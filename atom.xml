<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Mark Chang's Blog]]></title>
  <link href="http://ckmarkoh.github.io/atom.xml" rel="self"/>
  <link href="http://ckmarkoh.github.io/"/>
  <updated>2016-12-11T11:18:48+08:00</updated>
  <id>http://ckmarkoh.github.io/</id>
  <author>
    <name><![CDATA[Mark Chang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[類神經網路 -- Word2vec (Part 3 : Implementation)]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/08/29/neural-network-word2vec-part-3-implementation/"/>
    <updated>2016-08-29T11:17:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/08/29/neural-network-word2vec-part-3-implementation</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>本文接續 <a href="http://ckmarkoh.github.io/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation">word2vec (part2)</a> ，介紹如何根據推導出來的 <em>backward propagation</em> 公式，從頭到尾實作一個簡易版的 <em>word2vec</em> 。</p>

<p>本例的 input layer 採用 <em>skip-gram</em> ， output layer 採用 <em>negative sampling</em></p>

<p>本例用唐詩語料庫：https://github.com/ckmarkoh/coscup_nndl/blob/master/poem.txt</p>

<p>首先，載入所需的模組</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">json</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span><span class="p">,</span> <span class="n">OrderedDict</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">random</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">math</span>
</span></code></pre></td></tr></table></div></figure>

<!--more-->

<h2 id="build-dictionray">Build Dictionray</h2>

<p>再來是建立字典，即將每個字給一個id來對應。</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">LearnVocabFromTrainFile</span><span class="p">():</span>
</span><span class="line">		
</span><span class="line">    <span class="c"># 開啟唐詩語料庫</span>
</span><span class="line">    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">&quot;poem.txt&quot;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 統計唐詩語料庫中每個字出現的頻率</span>
</span><span class="line">    <span class="n">vcount</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
</span><span class="line">    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">():</span>
</span><span class="line">        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s">&quot;utf-8&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
</span><span class="line">            <span class="n">vcount</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 僅保留出現次數大於五的字，並按照出現次數排序</span>
</span><span class="line">    <span class="n">vcount_list</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">vcount</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
</span><span class="line">                         <span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 建立字典，將每個字給一個id ，字為 key, id 為 value</span>
</span><span class="line">    <span class="n">vocab_dict</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vcount_list</span><span class="p">)))</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 建立詞頻統計用的字典，給定某字，可查到其出現頻率</span>
</span><span class="line">    <span class="n">vocab_freq_dict</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">vcount_list</span><span class="p">))</span>
</span><span class="line">    <span class="k">return</span> <span class="n">vocab_dict</span><span class="p">,</span> <span class="n">vocab_freq_dict</span>
</span><span class="line">
</span><span class="line"><span class="n">vocab_dict</span><span class="p">,</span> <span class="n">vocab_freq_dict</span> <span class="o">=</span>  <span class="n">LearnVocabFromTrainFile</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>印出字典檔，每個字對應到一個id（編號）</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">for</span> <span class="n">w</span><span class="p">,</span><span class="n">wid</span> <span class="ow">in</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span class="line">    <span class="k">print</span> <span class="s">&quot;</span><span class="si">%s</span><span class="s"> : </span><span class="si">%s</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">wid</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">不 : 0
</span><span class="line">人 : 1
</span><span class="line">山 : 2
</span><span class="line">無 : 3
</span><span class="line">風 : 4
</span><span class="line">......
</span><span class="line">謏 : 5496
</span><span class="line">笮 : 5497
</span><span class="line">躠 : 5498
</span><span class="line">噆 : 5499
</span></code></pre></td></tr></table></div></figure>

<p>印出詞頻統計用的字典，給定某字，可查詢到其出現頻率：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">for</span> <span class="n">w</span><span class="p">,</span><span class="n">wfreq</span> <span class="ow">in</span> <span class="n">vocab_freq_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span class="line">    <span class="k">print</span> <span class="s">&quot;</span><span class="si">%s</span><span class="s"> : </span><span class="si">%s</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">wfreq</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">不 : 26426
</span><span class="line">人 : 20966
</span><span class="line">山 : 16056
</span><span class="line">無 : 15795
</span><span class="line">風 : 15618
</span><span class="line">...
</span><span class="line">謏 : 5
</span><span class="line">笮 : 5
</span><span class="line">躠 : 5
</span><span class="line">噆 : 5
</span></code></pre></td></tr></table></div></figure>

<h2 id="build-unigram-table">Build Unigram Table</h2>

<p>本例採用 <em>negative sampling</em> ，需要先建立 <em>unigram table</em> 以便進行 <em>negative sampling</em> 。</p>

<p>所謂的 <em>Unigram Table</em> 即是一個 <em>array</em> ，其中每個元素為某字的id，而某字的頻率，即為此id在此 <em>table</em> 中出現的次數的 0.75次方。</p>

<p>例如，id 為 5496 的字，詞頻為 5 ，則在此 <em>Unigram Table</em> 中，5496 的次數為：</p>

<script type="math/tex; mode=display">

5^{0.75} = 3.34 \approx 3

</script>

<p>由於 <em>array</em> 中的元素個數必須是整數，所以 5496 在 <em>Unigram Table</em> 中出現三次。</p>

<p>建立 <em>Unigram Table</em> 的程式碼如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">InitUnigramTable</span><span class="p">(</span><span class="n">vocab_freq_dict</span><span class="p">):</span>
</span><span class="line">    <span class="n">table_freq_list</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">0.75</span><span class="p">)),</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab_freq_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()))</span>
</span><span class="line">    <span class="n">table_size</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">table_freq_list</span><span class="p">])</span>
</span><span class="line">    <span class="n">table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">table_size</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
</span><span class="line">    <span class="n">offset</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">table_freq_list</span><span class="p">:</span>
</span><span class="line">        <span class="n">table</span><span class="p">[</span><span class="n">offset</span><span class="p">:</span><span class="n">offset</span> <span class="o">+</span> <span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">        <span class="n">offset</span> <span class="o">+=</span> <span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">    <span class="k">return</span> <span class="n">table</span>
</span><span class="line">
</span><span class="line"><span class="n">table</span> <span class="o">=</span> <span class="n">InitUnigramTable</span><span class="p">(</span><span class="n">vocab_freq_dict</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>得出的 <em>Unigram Table</em> 如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="p">[</span>   <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>
</span><span class="line"><span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>  <span class="o">...</span> <span class="p">,</span> <span class="mi">5495</span> <span class="mi">5495</span> <span class="mi">5495</span> <span class="mi">5496</span>  <span class="mi">5496</span> <span class="mi">5496</span> <span class="mi">5497</span> <span class="mi">5497</span> <span class="mi">5497</span> <span class="mi">5498</span>
</span><span class="line"><span class="mi">5498</span> <span class="mi">5498</span> <span class="mi">5499</span> <span class="mi">5499</span> <span class="mi">5499</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>

<h2 id="training-word2vec">Training word2vec</h2>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
<span class="line-number">61</span>
<span class="line-number">62</span>
<span class="line-number">63</span>
<span class="line-number">64</span>
<span class="line-number">65</span>
<span class="line-number">66</span>
<span class="line-number">67</span>
<span class="line-number">68</span>
<span class="line-number">69</span>
<span class="line-number">70</span>
<span class="line-number">71</span>
<span class="line-number">72</span>
<span class="line-number">73</span>
<span class="line-number">74</span>
<span class="line-number">75</span>
<span class="line-number">76</span>
<span class="line-number">77</span>
<span class="line-number">78</span>
<span class="line-number">79</span>
<span class="line-number">80</span>
<span class="line-number">81</span>
<span class="line-number">82</span>
<span class="line-number">83</span>
<span class="line-number">84</span>
<span class="line-number">85</span>
<span class="line-number">86</span>
<span class="line-number">87</span>
<span class="line-number">88</span>
<span class="line-number">89</span>
<span class="line-number">90</span>
<span class="line-number">91</span>
<span class="line-number">92</span>
<span class="line-number">93</span>
<span class="line-number">94</span>
<span class="line-number">95</span>
<span class="line-number">96</span>
<span class="line-number">97</span>
<span class="line-number">98</span>
<span class="line-number">99</span>
<span class="line-number">100</span>
<span class="line-number">101</span>
<span class="line-number">102</span>
<span class="line-number">103</span>
<span class="line-number">104</span>
<span class="line-number">105</span>
<span class="line-number">106</span>
<span class="line-number">107</span>
<span class="line-number">108</span>
<span class="line-number">109</span>
<span class="line-number">110</span>
<span class="line-number">111</span>
<span class="line-number">112</span>
<span class="line-number">113</span>
<span class="line-number">114</span>
<span class="line-number">115</span>
<span class="line-number">116</span>
<span class="line-number">117</span>
<span class="line-number">118</span>
<span class="line-number">119</span>
<span class="line-number">120</span>
<span class="line-number">121</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">vocab_dict</span><span class="p">,</span> <span class="n">vocab_freq_dict</span><span class="p">,</span> <span class="n">table</span><span class="p">):</span>
</span><span class="line">		
</span><span class="line">    <span class="n">total_words</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">vocab_freq_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
</span><span class="line">    <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab_dict</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 參數設定</span>
</span><span class="line">    <span class="n">layer1_size</span> <span class="o">=</span> <span class="mi">30</span> <span class="c"># hidden layer 的大小，即向量大小</span>
</span><span class="line">    <span class="n">window</span> <span class="o">=</span> <span class="mi">2</span> <span class="c"># 上下文寬度的上限</span>
</span><span class="line">    <span class="n">alpha_init</span> <span class="o">=</span> <span class="mf">0.025</span> <span class="c"># learning rate</span>
</span><span class="line">    <span class="n">sample</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="c"># 用來隨機丟棄高頻字用</span>
</span><span class="line">    <span class="n">negative</span> <span class="o">=</span> <span class="mi">10</span> <span class="c"># negative sampling 的數量</span>
</span><span class="line">    <span class="n">ite</span> <span class="o">=</span> <span class="mi">2</span> <span class="c"># iteration 次數</span>
</span><span class="line">
</span><span class="line">    <span class="c"># Weights 初始化</span>
</span><span class="line">    <span class="c"># syn0 : input layer 到 hidden layer 之間的 weights ，用隨機值初始化</span>
</span><span class="line">    <span class="c"># syn1 : hidden layer 到 output layer 之間的 weights ，用0初始化</span>
</span><span class="line">    <span class="n">syn0</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">layer1_size</span><span class="p">))</span> <span class="o">/</span> <span class="n">layer1_size</span>
</span><span class="line">    <span class="n">syn1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layer1_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 印出進度用</span>
</span><span class="line">    <span class="n">train_words</span> <span class="o">=</span> <span class="mi">0</span> <span class="c"># 總共訓練了幾個字</span>
</span><span class="line">    <span class="n">p_count</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">    <span class="n">avg_err</span> <span class="o">=</span> <span class="mf">0.</span>
</span><span class="line">    <span class="n">err_count</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">
</span><span class="line">    <span class="k">for</span> <span class="n">local_iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ite</span><span class="p">):</span>
</span><span class="line">        <span class="k">print</span> <span class="s">&quot;local_iter&quot;</span><span class="p">,</span> <span class="n">local_iter</span>
</span><span class="line">        <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">&quot;poem.txt&quot;</span><span class="p">)</span>
</span><span class="line">        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">():</span>
</span><span class="line">
</span><span class="line">            <span class="c">#用來暫存要訓練的字，一次訓練一個句子</span>
</span><span class="line">            <span class="n">sen</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">
</span><span class="line">            <span class="c"># 取出要被訓練的字</span>
</span><span class="line">            <span class="k">for</span> <span class="n">word_raw</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s">&quot;utf-8&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
</span><span class="line">                <span class="n">last_word</span> <span class="o">=</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word_raw</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">                <span class="c"># 丟棄字典中沒有的字（頻率太低）</span>
</span><span class="line">                <span class="k">if</span> <span class="n">last_word</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
</span><span class="line">                    <span class="k">continue</span>
</span><span class="line">                <span class="n">cn</span> <span class="o">=</span> <span class="n">vocab_freq_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word_raw</span><span class="p">)</span>
</span><span class="line">                <span class="n">ran</span> <span class="o">=</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cn</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">sample</span> <span class="o">*</span> <span class="n">total_words</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">*</span> <span class="p">(</span><span class="n">sample</span> <span class="o">*</span> <span class="n">total_words</span><span class="p">)</span> <span class="o">/</span> <span class="n">cn</span>
</span><span class="line">
</span><span class="line">                <span class="c"># 根據字的頻率，隨機丟棄，頻率越高的字，越有機會被丟棄</span>
</span><span class="line">                <span class="k">if</span> <span class="n">ran</span> <span class="o">&lt;</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">():</span>
</span><span class="line">                    <span class="k">continue</span>
</span><span class="line">                <span class="n">train_words</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">
</span><span class="line">                <span class="c"># 將要被訓練的字加到 sen</span>
</span><span class="line">                <span class="n">sen</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">last_word</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">            <span class="c"># 根據訓練過的字數，調整 learning rate</span>
</span><span class="line">            <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha_init</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">train_words</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">ite</span> <span class="o">*</span> <span class="n">total_words</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
</span><span class="line">            <span class="k">if</span> <span class="n">alpha</span> <span class="o">&lt;</span> <span class="n">alpha_init</span> <span class="o">*</span> <span class="mf">0.0001</span><span class="p">:</span>
</span><span class="line">                <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha_init</span> <span class="o">*</span> <span class="mf">0.0001</span>
</span><span class="line">
</span><span class="line">            <span class="c"># 逐一訓練 sen 中的字</span>
</span><span class="line">            <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sen</span><span class="p">):</span>
</span><span class="line">
</span><span class="line">            		<span class="c"># 隨機調整 window 大小</span>
</span><span class="line">                <span class="n">b</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">window</span><span class="p">)</span>
</span><span class="line">                <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span class="line">
</span><span class="line">                    <span class="c"># input 為 window 範圍中，上下文的某一字</span>
</span><span class="line">                    <span class="k">if</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">c</span> <span class="o">==</span> <span class="n">a</span> <span class="ow">or</span> <span class="n">c</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sen</span><span class="p">):</span>
</span><span class="line">                        <span class="k">continue</span>
</span><span class="line">                    <span class="n">last_word</span> <span class="o">=</span> <span class="n">sen</span><span class="p">[</span><span class="n">c</span><span class="p">]</span>
</span><span class="line">										
</span><span class="line">                    <span class="c"># h_err 暫存 hidden layer 的 error 用</span>
</span><span class="line">                    <span class="n">h_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layer1_size</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">                    <span class="c"># 進行 negative sampling</span>
</span><span class="line">                    <span class="k">for</span> <span class="n">negcount</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">negative</span><span class="p">):</span>
</span><span class="line">
</span><span class="line">                    		<span class="c"># positive example，從 sen 中取得，模型要輸出 1</span>
</span><span class="line">                        <span class="k">if</span> <span class="n">negcount</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">                            <span class="n">target_word</span> <span class="o">=</span> <span class="n">word</span>
</span><span class="line">                            <span class="n">label</span> <span class="o">=</span> <span class="mi">1</span>
</span><span class="line">
</span><span class="line">                        <span class="c"># negative example，從 table 中抽樣，模型要輸出 0 </span>
</span><span class="line">                        <span class="k">else</span><span class="p">:</span>
</span><span class="line">                            <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
</span><span class="line">                                <span class="n">target_word</span> <span class="o">=</span> <span class="n">table</span><span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">table</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
</span><span class="line">                                <span class="k">if</span> <span class="n">target_word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sen</span><span class="p">:</span>
</span><span class="line">                                    <span class="k">break</span>
</span><span class="line">                            <span class="n">label</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">
</span><span class="line">                        <span class="c"># 模型預測結果</span>
</span><span class="line">                        <span class="n">o_pred</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">syn0</span><span class="p">[</span><span class="n">last_word</span><span class="p">,</span> <span class="p">:],</span> <span class="n">syn1</span><span class="p">[:,</span> <span class="n">target_word</span><span class="p">])))</span>
</span><span class="line">
</span><span class="line">                        <span class="c"># 預測結果和標準答案的差距</span>
</span><span class="line">                        <span class="n">o_err</span> <span class="o">=</span> <span class="n">o_pred</span> <span class="o">-</span> <span class="n">label</span>
</span><span class="line">
</span><span class="line">                        <span class="c"># backward propagation</span>
</span><span class="line">                        <span class="c"># 此部分請參照 word2vec part2 的公式推導結果</span>
</span><span class="line">
</span><span class="line">                        <span class="c"># 1.將 error 傳遞到 hidden layer                        </span>
</span><span class="line">                        <span class="n">h_err</span> <span class="o">+=</span> <span class="n">o_err</span> <span class="o">*</span> <span class="n">syn1</span><span class="p">[:,</span> <span class="n">target_word</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">                        <span class="c"># 2.更新 syn1</span>
</span><span class="line">                        <span class="n">syn1</span><span class="p">[:,</span> <span class="n">target_word</span><span class="p">]</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">o_err</span> <span class="o">*</span> <span class="n">syn0</span><span class="p">[</span><span class="n">last_word</span><span class="p">]</span>
</span><span class="line">                        <span class="n">avg_err</span> <span class="o">+=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">o_err</span><span class="p">)</span>
</span><span class="line">                        <span class="n">err_count</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">
</span><span class="line">                    <span class="c"># 3.更新 syn0</span>
</span><span class="line">                    <span class="n">syn0</span><span class="p">[</span><span class="n">last_word</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">h_err</span>
</span><span class="line">
</span><span class="line">                    <span class="c"># 印出目前結果</span>
</span><span class="line">                    <span class="n">p_count</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">                    <span class="k">if</span> <span class="n">p_count</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">                        <span class="k">print</span> <span class="s">&quot;Iter: </span><span class="si">%s</span><span class="s">, Alpha </span><span class="si">%s</span><span class="s">, Train Words </span><span class="si">%s</span><span class="s">, Average Error: </span><span class="si">%s</span><span class="s">&quot;</span> \
</span><span class="line">                              <span class="o">%</span> <span class="p">(</span><span class="n">local_iter</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">train_words</span><span class="p">,</span> <span class="n">avg_err</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">err_count</span><span class="p">))</span>
</span><span class="line">                        <span class="n">avg_err</span> <span class="o">=</span> <span class="mf">0.</span>
</span><span class="line">                        <span class="n">err_count</span> <span class="o">==</span> <span class="mf">0.</span>
</span><span class="line">
</span><span class="line">        <span class="c"># 每一個 iteration 儲存一次訓練完的模型</span>
</span><span class="line">        <span class="n">model_name</span> <span class="o">=</span> <span class="s">&quot;w2v_model_blog_</span><span class="si">%s</span><span class="s">.json&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">local_iter</span><span class="p">)</span>
</span><span class="line">        <span class="k">print</span> <span class="s">&quot;save model: </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</span><span class="line">        <span class="n">fm</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="s">&quot;w&quot;</span><span class="p">)</span>
</span><span class="line">        <span class="n">fm</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">syn0</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>
</span><span class="line">        <span class="n">fm</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>開始訓練：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">train</span><span class="p">(</span><span class="n">vocab_dict</span><span class="p">,</span> <span class="n">vocab_freq_dict</span><span class="p">,</span> <span class="n">table</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>輸出結果如下，可以看到，當訓練過的字數增加時， Error 也跟著降低</p>

<p>大概要花幾十分鐘左右訓練完</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">Iter: 0, Alpha 0.0249923666923, Train Words 475200, Average Error: 0.499999254842
</span><span class="line">Iter: 0, Alpha 0.0249846739501, Train Words 954100, Average Error: 0.249998343836
</span><span class="line">Iter: 0, Alpha 0.0249771900316, Train Words 1420000, Average Error: 0.166660116256
</span><span class="line">Iter: 0, Alpha 0.0249693430813, Train Words 1908500, Average Error: 0.124949913475
</span><span class="line">Iter: 0, Alpha 0.024961329072, Train Words 2407400, Average Error: 0.0993522008349
</span><span class="line">Iter: 0, Alpha 0.0249531817368, Train Words 2914600, Average Error: 0.0787704454331
</span><span class="line">Iter: 0, Alpha 0.0249453540624, Train Words 3401900, Average Error: 0.06351951221
</span><span class="line">Iter: 0, Alpha 0.0249377801891, Train Words 3873400, Average Error: 0.0495117808015
</span><span class="line">..........
</span></code></pre></td></tr></table></div></figure>

<h2 id="show-result">Show Result</h2>

<p>檢視 word2vec 訓練結果的方法，即是看使用 <em>cosine similarity</em> 計算，是否能得出與某字語意相近的字。</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="c"># 讀取訓練好的模型</span>
</span><span class="line"><span class="n">f2</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">&quot;w2v_model_1.json&quot;</span><span class="p">,</span> <span class="s">&quot;r&quot;</span><span class="p">)</span>
</span><span class="line"><span class="n">w2v_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="s">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">f2</span><span class="o">.</span><span class="n">readlines</span><span class="p">())))</span>
</span><span class="line"><span class="n">f2</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span><span class="line">
</span><span class="line"><span class="n">vocab_dict_reversed</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">([(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
</span><span class="line">
</span><span class="line"><span class="c"># 計算 cosine similarity 最高的前五字</span>
</span><span class="line"><span class="k">def</span> <span class="nf">get_top</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
</span><span class="line">    <span class="n">wid</span> <span class="o">=</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 將某字與模型中所有的字向量做內積</span>
</span><span class="line">    <span class="n">dot_result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2v_model</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">w2v_model</span><span class="p">[</span><span class="n">wid</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</span><span class="line">    <span class="n">norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 計算 cosine similarity</span>
</span><span class="line">    <span class="n">cosine_result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">dot_result</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">norm</span><span class="o">*</span><span class="n">norm</span><span class="p">[</span><span class="n">wid</span><span class="p">])</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 根據 cosine similarity 的值排序</span>
</span><span class="line">    <span class="n">final_result</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">wid</span><span class="p">,</span>
</span><span class="line">                          <span class="p">[(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cosine_result</span><span class="p">)]),</span>
</span><span class="line">                          <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class="line">    <span class="k">print</span> <span class="n">word</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 印出語意最接近的前五字，以及其 cosine similarity</span>
</span><span class="line">    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">final_result</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
</span><span class="line">        <span class="k">print</span> <span class="n">vocab_dict_reversed</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>分別計算「山、峰、河、日」這四字語意最相近的字</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">get_top</span><span class="p">(</span><span class="s">u&quot;山&quot;</span><span class="p">)</span>
</span><span class="line"><span class="n">get_top</span><span class="p">(</span><span class="s">u&quot;峰&quot;</span><span class="p">)</span>
</span><span class="line"><span class="n">get_top</span><span class="p">(</span><span class="s">u&quot;河&quot;</span><span class="p">)</span>
</span><span class="line"><span class="n">get_top</span><span class="p">(</span><span class="s">u&quot;日&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下，可看出，計算所得出語意最相近的字，實際上，語意也相近，例如，山和峰、嶺的語意都很接近。</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">山
</span><span class="line">嶺 0.854901128361
</span><span class="line">嵩 0.846620438864
</span><span class="line">峰 0.842831270385
</span><span class="line">岡 0.838129842909
</span><span class="line">嶂 0.834701215189
</span><span class="line">峰
</span><span class="line">山 0.842831270385
</span><span class="line">嶽 0.83917452917
</span><span class="line">嶺 0.8219837161
</span><span class="line">頂 0.821088331571
</span><span class="line">嶂 0.809565794884
</span><span class="line">河
</span><span class="line">湟 0.787726187693
</span><span class="line">涇 0.770652269018
</span><span class="line">淮 0.751135710239
</span><span class="line">川 0.742243126005
</span><span class="line">汾 0.740643816278
</span><span class="line">日
</span><span class="line">旦 0.869047480855
</span><span class="line">又 0.842383624714
</span><span class="line">曛 0.830549707539
</span><span class="line">夕 0.826327222048
</span><span class="line">暉 0.82616774597
</span></code></pre></td></tr></table></div></figure>

<p>向量加減運算後的 <em>cosine similarity</em> ，例如： 女 + 父 - 男 = 母</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">get_calculated_top</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">w3</span><span class="p">):</span>
</span><span class="line">    <span class="n">wid1</span><span class="p">,</span> <span class="n">wid2</span><span class="p">,</span> <span class="n">wid3</span> <span class="o">=</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">w1</span><span class="p">),</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">w2</span><span class="p">),</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">w3</span><span class="p">)</span>
</span><span class="line">    <span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">,</span> <span class="n">v3</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="p">[</span><span class="n">wid1</span><span class="p">],</span> <span class="n">w2v_model</span><span class="p">[</span><span class="n">wid2</span><span class="p">],</span> <span class="n">w2v_model</span><span class="p">[</span><span class="n">wid3</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 得出加減運算後的向量</span>
</span><span class="line">    <span class="n">combined_vec</span> <span class="o">=</span> <span class="n">v1</span> <span class="o">+</span> <span class="p">(</span><span class="n">v2</span> <span class="o">-</span> <span class="n">v3</span><span class="p">)</span>
</span><span class="line">    <span class="n">dot_result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2v_model</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">combined_vec</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</span><span class="line">    <span class="n">norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</span><span class="line">    <span class="n">cvec_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">combined_vec</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
</span><span class="line">    <span class="n">cosine_result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">dot_result</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">norm</span> <span class="o">*</span> <span class="n">cvec_norm</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="n">final_result</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">wid1</span><span class="p">,</span> <span class="n">wid2</span><span class="p">,</span> <span class="n">wid3</span><span class="p">],</span>
</span><span class="line">                                 <span class="p">[(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cosine_result</span><span class="p">)]),</span>
</span><span class="line">                          <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class="line">    <span class="k">print</span> <span class="s">&quot;</span><span class="si">%s</span><span class="s"> + </span><span class="si">%s</span><span class="s"> - </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">w3</span><span class="p">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">final_result</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
</span><span class="line">        <span class="k">print</span> <span class="n">vocab_dict_reversed</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">get_calculated_top</span><span class="p">(</span><span class="s">u&quot;女&quot;</span><span class="p">,</span> <span class="s">u&quot;父&quot;</span><span class="p">,</span> <span class="s">u&quot;男&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下，如預期，運算結果的語意接近「母」：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">女 + 父 - 男
</span><span class="line">母 0.731002049447
</span><span class="line">娥 0.707469857054
</span><span class="line">客 0.69027387716
</span><span class="line">娃 0.687831493041
</span><span class="line">侶 0.681667240226
</span></code></pre></td></tr></table></div></figure>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[類神經網路 -- Word2vec (Part 2 : Backward Propagation)]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation/"/>
    <updated>2016-07-12T09:21:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>本文接續 <a href="http://ckmarkoh.github.io/blog/2016/07/12/neural-network-word2vec-part-1-overview">word2vec (part1)</a> ，介紹 <em>word2vec</em> 訓練過程的 <em>backward propagation</em> 公式推導。</p>

<p><em>word2vec</em> 的訓練過程中，輸出的結果，跟上下文有關的字，在 <em>output layer</em> 輸出為 1 ，跟上下文無關的字，在 <em>output layer</em> 輸出為 0。 在此，把跟上下文有關的，稱為 <em>positive example</em> ，而跟上下文無關的，稱為 <em>negative example</em> 。</p>

<p>根據 <a href="http://ckmarkoh.github.io/blog/2016/07/12/neural-network-word2vec-part-1-overview">word2vec (part1)</a> 中提到的例子， <em>cat</em> 的向量為 <script type="math/tex">\textbf{v}_2</script> ， <em>run</em> 的向量為 <script type="math/tex">\textbf{w}_3</script> ， <em>fly</em> 的向量為 <script type="math/tex">\textbf{w}_4</script> ，由於 <em>cat</em> 的上下文有 <em>run</em> ，所以 <em>run</em> 為 <em>positive example</em> ，而 <em>cat</em> 的上下文沒有 <em>fly</em> ，所以 <em>fly</em> 為 <em>negative example</em> ，如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00187.png" alt="" /></p>

<!--more-->

<h2 id="objective-function">Objective Function</h2>

<p>訓練類神經網路需要有個目標函數，如果希望 <em>positive example</em> 輸出為 1 ， <em>negative example</em> 輸出為 0，則可以將以下函數 <script type="math/tex">J</script> 做最小化。</p>

<script type="math/tex; mode=display">

J = - \text{log}(\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{pos} }}) - \sum_{neg} \text{log} ( 1- \frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{neg} }} )

</script>

<p>其中 <script type="math/tex">\textbf{v}_I</script> 為輸入端的字向量，而 <script type="math/tex">\textbf{w}_{pos}</script> 和 <script type="math/tex">\textbf{w}_{neg}</script> 為輸出端的字向量。 <script type="math/tex">\textbf{w}_{pos}</script> 為 <em>positive example</em> ，而  <script type="math/tex">\textbf{w}_{neg}</script> 為 <em>negative example</em> 。通常，對於每筆 <script type="math/tex">\textbf{v}_I</script> 而言，會找一個 <em>positive example</em> 和多個 <em>negative example</em> ，因此用 <script type="math/tex">\sum</script> 將這些 <em>negative example</em> 算出的結果給加起來。</p>

<p>先看這公式前半部的部分：</p>

<script type="math/tex; mode=display">

-\text{log}(\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{pos} }})

</script>

<p>從以上公式得知，當 <script type="math/tex">\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{pos} }} = 0</script> 時，  <script type="math/tex">J</script> 會趨近無限大，而當 <script type="math/tex">\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{pos} }} = 1</script> 時 ，  <script type="math/tex">J</script> 會趨近 0 ，所以，降低 <script type="math/tex">J</script> 的值，會迫使  <script type="math/tex">\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{pos} }}</script> 接近 1 。</p>

<p>再來看另一部分：</p>

<script type="math/tex; mode=display">

-\text{log}(1 - \frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{neg} }})

</script>

<p>當 <script type="math/tex">\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{neg} }} = 1</script> 時 <script type="math/tex">J</script> 會趨近無限大，反之亦然，同理，降低 <script type="math/tex">J</script> 的值，會迫使  <script type="math/tex">1- \frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{neg} }}</script> 接近 0 。</p>

<h2 id="backward-propagation">Backward Propagation</h2>

<p>至於要怎麼調整 <script type="math/tex">\textbf{v}</script> 和  <script type="math/tex">\textbf{w}</script> 的值，才能讓 <script type="math/tex">J</script> 變小？ 就是要用到 <em>backward propagation</em> 。</p>

<h3 id="positive-example">Positive Example</h3>

<p>這邊先看 <em>positive example</em> 的部分， 輸入端為 <em>dog</em> ，輸出端為 <em>run</em> ，在進行偏微分公式推導之前，先定義一些符號，以便之後推導：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x_{13} = \textbf{v}_1 \cdot \textbf{w}_3 \\

& y_{13} = \dfrac{1}{1+e^{-x_{13}}}  \\

& J = - \text{log} (y_{13} )

\end{align}

 %]]&gt;</script>

<p>這些符號，如下圖所示， <script type="math/tex">x_{13}</script> 是 <em>output layer</em> 在 <em>sigmoid</em> 之前的值， <script type="math/tex">y_{13} </script> 是通過 <em>sigmoid</em> 後的值， <script type="math/tex">J</script> 是針對這筆 <em>positive example</em> 所算出來的 <em>cost function</em> 。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00188.png" alt="" /></p>

<p>如果要更新 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_3</script> 的值，讓 <script type="math/tex">J</script> 變小，就要用 <em>gradient descent</em> 的方式，過程如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{v}_1 \leftarrow \textbf{v}_1 - \eta \nabla_{\textbf{v}_1}  J  \\

& \textbf{w}_3 \leftarrow \textbf{w}_3 - \eta \nabla_{\textbf{w}_1}  J   

\end{align}

 %]]&gt;</script>

<p>想瞭解更多關於 <em>gradient descent</em> ，請參考：<a href="http://ckmarkoh.github.io/blog/2015/12/23/optimization-method-adagrad">Gradient Descent &amp; AdaGrad </a></p>

<p>其中， <script type="math/tex">\eta</script> 為 <em>learning rate</em> ，為一常數，就是決定每一步要走多大，至於 <script type="math/tex">\nabla_{\textbf{v}_1} J </script> 這項要怎麼算？</p>

<p>先看看它每個維度上的值：</p>

<script type="math/tex; mode=display">

\nabla_{\textbf{v}_1} J = 

\begin{bmatrix}

\frac{\partial J}{\partial v_{11}} \\

\frac{\partial J}{\partial v_{12}} \\

\frac{\partial J}{\partial v_{13}} \\

\end{bmatrix}

</script>

<p>先看 <script type="math/tex">\frac{\partial J}{\partial v_{11}}</script> 這項，可以用 <em>chain rule</em> 把它拆開：</p>

<script type="math/tex; mode=display">

\frac{\partial J}{\partial v_{11} } = \frac{\partial J}{\partial y_{13}} \times \frac{\partial y_{13}}{\partial x_{13}}  \times \frac{\partial x_{13}}{\partial v_{11}}

</script>

<p>將 <script type="math/tex">\frac{\partial J}{\partial v_{11} }</script> 拆成 <script type="math/tex">\frac{\partial J}{\partial y_{13}}</script> 、 <script type="math/tex">\frac{\partial y_{13}}{\partial x_{13}}</script> 和 <script type="math/tex">\frac{\partial x_{13}}{\partial v_{11}}</script> 這三項。而這三項的值可分別求出來：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \frac{\partial J}{\partial y_{13}} = \frac{\partial   - \text{log} (y_{13} )}{\partial y_{13}} = \frac{-1}{y_{13}} \\ 

& \frac{\partial y_{13}}{\partial x_{13}} = \frac{\partial (\frac{1}{1+e^{-x_{13}}})}{\partial x_{13}} = \frac{1}{1+e^{-x_{13}}}( 1- \frac{1}{1+e^{-x_{13}}}) = y_{13} ( 1- y_{13}) \\

& \frac{\partial x_{13}}{\partial v_{11}} = \frac{\partial \textbf{v}_{1} \cdot \textbf{w}_3} {\partial v_{11}} = w_{31}


\end{align}

 %]]&gt;</script>

<p>代回這三項的結果到 <script type="math/tex">\frac{\partial J}{\partial v_{11} }</script> ，得出：</p>

<script type="math/tex; mode=display">

\frac{\partial J}{\partial v_{11} } = \frac{-1}{y_{13}} \times y_{13} ( 1- y_{13}) \times  w_{31} = ( y_{13} - 1)  \times w_{31}

</script>

<p>而 <script type="math/tex">\frac{\partial J}{\partial v_{12}}</script> 和 <script type="math/tex">\frac{\partial J}{\partial v_{13}}</script> 也可用同樣方式得出其值， 如下：</p>

<script type="math/tex; mode=display">

\nabla_{\textbf{v}_1} J = 

\begin{bmatrix}

\frac{\partial J}{\partial v_{11}} \\

\frac{\partial J}{\partial v_{12}} \\

\frac{\partial J}{\partial v_{13}} \\

\end{bmatrix}

=

\begin{bmatrix}

 ( y_{13} - 1)  \times w_{31} \\

 ( y_{13} - 1)  \times w_{32} \\

 ( y_{13} - 1)  \times w_{33} \\

 \end{bmatrix}

 = 

  ( y_{13} - 1)  \textbf{w}_3

</script>

<p>因此，可得出 <script type="math/tex">\textbf{v}_1</script> 要調整的量：</p>

<script type="math/tex; mode=display">

\textbf{v}_1 \leftarrow \textbf{v}_1 - \eta  ( y_{13} - 1)  \textbf{w}_3

</script>

<p>同理， <script type="math/tex">\textbf{w}_3</script> 要調整的量：</p>

<script type="math/tex; mode=display">

\textbf{w}_3 \leftarrow \textbf{w}_3 - \eta  ( y_{13} - 1)   \textbf{v}_1

</script>

<p>其中，可以把 <script type="math/tex">( y_{13} - 1)</script> 看成是，模型預測出來的，和我們所想要的值，差距多少。因為在 <em>positive example</em> 的情況下，我們希望模型輸出結果 <script type="math/tex">y_{13}</script> 為 1 。如果  <script type="math/tex">y_{13} = 1</script> 表示模型預測對了，則不需要修正 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_3</script> ，如果  <script type="math/tex">y_{13} \neq 1</script> 時，才要修正 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_3</script> 。</p>

<p>還有，之所以把這過程，稱為 <em>backward propagation</em> ，是因為可以把 <em>chain rule</em> 拆解 <script type="math/tex">\nabla_{\textbf{v}_1} J </script> 的過程，看成是將 <script type="math/tex">\frac{\partial J}{\partial y_{13} }</script> 的值， 由 <em>output layer</em> 往前傳遞，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00189.png" alt="" /></p>

<p>想瞭解更多關於 <em>backward propagation</em> 的推導，請參考： <a href="http://ckmarkoh.github.io/blog/2015/05/28/neural-network-backward-propagation">Backward Propagation 詳細推導過程 </a></p>

<h3 id="negative-example">Negative Example</h3>

<p>再來看看 <em>negative example</em> 的部分， 輸入端為 <em>dog</em> ，輸出端為 <em>fly</em> ，在進行偏微分公式推導之前，先定義一些符號，以便之後推導：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x_{14} = \textbf{v}_1 \cdot \textbf{w}_4 \\

& y_{14} = \dfrac{1}{1+e^{-x_{14}}}  \\

& J = - \text{log} (1 - y_{14} )

\end{align}

 %]]&gt;</script>

<p>這些符號，如下圖所示， <script type="math/tex">x_{14}</script> 是 <em>output layer</em> 在 <em>sigmoid</em> 之前的值， <script type="math/tex">y_{14} </script> 是通過 <em>sigmoid</em> 後的值， <script type="math/tex">J</script> 是針對這筆 <em>positive example</em> 所算出來的 <em>cost function</em> 。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00190.png" alt="" /></p>

<p>同之前 <em>positive example</em> ，如果要更新 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_4</script> 的值，讓 <script type="math/tex">J</script> 變小，就要用 <em>gradient descent</em> 的方式：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{v}_1 \leftarrow \textbf{v}_1 - \eta \nabla_{\textbf{v}_1}  J  \\

& \textbf{w}_4 \leftarrow \textbf{w}_4 - \eta \nabla_{\textbf{w}_1}  J   

\end{align}

 %]]&gt;</script>

<p>剩下的推導和 <em>positive example</em> 時，幾乎一樣，只有 <script type="math/tex">J</script> 不一樣。此處只需推導 <script type="math/tex">\frac{\partial J}{\partial y_{14}}</script> 的結果。</p>

<script type="math/tex; mode=display">

\frac{\partial J}{\partial y_{14}} = \frac{\partial   - \text{log} (1 - y_{14} )}{\partial y_{14}} = \frac{1}{1 - y_{14}} 

</script>

<p>代回此結果到 <script type="math/tex">\frac{\partial J}{\partial v_{11} }</script> ，得出：</p>

<script type="math/tex; mode=display">

\frac{\partial J}{\partial v_{11} } = \frac{1}{ 1- y_{14}} \times y_{14} ( 1- y_{14}) \times  w_{41} = ( y_{14} - 0)  \times w_{41}

</script>

<p>於是可以得出要修正的量：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{v}_1 \leftarrow \textbf{v}_1 - \eta  ( y_{14} - 0)  \textbf{w}_4 \\ 

& \textbf{w}_4 \leftarrow \textbf{w}_4 - \eta  ( y_{14} - 0)   \textbf{v}_1

\end{align}

 %]]&gt;</script>

<p>其中，可以把 <script type="math/tex">( y_{14} - 0)</script> 看成是，模型預測出來的，和我們所想要的值，差距多少。因為在 <em>negative example</em> 的情況下，我們希望模型輸出結果 <script type="math/tex">y_{14}</script> 為 0 。如果  <script type="math/tex">y_{13} = 0</script> 表示模型預測對了，則不需要修正 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_4</script> ，如果  <script type="math/tex">y_{14} \neq 0</script> 時，才要修正 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_4</script> 。</p>

<h2 id="further-reading">Further Reading</h2>

<p>關於如何從無到有，不使用任何自動微分的套件，實作一個 <em>word2vec</em> ，請看：<a href="http://ckmarkoh.github.io/blog/2016/08/29/neural-network-word2vec-part-3-implementation">word2vec (part3)</a></p>

<p>註： 實際上， <em>word2vec</em> 的 <em>input layer</em> 和 <em>output layer</em> 各有兩種架構。<em>input layer</em> 有 <em>cbow</em> 和 <em>skip-gram</em> 兩種，<em>output layer</em> 有 <em>hierarchical sofrmax</em> 和 <em>negative sampling</em> 兩種，本文所寫的為 <em>skip-gram</em> 搭配 <em>negative sampling</em> 的架構。關於 <em>hierarchical softmax</em> 請參考<a href="http://ckmarkoh.github.io/blog/2015/05/23/-hierarchical-probabilistic-neural-networks-neural-network-language-model">hierarchical sofrmax</a> 。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[類神經網路 -- Word2vec (Part 1 : Overview)]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/07/12/neural-network-word2vec-part-1-overview/"/>
    <updated>2016-07-12T09:19:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/07/12/neural-network-word2vec-part-1-overview</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>文字的語意可以用向量來表示，在上一篇 <a href="http://ckmarkoh.github.io/blog/2016/07/10/nlp-vector-space-semantics"> Vector Space of Semantics </a> 中提到，如果把每種字當成一個維度，假設總共有一萬總字，那向量就會有一萬個維度。有兩種方法可降低維度，分別是 <em>singular value decomposition</em> 和 <em>word2vec</em> 。</p>

<p>本文講解 <em>word2vec</em> 的原理。 <em>word2vec</em> 流程，總結如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00191.png" alt="" /></p>

<p>首先，將文字做 <em>one-hot encoding</em> ，然後再用 <em>word2vec</em> 類神經網路計算，求出壓縮後（維度降低後）的語意向量。</p>

<!--more-->

<h2 id="one-hot-encoding">One-Hot Encoding</h2>

<p>一開始，不知道哪個字和哪個字語意相近，所以就假設每個字的語意是不相干的。也就是說，每個字的向量都是互相垂直。</p>

<p>這邊舉個比較簡單的例子，假設總字彙量只有 4 個， 分別為 <em>dog, cat, run, fly</em> ，那麼，經過 <em>one-hot encoding</em> 的結果如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00192.png" alt="" /></p>

<p>如上圖， <em>dog</em> 的向量為 (1,0,0,0) ，只有在第一個維度是 1 ，其他維度是 0 ，而 <em>cat</em> 的向量為 (0,1,0,0) 只有在第一個維度是 1 ，其他維度是 0 。</p>

<p>也就是說，每個字都有一個代表它的維度，而它 <em>one hot encoding</em> 的結果，只有在那個維度上是 1 ，其他維度都是 0 。這樣的話，任意兩個 <em>one hot encoding</em> 的向量內積結果，都會是 0 ，內積結果為 0 ，表示兩向量是垂直的。</p>

<p>註：實際應用中，字彙量即是語料庫中的單字種類，通常會有幾千個甚至一萬個以上。</p>

<h2 id="word2vec">word2vec</h2>

<p><em>word2vec</em> 的神經網路架構如下，總共有三層， <em>input layer</em> 和 <em>output layer</em> 一樣大，中間的 <em>hidden layer</em> 比較小。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00193.png" alt="" /></p>

<p>如上圖，總字彙量有 4 個，那麼 <em>input layer</em> 和 <em>output layer</em> 的維度為 4， 每個維度分別代表一個字。 如果想要把向量維度降至三維， <em>hidden layer</em> 的維度為 3。</p>

<p>另外要注意的是， <em>hidden layer</em> 沒有非線性的 <em>activation funciton</em> ，而 <em>output layer</em> 的 <em>activation function</em> 是 <em>sigmoid</em> ，這兩點會有什麼影響，之後會提到。</p>

<p>其中，在 <em>input layer</em> 和 <em>hidden layer</em> 之間， 有 <script type="math/tex">4 \times 3</script> 個 <em>weight</em> ，在此命名為 <script type="math/tex">\textbf{V}</script> ，而在 <em>hidden layer</em> 到 <em>output layer</em> 之間， 有 <script type="math/tex">3 \times 4</script> 個 <em>weight</em> ，在此命名為 <script type="math/tex">\textbf{W}</script> 。</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\textbf{V}=

\begin{bmatrix}

    v_{11} & v_{12} & v_{13}  \\

    v_{21} & v_{22} & v_{23}  \\

    v_{31} & v_{32} & v_{33}  \\

    v_{41} & v_{42} & v_{43}  \\

\end{bmatrix}

\mspace{30mu}

\textbf{W}^T=

\begin{bmatrix}

    w_{11} & w_{12} & w_{13}  \\

    w_{21} & w_{22} & w_{23}  \\

    w_{31} & w_{32} & w_{33}  \\

    w_{41} & w_{42} & w_{43}  \\

\end{bmatrix}

 %]]&gt;</script>

<p>由於 <em>input</em> 是 <em>one hot encoding</em> 的向量，又因為 <em>hidden layer</em> 沒有 <em>sigmoid</em> 之類的非線性 <em>activation function</em>。 輸入到類神經網路後，在 <em>hidden layer</em> 所取得的值，即是 <script type="math/tex">\textbf{V}</script> 中某個橫排的值，如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00194.png" alt="" /></p>

<p>例如，輸入的是 <em>dog</em> 的 <em>one hot encoding</em> ，只有在第 1 個維度是 1 ，與 <script type="math/tex">\textbf{V}</script> 作矩陣相乘後，在 <em>hidden layer</em> 取得的值是 <script type="math/tex">\textbf{V}</script> 中的第一個橫排： <script type="math/tex">(v_{11}, v_{12}, v_{13})</script> ，這個向量就是 <em>dog</em> 壓縮後的語意向量。運算過程如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{bmatrix}

1 & 0 & 0 & 0 

\end{bmatrix}

\begin{bmatrix}

    v_{11} & v_{12} & v_{13}  \\

    v_{21} & v_{22} & v_{23}  \\

    v_{31} & v_{32} & v_{33}  \\

    v_{41} & v_{42} & v_{43}  \\

\end{bmatrix}

= 

\begin{bmatrix}

v_{11} & v_{12} & v_{13}

\end{bmatrix}

 %]]&gt;</script>

<p>因此， <script type="math/tex">\textbf{V}</script> 中的某個橫排，就是某個字的語意向量。從反方向來看，由於 <em>output layer</em> 也是對應到字彙的 <em>one hot encoding</em> 因此， <script type="math/tex">\textbf{W}^T</script> 中的某個橫排，就是某個字的語意向量。</p>

<p>所以，一個字分別在 <script type="math/tex">\textbf{V}</script> 和 <script type="math/tex">\textbf{W}</script> 中各有一個語意向量。但通常會選擇 <script type="math/tex">\textbf{V}</script> 中的語意向量，作為 <em>word2vec</em> 的輸出結果。</p>

<h2 id="initializing-word2vec">Initializing word2vec</h2>

<p>至於如何訓練這個類神經網路？ 訓練一個類神經網路的過程，第一步就是要先將 <em>weight</em> 作初始化。初始化即是隨機給每個 <em>weight</em> 不同的數值，這些數值介於 <script type="math/tex"> -N \sim N</script> 之間。</p>

<p>因此，在還沒開始訓練之前，這些向量的方向都是隨機的，跟語意無關。</p>

<p>舉 <em>dog</em> 和 <em>cat</em> 在 <script type="math/tex">\textbf{V}</script> 中的向量，為 <script type="math/tex">\textbf{V}_1,\textbf{V}_2</script> ，以及 <em>run</em> 和 <em>fly</em> 在 <script type="math/tex">\textbf{W}</script> 中的向量 為 <script type="math/tex">\textbf{W}_3,\textbf{W}_4</script> ，為例：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00195.png" alt="" /></p>

<p>由於 <script type="math/tex">\textbf{V}</script> 和 <script type="math/tex">\textbf{W}</script> 都是隨機初始化的，因此 <script type="math/tex">\textbf{V}_1, \textbf{V}_2, \textbf{W}_3, \textbf{W}_4 </script> 這些向量的方向都是隨機的，跟語意無關，如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00196.png" alt="" /></p>

<h2 id="training-word2vec">Training word2vec</h2>

<p>訓練 <em>word2vec</em> 的目的，是希望讓語意向量真的跟語意有關。，在上一篇 <a href="http://ckmarkoh.github.io/blog/2016/07/10/nlp-vector-space-semantics"> Vector Space of Semantics </a> 中提到，某字的語意，可從其上下文有哪些字來判斷。因此，可以用某字上下文的字，來做訓練，讓語意向量能抓到文字的語意。</p>

<p>若 <em>dog</em> 的上下文中有 <em>run</em> ， 令 <em>dog</em> 為 <em>word2vec</em> 的 <em>input</em> ， <em>run</em> 為 <em>output</em> 則輸入類神經網路後，在 <em>run</em> 的位置，在經過 <em>sigmoid</em> 之前，得到的結果是 <script type="math/tex">\textbf{V}_1</script> 和 <script type="math/tex">\textbf{W}_3</script> 的內積。經過了，<em>sigmoid</em> ，得到的值為：</p>

<script type="math/tex; mode=display">

\frac{1}{1+e^{- \textbf{V}_1 \cdot \textbf{W}_3}}

</script>

<p>由於 <em>run</em> 出現在 <em>dog</em> 的上下文中，所以要訓練類神經網路，在 <em>run</em> 位置可以輸出 1，如下：</p>

<script type="math/tex; mode=display">

\frac{1}{1+e^{- \textbf{V}_1 \cdot \textbf{W}_3}} \approx 1

</script>

<p>過程如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00197.png" alt="" /></p>

<p>根據上圖，如果要讓 <em>run</em> 的位置輸出為 1 ，則 <script type="math/tex">\textbf{V}_1</script> 和 <script type="math/tex">\textbf{W}_3</script> 的內積要越大越好。</p>

<p>內積要大，就是向量角度要越小，訓練過程中，會修正這兩個向量的角度，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00198.png" alt="" /></p>

<p>上圖左方為先正之前，各向量的方向，上圖右方為修正之後的方向，其中，深藍色為修正後的，淺藍色為修正前的，畫在一起以便作比較。修正完後， <script type="math/tex">\textbf{V}_1</script> 和 <script type="math/tex">\textbf{W}_3</script> 的角度又更接近了。</p>

<p>同理，若 <em>cat</em> 的上下文中有 <em>run</em> ，則用 <em>word2vec</em> 做同樣訓練，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00199.png" alt="" /></p>

<p>修正向量的角度後，<script type="math/tex">\textbf{V}_2</script> 和 <script type="math/tex">\textbf{W}_3</script> 的角度會更接近，結果如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00200.png" alt="" /></p>

<p>不過，以上訓練方法有個問題，就是訓練完後， <em>所有的向量都會位於同一條直線上，而無法分辨出每個字語意的差異</em> 。如果要讓 <em>word2vec</em> 學會分辨語意的差異，就需要加入反例，也就是 <em>不是出現在上下文的字</em> 。</p>

<p>如果 <em>dog</em> 的上下文中，不會出現 <em>fly</em> ， <em>fly</em> 的向量為 <script type="math/tex">\textbf{W}_4</script> ，將 <em>dog</em> 輸入類神經網路後，在 <em>fly</em> 的位置，訓練其輸出結果為 0 ，如下：</p>

<script type="math/tex; mode=display">

\frac{1}{1+e^{- \textbf{V}_1 \cdot \textbf{W}_3}} \approx 0

</script>

<p>如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00201.png" alt="" /></p>

<p>如果要讓輸出結果接近 0，則 <script type="math/tex">\textbf{V}_1</script> 和 <script type="math/tex">\textbf{W}_4</script> 的內積要越小越好，也就是說，它們之間的角度要越大越好。修正這兩個向量的角度，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00202.png" alt="" /></p>

<p>同理，若 <em>cat</em> 的上下文中沒有 <em>fly</em> ，則訓練其輸出 0 ：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00203.png" alt="" /></p>

<p>修正  <script type="math/tex">\textbf{V}_2</script> 和 <script type="math/tex">\textbf{W}_4</script> 的夾角，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00204.png" alt="" /></p>

<p>訓練後，得出的這些語意向量，語意相近的，夾角越小，語意相差越遠的，夾角越大，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00205.png" alt="" /></p>

<h2 id="further-reading">Further Reading</h2>

<ol>
  <li>
    <p><em>word2vec</em> 的 <em>backward propagation</em> 公式要怎麼推導，請看：<a href="http://ckmarkoh.github.io/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation">word2vec (part2 : Backward Propagation)</a></p>
  </li>
  <li>
    <p>如何從無到有，不使用任何自動微分的套件，實作一個 <em>word2vec</em> ，請看：<a href="http://ckmarkoh.github.io/blog/2016/08/29/neural-network-word2vec-part-3-implementation">word2vec (part3 : Implementation)</a></p>
  </li>
</ol>

<p>註： 實際上， <em>word2vec</em> 的 <em>input layer</em> 和 <em>output layer</em> 各有兩種架構。<em>input layer</em> 有 <em>cbow</em> 和 <em>skip-gram</em> 兩種，<em>output layer</em> 有 <em>hierarchical sofrmax</em> 和 <em>negative sampling</em> 兩種，本文所寫的為 <em>skip-gram</em> 搭配 <em>negative sampling</em> 的架構。關於 <em>hierarchical softmax</em> 請參考<a href="http://ckmarkoh.github.io/blog/2015/05/23/-hierarchical-probabilistic-neural-networks-neural-network-language-model">hierarchical sofrmax</a> 。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[自然語言處理 -- Vector Space of Semantics]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/07/10/nlp-vector-space-semantics/"/>
    <updated>2016-07-10T14:06:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/07/10/nlp-vector-space-semantics</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>如果要判斷某個字的語意，可以用它鄰近的字來判斷，例如以下句子：</p>

<blockquote>
  <p>The dog run.
A cat run.
A dog sleep.
The cat sleep.
A dog bark.
The cat meows.
The bird fly.
A bird sleep.</p>
</blockquote>

<p>由於 <em>dog</em> 和 <em>cat</em> 這兩個字出現在類似的上下文情境中，因此，可以判斷出 <em>dog</em> 和 <em>cat</em> 語意相近。</p>

<p>如果要能夠用數學運算，來計算語意相近程度，可以把文字的語意用向量表示，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{array}{c|c c c c c c c c c }

     &a &bark &bird &cat &dog &fly &meow & run & sleep & the \\ \hline

 dog &2 &1 &0 &0 &0 &0 &0 &1 &1 &1 \\

 cat &1 &0 &0 &0 &0 &0 &1 &1 &1 &2 \\

 bird &1 &0 &0 &0 &0 &1 &0 &0 &1 &1 

\end{array}

 %]]&gt;</script>

<p>其中， <em>dog</em> 的向量為  ( 2, 1, 0, 0, 0, 0, 0, 1, 1, 1 ) ，第一個維度表示 <em>dog</em> 在 <em>a</em> 旁邊的次數有 2 次，第二個維度表示在 <em>bark</em> 旁邊的次數有 1 次，以此類推。</p>

<!--more-->

<h2 id="vector-space-of-semantics">Vector Space of Semantics</h2>

<p>有了向量就可以用 <em>cosine similarity</em> 來計算語意相近程度。</p>

<p>給定兩向量 <script type="math/tex">A = (a_1,a_2,...,a_n)</script> 和<script type="math/tex">B = (b_1,b_2,...,b_n)</script> ，則這兩向量的 <em>cosine similarity</em> 為：</p>

<script type="math/tex; mode=display">

\frac{A \cdot B}{|A||B|}=  \frac{\sum a_i b_i}{\sqrt{\sum_i a_i^2}\sqrt{\sum_i b_i^2}}

</script>

<p><em>cosine similarity</em> 算出來的值，即是計算 <script type="math/tex">A</script> 和 <script type="math/tex">B</script> 兩向量的夾角的 <em>cosine</em> 值。 <em>cosine</em> 值越接近 1 ，表示兩向量夾角越小，表示兩向量的語意越接近。</p>

<p>根據以上例子， <em>dog</em> ( 2, 1, 0, 0, 0, 0, 0, 1, 1, 1 ) 和 <em>cat</em> ( 1, 0, 0, 0, 0, 0, 1, 1, 1, 2 )  的 <em>cosine similarity</em> 為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \frac{ 2 \times 1 + 1 \times 0 + 0 \times 0 + 0 \times 0 + 0 \times 0 + 0 \times 0 + 0 \times 1 + 1 \times 1 + 1 \times 1 + 1 \times 2 }{ \sqrt{ 2^2 + 1^2 + 0^2 + 0^2 + 0^2 + 0^2 + 0^2 + 1^2 + 1^2 + 1^2 } \sqrt{ 1^2 + 0^2 + 0^2 + 0^2 + 0^2 + 0^2 + 1^2 + 1^2 + 1^2 + 2^2} } \\

& = \frac{ 6 }{ \sqrt{ 8 } \sqrt{ 8} } 

= 0.75 

\end{align}

 %]]&gt;</script>

<p>而 <em>bird</em> 的向量為：( 1, 0, 0, 0, 0, 1, 0, 0, 1, 1 )  ，計算 <em>dog</em>  和  <em>bird</em> 的 <em>cosine similarity</em> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \frac{ 2 \times 1 + 1 \times 0 + 0 \times 0 + 0 \times 0 + 0 \times 0 + 0 \times 1 + 0 \times 0 + 1 \times 0 + 1 \times 1 + 1 \times 1 }{ \sqrt{ 2^2 + 1^2 + 0^2 + 0^2 + 0^2 + 0^2 + 0^2 + 1^2 + 1^2 + 1^2 } \sqrt{ 1^2 + 0^2 + 0^2 + 0^2 + 0^2 + 1^2 + 0^2 + 0^2 + 1^2 + 1^2} }  \\

& = \frac{ 4 }{ \sqrt{ 8 } \sqrt{ 4} } 

\approx 0.707106781187 

\end{align}

 %]]&gt;</script>

<p>由於 0.75 &gt; 0.707 ，因此 <em>dog</em> 和 <em>cat</em> 語意比較接近， <em>dog</em> 和 <em>bird</em> 語意比較不同。</p>

<p>此種語意向量有個缺點，就是向量的維度等於總字彙量。例如英文單字種共有好幾萬種，那麼，向量就有好幾萬個維度，向量維度過大的缺點，會造成資料過度稀疏，以及占記憶體的空間。</p>

<p>降低向量維度的方式，有兩種方法，分別是：</p>

<ol>
  <li>
    <p><a href="https://ccjou.wordpress.com/2009/09/01/%E5%A5%87%E7%95%B0%E5%80%BC%E5%88%86%E8%A7%A3-svd/">singular value decompisition (SVD)</a></p>
  </li>
  <li>
    <p><a href="http://blog.csdn.net/itplus/article/details/37969519">word2vec</a></p>
  </li>
</ol>

<p>本文先不詳細介紹這部分。</p>

<h2 id="implementation">Implementation</h2>

<p>在此實作將文字轉成向量，並用 SVD 降為至二維，作視覺化</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</span><span class="line">
</span><span class="line"><span class="n">text</span> <span class="o">=</span> <span class="p">[</span>
</span><span class="line">    <span class="p">[</span><span class="s">&quot;the&quot;</span><span class="p">,</span> <span class="s">&quot;dog&quot;</span><span class="p">,</span> <span class="s">&quot;run&quot;</span><span class="p">,</span> <span class="p">],</span>
</span><span class="line">    <span class="p">[</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;cat&quot;</span><span class="p">,</span> <span class="s">&quot;run&quot;</span><span class="p">,</span> <span class="p">],</span>
</span><span class="line">    <span class="p">[</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;dog&quot;</span><span class="p">,</span> <span class="s">&quot;sleep&quot;</span><span class="p">,</span> <span class="p">],</span>
</span><span class="line">    <span class="p">[</span><span class="s">&quot;the&quot;</span><span class="p">,</span> <span class="s">&quot;cat&quot;</span><span class="p">,</span> <span class="s">&quot;sleep&quot;</span><span class="p">,</span> <span class="p">],</span>
</span><span class="line">    <span class="p">[</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;dog&quot;</span><span class="p">,</span> <span class="s">&quot;bark&quot;</span><span class="p">,</span> <span class="p">],</span>
</span><span class="line">    <span class="p">[</span><span class="s">&quot;the&quot;</span><span class="p">,</span> <span class="s">&quot;cat&quot;</span><span class="p">,</span> <span class="s">&quot;meows&quot;</span><span class="p">,</span> <span class="p">],</span>
</span><span class="line">    <span class="p">[</span><span class="s">&quot;the&quot;</span><span class="p">,</span> <span class="s">&quot;bird&quot;</span><span class="p">,</span> <span class="s">&quot;fly&quot;</span><span class="p">,</span> <span class="p">],</span>
</span><span class="line">    <span class="p">[</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;bird&quot;</span><span class="p">,</span> <span class="s">&quot;sleep&quot;</span><span class="p">,</span> <span class="p">],</span>
</span><span class="line"><span class="p">]</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">build_word_vector</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
</span><span class="line">    <span class="n">word2id</span> <span class="o">=</span> <span class="p">{</span><span class="n">w</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="nb">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">,</span> <span class="n">text</span><span class="p">)))))}</span>
</span><span class="line">    <span class="n">id2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">word2id</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</span><span class="line">    <span class="n">wvectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">word2id</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2id</span><span class="p">)))</span>
</span><span class="line">    <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
</span><span class="line">        <span class="k">for</span> <span class="n">word1</span><span class="p">,</span> <span class="n">word2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sentence</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">sentence</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
</span><span class="line">            <span class="n">id1</span><span class="p">,</span> <span class="n">id2</span> <span class="o">=</span> <span class="n">word2id</span><span class="p">[</span><span class="n">word1</span><span class="p">],</span> <span class="n">word2id</span><span class="p">[</span><span class="n">word2</span><span class="p">]</span>
</span><span class="line">            <span class="n">wvectors</span><span class="p">[</span><span class="n">id1</span><span class="p">,</span> <span class="n">id2</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">            <span class="n">wvectors</span><span class="p">[</span><span class="n">id2</span><span class="p">,</span> <span class="n">id1</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">    <span class="k">return</span> <span class="n">wvectors</span><span class="p">,</span> <span class="n">word2id</span><span class="p">,</span> <span class="n">id2word</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">cosine_sim</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">):</span>
</span><span class="line">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))))</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">visualize</span><span class="p">(</span><span class="n">wvectors</span><span class="p">,</span> <span class="n">id2word</span><span class="p">):</span>
</span><span class="line">    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</span><span class="line">    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
</span><span class="line">    <span class="n">U</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">Vh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">wvectors</span><span class="p">)</span>
</span><span class="line">    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
</span><span class="line">    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">id2word</span><span class="p">:</span>
</span><span class="line">        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">U</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">U</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">id2word</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>本程式中， <code>text</code> 是輸入的文字， <code>build_word_vector</code> 將文字轉成向量：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; wvectors, word2id, <span class="nv">id2word</span> <span class="o">=</span> build_word_vector<span class="o">(</span>text<span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>其中， <code>wvectors</code> 是根據前後文統計後，得出各文字的向量：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; print wvectors
</span><span class="line"><span class="o">[[</span> 0.  0.  1.  1.  2.  0.  0.  0.  0.  0.<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> 1.  0.  0.  0.  0.  1.  0.  0.  1.  1.<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> 1.  0.  0.  0.  0.  0.  1.  1.  1.  2.<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> 2.  1.  0.  0.  0.  0.  0.  1.  1.  1.<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> 0.  0.  0.  1.  1.  0.  0.  0.  0.  0.<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> 0.  0.  1.  1.  1.  0.  0.  0.  0.  0.<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> 0.  0.  1.  2.  1.  0.  0.  0.  0.  0.<span class="o">]]</span>
</span></code></pre></td></tr></table></div></figure>

<p>每一橫排（或直排）代表著某個字的向量，但從它看不出是哪個字，所以 <code>word2id</code> 則是給定文字，找出是第幾個向量，而 <code>id2word</code> 則是給定第幾個向量，找出所對應的文字。</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; print word2id
</span><span class="line"><span class="o">{</span><span class="s1">&#39;a&#39;</span>: 0, <span class="s1">&#39;fly&#39;</span>: 5, <span class="s1">&#39;run&#39;</span>: 7, <span class="s1">&#39;the&#39;</span>: 9, <span class="s1">&#39;dog&#39;</span>: 4, <span class="s1">&#39;cat&#39;</span>: 3,
</span><span class="line"><span class="s1">&#39;meows&#39;</span>: 6, <span class="s1">&#39;sleep&#39;</span>: 8, <span class="s1">&#39;bark&#39;</span>: 1, <span class="s1">&#39;bird&#39;</span>: 2<span class="o">}</span>
</span><span class="line">
</span><span class="line">&gt;&gt;&gt; print id2word
</span><span class="line"><span class="o">{</span>0: <span class="s1">&#39;a&#39;</span>, 1: <span class="s1">&#39;bark&#39;</span>, 2: <span class="s1">&#39;bird&#39;</span>, 3: <span class="s1">&#39;cat&#39;</span>, 4: <span class="s1">&#39;dog&#39;</span>, 5: <span class="s1">&#39;fly&#39;</span>,
</span><span class="line">6: <span class="s1">&#39;meows&#39;</span>, 7: <span class="s1">&#39;run&#39;</span>, 8: <span class="s1">&#39;sleep&#39;</span>, 9: <span class="s1">&#39;the&#39;</span><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>

<p>例如 <em>dog</em> 是在 <code>wvectors</code> 中， 第 5 排的向量（註：index 從0開始算），用 <code>word2id</code> 可從 <code>wvector</code> 中，取出其對應向量：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; print wvectors<span class="o">[</span>word2id<span class="o">[</span><span class="s2">&quot;dog&quot;</span><span class="o">]]</span>
</span><span class="line"><span class="o">[</span> 2.  1.  0.  0.  0.  0.  0.  1.  1.  1.<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>程式碼中的 <code>cosine_sim</code> ，則可計算兩向量間的 <em>cosine similarity</em> ，例如，</p>

<p>計算 <em>dog</em> 和 <em>cat</em> 與 <em>dog</em> 和 <em>bird</em> 之間的  <em>cosine similarity</em>  ：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; print cosine_sim<span class="o">(</span>wvectors<span class="o">[</span>word2id<span class="o">[</span><span class="s2">&quot;dog&quot;</span><span class="o">]]</span>, wvectors<span class="o">[</span>word2id<span class="o">[</span><span class="s2">&quot;cat&quot;</span><span class="o">]])</span>
</span><span class="line">0.75
</span><span class="line">
</span><span class="line">&gt;&gt;&gt; print cosine_sim<span class="o">(</span>wvectors<span class="o">[</span>word2id<span class="o">[</span><span class="s2">&quot;dog&quot;</span><span class="o">]]</span>, wvectors<span class="o">[</span>word2id<span class="o">[</span><span class="s2">&quot;bird&quot;</span><span class="o">]])</span>
</span><span class="line">0.707106781187
</span></code></pre></td></tr></table></div></figure>

<p>再來是用 <code>visualize</code> 將這些語意向量在平面上作視覺化：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; visualize<span class="o">(</span>wvectors, id2word<span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>在平面上作視覺化的方法，是用 SVD 將語意向量降為至二維，就可以把這些向量所對應的字，畫在平面上，結果如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00186.png" alt="" /></p>

<p>此結果顯示，   <em>dog</em> 、 <em>cat</em> 和 <em>bird</em> 是語意相近的一群， <em>a</em> 和 <em>the</em> 語意相近，以此類推。</p>

<h2 id="further-reading">Further Reading</h2>

<p>Simple Word Vector representations</p>

<p>http://cs224d.stanford.edu/lectures/CS224d-Lecture2.pdf</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[機率圖模型 -- Gibbs Sampling]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/07/09/pgm-gibbs-sampling/"/>
    <updated>2016-07-09T08:36:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/07/09/pgm-gibbs-sampling</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p><em>Gibbs Sampling</em> 是一種類似於 <a href="http://ckmarkoh.github.io/blog/2016/07/07/pgm-metropolis-hasting">Metropolis Hasting</a> 的抽樣方式，也是根據機率分佈來建立 <em>Markov Chain</em> ，並在 <em>Markov Chain</em> 上行走，抽樣出機率分佈。</p>

<p>設一 <em>Markov Chain</em> ， 有 <em>a</em> 和 <em>b</em> 兩個 <em>state</em> ，它們的值分別為 <script type="math/tex">p(a)</script> 和 <script type="math/tex">p(b)</script> ，而它們之間的轉移機率，分別為 <script type="math/tex">q_1(a,b)</script> 和 <script type="math/tex">q_2(a,b)</script> ，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00179.png" alt="" /></p>

<p>達平衡時，會滿足以下條件：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&

\begin{bmatrix} 

p(a) \\

p(b)

\end{bmatrix}

=

\begin{bmatrix} 

1-q_1(a,b) & q_2(a,b) \\

q_1(a,b) & 1-q_2(a,b)

\end{bmatrix}

\begin{bmatrix} 

p(a) \\

p(b)

\end{bmatrix} \\

& \Rightarrow

\begin{cases}

p(a)(1-q_1(a,b) +p(b)q_2(a,b) = p(a)  \\

p(a)q_1(a,b) + p(b)(1-q_2(a,b)) = p(b) 

\end{cases}\\

& \Rightarrow p(a)q_1(a,b) = p(b)q_2(a,b)

\end{align}


 %]]&gt;</script>

<p>因此，達到平衡時，得出 <a name="eq1">（公式一）</a> ：</p>

<script type="math/tex; mode=display">

 p(a)q_1(a,b) = p(b)q_2(a,b)

</script>

<p>在  <a href="http://ckmarkoh.github.io/blog/2016/07/07/pgm-metropolis-hasting">Metropolis Hasting</a> 這篇有提到，可以利用 <em>Markov Chain</em> 最終會達到平衡的特性，來為某機率分佈 <script type="math/tex">p(x)</script> 抽樣。</p>

<p>但是 <em>Metropolis Hasting</em> 抽樣時，需要先用 <em>proposed distribution</em> 計算出下一個時間點可能的值，然後 <em>acceptance probability</em> 來拒絕它，因為計算出來的值會被拒絕，所以造成計算上的浪費。</p>

<p>而對於一高維度的機率分佈 <script type="math/tex">p(x_1,x_2,...,x_n)</script> ，可以用另一種方式來建立 <em>Markov Chain</em> ，則不會有這個問題。這種方法為 <em>Gibbs Sampling</em> 。</p>

<!--more-->

<h2 id="gibbs-sampling">Gibbs Sampling</h2>

<p>首先，先考慮二維的空間，設一機率分佈 <script type="math/tex">p(X,Y)</script> ，若此機率分佈的 <em>Random Variable</em> 有四個值，分別為 <script type="math/tex">A(x_1,y_1)</script> ， <script type="math/tex">B(x_2,y_1)</script> ， <script type="math/tex">C(x_1,y_2)</script> 和 <script type="math/tex">D(x_2,y_2)</script> 。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00180.png" alt="" /></p>

<p>首先，看看如何在  <em>A</em> 和 <em>B</em> 建立 <em>Markov Chain</em> 。由於 <em>A</em> 和 <em>B</em> 只有在 <script type="math/tex">x</script> 維度上的值不一樣，則：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& p(x_1,y_1)p(x_2|y_1)=p(y_1)p(x_1|y_1)p(x_2|y_1) \\

& p(x_2,y_1)p(x_1|y_1)=p(y_1)p(x_2|y_1)p(x_1|y_1)

\end{align}

 %]]&gt;</script>

<p>由於兩式的等號右邊一樣，則可得出：</p>

<script type="math/tex; mode=display">

\begin{align}

p(x_1,y_1)p(x_2|y_1)=p(x_2,y_1)p(x_1|y_1)

\end{align}

</script>

<p>此結果符合 <a href="#eq1">（公式一）</a> 的 <em>Markov Chain</em> 平衡狀態。根據此結果，建立以下的 <em>Markov Chain</em> ：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00181.png" alt="" /></p>

<p>其中， <script type="math/tex">p(x_1\mid y_1)+p(x_2\mid y_1) = 1</script> 。</p>

<p>若在此 <em>Markov Chain</em> 上行走，如果走得夠多次的話，走到 <em>A</em> 和走到 <em>B</em> 的次數比，會是 <script type="math/tex">p(x_1,y_1) : p(x_2,y_1)</script> 。因此，用此 <em>Markov Chain</em> 得出的次數比，就相當於從機率分佈 <script type="math/tex">p(X,Y)</script> 抽樣後，  <em>A</em> 和 <em>B</em>  所抽出的次數比。</p>

<p>也可用同樣方法，在 <em>A</em> 和 <em>C</em> 之間建立 <em>Markov Chain</em>：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00182.png" alt="" /></p>

<p>同理，<em>D</em> 和 <em>B</em> 或 <em>C</em> 之間，只有一個維度是不一樣的值，也可建立這樣的 <em>Markov Chain</em> 。</p>

<p>結合以上 <em>Markov Chain</em> ，若要對 <script type="math/tex">p(X,Y)</script> 進行抽樣，則可先固定 <script type="math/tex">y</script> 的值，先在 <script type="math/tex">x</script> 軸上抽樣， 再固定 <script type="math/tex">x</script> 的值，在  <script type="math/tex">y</script> 軸上抽樣，如此交替進行，即為 <em>Gibbs Sampling</em> 的方法。</p>

<p>舉個例子，假設有一聯合分佈 <script type="math/tex">p(X,Y)</script> ，其機率值如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{array}{c | c c}

p(X,Y) & Y=0  & Y=1  \\ \hline

X=0  & 0.5 & 0.2 \\

X=1  & 0.1 & 0.2 \\

\end{array}

 %]]&gt;</script>

<p>這些點在座標上的位置如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00183.png" alt="" /></p>

<p>先把 <script type="math/tex">p(X\mid Y)</script> 和 <script type="math/tex">p(Y\mid X)</script> 計算出來，以便之後使用。</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&

\begin{array}{c | c c}

p(X|Y) & Y=0  & Y=1  \\ \hline

X=0  & \frac{5}{6} & \frac{1}{2} \\

X=1  & \frac{1}{6} & \frac{1}{2} \\

\end{array} 

&

\begin{array}{c | c c}

p(Y|X) & Y=0  & Y=1  \\ \hline

X=0  & \frac{5}{7} & \frac{2}{7} \\

X=1  & \frac{1}{3} & \frac{2}{3} \\

\end{array}

\end{align}

 %]]&gt;</script>

<p>然後，進行 <em>gibbs sampling</em> ，首先，從 (0,0) 開始，固定 y 軸，在 x 軸上抽樣，用 <script type="math/tex">p(X\mid Y=0)</script> 的值，建立從 (0,0) 在 x 軸上轉移的 <em>Markov Chain</em> ，如下： </p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00184.png" alt="" /></p>

<p>根據此 <em>Marokv Chain</em> ， 從 <script type="math/tex">p(X\mid  Y=0)</script> 中，抽出 <script type="math/tex">X</script> ， 假設抽出的 <script type="math/tex">X</script> 值為 1，則抽出來的點為 (1,0) ，下一步則是要固定 x 軸，在 y 軸上抽樣。用 <script type="math/tex">p(Y\mid Y=1)</script> 的值，建立從 (1,0) 在 y 軸上轉移的 <em>Markov Chain</em> ，如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00185.png" alt="" /></p>

<p>根據此 <em>Marokv Chain</em> ， 從 <script type="math/tex">p(Y\mid  X=1)</script> 中，抽出 <script type="math/tex">Y</script>， 假設抽出的 <script type="math/tex">Y</script> 值為 1，則抽出來的點為 (1,1)，到目前為止，抽出來的樣本序列為這樣：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"><span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>1,0<span class="o">)</span> <span class="o">(</span>1,1<span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>然後再固定 y 軸，在 x 軸上抽樣，這樣持續抽樣下去，抽到最後，得出以下序列：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"><span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>1,0<span class="o">)</span> <span class="o">(</span>1,1<span class="o">)</span> <span class="o">(</span>0,1<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>1,0<span class="o">)</span>
</span><span class="line"><span class="o">(</span>1,1<span class="o">)</span> <span class="o">(</span>1,1<span class="o">)</span> <span class="o">(</span>1,1<span class="o">)</span> <span class="o">(</span>1,1<span class="o">)</span> <span class="o">(</span>1,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span>
</span><span class="line"><span class="o">(</span>0,1<span class="o">)</span> <span class="o">(</span>1,1<span class="o">)</span> <span class="o">(</span>1,1<span class="o">)</span> <span class="o">(</span>0,1<span class="o">)</span> <span class="o">(</span>0,1<span class="o">)</span> <span class="o">(</span>0,1<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> ...
</span></code></pre></td></tr></table></div></figure>

<p>最後，統計抽樣序列中各個點的出現次數， (0,0) (1,0) (1,1) (0,1) 這四個點的次數比會接近 0.5 : 0.1 : 0.2 : 0.2</p>

<p><em>Gibbs Sampling</em> 也可用在高維度的機率分佈 <script type="math/tex">p(x_1,x_2,...,x_n)</script> 。抽樣時，先從第一個維度上抽樣，固定其他維度，用 <script type="math/tex">p(x_1\mid x_2,...,x_{n})</script> 抽出下一個樣本，然後再從第二個維度抽樣，固定其他維度，用 <script type="math/tex">p(x_2\mid x_1,x_3,...,x_{n})</script> 抽出下一個樣本，如此一直循環下去。</p>

<p>整個抽樣過程的 pseudo code 如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

1 & \text{set }t = 0 \\

2 & \text{generate an initial state } \textbf{x}_0 \in \text{domain of }p(x_1,x_2,...,x_n) \\

3 & \text{repeat until }t = M \\

4 & \mspace{30mu}\text{for each dimension }i = 1...n \\

5 & \mspace{60mu}\text{draw } \textbf{x}_t \text{ from } p(x_i|x_1,x_2,\dots,x_{i-1},x_{i+1},\dots,x_n) \\

6 & \mspace{60mu}\text{set }t = t+1

\end{align}

 %]]&gt;</script>

<p>此處流程大致上和 <em>Metropolis Hasting</em> 類似， 而 <script type="math/tex">p(x_i\mid x_1,x_2,\dots,x_{i-1},x_{i+1},\dots,x_n)</script> 就相當於是  <em>Metropolis Hasting</em> 中的 <em>proposed distribution</em> ，但在 <em>Gibbs Sampling</em> 中 ， <script type="math/tex">p(x_i\mid x_1,x_2,\dots,x_{i-1},x_{i+1},\dots,x_n)</script> 得出的值，就直接是抽樣結果了，不需要再算一個 <em>acceptance probability</em> 來判斷是否要接受或拒絕它。因此，不會造成計算上的浪費。</p>

<h2 id="implementation">Implementation</h2>

<p>此例實作先前提到的  <script type="math/tex">p(X,Y)</script> 抽樣：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{array}{c | c c}

p(X,Y) & Y=0  & Y=1  \\ \hline

X=0  & 0.5 & 0.2 \\

X=1  & 0.1 & 0.2 \\

\end{array}

 %]]&gt;</script>

<p>程式碼如下：</p>

<figure class="code"><figcaption><span>gibbssamp.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">gibbssamp</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span><span class="line">    <span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]])</span>
</span><span class="line">    <span class="n">condi</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span><span class="p">]</span>
</span><span class="line">    <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</span><span class="line">    <span class="n">samples</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span><span class="line">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
</span><span class="line">            <span class="n">one_prob</span> <span class="o">=</span> <span class="n">condi</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">[(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]]</span>
</span><span class="line">            <span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">one_prob</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">            <span class="n">samples</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
</span><span class="line">    <span class="k">return</span> <span class="n">samples</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">count</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
</span><span class="line">    <span class="n">c</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
</span><span class="line">    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">:</span>
</span><span class="line">        <span class="n">c</span><span class="o">.</span><span class="n">update</span><span class="p">([</span><span class="s">&quot;(</span><span class="si">%s</span><span class="s">,</span><span class="si">%s</span><span class="s">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">])])</span>
</span><span class="line">    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">c</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
</span><span class="line">        <span class="k">print</span> <span class="n">k</span><span class="p">,</span> <span class="n">c</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>其中，程式碼中的 <code>P</code> 為機率分佈 <script type="math/tex">p(X,Y)</script> ， <code>condi[0]</code> 為 <script type="math/tex">p(X\mid Y)</script> ， <code>condi[1]</code> 為 <script type="math/tex">p(Y\mid X)</script> 。 <code>gibbssamp</code> 為執行 <em>Gibbs Sampling</em> 的主要函數。</p>

<p>執行 <code>gibbssamp</code> 抽出 1000 * 2 個樣本，如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; <span class="nv">X</span> <span class="o">=</span> gibbssamp<span class="o">(</span>1000<span class="o">)</span>
</span><span class="line">&gt;&gt;&gt; print X
</span><span class="line"><span class="o">[[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>1, 0<span class="o">]</span>, <span class="o">[</span>1, 1<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>,
</span><span class="line"><span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>1, 1<span class="o">]</span>, <span class="o">[</span>1, 1<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>1, 1<span class="o">]</span>, <span class="o">[</span>1, 1<span class="o">]</span>,
</span><span class="line"><span class="o">[</span>1, 1<span class="o">]</span>, <span class="o">[</span>1, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>1, 0<span class="o">]</span>, <span class="o">[</span>1, 1<span class="o">]</span>,
</span><span class="line"><span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>,
</span><span class="line"><span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>1, 0<span class="o">]</span>, <span class="o">[</span>1, 1<span class="o">]</span>, <span class="o">[</span>1, 1<span class="o">]</span>, <span class="o">[</span>1, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span> ...
</span></code></pre></td></tr></table></div></figure>

<p>用 <code>count</code> 用來統計抽樣結果：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; count<span class="o">(</span>X<span class="o">)</span>
</span><span class="line"><span class="o">(</span>0,0<span class="o">)</span> 1043
</span><span class="line"><span class="o">(</span>1,0<span class="o">)</span> 172
</span><span class="line"><span class="o">(</span>0,1<span class="o">)</span> 387
</span><span class="line"><span class="o">(</span>1,1<span class="o">)</span> 398
</span></code></pre></td></tr></table></div></figure>

<p>統計結果顯示， (0,0) (1,0) (0,1) (1,1) 的比率會接近 0.5 : 0.1 : 0.2 : 0.2</p>

<h2 id="further-reading">Further Reading</h2>

<p>The Gibbs Sampler</p>

<p>https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/</p>

<p>LDA-math-MCMC 和 Gibbs Sampling(1)</p>

<p>http://www.52nlp.cn/lda-math-mcmc-%e5%92%8c-gibbs-sampling1</p>

<p>LDA-math-MCMC 和 Gibbs Sampling(2)</p>

<p>http://www.52nlp.cn/lda-math-mcmc-%e5%92%8c-gibbs-sampling2</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[機率圖模型 -- Metropolis Hasting]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/07/07/pgm-metropolis-hasting/"/>
    <updated>2016-07-07T17:21:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/07/07/pgm-metropolis-hasting</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>對一個 <em>Markov Chain</em> 而言，不論起始狀態為多少，最後會達到一個穩定平衡的狀態。</p>

<p>舉個例子，以下為一 <em>Markov Chain</em></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00172.png" alt="" /></p>

<p>則此 <em>Markov Chain</em> 達平衡狀態時， <script type="math/tex">A,B</script> 的比率為<script type="math/tex"> 5:6 </script>，也就是說：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&

\begin{bmatrix} 

A \\

B

\end{bmatrix}

=

\begin{bmatrix} 

0.4 & 0.5 \\

0.6 & 0.5 

\end{bmatrix}

\begin{bmatrix} 

A \\

B

\end{bmatrix} \\

& \Rightarrow

\begin{cases}

4A+5B = 10A \\

6A+5B = 10B 

\end{cases}\\

& \Rightarrow 6A = 5B

\end{align}

 %]]&gt;</script>

<p>不管此 <em>Markov Chain</em> 的起始狀態如何，最後達平衡狀態時， <script type="math/tex">A,B</script> 的比率一定為<script type="math/tex"> 5:6 </script>。因此，如果在這 <em>Markov Chain</em> 的  <script type="math/tex">A</script> 和 <script type="math/tex">B</script> 任一一個點開始走，假設走的次數夠多的話，走到 <script type="math/tex">A</script> 和走到 <script type="math/tex">B</script> 的比例。會是 <script type="math/tex">5 : 6</script> 。因此，可利用 <em>Markov Chain</em> 最後會收斂到一穩定狀態的特性，來進行抽樣。</p>

<p><em>Metropolis Sampler</em> 即是給定一機率分佈函數，從這機率函數建立 <em>Markov Chain</em> ，然後再用建立出來的 <em>Markov Chain</em> 來進行抽樣。</p>

<!--more-->

<h2 id="metropolis-sampler">Metropolis Sampler</h2>

<p>設某一機率分佈 <script type="math/tex">p(X)</script> ，起始值為 <script type="math/tex">x_{0}</script> 。隨機變數的值為 <script type="math/tex"> X = \{ a_{1}, a_{2}, ..., a_{n}\} </script>。</p>

<p>抽樣過程如下：</p>

<p>設目前時間點 <script type="math/tex">t</script> 抽出的值為 <script type="math/tex">x_{t}=a_{i}</script> 然後，從一機率分佈（稱為 <em>proposal distribution</em>），<script type="math/tex">q(x_{t+1}\mid x_{t})</script> 中，抽出一個值，為 <script type="math/tex">a_{j}</script> ，其中 <script type="math/tex">q(x_{t+1}\mid x_{t})</script> 滿足以下對稱性即可：</p>

<script type="math/tex; mode=display">

q(x_{t+1}|x_{t}) = q(x_{t}|x_{t+1})

</script>

<p><script type="math/tex">q(x_{t+1}\mid x_{t})</script> 可以是比較好計算的函數，用於快速產生下一個可能的樣本 <script type="math/tex">a_{j}</script> ，  <script type="math/tex">a_{j}</script> 為 <em>proposed state</em> ，意思就是說，想從 <script type="math/tex">q(x_{t+1}\mid x_{t})</script> 「提出」一個值，看看這個值能不能成為 <script type="math/tex">x_{t+1}</script> 的值，但實際上，這時還不知道 <script type="math/tex">a_{j}</script> 適不適合為<script type="math/tex">x_{t+1}</script> 的值，所以要做以下運算。</p>

<p>如果 <script type="math/tex">p(a_{j}) \geq p(a_{i})</script> ，則：</p>

<script type="math/tex; mode=display">

x_{t+1} = a_{j}

</script>

<p>如果 <script type="math/tex">% &lt;![CDATA[
p(a_{j}) < p(a_{i}) %]]&gt;</script> 則：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x_{t+1} = \begin{cases}

   a_{j} & \text{with probability of } \alpha  \\

   a_{i} & \text{with probability of } 1- \alpha 

\end{cases}\\

& \text{where }  \alpha = \frac{p(a_j)}{p(a_{i})}

\end{align}


 %]]&gt;</script>

<p>其中， <script type="math/tex">\alpha</script> 稱為 <em>acceptance probability</em> ，意思是，接受 <script type="math/tex">x_{t+1}</script> 的值為 <script type="math/tex">a_{j}</script> 的機率有多少？</p>

<p>此方法可以看成是在 <script type="math/tex">a_{i}</script> 與 <script type="math/tex">a_{j}</script> 之間，建立 <em>Markov Chain</em> ：</p>

<p>如果想從 <script type="math/tex">a_{i}</script> 轉移到 <script type="math/tex">a_{j}</script> ，且， <script type="math/tex">p(a_{j}) \geq p(a_{i})</script> ，則建立出的 <em>Markov Chain</em> ，如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00173.png" alt="" /></p>

<p>由於 <script type="math/tex">p(a_{j}) \geq p(a_{i})</script> ，所以一定會接受 從 <script type="math/tex">a_{i}</script> 到 <script type="math/tex">a_{j}</script> 的轉移。</p>

<p>呈上，如果想從 <script type="math/tex">a_{j}</script> 轉移回 <script type="math/tex">a_{i}</script> ，則建立出的 <em>Markov Chain</em> ，如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00174.png" alt="" /></p>

<p>由於 <script type="math/tex">% &lt;![CDATA[
p(a_{i}) < p(a_{j}) %]]&gt;</script>  ，所以從 <script type="math/tex">a_{j}</script> 轉到 <script type="math/tex">a_{i}</script> 的轉移，不一定會成立，而是由 <em>acceptance probability</em> 來決定。</p>

<p>把以上兩個合起來，得出 <script type="math/tex">a_{i}</script> 與 <script type="math/tex">a_{j}</script> 之間的  <em>Markov Chain</em> ，如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00175.png" alt="" /></p>

<p>假設在這 <em>Markov Chain</em> 上，隨機走動，設走到 <script type="math/tex">a_i</script> 的機率為 <script type="math/tex">A</script> ，走到 <script type="math/tex">a_j</script> 的機率為 <script type="math/tex">B</script> ，則達平衡時：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&

\begin{bmatrix} 

A \\

B

\end{bmatrix}

=

\begin{bmatrix} 

0 & \frac{p(a_i)}{p(a_j)} \\

1 & 1 -  \frac{p(a_i)}{p(a_j)}

\end{bmatrix}

\begin{bmatrix} 

A \\

B

\end{bmatrix} \\

& \Rightarrow

\begin{cases}

\frac{p(a_i)}{p(a_j)} B = A \\

A + (1 -  \frac{p(a_i)}{p(a_j)})B = B 

\end{cases}\\

& \Rightarrow A : B = p(a_i) : p(a_j)

\end{align}

 %]]&gt;</script>

<p>因此 <em>Markov Chain</em> 達到平衡狀態時， 走到 <script type="math/tex">a_i</script> 和走到 <script type="math/tex">a_j</script> 的次數比，會是 <script type="math/tex">p(a_i):p(a_j)</script> 。</p>

<p>舉個簡單的例子，假設 <script type="math/tex">p(x)</script> 如下， <script type="math/tex">X</script> 有0.8的機率為 0 ，有0.2的機率為 1 ，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


p(x) = \begin{cases}

0.2 & X=0 \\

0.8 & X=1

\end{cases}\\

 %]]&gt;</script>

<p>則建立出來的 <em>Markov Chain</em> 如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00176.png" alt="" /></p>

<p>從這 <em>Markov Chain</em> 上，從 0 開始走，會先轉移到 1 ，走過的序列如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">01
</span></code></pre></td></tr></table></div></figure>

<p>到 1 以後，有 0.75的機率會走回 1，有0.25會再走到 0 ，走到 1 和走到 0 的比率是 3:1 ， 假設走回 1 三次以後才走回0 ，則走過的序列如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">011110
</span></code></pre></td></tr></table></div></figure>

<p>這樣一直走下去，則</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">01111011110111101111...
</span></code></pre></td></tr></table></div></figure>

<p>序列中任取一個數，則有0.8的機率為 0 ，有0.2的機率為 1 ，和 <script type="math/tex">p(x)</script> 的機率分佈一樣。</p>

<p>因此，建立了  <script type="math/tex">a_{i}</script> 與 <script type="math/tex">a_{j}</script> 之間的 <em>Markov Chain</em> 之後，就可以在這兩個點上面來回行走，抽出適當比例的樣本。</p>

<p>可以把 <script type="math/tex">X</script> 只有兩個值的機率分佈，推廣到多個值，在 <script type="math/tex"> X = \{ a_{1}, a_{2}, ..., a_{n}\} </script> 中的任兩個值都可以建立這樣的 <em>Markov Chain</em> ，在這些 <em>Markov Chain</em> 中，來回行走，最後達平衡時，抽出來的序列就等同於從 <script type="math/tex">p(X)</script> 的機率分佈中抽樣。</p>

<p>整個抽樣過程的 <em>pseudo code</em> 如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

1 & \text{set }t = 0 \\

2 & \text{generate an initial state } x_0 \in \{ a_{1}, a_{2}, ..., a_{n}\} \\

3 & \text{repeat until }t = M \\

4 & \mspace{30mu} \text{generate a proposal state } a_{j} \text{ from } q(x_{t+1} | x_t ) \\

5 & \mspace{30mu} \text{calculate the acceptance probability }\alpha = \text{min} \left(1,\frac{p(a_{j})}{p(x_t)} \right) \\

6 & \mspace{30mu} \text{draw a random number } u \text{ from } \text{Unif}(0,1) \\

7 & \mspace{30mu} \text{if }u \leq \alpha \\

8 & \mspace{60mu} \text{ accept the proposal state } a_{j} \text{ and set } x_{t+1}= a_{j} \\

9 & \mspace{30mu} \text{else set } \\

10 & \mspace{60mu} x_{t+1} = x_{t}

\end{align}

 %]]&gt;</script>

<p>其中， <script type="math/tex">M</script> 為抽出的樣本數，而從 <em>Unif(0,1)</em> 抽出的 <script type="math/tex">u</script> ，目的是要讓 <script type="math/tex">x_{t+1}= a_{j}</script> 成立的機率為 <script type="math/tex">\alpha</script> 。</p>

<h2 id="metropolis-hasting">Metropolis Hasting</h2>

<p>由於 <em>Metropolis Sampler</em> 的 <em>proposed distribution</em> 一定要滿足對稱性，如果 <script type="math/tex">p(x)</script> 為非對稱性的機率分佈，例如 <em>Gamma distribution</em> ，就不太適合用對稱性的 <em>proposed distribution</em> 來進行抽樣。</p>

<p><em>Metropolis Hasting</em> 是一種更 <em>General</em> 的抽樣方式，它可用於以下情形：</p>

<script type="math/tex; mode=display">

q(x_{t+1}|x_{t}) \neq q(x_{t}|x_{t+1})

</script>

<p>如果 <script type="math/tex">q(x_{t+1}\mid x_{t})</script> 不對稱，例如 <script type="math/tex">q(x_{t+1}\mid x_{t}) > q(x_{t}\mid x_{t+1}) </script> 這種情況，表示從 <script type="math/tex">x_{t}</script> 的位置「提出」走到 <script type="math/tex">x_{t+1}</script> 的位置，機率是比從 <script type="math/tex">x_{t+1}</script> 「提出」走回 <script type="math/tex">x_{t}</script> 還高。所以也要把這個機率的差異考慮到 <em>Markov Chain</em> 裡面，因此要加入 <em>correction factor</em> <script type="math/tex">c</script> ：</p>

<script type="math/tex; mode=display">

c = \frac{q(x_{t}|x_{t+1})}{q(x_{t+1}|x_{t})}

</script>

<p>將 <script type="math/tex">c</script> 乘上 <em>proposed probability</em>  ，來修改 <em>acceptance probability</em> ：</p>

<script type="math/tex; mode=display">

\alpha = \frac{p(a_j)}{p(a_{i})} \times c = \frac{p(a_j)}{p(a_{i})} \times \frac{q(a_i|a_j)}{q(a_j|a_i)}

</script>

<p>如果要在 <script type="math/tex">a_i</script> 與 <script type="math/tex">a_j</script> 之間，建立 <em>Markov Chain</em> ，且 <script type="math/tex">p(a_j)q(a_i\mid a_j) > p(a_{i})q(a_j\mid a_i)</script>，則建立出的 Markov Chain ，如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00177.png" alt="" /></p>

<p>除了 <em>acceptance probability</em> 要加上 <em>correction factor</em> 之外， <em>Metropolis Hasting</em> 的抽樣過程，幾乎和 <em>Metropolis Sampler</em> 一樣。整個抽樣過程的 <em>pseudo code</em> 如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

1 & \text{set }t = 0 \\

2 & \text{generate an initial state } x_0 \in \{ a_{1}, a_{2}, ..., a_{n}\} \\

3 & \text{repeat until }t = M \\

4 & \mspace{30mu} \text{generate a proposal state } a_{j} \text{ from } q(x_{t+1} | x_t ) \\

5 & \mspace{30mu} \text{calculate the proposal correction factor }c = \frac{q(x_{t-1} | a_j) }{q(a_j|x_{t-1})} \\

6 & \mspace{30mu} \text{calculate the acceptance probability }\alpha = \text{min} \left(1,\frac{p(a_{j})}{p(x_t)} \times c \right) \\

7 & \mspace{30mu} \text{draw a random number } u \text{ from } \text{Unif}(0,1) \\

8 & \mspace{30mu} \text{if }u \leq \alpha \\

9 & \mspace{60mu} \text{ accept the proposal state } a_{j} \text{ and set } x_{t+1}= a_{j} \\

10 & \mspace{30mu} \text{else set } \\

11 & \mspace{60mu} x_{t+1} = x_{t}

\end{align}

 %]]&gt;</script>

<h2 id="implementation">Implementation</h2>

<p>再來，進入實作的部分</p>

<h4 id="metropolis-sampler-1">Metropolis Sampler</h4>

<p>首先，從 <em>Metropolis Sampler</em> 開始，用 <em>Metropolis Sampler</em> 對以下 <script type="math/tex">p(x)</script> 進行抽樣：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


p(x) = \begin{cases}

0.2 & X=0 \\

0.8 & X=1

\end{cases}\\

 %]]&gt;</script>

<p>程式碼如下：</p>

<figure class="code"><figcaption><span>matrosamp.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">random</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">collections</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">metrosamp</span><span class="p">(</span><span class="n">ITER</span><span class="p">):</span>
</span><span class="line">    <span class="n">P</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">}</span>
</span><span class="line">    <span class="n">Q</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
</span><span class="line">    <span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">    <span class="n">xt</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ITER</span><span class="p">):</span>
</span><span class="line">        <span class="n">xtp1</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="n">xt</span><span class="p">]</span>
</span><span class="line">        <span class="n">alpha</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">P</span><span class="p">[</span><span class="n">xtp1</span><span class="p">]</span> <span class="o">/</span> <span class="n">P</span><span class="p">[</span><span class="n">xt</span><span class="p">])</span>
</span><span class="line">        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">alpha</span><span class="p">:</span>
</span><span class="line">            <span class="n">xt</span> <span class="o">=</span> <span class="n">xtp1</span>
</span><span class="line">        <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">X</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">count</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
</span><span class="line">    <span class="n">counter</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">counter</span><span class="p">:</span>
</span><span class="line">        <span class="k">print</span> <span class="n">key</span><span class="p">,</span> <span class="n">counter</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>程式碼中的 <code>P</code> 即 <script type="math/tex">p(x)</script> ， 而 <code>Q</code> 則是 <em>proposed distribution</em> ， <script type="math/tex">q(x_{t}\mid x_{t-1})</script> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& q(x_{t} = 0 |x_{t-1}=1) = 1 \\

& q(x_{t} = 1|x_{t-1}=0) = 1 \\

\end{align}

 %]]&gt;</script>

<p>也就是說，位於 0 時， <em>proposed distribution</em> 會「提出」走到 1 ，位於 1 時會提出走到 0 。</p>

<p>而進行 <em>Metropolis Sampler</em> 抽樣的演算法為程式碼中的 <code>metrosamp</code> ，抽樣結果儲存於 <code>X</code> ：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; <span class="nv">X</span> <span class="o">=</span> metrosamp<span class="o">(</span>10000<span class="o">)</span>
</span><span class="line">&gt;&gt;&gt; print X
</span><span class="line"><span class="o">[</span>1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,....<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>用count` 則是將結果做統計：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; count<span class="o">(</span>X<span class="o">)</span>
</span><span class="line"><span class="m">0</span> 2027
</span><span class="line"><span class="m">1</span> 7973
</span></code></pre></td></tr></table></div></figure>

<p>0 和 1 的比例，接近 0.2 和 0.8 。</p>

<h4 id="metropolis-hasting-1">Metropolis Hasting</h4>

<p>再來舉個 <em>Metropolis Hasting</em> 的例子。</p>

<p><em>Metropolis Hasting</em> 也可以用在連續的機率分佈，此例用 <em>Metropolis Hasting</em> 來對 <em>gamma distribution</em> 進行抽樣，程式碼如下：</p>

<figure class="code"><figcaption><span>metrohast.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">random</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">collections</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">math</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">scipy.stats</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">p_func_raw</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
</span><span class="line">    <span class="n">S1</span> <span class="o">=</span> <span class="p">((</span><span class="n">b</span> <span class="o">**</span> <span class="n">a</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
</span><span class="line">    <span class="n">S2</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span><span class="line">    <span class="n">S3</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">b</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">S1</span> <span class="o">*</span> <span class="n">S2</span> <span class="o">*</span> <span class="n">S3</span>  <span class="c"># * S4</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">p_func</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
</span><span class="line">    <span class="k">return</span> <span class="n">p_func_raw</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">q_func</span><span class="p">(</span><span class="n">beta</span><span class="p">):</span>
</span><span class="line">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">q_func_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
</span><span class="line">    <span class="k">return</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">metrohast</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
</span><span class="line">    <span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">    <span class="n">beta</span> <span class="o">=</span> <span class="mf">5.</span>
</span><span class="line">    <span class="n">xt</span> <span class="o">=</span> <span class="n">beta</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
</span><span class="line">        <span class="n">aj</span> <span class="o">=</span> <span class="n">q_func</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
</span><span class="line">        <span class="n">c</span> <span class="o">=</span> <span class="n">q_func_pdf</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">/</span> <span class="n">q_func_pdf</span><span class="p">(</span><span class="n">aj</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
</span><span class="line">        <span class="n">alpha</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="p">(</span><span class="n">p_func</span><span class="p">(</span><span class="n">aj</span><span class="p">)</span> <span class="o">/</span> <span class="n">p_func</span><span class="p">(</span><span class="n">xt</span><span class="p">))</span> <span class="o">*</span> <span class="n">c</span><span class="p">)</span>
</span><span class="line">        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">alpha</span><span class="p">:</span>
</span><span class="line">            <span class="n">xt</span> <span class="o">=</span> <span class="n">aj</span>
</span><span class="line">        <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">X</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">draw</span><span class="p">(</span><span class="n">S</span><span class="p">):</span>
</span><span class="line">    <span class="n">n</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">patches</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">normed</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s">&#39;b&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bins</span><span class="p">,</span> <span class="p">[</span><span class="n">p_func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">bins</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>本例實作一個 <em>gamma distribution</em> 的抽樣：</p>

<script type="math/tex; mode=display">

Gamma(x,a,b) = \frac{ b^a }{\Gamma(a)}x^{a-1}e^{-bx}

</script>

<p>在本例中，令 a=2, b=1</p>

<script type="math/tex; mode=display">

p(x) = Gamma(x,2,1)

</script>

<p>程式碼中， <code>p_func</code> 為 <script type="math/tex">p(x)</script></p>

<p>而抽樣所用的 <em>proposed distribution</em> 為 <em>exponantial distribution</em> ：</p>

<script type="math/tex; mode=display">

exp(x,\beta) =  \frac{1}{\beta} e^{-\frac{x}{\beta}}

</script>

<p>在本例中，令 <script type="math/tex">\beta=5</script> </p>

<script type="math/tex; mode=display">

q(x) = exp(x,5) 

</script>

<p>此例的 <script type="math/tex">q(x)</script> 跟上一時間點的 <script type="math/tex">x</script> 值無關，即： <script type="math/tex">q(x_{t+1}) = q(x_{t+1}\mid x_{t})</script> 。</p>

<p>程式碼中， <code>q_func</code> 和 <code>q_func_pdf</code> 為 <script type="math/tex">q(x)</script> 。其中， <code>q_func</code> 用於產生 <script type="math/tex">a_j</script> ，而 <code>q_func_pdf</code> 用於計算 <em>correction factor</em> 。</p>

<p>而進行 <em>Metropolis Hasting</em> 抽樣的演算法為程式碼中的 <code>metrohast</code> ，抽樣結果儲存於 <code>X</code> 。<code>draw</code> 則是將結果畫成 <em>Histogram</em> 。</p>

<p>用 <code>metrohast</code> 執行抽樣，並用 <code>draw</code> 畫出結果：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; <span class="nv">X</span> <span class="o">=</span> metrohast<span class="o">(</span>10000<span class="o">)</span>
</span><span class="line">&gt;&gt;&gt; draw<span class="o">(</span>X<span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00178.png" alt="" /></p>

<p>其中，紅色的線為 <em>Gamma Distribution</em> 實際上的值，藍色的線為 <em>Metropolis Hasting</em> 的抽樣結果。</p>

<h2 id="further-reading">Further Reading</h2>

<p>Metropolis Sampler</p>

<p>https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/</p>

<p>Metropolis Hasting</p>

<p>https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/</p>

<p>LDA-math-MCMC 和 Gibbs Sampling(1)</p>

<p>http://www.52nlp.cn/lda-math-mcmc-%e5%92%8c-gibbs-sampling1</p>

<p>LDA-math-MCMC 和 Gibbs Sampling(2)</p>

<p>http://www.52nlp.cn/lda-math-mcmc-%e5%92%8c-gibbs-sampling2</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[機率圖模型 -- Variational Inference]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/06/20/pgm-variational-inference/"/>
    <updated>2016-06-20T02:42:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/06/20/pgm-variational-inference</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>若某個有 <script type="math/tex">m</script> 個維度的 data <script type="math/tex">\textbf{x} = \{x_1,x_2,...,x_m\}</script> 的產生，是跟某個有 <script type="math/tex">n</script>個維度的 hidden variable <script type="math/tex">\textbf{z} = \{z_1,z_2,...,z_n\} </script> 有關，在機率圖模型，表示成：  </p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00171.png" alt="" /></p>

<p>從這機率圖形，可得出 hidden variable <script type="math/tex">\textbf{z}</script> 和 <script type="math/tex">\textbf{x}</script> 的聯合分佈機率：</p>

<script type="math/tex; mode=display">

p(\textbf{x},\textbf{z})  = p(\textbf{z})p(\textbf{x}|\textbf{z})

</script>

<p>若是給定 hidden variable <script type="math/tex">\textbf{z}</script> 的值，則可產生 data <script type="math/tex">\textbf{x}</script> ，如下：</p>

<script type="math/tex; mode=display">

p(\textbf{x}|\textbf{z}) 

</script>

<p>但如果給定 data <script type="math/tex">\textbf{x}</script> 的值，這組 data 所對應到的 hidden variable 的值，如下：</p>

<script type="math/tex; mode=display">

p(\textbf{z}|\textbf{x}) = \frac{p(\textbf{x} , \textbf{z})}{p(\textbf{x})}  =\frac{p(\textbf{x} | \textbf{z}) p(\textbf{z}) }{\int p(\textbf{x} | \textbf{z})p(\textbf{z}) \text{d}\textbf{z}} 


</script>

<p>其中，分母 <script type="math/tex">\int p(\textbf{x} \mid  \textbf{z})p(\textbf{z}) \text{d}\textbf{z}</script> 積分如果無法算出來的時候，就無法直接算出 <script type="math/tex">p(\textbf{z}\mid \textbf{x})</script> 的值，則要用估計的方法來計算 <script type="math/tex">p(\textbf{z}\mid \textbf{x})</script> 的值。</p>

<p>Variational Inference 用來估計 <script type="math/tex">p(\textbf{z}\mid \textbf{x})</script> 的值 。</p>

<!--more-->

<p><em>Variational Inference</em> 的做法是，不直接把 <script type="math/tex">p(\textbf{z}\mid \textbf{x})</script> 求出，而是用一個較好算的 <script type="math/tex">q(\textbf{z}\mid \mathbf{\theta})</script> 來求出近似解。其中， <script type="math/tex">\mathbf{\theta}</script> 為參數，調整此參數可以讓 <script type="math/tex">q(\textbf{z}\mid \mathbf{\theta})</script>。較接近 <script type="math/tex">p(\textbf{z}\mid \textbf{x})</script> 。</p>

<p>求近似解的方法，是讓 <script type="math/tex">q(\textbf{z}\mid \mathbf{\theta})</script> 和 <script type="math/tex">p(\textbf{z}\mid \textbf{x})</script> 這兩個機率分佈的 <em>KL Divergence</em> 越小越好：<a name="eq1">（公式一）</a></p>

<script type="math/tex; mode=display">

\min_{\theta} D_{KL}[  q(\textbf{z}| \mathbf{\theta} )  ||   p(\textbf{z}|\textbf{x})   ]

</script>

<h2 id="evidence-lower-bound-elob">Evidence Lower Bound (ELOB)</h2>

<p>由於<a href="#eq1">（公式一）</a>中的 <script type="math/tex">p(\textbf{z}\mid \textbf{x})</script> 無法直接計算出，因此這個</p>

<p><em>KL Divergence</em> 的值也無法算出來。但可以把它稍微整理一下，如下<a name="eq2">（公式二）</a>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}


&

D_{KL}[q(\textbf{z})||p(\textbf{z}|\textbf{x})] = \int

q(\textbf{z}) \text{log}  \frac{q(\textbf{z})}{p(\textbf{z}|\textbf{x})} \text{d}\textbf{z}

\\&


=  \int

q(\textbf{z}) \text{log} \frac{  q(\textbf{z}) p(\textbf{x}) }{ p(\textbf{x},\textbf{z})  } \text{d}\textbf{z}

\\

&


=  \int

q(\textbf{z}) \text{log} \frac{  q(\textbf{z})  }{ p(\textbf{x},\textbf{z})  } \text{d}\textbf{z}

+ \int q(\textbf{z})\text{log} {p(\textbf{x})} \text{d}\textbf{z}

\\


&

=  \int 

q(\textbf{z})

\bigg(\text{log}   q(\textbf{z})

-\text{log} p(\textbf{z},\textbf{x} )\bigg)  \text{d} \textbf{z}

+ \text{log} p(\textbf{x})

\\

&

= -\bigg( E_{q(\textbf{z}) }[\text{log}p(\textbf{x},\textbf{z})] - E_{ q(\textbf{z} ) }[\text{log}q(\textbf{z})]\bigg) + \text{log}p(\textbf{x})


\end{align}

 %]]&gt;</script>

<p>註：以上省略 <script type="math/tex">\theta</script></p>

<p>定義 Evidence Lower Bound (ELOB) <script type="math/tex">L[q(\textbf{z})]</script> 為：</p>

<script type="math/tex; mode=display">

L[q(\textbf{z})]

   =  E_{q(\textbf{z}) }[\text{log}p(\textbf{x},\textbf{z})] - E_{ q(\textbf{z} ) }[\text{log}q(\textbf{z})]


</script>

<p>將 <script type="math/tex">L[q(\textbf{z})]</script> 代入<a href="#eq2">（公式二）</a>的推導結果，得出：</p>

<script type="math/tex; mode=display">

D_{KL}[q(\textbf{z})||p(\textbf{z}|\textbf{x})]  = -L[q(\textbf{z})]

 + \text{log}p(\textbf{x})

</script>

<p>因此：</p>

<script type="math/tex; mode=display">

\text{log}p(\textbf{x})  = D_{KL}[q(\textbf{z})||p(\textbf{z}|\textbf{x})] +  L[q(\textbf{z})]

</script>

<p>由於 <script type="math/tex">p(\textbf{x})</script> 為一個固定的機率分佈，因此 <script type="math/tex">\text{log}p(\textbf{x})</script> 為常數。</p>

<p>因此，只要將 <script type="math/tex">L[q(\textbf{z})]</script> 的最大化，即可將 <script type="math/tex">D_{KL}[q(\textbf{z})\mid \mid p(\textbf{z}\mid \textbf{x})] </script> 最小化。</p>

<p>而 <script type="math/tex">L[q(\textbf{z})]</script> 中的 <script type="math/tex">p(\textbf{x}, \textbf{z})</script> 是可以算得出來的，因此目標函數為：</p>

<script type="math/tex; mode=display">

\max_{\theta} L[q(\textbf{z} | \theta )]


</script>

<h2 id="mean-field-variational-inference">Mean-Field Variational Inference</h2>

<p><em>Variational Inference</em> 有很多種作法，其中一種常見的作法為 <em>Mean-Field Variational Inference</em> ，這種方法是，假設 <script type="math/tex">q(\textbf{z}\mid  \mathbf{\theta} )</script> 中的每個維度都是獨立的。這樣會比較容易求出它的值，即：<a name="eq3">（公式三）</a></p>

<script type="math/tex; mode=display">

q(\textbf{z}|\mathbf{\theta}) = \prod_{i} q_{i}(z_{i}| \theta_{i} ) 

</script>

<p>每個獨立的 <script type="math/tex">q(z_{i}\mid \theta)</script> 都是機率分佈，即積分結果為1：</p>

<script type="math/tex; mode=display">

\forall i  ,  \int q_{i}(z_{i}| \theta_{i} )  \text{d} z_{i} = 1

</script>

<h2 id="maximizing-elob">Maximizing ELOB</h2>

<p>再來，用 <em>Mean-Field Variational Inference</em> 的方法，把  <script type="math/tex">L[q(\textbf{z})]</script>  整理一下。</p>

<p><script type="math/tex">L[q(\textbf{z})]</script> 可分成兩部分：<script type="math/tex"> E_{q(\textbf{z}) }[\text{log}p(\textbf{x},\textbf{z})]</script> 和 <script type="math/tex">  E_{ q(\textbf{z} ) }[\text{log}q(\textbf{z})] </script></p>

<p>現在，要分別把<a href="#eq3">（公式三）</a>代入這兩項，並稍做整理，讓它們的值可以被求出來。</p>

<p>首先，來看 <script type="math/tex"> E_{q(\textbf{z}) }[\text{log}p(\textbf{x},\textbf{z})]</script> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& E_{q(\textbf{z}) }[\text{log}p(\textbf{x},\textbf{z})] = \int \prod_{i} q_{i}(z_{i}) \text{log} p (\textbf{z},\textbf{x}) \text{d}\textbf{z}

\\

&

= \int_{z_{1}}\int_{z_{2}} ... \int_{z_{n}} \int \prod_{i} q_{i}(z_{i}) \text{log} p (\textbf{z},\textbf{x}) \text{d}z_{1} \text{d}z_{2}...\text{d}z_{n}

\end{align}

 %]]&gt;</script>

<p>挑出 <script type="math/tex">\textbf{z}</script> 中的某個元素： <script type="math/tex">z_{j}</script> ，整理一下，繼續以上推導過程：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& = \int_{z_{j}} q_{j}(z_{j}) \bigg( \int... \int_{z_{i\neq j }} \prod_{i} q(z_{i}) \text{log} p (\textbf{z},\textbf{x}) \prod_{i \neq j} \text{d}z_{i} \bigg) \text{d}z_{j}

\\

& =  \int_{z_{j}} q_{j}(z_{j}) \bigg( \int... \int_{z_{i\neq j }} \text{log} p (\textbf{z},\textbf{x}) \prod_{i \neq j } q(z_{i})  \text{d}z_{i} \bigg) \text{d}z_{j}

\end{align}

 %]]&gt;</script>

<p>由於 <script type="math/tex"> \prod_{i \neq j } q(z_{i})</script> 是 joint distribution，可以把 <script type="math/tex">p (\textbf{z},\textbf{x})</script> 放到此機率分布的期望值裡，繼續以上推導過程：<a name="eq4">（公式四）</a></p>

<script type="math/tex; mode=display">

=  \int q_{j}(z_{j}) E_{q_{i \neq j}(z)}\bigg[  \text{log} p (\textbf{z},\textbf{x}) \  \bigg] \text{d}z_{j}

</script>

<p>由於 <script type="math/tex">E_{q_{i \neq j}(z)}\bigg[  \text{log} p (\textbf{z},\textbf{x}) \  \bigg]</script> 中，把 <script type="math/tex">p (\textbf{z},\textbf{x})</script> 中不是 <script type="math/tex">z_{j}</script> 的 <script type="math/tex">z_{i}</script> 全都消掉了，所以算完後的 <script type="math/tex">p (\textbf{z},\textbf{x})</script> 只會剩 <script type="math/tex">z_{j}</script> ，令：</p>

<script type="math/tex; mode=display">

\text{log} \tilde{p}(\textbf{x},z_{j}) =  E_{q_{i \neq j}(z)}\bigg[  \text{log} p (\textbf{z},\textbf{x}) \  \bigg] 

</script>

<p>將其代入<a href="#eq4">（公式四）</a>，繼續推導過程，得出<a name="eq5">公式五</a>：</p>

<script type="math/tex; mode=display">

 E_{q(\textbf{z}) }[\text{log}p(\textbf{x},\textbf{z})]  =  \int q_{j}(z_{j})   \text{log} \tilde{p}(\textbf{x},z_{j}) \text{d}z_{j}

</script>

<p>再來看 <script type="math/tex"> E_{ q(\textbf{z} ) }[\text{log}q(\textbf{z})]</script>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& E_{ q(\textbf{z} ) }[\text{log}q(\textbf{z})] = \int \prod_{i} q_{i}(z_{i}) \text{log} \prod_{i} q (z_{i}) \text{d}\textbf{z}

\\

&

= \int \prod_{i} q_{i}(z_{i}) \sum_{i} \text{log} q (z_{i}) \text{d}\textbf{z}

\\

&

= \sum_{i} \bigg(\prod_{k \neq i} \int_{k}q_{k}(z_{k})\text{d}z_{k}  \int  q_{i}(z_{i}) \text{log} q (z_{i}) \text{d}z_{i}  \bigg)

\\

&

= \sum_{i} \bigg(  \int  q_{i}(z_{i}) \text{log} q (z_{i}) \text{d}z_{i}  \bigg)

\end{align}

 %]]&gt;</script>

<p>挑出 <script type="math/tex">\textbf{z}</script> 中的某個元素： <script type="math/tex">z_{j}</script> ，整理一下，繼續以上推導過程：</p>

<script type="math/tex; mode=display">

=  \int  q_{j}(z_{j}) \text{log} q (z_{j}) \text{d}z_{j} + \sum_{i \neq j } \bigg(  \int  q_{i}(z_{i}) \text{log} q (z_{i}) \text{d}z_{i}  \bigg)

</script>

<p>不涉及 <script type="math/tex">j</script>的部分，就當 <script type="math/tex">const</script> 來處理，整理以上公式，得出：<a name="eq6">（公式六）</a></p>

<script type="math/tex; mode=display">

E_{ q(\textbf{z} ) }[\text{log}q(\textbf{z})] =  \int  q_{j}(z_{j}) \text{log} q (z_{j}) \text{d}z_{j} + const

</script>

<p>將 <a href="#eq5">（公式五）</a>和<a href="#eq6">（公式六）</a> 代入 <script type="math/tex">L[q(\textbf{z})]</script> ，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& L[q(\textbf{z})] =\int q_{j}(z_{j})   \text{log} \tilde{p}(\textbf{x},z_{j}) \text{d}z_{j}- \int  q_{j}(z_{j}) \text{log} q (z_{j}) \text{d}z_{j} - const

\\

&

= \int q_{j}(z_{j})   \text{log} \frac{ \tilde{p}(\textbf{x},z_{j}) } { q_{j}(z_{j}) } \text{d} z_{j} - const

\\

& 

= -D_{KL}[\tilde{p}(\textbf{x},z_{j})|| q_{j}(z_{j} ) ] - const

\end{align}

 %]]&gt;</script>

<p>根據以上結果，要將 <script type="math/tex"> L[q(\textbf{z})] =\int q_{j}(z_{j}) </script> 最大化，則：</p>

<script type="math/tex; mode=display">

D_{KL}[\tilde{p}(\textbf{x},z_{j})|| q_{j}(z_{j} ) ]  = 0

</script>

<p>因此：</p>

<script type="math/tex; mode=display">

q_{j}(z_{j}) = \tilde{p}(\textbf{x},z_{j}) 

</script>

<p>由於 <script type="math/tex">z_{j}</script> 有 <script type="math/tex">n</script> 個， 整個運算過程，有點類似 <em>EM</em> 演算法，就是挑選某個 <script type="math/tex">q_{j}(z_{j})</script> 根據以上公式，來更新其值，而其他的 <script type="math/tex">q_{i}(z_{i})</script> 則固定。這樣依序更新每個 <script type="math/tex">q_{j}(z_{j})</script> ，一直循環下去，則最後收斂的結果，即為 <script type="math/tex">L[q(\textbf{z})]</script> 的 <em>Local Maximum</em> 。</p>

<h2 id="reference">Reference</h2>

<p>A Tutorial on Variational Bayesian Inference</p>

<p>http://www.orchid.ac.uk/eprints/40/1/fox_vbtut.pdf</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Optimization Method -- AdaDelta]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/02/08/optimization-method-adadelta/"/>
    <updated>2016-02-08T16:13:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/02/08/optimization-method-adadelta</id>
    <content type="html"><![CDATA[<h2 id="adagrad">AdaGrad</h2>

<p>本文接續 <a href="http://ckmarkoh.github.io/blog/2015/12/23/optimization-method-adagrad">Optimization Method – Gradient Descent &amp; AdaGrad </a>。所提到的 <em>AdaGrad</em> ，及改良它的方法 – <em>AdaDelta</em> 。</p>

<p>在機器學習最佳化過程中，用 <em>AdaGrad</em> 可以隨著時間來縮小 <em>Learning Rage</em> ，以達到較好的收斂效果。<em>AdaGrad</em> 的公式如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{G}_{t} = \sum_{n=0}^{t} \textbf{g}_{n}^{2} \\

& \textbf{x}_{t+1} = \textbf{x}_{t} - \frac{\eta}{\sqrt{\textbf{G}_{t}}} \textbf{g}_{t} \\

\end{align}

 %]]&gt;</script>

<p>不過， <em>AdaGrad</em> 有個缺點，由於 <script type="math/tex">\textbf{g}_{n}^{2}</script> 恆為正，故 <script type="math/tex">\textbf{G}_{t} </script> 只會隨著時間增加而遞增，所以 <script type="math/tex">\frac{\eta}{\sqrt{\textbf{G}_{t}}} </script> 只會隨著時間增加而一直遞減，如果 <em>Learning Rate</em> <script type="math/tex">\eta</script>的值太小，則 <em>AdaGrad</em> 會較慢才收斂。</p>

<p>舉個例子，如果目標函數為 <script type="math/tex"> f(x,y) = y^2 - x^2  </script> ，起始點為 <script type="math/tex">(x,y) = (0.001,4)</script> ， <em>Learning Rate</em> <script type="math/tex">\eta=0.5</script> ，則整個最佳化的過程如下圖，曲面為目標函數，紅色的點為 <script type="math/tex">(x,y)</script> ：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00157.png" alt="" /></p>

<!--more-->

<p>動畫版：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00158.gif" alt="" /></p>

<p>從上圖來看，一開始紅色點的下降速度很快，但越後面則越慢。</p>

<p>為了解決此問題，在調整 <em>Learning Rate</em> 時，不要往前一直加到最初的時間點，而只要往前加到某段時間即可。</p>

<p>但如果要從某段時間點的 <script type="math/tex">\textbf{g}_{t}</script> 開始累加，則需要儲存某個時間點之後開始的每個 <script type="math/tex">\textbf{g}_{t}</script> ，這樣會造成記憶體的浪費。有種較簡便的做法，即是用衰減係數 <script type="math/tex">\rho</script> ，將上一時間點的  <script type="math/tex">\textbf{G}_{t-1}</script> 乘上 <script type="math/tex"> \rho</script> ，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{G}_{t} = \rho \textbf{G}_{t-1} + (1 - \rho) \textbf{g}_{t}^{2} \\

& 0 < \rho < 1 \\

\end{align}

 %]]&gt;</script>

<p>藉由衰減係數 <script type="math/tex">\rho</script> ，可讓較早期時間點累加的 <script type="math/tex">\textbf{g}_{t}^{2}</script> 衰減至 0 ，因此，不會使得 <em>Learning Rate</em> 只隨著時間而一直遞減。</p>

<h2 id="correct-units-of-x">Correct Units of ΔX</h2>

<p><em>Adagrad</em> 還有另一個問題，就是 <script type="math/tex">\textbf{x}</script> 的修正量– <script type="math/tex">\Delta{\textbf{x}}</script> 為 <script type="math/tex">\frac{\eta}{\sqrt{\textbf{G}_{t}}} \textbf{g}_{t}</script> ，假設它如果有「單位」的話，它的單位會與 <script type="math/tex">\textbf{x}</script> 不同。 因 <script type="math/tex">\Delta{\textbf{x}}</script>  的單位與 <script type="math/tex">\textbf{g}</script> 的單位相同，而會和 <script type="math/tex">\textbf{x}</script> 不同，因為：</p>

<script type="math/tex; mode=display">

   \text{ units of }\Delta{\textbf{x}}  \propto  \text{ units of } \textbf{g} \propto  \dfrac{\partial f}{\partial x } \propto \frac{1}{  \text{ units of } \textbf{x} }

</script>

<p>註：在此假設 <script type="math/tex">f</script> 無單位。</p>

<p>相較之下， <a href="http://ckmarkoh.github.io/blog/2016/01/25/optimization-method-newton"><em>Newton’s Method</em></a> 中， <script type="math/tex">\Delta{\textbf{x}} =  \eta   \textbf{H}^{-1} \textbf{g}</script>， <script type="math/tex">\Delta{\textbf{x}}</script> 的單位與 <script type="math/tex">\textbf{x}</script> 的單位相同，因為：</p>

<script type="math/tex; mode=display">

   \text{ units of }\Delta{\textbf{x}}  \propto  \text{ units of } \textbf{H}^{-1} \textbf{g} \propto 

   \frac{

   \dfrac{\partial f}{\partial x }

   }

   {   \dfrac{\partial^{2} f}{\partial x^{2} }

   }

   \propto   \text{ units of } \textbf{x} 


</script>

<p>但 <em>Newton’s Method</em> 的缺點是，二次微分 <em>Hessian</em> 矩陣的反矩陣 <script type="math/tex">\textbf{H}^{-1}</script> ，計算時間複雜度太高。如果只是為了要單位相同，是沒必要這樣算。</p>

<p>想要簡易求出  <script type="math/tex">\textbf{H}^{-1}</script> 的單位，稍微整理一下以上公式，得出：</p>

<script type="math/tex; mode=display">

   \Delta{\textbf{x}}  \propto  \frac{\dfrac{\partial f}{\partial x } } {\dfrac{\partial^{2} f}{\partial x^{2}}} \Rightarrow  \textbf{H}^{-1} \propto \frac{1 } {\dfrac{\partial^{2} f}{\partial x^{2}}} 

     \propto

   

   \dfrac{\Delta{x}}{\dfrac{\partial f}{\partial x }} \propto  \dfrac{\Delta{x}}{\textbf{g}}  


</script>

<p>因此，若要簡易求出  <script type="math/tex">\textbf{H}^{-1} </script> 的單位，只要算 <script type="math/tex">\dfrac{\Delta{x}}{\textbf{g}}  </script> 即可。</p>

<p>註：如果看不懂這段在寫什麼，請參考<a href="http://arxiv.org/abs/1212.5701">Matthew D. Zeiler. ADADELTA: AN ADAPTIVE LEARNING RATE METHOD.</a></p>

<h2 id="adadelta">AdaDelta</h2>

<p><em>AdaDelta</em> 解決了 <em>AdaGrad</em> 會發生的兩個問題：</p>

<p>(1) <em>Learning Rate</em> 只會隨著時間而一直遞減下去</p>

<p>(2) <script type="math/tex">\Delta{\textbf{x}}</script> 與 <script type="math/tex">\textbf{x}</script> 的單位不同</p>

<p><em>AdaDelta</em> 的公式如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{G}_{t} = \rho \textbf{G}_{t-1} + (1 - \rho) \textbf{g}_{t}^{2} \\


& \Delta \textbf{x}_{t} = - \frac{\sqrt{\textbf{D}_{t - 1}+\epsilon}}{\sqrt{\textbf{G}_{t}+\epsilon}} \textbf{g}_{t} \\


& \textbf{D}_{t} = \rho \textbf{D}_{t-1} + (1 - \rho) \Delta \textbf{x}_{t}^{2} \\


& \textbf{x}_{t+1} = \textbf{x}_{t} + \Delta{x}_{t} \\


\end{align}

 %]]&gt;</script>

<p>其中， <script type="math/tex">\rho</script> 和  <script type="math/tex">\epsilon</script> 為常數。 <script type="math/tex">\rho</script> 的作用為「衰減係數」，而 <script type="math/tex">\epsilon</script> 是為了避免 <script type="math/tex">\frac{\sqrt{\textbf{D}_{t - 1}+\epsilon}}{\sqrt{\textbf{G}_{t}+\epsilon}}</script> 的分母為 0 。</p>

<p>此處的 <script type="math/tex"> \textbf{G}_{t} </script> 有點類似 <em>AdaGrad</em> 裡面的  <script type="math/tex"> \textbf{G}_{t} </script> ，但如前面所述，  <em>AdaDelta</em> 的不是直接把 <script type="math/tex">\textbf{g}_{t}^2</script> 直接累加上去，而是藉由衰減係數 <script type="math/tex">\rho</script> ，可讓較早期時間點累加的 <script type="math/tex">\textbf{g}_{t}^{2}</script> 衰減至 0 ，因此，不會使得 <em>Learning Rate</em> 只隨著時間一直遞減下去。</p>

<p>而 <script type="math/tex">\textbf{D}_{t}</script> 的作用，則是使 <script type="math/tex">\Delta{\textbf{x}}</script> 與 <script type="math/tex">\textbf{x}</script> 有相同的單位，因為 <script type="math/tex"> \frac{\sqrt{\textbf{D}_{t - 1}+\epsilon}}{\sqrt{\textbf{G}_{t}+\epsilon}}</script> 與 <script type="math/tex">\textbf{H}^{-1}</script> 具有相同單位，如下：</p>

<script type="math/tex; mode=display">

 \frac{\sqrt{\textbf{D}_{t - 1}+\epsilon}}{\sqrt{\textbf{G}_{t}+\epsilon}} \propto  \dfrac{\Delta{x}}{\textbf{g}}  \propto  \textbf{H}^{-1}

</script>

<p>根據前一段的結果，若 <script type="math/tex">\Delta{\textbf{x}}  \propto   \textbf{H}^{-1} \textbf{g}</script>，則 <script type="math/tex">\Delta{\textbf{x}}</script> 與 <script type="math/tex">\textbf{x}</script> 的單位相同。</p>

<p>另外，<script type="math/tex">\textbf{D}_{t}</script> 可累加過去時間點的 <script type="math/tex">\Delta{\textbf{x}}</script> ，這樣所造成的效果，有點類似  <a href="http://ckmarkoh.github.io/blog/2016/01/16/optimization-method-momentum"><em>Gradient Descent with Momentum</em></a> ，使得現在時間點的 <script type="math/tex">\Delta{\textbf{x}}</script> ，具有過去時間點的動量。</p>

<p>實際帶數字進去算一次 <em>AdaDelta</em> 。舉前述例子，假設 <script type="math/tex"> f(x,y) = y^2 - x^2  </script> ，起始參數為 <script type="math/tex">(x,y) = (0.001,4)</script> ，則畫出來的圖形如下圖，藍色點為起始點位置：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00161.png" alt="" /></p>

<p>用 <em>AdaDelta</em> 最佳化方法，初始值設 <script type="math/tex">\textbf{G}_{0} = [0,0 ]^{T}  </script> ， <script type="math/tex"> \textbf{D}_{0} = [0,0 ]^{T} </script> ，設參數 <script type="math/tex">\rho = 0.5</script> ， <script type="math/tex">\epsilon = 0.1 </script> ，更新 <script type="math/tex">x,y </script> 的值，如下，（註：以下的向量 <script type="math/tex">\textbf{G}</script> 、 <script type="math/tex">\textbf{D}</script> 、 <script type="math/tex">\Delta \textbf{x}</script> 等等的加減乘除運算，皆為 <em>Element-wise Operation</em> ）：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{g}_{1} = 

\begin{bmatrix}

-2x_{0} \\[0.3em]

2y_{0} \\[0.3em]

\end{bmatrix}

=

\begin{bmatrix}

-2 \times 0.001 \\[0.3em]

2 \times 4 \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

-0.002 \\[0.3em]

8 \\[0.3em]

\end{bmatrix} \\



& \textbf{G}_{1} = 0.5 

\begin{bmatrix}

0  \\[0.3em]

0  \\[0.3em]

\end{bmatrix}

+ (1-0.5 ) 

\begin{bmatrix}

(-0.002)^{2}  \\[0.3em]

8^{2}  \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

2 \times 10^{-6}  \\[0.3em]

32  \\[0.3em]

\end{bmatrix} \\


& \Delta \textbf{x}_{1} = - 

\frac{\sqrt{

\begin{bmatrix}

0   \\[0.3em]

0 \\[0.3em]

\end{bmatrix} 

+ 0.1}}

{\sqrt{

\begin{bmatrix}

2 \times 10^{-6}  \\[0.3em]

32 \\[0.3em]

\end{bmatrix} 

 + 0.1}}

\begin{bmatrix}

-0.002  \\[0.3em]

8  \\[0.3em]

\end{bmatrix} 

= 

\begin{bmatrix}

0.00199998 \\[0.3em]

-0.44651646 \\[0.3em]

\end{bmatrix} \\


& \textbf{D}_{1} = 0.5 

\begin{bmatrix}

0  \\[0.3em]

0  \\[0.3em]

\end{bmatrix}

+ (1-0.5 ) 

\begin{bmatrix}

0.00199998^{2}  \\[0.3em]

(-0.44651646)^{2}  \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

1.99996 \times 10^{-6}  \\[0.3em]

0.09968847  \\[0 .3em]

\end{bmatrix} \\


&

\begin{bmatrix}

x_{1} \\[0 .3em]

y_{1} \\[0 .3em]

\end{bmatrix}

=

\textbf{x}_{1} = 

\begin{bmatrix}

0.001 \\[0.3em]

4 \\[0 .3em]

\end{bmatrix}

+ 

\begin{bmatrix}

0.00199998 \\[0.3em]

-0.44651646 \\[0.3em]

\end{bmatrix}

= 

\begin{bmatrix} 

0.00299998 \\[0.3em]

3.55348354 \\[0.3em]

\end{bmatrix}

\end{align}


 %]]&gt;</script>

<p>更新 <script type="math/tex">x,y</script> 的值， <script type="math/tex">x,y = 0.00299998, 3.55348354 \approx 0.00300,3.55348  </script> ，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00160.png" alt="" /></p>

<p>再往下走一步， 計算 <script type="math/tex">x,y</script> 的值，如下：  </p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{g}_{2} = 

\begin{bmatrix}

-2x_{1} \\[0.3em]

2y_{1} \\[0.3em]

\end{bmatrix}

=

\begin{bmatrix}

-2 \times 0.00299998 \\[0.3em]

2 \times 3.55348354 \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

-0.00599996 \\[0.3em]

7.10696708 \\[0.3em]

\end{bmatrix} \\



& \textbf{G}_{2} = 0.5 

\begin{bmatrix}

2 \times 10^{-6}  \\[0.3em]

32  \\[0.3em]

\end{bmatrix}

+ (1-0.5 ) 

\begin{bmatrix}

(-0.00599996)^{2}  \\[0.3em]

7.10696708^{2}  \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

1.89997600 \times 10^{-5}  \\[0.3em]

41.25449057  \\[0.3em]

\end{bmatrix} \\



& \Delta \textbf{x}_{2} = - 

\frac{\sqrt{

\begin{bmatrix}

1.99996 \times 10^{−6}   \\[0.3em]

0.09968847 \\[0.3em]

\end{bmatrix} 

+ 0.1}}

{\sqrt{

\begin{bmatrix}

1.89997600 \times 10^{-6}  \\[0.3em]

41.25449057 \\[0.3em]

\end{bmatrix} 

 + 0.1}}

\begin{bmatrix}

-0.00599996 \\[0.3em]

7.10696708 \\[0.3em]

\end{bmatrix} 

= 

\begin{bmatrix}

0.00599945 \\[0.3em]

-0.49385501\\[0.3em]

\end{bmatrix} \\


& \textbf{D}_{2} = 0.5 

\begin{bmatrix}

1.99996 \times 10^{−6}  \\[0.3em]

0.09968847  \\[0.3em]

\end{bmatrix}

+ (1-0.5 ) 

\begin{bmatrix}

0.00599945^{2}  \\[0.3em]

(-0.49385501)^{2}  \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

1.89966806 \times 10^{-5}  \\[0.3em]

0.17179062  \\[0 .3em]

\end{bmatrix} \\


&

\begin{bmatrix}

x_{2} \\[0 .3em]

y_{2} \\[0 .3em]

\end{bmatrix}

=

\textbf{x}_{2} = 

\begin{bmatrix}

0.00299998 \\[0.3em]

3.55348354 \\[0 .3em]

\end{bmatrix}

+ 

\begin{bmatrix}

0.00599945 \\[0.3em]

−0.49385501 \\[0.3em]

\end{bmatrix}

= 

\begin{bmatrix} 

0.00899943 \\[0.3em]

3.05962853 \\[0.3em]

\end{bmatrix}

\end{align}


 %]]&gt;</script>

<p>更新 <script type="math/tex">x,y</script> 的值， <script type="math/tex">x,y = 0.00899943, 3.05962853 \approx 0.00900,3.05963  </script> ，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00161.png" alt="" /></p>

<p>重複以上循環，整個過程如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00162.png" alt="" /></p>

<p>動畫版：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00163.gif" alt="" /></p>

<p>將 <em>Gradient Descent</em> （綠） ， <em>AdaGrad</em> （紅） 和 <em>AdaDelta</em> （藍） 畫在同一張圖上比較看看： </p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00164.gif" alt="" /></p>

<p>從上圖可看出， <em>AdaDelta</em> 的 <em>Learning Rate</em> 會隨著坡度而適度調整，不會一直遞減下去，也不會像 <em>Gradient Descent</em> 一樣，容易卡在 <em>saddle point</em> （請見<a href="http://ckmarkoh.github.io/blog/2015/12/23/optimization-method-adagrad"> Optimization Method – Gradient Descent &amp; AdaGrad </a>）。</p>

<h2 id="implementation">Implementation</h2>

<p>再來進入實作的部分</p>

<p>首先，開啟新的檔案 adadelta.py 並貼上以下程式碼：</p>

<figure class="code"><figcaption><span>adadelta.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
<span class="line-number">61</span>
<span class="line-number">62</span>
<span class="line-number">63</span>
<span class="line-number">64</span>
<span class="line-number">65</span>
<span class="line-number">66</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">sqrt</span>
</span><span class="line">
</span><span class="line"><span class="n">XT</span> <span class="o">=</span> <span class="mf">0.001</span>
</span><span class="line"><span class="n">YT</span> <span class="o">=</span> <span class="mi">4</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
</span><span class="line">  <span class="k">return</span> <span class="p">(</span><span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">func_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
</span><span class="line">  <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">y</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s">&#39;r&#39;</span><span class="p">):</span>
</span><span class="line">  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
</span><span class="line">  <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">&#39;3d&#39;</span><span class="p">,</span>
</span><span class="line">        <span class="n">elev</span><span class="o">=</span><span class="mf">35.</span><span class="p">,</span> <span class="n">azim</span><span class="o">=-</span><span class="mi">30</span><span class="p">)</span>
</span><span class="line">  <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">))</span>
</span><span class="line">  <span class="n">Z</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
</span><span class="line">  <span class="n">surf</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">rstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span class="line">    <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</span><span class="line">  <span class="n">ax</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="o">-</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
</span><span class="line">  <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">,</span> <span class="n">func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">),</span><span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">&#39;o&#39;</span> <span class="p">)</span>
</span><span class="line">  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">&quot;x=</span><span class="si">%.5f</span><span class="s">, y=</span><span class="si">%.5f</span><span class="s">, f(x,y)=</span><span class="si">%.5f</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="n">func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">)))</span>
</span><span class="line">  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span><span class="line">  <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">run_adagrad</span><span class="p">():</span>
</span><span class="line">  <span class="n">xt</span><span class="p">,</span> <span class="n">yt</span> <span class="o">=</span> <span class="n">XT</span><span class="p">,</span> <span class="n">YT</span>
</span><span class="line">  <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.5</span>
</span><span class="line">  <span class="n">Gxt</span><span class="p">,</span> <span class="n">Gyt</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
</span><span class="line">  <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
</span><span class="line">    <span class="n">gxt</span><span class="p">,</span> <span class="n">gyt</span> <span class="o">=</span> <span class="n">func_grad</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">)</span>
</span><span class="line">    <span class="n">Gxt</span> <span class="o">+=</span> <span class="n">gxt</span><span class="o">**</span><span class="mi">2</span>
</span><span class="line">    <span class="n">Gyt</span> <span class="o">+=</span> <span class="n">gyt</span><span class="o">**</span><span class="mi">2</span>
</span><span class="line">    <span class="n">xt</span> <span class="o">-=</span> <span class="n">eta</span><span class="o">*</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="p">(</span><span class="n">Gxt</span><span class="o">**</span><span class="mf">0.5</span><span class="p">))</span><span class="o">*</span><span class="n">gxt</span>
</span><span class="line">    <span class="n">yt</span> <span class="o">-=</span> <span class="n">eta</span><span class="o">*</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="p">(</span><span class="n">Gyt</span><span class="o">**</span><span class="mf">0.5</span><span class="p">))</span><span class="o">*</span><span class="n">gyt</span>
</span><span class="line">    <span class="k">if</span> <span class="n">xt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">xt</span> <span class="o">&gt;</span> <span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">:</span>
</span><span class="line">      <span class="k">break</span>
</span><span class="line">    <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">run_adadelta</span><span class="p">():</span>
</span><span class="line">  <span class="n">xt</span><span class="p">,</span> <span class="n">yt</span> <span class="o">=</span> <span class="n">XT</span><span class="p">,</span> <span class="n">YT</span>
</span><span class="line">  <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>
</span><span class="line">  <span class="n">rho</span> <span class="o">=</span> <span class="mf">0.5</span>
</span><span class="line">  <span class="n">Gxt</span><span class="p">,</span> <span class="n">Gyt</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span>
</span><span class="line">  <span class="n">Dxt</span><span class="p">,</span> <span class="n">Dyt</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span>
</span><span class="line">  <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;b&#39;</span><span class="p">)</span>
</span><span class="line">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
</span><span class="line">    <span class="n">gxt</span><span class="p">,</span> <span class="n">gyt</span> <span class="o">=</span> <span class="n">func_grad</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">)</span>
</span><span class="line">    <span class="n">Gxt</span><span class="p">,</span> <span class="n">Gyt</span> <span class="o">=</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">Gxt</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">rho</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">gxt</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="p">,</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">Gyt</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">rho</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">gyt</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</span><span class="line">    <span class="n">dxt</span><span class="p">,</span> <span class="n">dyt</span>  <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">sqrt</span><span class="p">(</span><span class="n">Dxt</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">Gxt</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">))</span> <span class="o">*</span> <span class="n">gxt</span> <span class="p">,</span> \
</span><span class="line">                <span class="o">-</span><span class="p">(</span><span class="n">sqrt</span><span class="p">(</span><span class="n">Dyt</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">Gyt</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">))</span> <span class="o">*</span> <span class="n">gyt</span>
</span><span class="line">    <span class="n">Dxt</span><span class="p">,</span> <span class="n">Dyt</span> <span class="o">=</span>  <span class="n">rho</span> <span class="o">*</span> <span class="n">Dxt</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">rho</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dxt</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="p">,</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">Dyt</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">rho</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dyt</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</span><span class="line">    <span class="n">xt</span> <span class="o">+=</span> <span class="n">dxt</span>
</span><span class="line">    <span class="n">yt</span> <span class="o">+=</span> <span class="n">dyt</span>
</span><span class="line">    <span class="k">if</span> <span class="n">xt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">xt</span> <span class="o">&gt;</span> <span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">:</span>
</span><span class="line">      <span class="k">break</span>
</span><span class="line">    <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;b&#39;</span><span class="p">)</span>
</span><span class="line">
</span></code></pre></td></tr></table></div></figure>

<p>其中， <code>func(x,y)</code> 為目標函數， <code>func_grad(x,y)</code> 為目標函數的 gradient ，而 <code>plot_func(xt,yt,c='r')</code> 可畫出目標函數的曲面， <code>run_adagrad()</code> 用來執行 <em>AdaGrad</em> ， <code>run_adadelta()</code> 用來執行 <em>AdaDelta</em> 。</p>

<p>到 python console 執行：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; import adadelta
</span></code></pre></td></tr></table></div></figure>

<p>執行 <em>AdaGrad</em> ，指令如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; adadelta.run_adagrad<span class="o">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00165.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00166.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00167.png" alt="" /></p>

<p>以此類推</p>

<p>執行 <em>AdaDelta</em> ，指令如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; adadelta.run_adadelta<span class="o">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00168.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00169.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00170.png" alt="" /></p>

<p>以此類推</p>

<h2 id="reference">Reference</h2>

<p><a href="http://arxiv.org/abs/1212.5701">Matthew D. Zeiler. ADADELTA: AN ADAPTIVE LEARNING RATE METHOD.</a></p>

<p><a href="http://imgur.com/a/Hqolp">Visualizing Optimization Algos</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Optimization Method -- Newton's Method for Optimization]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/01/25/optimization-method-newton/"/>
    <updated>2016-01-25T16:56:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/01/25/optimization-method-newton</id>
    <content type="html"><![CDATA[<h2 id="gradient-descent">Gradient Descent</h2>

<p>機器學習中，用 <em>Gradient Descent</em> 是解最佳化問題，最基本的方法。關於Gradient Descent的公式，請參考：<a href="http://ckmarkoh.github.io/blog/2015/12/23/optimization-method-adagrad">Optimization Method – Gradient Descent &amp; AdaGrad</a></p>

<p>對於 <em>Cost function</em> <script type="math/tex">f(\textbf{x})</script> ，在 <script type="math/tex">\textbf{x} = \textbf{x}_{t}</script> 時， <em>Gradient Descent</em>  走的方向為  <script type="math/tex">  -\nabla f(\textbf{x})</script> 。也就是，用泰勒展開式展開後，用一次微分 <script type="math/tex">f(\textbf{x})</script> 來趨近的方向，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00144.png" alt="" /></p>

<p>註：考慮到 <script type="math/tex">\textbf{x}</script> 為向量的情形，故一次微分寫成  <script type="math/tex">\nabla f(\textbf{x})</script> 。 </p>

<p>其中， <script type="math/tex">f(\textbf{x})</script> 為原本的 <em>Cost function</em> ，而 <script type="math/tex">\tilde{f}(\textbf{x})</script> 為泰勒展開式取一次微分逼近的。 而 <em>Gradient Descent</em> 走的方向為 <script type="math/tex"> - \nabla f(\textbf{x}) </script> ，為沿著 <script type="math/tex">\tilde{f}(\textbf{x})</script> 的方向。</p>

<!--more-->

<p>這會有個問題，如果原本的 <em>Cost function</em> 為較高次函數，只用一次項來逼近是不夠的，有時候，失真情形很嚴重，例如， <em>Cost function</em> 為橢圓 <script type="math/tex">f(x,y) = x^{2}+9y^{2} </script>， 此函數的等高線圖，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00145.png" alt="" /></p>

<p>如果起始點為 <script type="math/tex">(x,y) = (-4,2.5)</script> ，沿著 <script type="math/tex"> - \nabla f(x) </script> 的方向走，也就是說，走梯度最陡的方向（即與等高線垂直的方向），可能會需要多次折返，才能走到最小值，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00146.gif" alt="" /></p>

<h2 id="second-order-taylor-approximation">Second-Order Taylor Approximation</h2>

<p>根據前面的例子得知，只考慮一次微分項，是不夠的，現在要來考慮二次微分項。</p>

<p>對於 <em>Cost function</em> <script type="math/tex">f(\textbf{x})</script> ， 在 <script type="math/tex">\textbf{x} = \textbf{x}_{t}</script> 時，用泰勒展開式展開後，分別用一次微分與二次微分來逼近，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00147.png" alt="" /></p>

<p>其中，橘色的 <script type="math/tex">\tilde{f}(\textbf{x})</script> 為只用了一次微分的逼近，而紫色的 <script type="math/tex">\hat{f}(\textbf{x})</script> 為用了一次與二次微分向的逼近，由此可見， <script type="math/tex">\hat{f}(\textbf{x})</script>  較 <script type="math/tex">\tilde{f}(\textbf{x})</script> 接近原本的 <script type="math/tex">f(\textbf{x})</script></p>

<p>如果要求 <script type="math/tex">f(\textbf{x})</script> 的最小值，可以往 <script type="math/tex">\hat{f}(\textbf{x})</script> 為最小值的方向，一步一步走下去。要找出 <script type="math/tex">\hat{f}(\textbf{x})</script> 的最小值，即：</p>

<script type="math/tex; mode=display">

\min_{x} \hat{f} (\textbf{x})  =  \min_{\textbf{x}}  f(\textbf{x}_{t}) + \nabla f(\textbf{x}_{t})^{T}\textbf{x} + \frac{1}{2} \textbf{x}^{T} \nabla^{2}f(\textbf{x}_{t}) \textbf{x} 

</script>

<p>將 <script type="math/tex">\hat{f}(\textbf{x})</script> 對 <script type="math/tex">\textbf{x}</script> 微分，令微分結果為 <script type="math/tex">0</script> ，得：</p>

<script type="math/tex; mode=display">

0 =  \nabla f(\textbf{x}_{t}) +  \nabla^{2}f(\textbf{x}_{t}) \textbf{x} 

</script>

<p>得 </p>

<script type="math/tex; mode=display">

\textbf{x} = -  \nabla^{2}f(\textbf{x}_{t})^{-1} \nabla f(\textbf{x}_{t}) 

</script>

<p>可以用此 <script type="math/tex">\textbf{x}</script> ，來當作位於 <script type="math/tex">\textbf{x}=\textbf{x}_{t}</script> 時，想走往 <script type="math/tex">f(\textbf{x})</script> 的最小值，要走的方向（與距離）。用一次與二次微分所得出的方向，一步步走下去，最後走到最小值，這種方法即為 <em>Newton’s Method</em> 。</p>

<h2 id="newtons-method-for-optimization">Newton’s Method for Optimization</h2>

<p><em>Newton’s Method</em> 即是考慮二次微分的 <em>Gradient Descent</em> 方法，公式如下：</p>

<script type="math/tex; mode=display">

\textbf{x}_{t+1} \leftarrow \textbf{x}_{t} - \eta   \textbf{H}_{t}^{-1} \textbf{g}_{t}

</script>

<p>其中， <script type="math/tex"> \eta</script> 為 <em>Learning Rate</em> ， <script type="math/tex"> \textbf{H}_{t} = \nabla^{2}f(\textbf{x}_{t}) </script> （ 稱為 <em>Hessian</em> ）， <script type="math/tex">\textbf{g}_{t}=\nabla f(\textbf{x}_{t}) </script> （ 稱為 <em>Gradient</em> ）。</p>

<p>再來看看用 <em>Newton’s Method</em> 來解決 <em>Cost function</em> 為橢圓 <script type="math/tex">f(x,y) = x^{2}+9y^{2} </script> 的情形。首先，畫出起始點 <script type="math/tex">(-4, 2.5) </script> ，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00148.png" alt="" /></p>

<p>先來算 <script type="math/tex">\textbf{g}</script> 和 <script type="math/tex">\textbf{H}^{-1}</script> ，分別為：</p>

<script type="math/tex; mode=display">

\textbf{g} = 

\begin{bmatrix}

  \dfrac{ \partial f(x,y) }{\partial x}  \\[0.3em]

  \dfrac{\partial f(x,y) } {\partial y}  \\[0.3em]

\end{bmatrix} 

= 

\begin{bmatrix}

  2x  \\[0.3em]

  18y \\[0.3em]

\end{bmatrix} 

</script>

<script type="math/tex; mode=display">% &lt;![CDATA[


\textbf{H}^{-1} = 

\begin{bmatrix}

  \dfrac{ \partial^{2} f(x,y) }{\partial x^{2}}  &  

  \dfrac{ \partial^{2} f(x,y) }{\partial xy}

  \\[0.3em]

  \dfrac{ \partial^{2} f(x,y) } {\partial xy} &

  \dfrac{ \partial^{2} f(x,y) }{\partial y^{2}}   

  \\[0.3em]

\end{bmatrix} ^{-1}

= 

\begin{bmatrix}

  2  &  

  0

  \\[0.3em]

  0 &

  18 

  \\[0.3em]

\end{bmatrix} ^{-1}

=

\begin{bmatrix}

  \frac{1}{2}  &  

  0

  \\[0.3em]

  0 &

  \frac{1}{18} 

  \\[0.3em]

\end{bmatrix} 

 %]]&gt;</script>

<p>設 <script type="math/tex"> \eta = 0.5 </script> ，代入起始點  <script type="math/tex">(x_{0},y_{0}) = (-4, 2.5) </script> 、 <script type="math/tex">\textbf{g}</script> 和 <script type="math/tex">\textbf{H}^{-1}</script> 到 <em>Newton’s Method</em> 的公式： <script type="math/tex">\textbf{x}_{t+1} \leftarrow \textbf{x}_{t} - \eta   \textbf{H}_{t}^{-1} \textbf{g}_{t}</script> ，得：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{bmatrix}

  x_{1}

  \\[0.3em]

  y_{1} 

  \\[0.3em]

\end{bmatrix} 

= 

\begin{bmatrix}

  -4 

  \\[0.3em]

  2.5  

  \\[0.3em]

\end{bmatrix} 

- 0.5 

\begin{bmatrix}

  \frac{1}{2}  &  

  0

  \\[0.3em]

  0 &

  \frac{1}{18} 

  \\[0.3em]

\end{bmatrix} 

\begin{bmatrix}

  2 \times (-4)  \\[0.3em]

  18 \times 2.5 \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

  -2  

  \\[0.3em]

  1.25

  \\[0.3em]

\end{bmatrix} 


 %]]&gt;</script>

<p>更新圖上的點， <script type="math/tex">(x_{1},y_{1}) = (-2, 1.25) </script> ，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00155.png" alt="" /></p>

<p>再往下走一步，求 <script type="math/tex">(x_{2},y_{2})</script> 的值，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{bmatrix}

  x_{2}

  \\[0.3em]

  y_{2} 

  \\[0.3em]

\end{bmatrix} 

= 

\begin{bmatrix}

  -2 

  \\[0.3em]

  1.25  

  \\[0.3em]

\end{bmatrix} 

- 0.5 

\begin{bmatrix}

  \frac{1}{2}  &  

  0

  \\[0.3em]

  0 &

  \frac{1}{18} 

  \\[0.3em]

\end{bmatrix} 

\begin{bmatrix}

  2 \times (-2)  \\[0.3em]

  18 \times 1.25 \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

  -1  

  \\[0.3em]

  0.625

  \\[0.3em]

\end{bmatrix} 


 %]]&gt;</script>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00150.png" alt="" /></p>

<p>從以上過程發現，  <em>Newton’s Method</em>  方向不需要一直折返，可以直接往最小值處走下去 ，整個過程如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00151.gif" alt="" /></p>

<p>註：事實上，由於本例的 <em>Cost function</em> <script type="math/tex">f(x,y)</script> 為二次函數，如果是用二次的泰勒展開式逼近，則可以完全貼合 <script type="math/tex">f(x,y)</script> 。所以用  <em>Newton’s Method</em> 的話， 位於 <script type="math/tex">\textbf{x}_{t}</script> 時， <script type="math/tex"> -  \nabla^{2}f(\textbf{x}_{t})^{-1} \nabla f(\textbf{x}_{t}) </script> 即是泰勒展開式最小值的 <script type="math/tex">\textbf{x}</script> 解，也是 <script type="math/tex">f(x,y)</script> 的最小值解，如果設 <script type="math/tex"> \eta = 1 </script> ，只要走一步就可以走到最小值，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{bmatrix}

  x_{1}

  \\[0.3em]

  y_{1} 

  \\[0.3em]

\end{bmatrix} 

= 

\begin{bmatrix}

  -4 

  \\[0.3em]

  2.5  

  \\[0.3em]

\end{bmatrix} 

- 

\begin{bmatrix}

  \frac{1}{2}  &  

  0

  \\[0.3em]

  0 &

  \frac{1}{18} 

  \\[0.3em]

\end{bmatrix} 

\begin{bmatrix}

  2 \times (-4)  \\[0.3em]

  18 \times 2.5 \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

  0

  \\[0.3em]

  0

  \\[0.3em]

\end{bmatrix} 


 %]]&gt;</script>

<p>過程如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00152.png" alt="" /></p>

<h2 id="implementation">Implementation</h2>

<p>再來進入實作的部分</p>

<p>首先，開啟新的檔案 newtons.py 並貼上以下程式碼：</p>

<figure class="code"><figcaption><span>newtons.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
<span class="line-number">61</span>
<span class="line-number">62</span>
<span class="line-number">63</span>
<span class="line-number">64</span>
<span class="line-number">65</span>
<span class="line-number">66</span>
<span class="line-number">67</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</span><span class="line">
</span><span class="line"><span class="n">A</span> <span class="o">=</span> <span class="mi">1</span>
</span><span class="line"><span class="n">B</span> <span class="o">=</span> <span class="mi">9</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">obj_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span><span class="line">    <span class="n">z</span> <span class="o">=</span> <span class="n">A</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">B</span><span class="o">*</span><span class="p">(</span><span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">z</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">obj_func_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span><span class="line">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="o">*</span><span class="n">A</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">B</span><span class="o">*</span><span class="n">y</span><span class="p">])</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">obj_func_hessian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span><span class="line">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="o">*</span><span class="n">A</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
</span><span class="line">                     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">B</span><span class="p">]</span>
</span><span class="line">                     <span class="p">])</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">plot_func</span><span class="p">(</span><span class="n">xts</span><span class="p">,</span> <span class="n">yts</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
</span><span class="line">    <span class="n">delta</span> <span class="o">=</span> <span class="mf">0.1</span>
</span><span class="line">    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
</span><span class="line">    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
</span><span class="line">    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span class="line">    <span class="n">Z</span> <span class="o">=</span> <span class="n">obj_func</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
</span><span class="line">    <span class="n">CS</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s">&#39;gray&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xts</span><span class="p">,</span> <span class="n">yts</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">xt</span><span class="p">,</span> <span class="n">yt</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">xts</span><span class="p">,</span> <span class="n">yts</span><span class="p">):</span>
</span><span class="line">            <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&quot;x=</span><span class="si">%.5f</span><span class="s">, y=</span><span class="si">%.5f</span><span class="s">, f(x, y)=</span><span class="si">%.5f</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">xts</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yts</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">obj_func</span><span class="p">(</span><span class="n">xts</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yts</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])))</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">CS</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">run_gd</span><span class="p">():</span>
</span><span class="line">    <span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">])</span>
</span><span class="line">    <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
</span><span class="line">    <span class="n">xts</span> <span class="o">=</span> <span class="p">[</span><span class="n">xy</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span><span class="line">    <span class="n">yts</span> <span class="o">=</span> <span class="p">[</span><span class="n">xy</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
</span><span class="line">    <span class="n">plot_func</span><span class="p">(</span><span class="n">xts</span><span class="p">,</span> <span class="n">yts</span><span class="p">,</span> <span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">):</span>
</span><span class="line">        <span class="n">gxy</span> <span class="o">=</span> <span class="n">obj_func_grad</span><span class="p">(</span><span class="n">xy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xy</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line">        <span class="n">xy</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">gxy</span>
</span><span class="line">        <span class="n">xts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xy</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span><span class="line">        <span class="n">yts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xy</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line">        <span class="n">plot_func</span><span class="p">(</span><span class="n">xts</span><span class="p">,</span> <span class="n">yts</span><span class="p">,</span> <span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">run_newtons</span><span class="p">():</span>
</span><span class="line">    <span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">])</span>
</span><span class="line">    <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.5</span>
</span><span class="line">    <span class="n">xts</span> <span class="o">=</span> <span class="p">[</span><span class="n">xy</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span><span class="line">    <span class="n">yts</span> <span class="o">=</span> <span class="p">[</span><span class="n">xy</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
</span><span class="line">    <span class="n">plot_func</span><span class="p">(</span><span class="n">xts</span><span class="p">,</span> <span class="n">yts</span><span class="p">,</span> <span class="s">&#39;b&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">):</span>
</span><span class="line">        <span class="n">gxy</span> <span class="o">=</span> <span class="n">obj_func_grad</span><span class="p">(</span><span class="n">xy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xy</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line">        <span class="n">hxy</span> <span class="o">=</span> <span class="n">obj_func_hessian</span><span class="p">(</span><span class="n">xy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xy</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line">        <span class="n">deltax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">hxy</span><span class="p">),</span> <span class="n">gxy</span><span class="p">)</span>
</span><span class="line">        <span class="n">xy</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">deltax</span>
</span><span class="line">        <span class="n">xts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xy</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span><span class="line">        <span class="n">yts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xy</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line">        <span class="n">plot_func</span><span class="p">(</span><span class="n">xts</span><span class="p">,</span> <span class="n">yts</span><span class="p">,</span> <span class="s">&#39;b&#39;</span><span class="p">)</span>
</span><span class="line">
</span></code></pre></td></tr></table></div></figure>

<p>其中， <code>obj_func(x,y)</code> 為目標函數， <code>obj_func_grad(x,y)</code> 為 <script type="math/tex">\textbf{g}</script> ， <code>obj_func_hessian(x,y)</code>  <script type="math/tex">\textbf{H}</script> ，而 <code>plot_function(xt,yt,c='r')</code> 可畫出目標函數的等高線圖， <code>run_gd()</code> 用來執行 <em>Gradient Descent</em> ， <code>run_newtons()</code> 用來執行 <em>Newton’s Method</em> 。 <code>xy</code> 對應到前例的 <script type="math/tex">(x,y)</script> ，而 <code>eta</code> 為 <em>Learning Rate</em> 。 <code>for i in range(20)</code> 表示最多會跑20個迴圈。</p>

<p>到 python console 執行：</p>

<figure class="code"><figcaption><span>newtons.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">newtons</span>
</span></code></pre></td></tr></table></div></figure>

<p>執行 <em>Gradient Descent</em> ，指令如下：</p>

<figure class="code"><figcaption><span>newtons.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="o">&gt;&gt;&gt;</span> <span class="n">newtons</span><span class="o">.</span><span class="n">run_gd</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00153.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00154.png" alt="" /></p>

<p>以此類推</p>

<p>執行 <em>Newton’s Method</em> ，指令如下：</p>

<figure class="code"><figcaption><span>newtons.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="o">&gt;&gt;&gt;</span> <span class="n">netons</span><span class="o">.</span><span class="n">run_newtons</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00155.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00156.png" alt="" /></p>

<p>以此類推</p>

<h2 id="comment">Comment</h2>

<p><em>Newton’s Method</em> 需要計算二次微分 <em>Hessian</em> 矩陣的反矩陣，如果 <em>variable</em> 為高維度向量，則計算這個矩陣的時間複雜度會很高，而且很占記憶體空間，因此有人提出一些 <em>Hessian</em> 矩陣的近似求法，例如 <a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS"><em>L-BFGS</em></a> 。但如果用在像是 <em>Deep Learning</em> 這種有超多 <em>variable</em> 的模型，近似求法仍然太慢，因此解 <em>Deep Learning</em> 問題，通常只會用一次微分的方法，例如 <a href="http://ckmarkoh.github.io/blog/2015/12/23/optimization-method-adagrad"><em>Adagrad</em></a>之類的。</p>

<h2 id="reference">Reference</h2>

<p>本文參考至以下教科書：</p>

<p>Stephen Boyd &amp; Lieven Vandenberghe. Convex Optimization. Chapter 5 Duality.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Optimization Method -- Gradient Descent With Momentum]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/01/16/optimization-method-momentum/"/>
    <updated>2016-01-16T08:01:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/01/16/optimization-method-momentum</id>
    <content type="html"><![CDATA[<h2 id="gradient-descent">Gradient Descent</h2>

<p>在機器學習的過程中，常需要將 Cost Function 的值減小，通常用 Gradient Descent 來做最佳化的方法來達成。但是用 Gradient Descent 有其缺點，例如，很容易卡在 Local Minimum。</p>

<p><em>Gradient Descent</em> 的公式如下：</p>

<script type="math/tex; mode=display">

\textbf{x}_{t+1} \leftarrow \textbf{x}_{t} - \eta \textbf{g}_{t}

</script>

<p>關於Gradient Descent的公式解說，請參考：<a href="http://ckmarkoh.github.io/blog/2015/12/23/optimization-method-adagrad">Optimization Method – Gradient Descent &amp; AdaGrad</a></p>

<h2 id="getting-stuck-in-local-minimum">Getting Stuck in Local Minimum</h2>

<p>舉個例子，如果 Cost Function 為 <script type="math/tex">0.3y^{3}+y^{2}+0.3x^{3}+x^{2}</script> ，有 Local Minimum <script type="math/tex">(x=0,y=0)</script> ，畫出來的圖形如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00131.png" alt="" /></p>

<!--more-->

<p>當執行 Gradient Descent 的時候，則會卡在 Local Minimum，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00132.gif" alt="" /></p>

<p>解決卡在 Local Minimum 的方法，可加入 Momentum ，使它在 Gradient 等於零的時候，還可繼續前進。</p>

<h2 id="gradient-descent-with-momentum">Gradient Descent with Momentum</h2>

<p>Momentum 的概念如下： 當一顆球從斜坡上滾到平地時，球在平地仍會持續滾動，因為球具有動量，也就是說，它的速度跟上一個時間點的速度有關。</p>

<p>模擬 Momentum的方式很簡單，即是把上一個時間點用 Gradient 得出的變化量也考慮進去。</p>

<p><em>Gradient Descent with Momentum</em> 的公式如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \Delta_{\textbf{x},t+1 } \leftarrow  \beta \Delta_{\textbf{x},t } +  (1-\beta) \eta \textbf{g}_{t} 

& \text{, where }  0 <  \beta < 1 \\

\\

& \textbf{x}_{t+1} \leftarrow \textbf{x}_{t} - \Delta_{\textbf{x},t+1 } 

\end{align}

 %]]&gt;</script>

<p>其中 <script type="math/tex"> \Delta_{\textbf{x},t +1} </script> 為 <script type="math/tex">t+1</script> 時間點，修正 <script type="math/tex">\textbf{x}</script> 值所用的變化量，而 <script type="math/tex">\Delta_{\textbf{x},t }</script> 則是 <script type="math/tex">t</script> 時間點的修正量，而 <script type="math/tex"> \beta </script> 則是用來控制在 <script type="math/tex">t+1</script> 時間點中的 <script type="math/tex">\Delta_{\textbf{x},t+1}</script> 具有上個時間點的 <script type="math/tex">\Delta_{\textbf{x},t}</script> 值的比例。 好比說，在 <script type="math/tex">t+1</script> 時間點時，球的速度會跟 <script type="math/tex">t</script> 時間點有關。 而 <script type="math/tex">(1-\beta)</script> ，則是 <script type="math/tex">t+1</script> 時間點算出之 Gradient <script type="math/tex">\textbf{g}_{t}</script> 乘上 Learning Rate <script type="math/tex">\eta</script> 後，在 <script type="math/tex">\Delta_{\textbf{x},t+1}</script> 中所占的比例。</p>

<p>舉前述例子，若起始參數為 <script type="math/tex">(x=3,y=3)</script>  ，畫出目標函數，藍點為起始點 <script type="math/tex">(x,y)</script> 的位置：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00141.png" alt="" /></p>

<p>用 Gradient Descent with Momentum 來更新 <script type="math/tex">x,y</script> 的值，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \Delta_{x,t+1 } \leftarrow  \beta \Delta_{x,t } +  (1-\beta) \eta \dfrac{\partial f(x_{t},y_{t})}{\partial x_{t}} \\

& \Delta_{y,t+1 } \leftarrow  \beta \Delta_{y,t } +  (1-\beta) \eta \dfrac{\partial f(x_{t},y_{t})}{\partial y_{t}} \\

& x_{t+1} \leftarrow x_{t} - \Delta_{x,t+1 }  \\

& y_{t+1} \leftarrow y_{t} - \Delta_{y,t+1 } 


\end{align}

 %]]&gt;</script>

<p>化減後得：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \Delta_{x,t+1 } \leftarrow  \beta \Delta_{x,t } +  (1-\beta) \eta (0.9 x_{t}^{2} + 2x_{t} ) \\

& \Delta_{y,t+1 } \leftarrow  \beta \Delta_{y,t } +  (1-\beta) \eta (0.9 y_{t}^{2} + 2y_{t} ) \\

& x_{t+1} \leftarrow x_{t} - \Delta_{x,t+1 }  \\

& y_{t+1} \leftarrow y_{t} - \Delta_{y,t+1 } 


\end{align}

 %]]&gt;</script>

<p>設初始化值 <script type="math/tex">  \Delta_{x} = 0,  \Delta_{y} = 0 </script> ，參數 <script type="math/tex">\beta = 0.9, \eta = 0.2 </script> ，代入 <script type="math/tex"> x=3,y=3 </script> ，則：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \Delta_{x,1 } =  0.9 \times  0 +  (1-0.9)\times 0.2 \times (0.9\times 3^{2}+2\times 3) = 0.282 \\

& \Delta_{y,1 } =  0.9 \times  0 +  (1-0.9)\times 0.2 \times (0.9\times 3^{2}+2\times 3) = 0.282 \\

& x_{1} = 3 - \Delta_{x,1 } = 3 - 0.282 = 2.718 \\

& y_{1} = 3 - \Delta_{y,1 } = 3 - 0.282 = 2.718

\end{align}

 %]]&gt;</script>

<p>更新圖上的藍點，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00142.png" alt="" /></p>

<p>再往下走一步， <script type="math/tex"> x,y </script>  的值如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \Delta_{x,2 } =  0.9 \times  0.282 +  (1-0.9)\times 0.2 \times (0.9\times 2.718^{2}+2\times 2.718) = 0.4955 \\

& \Delta_{y,2 } =  0.9 \times  0.282 +  (1-0.9)\times 0.2 \times (0.9\times 2.718^{2}+2\times 2.718) = 0.4955\\

& x_{2} = 3 - \Delta_{x,2 } = 2.718  - 0.4955 = 2.2225 \\

& y_{2} = 3 - \Delta_{y,2 } = 2.718  - 0.4955 = 2.2225

\end{align}

 %]]&gt;</script>

<p>更新圖上的藍點，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00143.png" alt="" /></p>

<p>在以上兩步中，可發現 <script type="math/tex"> \Delta_{x }, \Delta_{y }</script> 的值逐漸變大。由於一開始 <script type="math/tex"> \Delta_{x }, \Delta_{y }</script> 都是零，它會跟前一個時間點的值有關，所以看起來就好像是球從斜坡上滾下來時，慢慢加速，而在球經過 Local Minimum時，也會慢慢減速，不會直接卡在 Local Minimum 。整個過程如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00136.png" alt="" /></p>

<p>動畫版：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00137.gif" alt="" /></p>

<h2 id="implementation">Implementation</h2>

<p>再來進入實作的部分</p>

<p>首先，開啟新的檔案 momentum.py 並貼上以下程式碼：</p>

<figure class="code"><figcaption><span>momentum.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
</span><span class="line">  <span class="k">return</span> <span class="p">(</span><span class="mf">0.3</span><span class="o">*</span><span class="n">y</span><span class="o">**</span><span class="mi">3</span><span class="o">+</span><span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="mf">0.3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="o">+</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">func_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
</span><span class="line">  <span class="k">return</span> <span class="p">(</span><span class="mf">0.9</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.9</span><span class="o">*</span><span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">y</span> <span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s">&#39;r&#39;</span><span class="p">):</span>
</span><span class="line">  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
</span><span class="line">  <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">&#39;3d&#39;</span><span class="p">,</span>
</span><span class="line">        <span class="n">elev</span><span class="o">=</span><span class="mf">7.</span><span class="p">,</span> <span class="n">azim</span><span class="o">=-</span><span class="mi">175</span><span class="p">)</span>
</span><span class="line">  <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">))</span>
</span><span class="line">  <span class="n">Z</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
</span><span class="line">  <span class="n">surf</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">rstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span class="line">    <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</span><span class="line">  <span class="n">ax</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
</span><span class="line">  <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">,</span> <span class="n">func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">),</span><span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">&#39;o&#39;</span> <span class="p">)</span>
</span><span class="line">  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">&quot;x=</span><span class="si">%.5f</span><span class="s">, y=</span><span class="si">%.5f</span><span class="s">, f(x,y)=</span><span class="si">%.5f</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="n">func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">)))</span>
</span><span class="line">  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span><span class="line">  <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">run_grad</span><span class="p">():</span>
</span><span class="line">  <span class="n">xt</span> <span class="o">=</span> <span class="mi">3</span>
</span><span class="line">  <span class="n">yt</span> <span class="o">=</span> <span class="mi">3</span>
</span><span class="line">  <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
</span><span class="line">  <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
</span><span class="line">    <span class="n">gxt</span><span class="p">,</span> <span class="n">gyt</span> <span class="o">=</span> <span class="n">func_grad</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">)</span>
</span><span class="line">    <span class="n">xt</span> <span class="o">=</span> <span class="n">xt</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">gxt</span>
</span><span class="line">    <span class="n">yt</span> <span class="o">=</span> <span class="n">yt</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">gyt</span>
</span><span class="line">    <span class="k">if</span> <span class="n">xt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">xt</span> <span class="o">&gt;</span> <span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">:</span>
</span><span class="line">      <span class="k">break</span>
</span><span class="line">    <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">run_momentum</span><span class="p">():</span>
</span><span class="line">  <span class="n">xt</span> <span class="o">=</span> <span class="mi">3</span>
</span><span class="line">  <span class="n">yt</span> <span class="o">=</span> <span class="mi">3</span>
</span><span class="line">  <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.2</span>
</span><span class="line">  <span class="n">beta</span> <span class="o">=</span> <span class="mf">0.9</span>
</span><span class="line">  <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;b&#39;</span><span class="p">)</span>
</span><span class="line">  <span class="n">delta_x</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">  <span class="n">delta_y</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
</span><span class="line">    <span class="n">gxt</span><span class="p">,</span> <span class="n">gyt</span> <span class="o">=</span> <span class="n">func_grad</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">)</span>
</span><span class="line">    <span class="n">delta_x</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">delta_x</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta</span><span class="p">)</span><span class="o">*</span><span class="n">eta</span><span class="o">*</span><span class="n">gxt</span>
</span><span class="line">    <span class="n">delta_y</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">delta_y</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta</span><span class="p">)</span><span class="o">*</span><span class="n">eta</span><span class="o">*</span><span class="n">gyt</span>
</span><span class="line">    <span class="n">xt</span> <span class="o">=</span> <span class="n">xt</span> <span class="o">-</span> <span class="n">delta_x</span>
</span><span class="line">    <span class="n">yt</span> <span class="o">=</span> <span class="n">yt</span> <span class="o">-</span> <span class="n">delta_y</span>
</span><span class="line">    <span class="k">if</span> <span class="n">xt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">xt</span> <span class="o">&gt;</span> <span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">:</span>
</span><span class="line">      <span class="k">break</span>
</span><span class="line">    <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;b&#39;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>其中， <code>func(x,y)</code> 為目標函數，<code>func_grad(x,y)</code> 為目標函數的 <em>gradient</em> ，而 <code>plot_func(xt,yt,c='r')</code> 可畫出目標函數的曲面， <code>run_grad()</code> 用來執行 <em>Gradient Descent</em> ， <code>run_momentum()</code> 用來執行 <em>Gradient Descent with Momentum</em> 。 <code>xt</code> 和 <code>yt</code> 對應到前例的 <script type="math/tex">(x,y)</script> ，而 <code>eta</code> 為 <em>Learning Rate</em> 。 <code>for i in range(20)</code> 表示最多會跑20個迴圈，而 <code>if xt &lt; -5 or yt &lt; -5 or xt &gt; 5 or yt &gt; 5</code> 表示，如果 <code>xt</code> 和 <code>yt</code> 超出邊界，則會先結束迴圈。</p>

<p>到 python console 執行：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; import momentum
</span></code></pre></td></tr></table></div></figure>

<p>執行 <em>Gradient Descent</em> ，指令如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; momentum.run_grad<span class="o">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00138.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00139.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00140.png" alt="" /></p>

<p>以此類推</p>

<p>執行 <em>Gradient Descent with Momentum</em> ，指令如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; momentum.run_momentum<span class="o">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00141.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00142.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00143.png" alt="" /></p>

<p>以此類推</p>

<h2 id="reference">Reference</h2>

<h4 id="visualizing-optimization-algos">Visualizing Optimization Algos</h4>

<p>http://imgur.com/a/Hqolp</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Optimization Method -- Gradient Descent & AdaGrad]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/12/23/optimization-method-adagrad/"/>
    <updated>2015-12-23T17:14:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/12/23/optimization-method-adagrad</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>在機器學習的過程中，常需要將 <em>Cost Function</em> 的值減小，需由最佳化的方法來達成。本文介紹 <em>Gradient Descent</em> 和 <em>AdaGrad</em> 兩種常用的最佳化方法。</p>

<h2 id="gradient-descent">Gradient Descent</h2>

<p><em>Gradient Descent</em> 的公式如下：</p>

<script type="math/tex; mode=display">

\textbf{x}_{t+1} \leftarrow \textbf{x}_{t} - \eta \textbf{g}_{t}

</script>

<p>其中， <script type="math/tex">\eta</script> 為 <em>Learning Rate</em> ， <script type="math/tex">\textbf{x} </script> 為最佳化時要調整的參數， <script type="math/tex">\textbf{g}</script> 為最佳化目標函數對 <script type="math/tex">\textbf{x}</script> 的梯度。 <script type="math/tex">\textbf{x}_{t}</script> 為調整之前的 <script type="math/tex">\textbf{x} </script> ，<script type="math/tex">\textbf{x}_{t+1}</script> 為調整之後的 <script type="math/tex">\textbf{x} </script> 。</p>

<p>舉個例子，如果目標函數為 <script type="math/tex"> f(x,y) = y^2 - x^2  </script> ，起始參數為 <script type="math/tex">(x,y) = (0.001,4)</script> ，則畫出來的圖形如下圖，曲面為目標函數，紅色的點為起始參數：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00126.png" alt="" /></p>

<!--more-->

<p>可藉由改變 <script type="math/tex">(x,y)</script> 來讓 <script type="math/tex">f(x,y)</script> 的值減小。 <em>Gradient Descent</em> 所走的方向為梯度最陡的方向，若 <script type="math/tex">eta=0.3</script> 則 ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& 	x \leftarrow  x - \eta  \dfrac{\partial f(x,y)}{\partial x}  \\

&  y \leftarrow  y - \eta  \dfrac{\partial f(x,y)}{\partial y} \\

\end{align}

 %]]&gt;</script>

<p>求出微分後得：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& 	x \leftarrow  x - \eta  \times (-2x)  \\

&  y \leftarrow  y - \eta  \times 2y \\

\end{align}

 %]]&gt;</script>

<p>代入數值，得：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x = 0.001 - 0.3 \times (-2) \times 0.001 = 0.0016 \\

& y = 4 - 0.3 \times 2 \times 4 = 1.6 \\

\end{align}

 %]]&gt;</script>

<p>更新完後的結果如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00127.png" alt="" /></p>

<p>從上圖可看出，紅點移動到比較低的地方，即 <script type="math/tex">f(x,y)</script> 變小了。</p>

<p>經過了數次改變 <script type="math/tex">(x,y)</script> 值的循環之後，<script type="math/tex">f(x,y)</script> 的值會越變越小，紅點移動的路徑如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00118.png" alt="" /></p>

<p>動畫版：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00119.gif" alt="" /></p>

<p>從上圖可發現，紅色的點會卡在 <script type="math/tex">(0,0)</script> 附近（也就是Saddle Point），過了一陣子後才會繼續往下滾。</p>

<h2 id="adagrad">AdaGrad</h2>

<p><em>Gradient Descent</em> 的缺點有：</p>

<p>(1) <em>Learning Rate</em> 不會隨著時間而減少</p>

<p>(2) <em>Learning Rate</em> 在每個方向是固定的</p>

<p>以上的(1)會使得在越接近近目標函數最小值時，越容易走過頭，(2)則會容易卡在目標函數的Saddle Point。</p>

<p>因為 <em>Gradient Descent</em> 只考慮目前的 <em>Gradient</em> ，如果可以利用過去時間在各個方向的 <em>Gradient</em> ，來調整現在時間點在各個方向的 <em>Learning Rate</em> ，則可避免以上兩種情型發生。</p>

<p><em>AdaGrad</em> 的公式如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{G}_{t} = \sum_{n=0}^{t} \textbf{g}_{n}^{2} \\

& \textbf{x}_{t+1} \leftarrow \textbf{x}_{t} - \frac{\eta}{\sqrt{\textbf{G}_{t}}} \textbf{g}_{t} \\

\end{align}

 %]]&gt;</script>

<p>其中，<script type="math/tex"> \textbf{G}_{t} </script> 為過去到現在所有時間點所有的 <script type="math/tex">\textbf{g}</script> 的平方和。由於  <script type="math/tex">\textbf{x}</script> ， <script type="math/tex">\textbf{g}</script>和 <script type="math/tex">\textbf{G}</script> 皆為向量，設 <script type="math/tex">x_{i}</script> ， <script type="math/tex">g_{i}</script> 和 <script type="math/tex">G_{i}</script> 各為其元素，則公式可寫成：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& G_{i,t} = \sum_{n=0}^{t} g_{i,n}^{2} \\

& x_{i,t+1} \leftarrow x_{i,t} - \frac{\eta}{\sqrt{G_{i,t}}} g_{i,t} \\

\end{align}

 %]]&gt;</script>

<p>這公式可修正以上兩個 <em>Gradient Descent</em> 的缺點：</p>

<p>1.若時間越久，則 <em>Gradient</em> 平方和越大，使得 <em>Learning Rate</em> 越小，這樣就可以讓 <em>Learning Rate</em> 隨著時間減少，而在接近目標函數的最小值時，比較不會走過頭。</p>

<p>2.若某方向從過去到現在時間點 <em>Gradient</em> 平方和越小，則 <em>Learning Rate</em> 要越大。（直覺上來講，過去時間點 <em>Gradient</em> 越小的方向，在未來可能越重要，這種概念有點類似<a href="http://ckmarkoh.github.io/blog/2014/04/14/natural-language-processing-tf-idf">tf-idf</a>，在越少文檔中出現的詞，可能越重要。）由於各方向的 <em>Learning Rate</em> 不同，比較不會卡在 <em>Saddle Point</em> 。</p>

<p>前述例子，起始參數為 <script type="math/tex">(x,y) = (0.001,4)</script> ，則畫出來的圖形如下圖，曲面為目標函數，藍點為起始參數：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00128.png" alt="" /></p>

<p>用 <em>AdaGrad</em> 來更新 <script type="math/tex">(x,y)</script> 的值，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& 	x_{t+1} \leftarrow  x_{t} - \frac{\eta}{\sqrt{\sum_{n=0}^{t}  

(\dfrac{\partial f(x_{n},y_{n})}{\partial x_{n}} )^{2} }} 

\dfrac{\partial f(x_{t},y_{t})}{\partial x_{t}}  \\


&  y_{t+1} \leftarrow  y_{t} - \frac{\eta}{\sqrt{\sum_{n=0}^{t}  

(\dfrac{\partial f(x_{n},y_{n})}{\partial y_{n}} )^{2}  }} 

\dfrac{\partial f(x_{t},y_{t})}{\partial y_{t}} \\

\end{align}

 %]]&gt;</script>

<p>化簡後得：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& 	x_{t+1} \leftarrow  x_{t} - \frac{\eta}{\sqrt{\sum_{n=0}^{t}  

( -2x_{n} )^{2} }} 

( -2x_{t} ) \\


&  y_{t+1} \leftarrow  y_{t} - \frac{\eta}{\sqrt{\sum_{n=0}^{t}  

( 2y_{n} )^{2} }} 

( 2y_{t} ) \\

\end{align}

 %]]&gt;</script>

<p>由於 <em>AdaGrad</em> 的 <em>Learning Rate</em> 會隨時間減小，所以初始化時可以給它較大的值，此例中，設 <script type="math/tex">\eta = 1.0</script></p>

<p>代入 <script type="math/tex">(x,y)</script> 的數值，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x = 0.001 -  \frac{1.0}{\sqrt{  ( (-2) \times 0.001 )^2  }} \times (-2) \times 0.001 = 1.001 \\

& x = 4 -  \frac{1.0}{\sqrt{  ( 2 \times 4 )^2  }} \times 2 \times 4 = 3 \\

\end{align}

 %]]&gt;</script>

<p>更新圖上的藍點，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00129.png" alt="" /></p>

<p>再往下走一步， <script type="math/tex">(x,y)</script> 的值如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x = 1.001 -  \frac{1.0}{\sqrt{  ( (-2) \times 0.001 )^2  + ( (-2) \times 1.001 )^2 }} \times (-2) \times 1.001 = 2.001 \\

& x = 3 -  \frac{1.0}{\sqrt{  ( 2 \times 4 )^2 +  ( 2 \times 3 )^2  }} \times 2 \times 3 = 2.4 \\

\end{align}

 %]]&gt;</script>

<p>更新圖上的藍點，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00130.png" alt="" /></p>

<p>經過了數次改變 <script type="math/tex">(x,y)</script> 值的循環之後，<script type="math/tex">f(x,y)</script> 的值會越變越小，藍點移動的路徑如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00123.png" alt="" /></p>

<p>動畫版：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00124.gif" alt="" /></p>

<p>由此可以發現， <em>AdaGrad</em> 不會卡在 <em>Saddle Point</em> 。</p>

<p>將 <em>Gradient Descent</em> 和 <em>AdaGrad</em> 畫在同一張圖上，比較兩者差異：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00125.gif" alt="" /></p>

<h2 id="implementation">Implementation</h2>

<p>再來進入實作的部分：</p>

<p>首先,開啟新的檔案 adagrad.py 並貼上以下程式碼</p>

<figure class="code"><figcaption><span>adagrad.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
</span><span class="line">  <span class="k">return</span> <span class="p">(</span><span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">func_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
</span><span class="line">  <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">y</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s">&#39;r&#39;</span><span class="p">):</span>
</span><span class="line">  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
</span><span class="line">  <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">&#39;3d&#39;</span><span class="p">,</span>
</span><span class="line">        <span class="n">elev</span><span class="o">=</span><span class="mf">35.</span><span class="p">,</span> <span class="n">azim</span><span class="o">=-</span><span class="mi">30</span><span class="p">)</span>
</span><span class="line">  <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">))</span>
</span><span class="line">  <span class="n">Z</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
</span><span class="line">  <span class="n">surf</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">rstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span class="line">    <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</span><span class="line">  <span class="n">ax</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="o">-</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
</span><span class="line">  <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">,</span> <span class="n">func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">),</span><span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">&#39;o&#39;</span> <span class="p">)</span>
</span><span class="line">  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">&quot;x=</span><span class="si">%.5f</span><span class="s">, y=</span><span class="si">%.5f</span><span class="s">, f(x,y)=</span><span class="si">%.5f</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="n">func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">)))</span>
</span><span class="line">  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span><span class="line">  <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">run_grad</span><span class="p">():</span>
</span><span class="line">  <span class="n">xt</span> <span class="o">=</span> <span class="mf">0.001</span>
</span><span class="line">  <span class="n">yt</span> <span class="o">=</span> <span class="mi">4</span>
</span><span class="line">  <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.3</span>
</span><span class="line">  <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
</span><span class="line">    <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span> <span class="o">=</span> <span class="n">func_grad</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">)</span>
</span><span class="line">    <span class="n">xt</span> <span class="o">=</span> <span class="n">xt</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">gx</span>
</span><span class="line">    <span class="n">yt</span> <span class="o">=</span> <span class="n">yt</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">gy</span>
</span><span class="line">    <span class="k">if</span> <span class="n">xt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">xt</span> <span class="o">&gt;</span> <span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">:</span>
</span><span class="line">      <span class="k">break</span>
</span><span class="line">    <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">run_adagrad</span><span class="p">():</span>
</span><span class="line">  <span class="n">xt</span> <span class="o">=</span> <span class="mf">0.001</span>
</span><span class="line">  <span class="n">yt</span> <span class="o">=</span> <span class="mi">4</span>
</span><span class="line">  <span class="n">eta</span> <span class="o">=</span> <span class="mf">1.0</span>
</span><span class="line">  <span class="n">Gxt</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">  <span class="n">Gyt</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">  <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;b&#39;</span><span class="p">)</span>
</span><span class="line">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
</span><span class="line">    <span class="n">gxt</span><span class="p">,</span><span class="n">gyt</span> <span class="o">=</span> <span class="n">func_grad</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">)</span>
</span><span class="line">    <span class="n">Gxt</span> <span class="o">+=</span> <span class="n">gxt</span><span class="o">**</span><span class="mi">2</span>
</span><span class="line">    <span class="n">Gyt</span> <span class="o">+=</span> <span class="n">gyt</span><span class="o">**</span><span class="mi">2</span>
</span><span class="line">    <span class="n">xt</span> <span class="o">=</span> <span class="n">xt</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="p">(</span><span class="n">Gxt</span><span class="o">**</span><span class="mf">0.5</span><span class="p">))</span><span class="o">*</span><span class="n">gxt</span>
</span><span class="line">    <span class="n">yt</span> <span class="o">=</span> <span class="n">yt</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="p">(</span><span class="n">Gyt</span><span class="o">**</span><span class="mf">0.5</span><span class="p">))</span><span class="o">*</span><span class="n">gyt</span>
</span><span class="line">    <span class="k">if</span> <span class="n">xt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">xt</span> <span class="o">&gt;</span> <span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">:</span>
</span><span class="line">      <span class="k">break</span>
</span><span class="line">    <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;b&#39;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>其中， <code>func(x,y)</code> 為目標函數，<code>func_grad(x,y)</code> 為目標函數的 <em>gradient</em> ，而 <code>plot_func(xt,yt,c='r')</code> 可畫出目標函數的曲面， <code>run_grad()</code> 用來執行 <em>Gradient Descent</em> ， <code>run_adagrad()</code> 用來執行 <em>AdaGrad</em> 。 <code>xt</code> 和 <code>yt</code> 對應到前例的 <script type="math/tex">(x,y)</script> ，而 <code>eta</code> 為 <em>Learning Rate</em> 。 <code>for i in range(20)</code> 表示最多會跑20個迴圈，而 <code>if xt &lt; -5 or yt &lt; -5 or xt &gt; 5 or yt &gt; 5</code> 表示，如果 <code>xt</code> 和 <code>yt</code> 超出邊界，則會先結束迴圈。</p>

<p>到 python console 執行：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; import adagrad
</span></code></pre></td></tr></table></div></figure>

<p>執行 <em>Gradient Descent</em> ，指令如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; adagrad.run_grad<span class="o">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00126.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00127.png" alt="" /></p>

<p>以此類推</p>

<p>執行 <em>Adagrad</em> ，指令如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; adagrad.run_adagrad<span class="o">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00128.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00129.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00130.png" alt="" /></p>

<p>以此類推</p>

<h2 id="reference">Reference</h2>

<h4 id="notes-on-adagrad">Notes on AdaGrad</h4>

<p>http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf</p>

<h4 id="visualizing-optimization-algos">Visualizing Optimization Algos</h4>

<p>http://imgur.com/a/Hqolp</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Convex Optimization -- Duality & KKT Conditions]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/11/05/convex-optimization-duality-and-kkt-conditions/"/>
    <updated>2015-11-05T13:18:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/11/05/convex-optimization-duality-and-kkt-conditions</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>在解「 <strong>有條件的最佳化問題</strong> 」時，有時需要把原本的問題轉換成對偶問題(Dual Problem)後，會比較好解。</p>

<p>如果對偶問題有最佳解，原本問題也有最佳解，且這兩個最佳解相同，則必須要滿足 <em>Karush-Kuhn-Tucker (KKT) Conditions</em>：</p>

<ol>
  <li>
    <p>Primal Feasibility </p>
  </li>
  <li>
    <p>Dual Feasibility</p>
  </li>
  <li>
    <p>Complementary Slackness</p>
  </li>
  <li>
    <p>Stationarity</p>
  </li>
</ol>

<p>至於這四項到底是什麼？講起來有點複雜。本文會先從對偶問題的概念開始介紹，再來講解這四個條件。</p>

<h2 id="the-lagrange-dual-function">The Lagrange dual function</h2>

<p>首先，講解一下什麼是對偶問題。</p>

<p>通常，有條件的最佳化問題，可寫成 <a name="eq1">＜公式一＞</a> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{aligned}

& \textbf{minimize} & f_{0}(x) & \\

& \textbf{subject to } & f_{i}(x) \leq 0 ,& i=1, ... ,m \\

& & h_{j}(x) = 0 , &j=1, ... ,p \\

\end{aligned}

 %]]&gt;</script>

<!--more-->

<p>其中, <script type="math/tex">f_{0}(x)</script> 是目標函數，而 <script type="math/tex"> f_{i}(x) \leq 0</script> 為不等式的條件限制， <script type="math/tex">h_{j}(x) = 0</script> 為等式的條件限制。也就是說，最佳化的目標是要求出 <script type="math/tex">x</script> 讓 <script type="math/tex">f_{0}(x)</script> 得出最小值，而這個 <script type="math/tex">x</script> ，必須滿足 <script type="math/tex"> f_{i}(x) \leq 0</script> 和 <script type="math/tex">h_{j}(x) = 0</script> 所限定的條件。若滿足這兩項條件，則表示這個最佳化問題有解，即為滿足 <strong>primal feasibility</strong> 。</p>

<p>此最佳化問題可以轉換成對偶問題，如下：</p>

<script type="math/tex; mode=display">

L(x, \lambda, \nu) = f_{0}(x) + \sum_{i=1}^{m} \lambda_{i} f_{i}(x) + \sum_{j=1}^{p} \nu_{j} h_{j}(x)

</script>

<p>其中，我們把 <script type="math/tex">L(x, \lambda, \nu)</script> 稱為 <em>Lagrangian</em> ，<script type="math/tex"> \lambda_{i} </script> 和 <script type="math/tex">\nu_{j}</script> 稱為 <em>Lagrange multiplier</em> 。</p>

<p>所謂的對偶函數( <em>Lagrange dual function</em> )，即是在給定了 <em>Lagrange multiplier</em> 之下，改變 <script type="math/tex">x</script> 來得出 <em>Lagrangian</em> 最小值，如下：</p>

<script type="math/tex; mode=display">

g(\lambda, \nu) = \inf_{x}( L(x, \lambda, \nu) ) = \inf_{x}( f_{0}(x) + \sum_{i=1}^{m} \lambda_{i} f_{i}(x) + \sum_{j=1}^{p} \nu_{j} h_{j}(x) )

</script>

<p>其中， <script type="math/tex">g(\lambda, \nu) </script> 為對偶函數，而 <script type="math/tex">\inf_{x}</script> 即是在改變 <script type="math/tex">x</script> 的情況下，找出最小值。</p>

<h2 id="lower-bounds-on-optimal-value">Lower bounds on optimal value</h2>

<p>接著來證明，對偶函數可當作原本問題的 <em>Lower Bound</em> 。設原本問題<a href="#eq1">公式一</a>的最佳解為 <script type="math/tex">p^{\star}</script> ，則在所有 <script type="math/tex"> \lambda_{i} \geq 0 </script> （記作 <script type="math/tex">\lambda \succeq 0 </script> ）和  <script type="math/tex">\nu_{j}</script> 為任意數的情況下，會滿足以下條件：</p>

<script type="math/tex; mode=display">

g(\lambda, \nu) \leq p^{\star}

</script>

<p>此性質不難證明，對於所有滿足 <script type="math/tex"> f_{i}(\tilde{x}) \leq 0</script> 和 <script type="math/tex">h_{j}(\tilde{x}) = 0</script> 這兩個條件的 <script type="math/tex">\tilde{x}</script> ，必滿足：</p>

<script type="math/tex; mode=display">

\sum_{i=1}^{m} \lambda_{i} f_{i}(\tilde{x}) + \sum_{j=1}^{p} \nu_{j} h_{j}(\tilde{x}) \leq 0

</script>

<p>則：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{aligned}

& g(\lambda, \nu) = \inf_{x}( L(x, \lambda, \nu) ) = \inf_{x}( f_{0}(x) + \sum_{i=1}^{m} \lambda_{i} f_{i}(x) + \sum_{j=1}^{p} \nu_{j} h_{j}(x) ) \\

& \leq L(\tilde{x}, \lambda, \nu) =  f_{0}(\tilde{x}) + \sum_{i=1}^{m} \lambda_{i} f_{i}(\tilde{x}) + \sum_{j=1}^{p} \nu_{j} h_{j}(\tilde{x}) \\

&\leq  f_{0}(\tilde{x})

\end{aligned}

 %]]&gt;</script>

<p>設原本問題<a href="#eq1">公式一</a>最佳解時的 <script type="math/tex">x</script> 為 <script type="math/tex">x^{\star}</script> ，由於 <script type="math/tex">x^{\star}</script> 必須滿足 滿足 <script type="math/tex"> f_{i}(x^{\star}) \leq 0</script> 和 <script type="math/tex">h_{j}(x^{\star}) = 0</script> 這兩個條件，又因 <script type="math/tex">\lambda \succeq 0 </script> ，故 <script type="math/tex">g(\lambda, \nu) \leq p^{\star}</script> 成立。</p>

<p>舉個例子，下圖中，黑色的實線為目標函數 <script type="math/tex">f_{0}(x)</script> ，此最佳化問題有一個不等式條件限制 <script type="math/tex">f_{1}(x)</script> ，用綠色虛線表示，而滿足於  <script type="math/tex">f_{1}(x) \leq 0 </script> 的 <script type="math/tex">x</script> 範圍落在 <script type="math/tex">[-0.46~0.46]</script> 之間，範圍用兩條鉛直的紅色虛線表示。 紫色的圓圈為最佳解的點 <script type="math/tex">x^{\star} = -0.46, p^{\star} = 1.54</script> 。此最佳化問題沒有等式條件，故沒有 <script type="math/tex">h_{j}</script> 及  <script type="math/tex">\nu_{j}</script> 。則此問題的 <em>Lagrangian</em> 為 <script type="math/tex">L(x, \lambda)</script> 。藍色的點為 <script type="math/tex">\lambda = 0.1, 0.2, ... , 1.0 </script> 所畫出來的 <em>Lagrangian</em> 圖形。 </p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00114.png" alt="" /></p>

<p>在上圖中，兩條紅色虛線之間的範圍內，即 <script type="math/tex">x</script> 滿足不等式的條件限制 <script type="math/tex">f_{1}(x) \leq 0</script> ，此時藍色的虛線皆位於黑色線的下方，滿足 <script type="math/tex">L(x, \lambda) \leq f_{0}(x)</script> 。 </p>

<p>下圖畫出改變不同 <script type="math/tex">\lambda</script> 值時，對偶函數 <script type="math/tex"> g(\lambda) = \inf_{x}( L(x, \lambda) ) </script> 的值，其中，黑色的實線為 <script type="math/tex"> g(\lambda) </script> ，紫色的虛線為 <script type="math/tex">p^{\star}</script> 的值（<script type="math/tex">p^{\star} = 1.54</script> ）。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00115.png" alt="" /></p>

<p>根據此圖，黑色的線恆在紫色的虛線下方，滿足 <script type="math/tex">g(\lambda) \leq p^{\star}</script> 。</p>

<h2 id="the-lagrange-dual-problem">The Lagrange dual problem</h2>

<p>所謂的對偶問題（Lagrange dual problem）即是把原本的問題轉成對偶函數之後，來解最佳化問題。</p>

<p>從前面結論可得知，對偶函數若滿足 <script type="math/tex">\lambda \succeq 0 </script> （也就是所有的 <script type="math/tex">\lambda_{i}</script> 都滿足 <script type="math/tex">\lambda_{i} \geq 0</script> ）的條件，則必足 <script type="math/tex">g(\lambda, \nu) \leq p^{\star}</script> 。如果想找出最接近 <script type="math/tex">p^{\star}</script> 的對偶函數解，則可藉由改變 <script type="math/tex">\lambda, \nu</script> ，來找出 <script type="math/tex">g(\lambda, \nu) </script> 的最大值。此對偶問題如下 <a name="eq2">＜公式二＞</a>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{aligned}

& \textbf{maximize} & g(\lambda, \nu) & \\

& \textbf{subject to } & \lambda \succeq 0 \\

\end{aligned}

 %]]&gt;</script>

<p>如果可以找出一組解，滿足 <script type="math/tex">\lambda \succeq 0 </script> ，可得出 <script type="math/tex">g(\lambda, \nu) </script> 的最大值 <script type="math/tex">d^{\star}</script> ，則此問題滿足 <strong>dual feasibility</strong>。</p>

<p>根據對偶問題的解 <script type="math/tex">d^{\star}</script> 和原本問題的解 <script type="math/tex">p^{\star}</script> 的關係，可將對偶性質分為 <em>weak duality</em> 和  <em>strong duality</em> 兩類：</p>

<h4 id="weak-duality">Weak duality</h4>

<p>若對偶問題的解，小於或等於原本問題的解，即滿足 <em>weak duality</em> ：</p>

<script type="math/tex; mode=display">

d^{\star} \leq p^{\star} 

</script>

<p>根據前面段落 <em>Lower bounds on optimal value</em> 所推導的結論， <em>Weak duality</em> 必定成立。</p>

<h4 id="strong-duality">Strong duality</h4>

<p>若對偶問題的解，等於原本問題的解，即滿足 <em>strong duality</em> ：</p>

<script type="math/tex; mode=display">

d^{\star} = p^{\star} 

</script>

<p>這個條件不一定會成立，如果要成立的話，原本問題<a href="#eq1">公式一</a>的中的 <script type="math/tex">f_{0}(x)</script> 和 <script type="math/tex">f_{i}(x)</script> 要是凸函數，但即使滿足此條件，仍須滿足其他條件才能使 <em>strong duality</em> 成立，這講起來比較複雜，在此先不提。</p>

<h2 id="complementary-slackness">Complementary Slackness</h2>

<p>設原本問題最佳解的 <script type="math/tex">x</script> 為 <script type="math/tex">x^{\star}</script> ，對偶問題最佳解的 <script type="math/tex">(\lambda, \nu)</script> 為 <script type="math/tex">(\lambda^{\star}, \nu^{\star}) </script> ，根據對偶函數 <script type="math/tex">g(\lambda^{\star}, \nu^{\star})</script> 的定義，和前面段落 <em>Lower bounds on optimal value</em> 所推導出的結論，可得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{aligned}

& g(\lambda^{\star}, \nu^{\star}) = \inf_{x}( L(x, \lambda^{\star}, \nu^{\star}) ) \\

& \leq f_{0}(x^{\star}) + \sum_{i=1}^{m} \lambda_{i}^{\star} f_{i}(x^{\star}) + \sum_{j=1}^{p} \nu_{j}^{\star} h_{j}(x^{\star})  \\

& \leq f_{0}(x^{\star}) 

\end{aligned}

 %]]&gt;</script>

<p>若對偶問題滿足 <em>strong duality</em> ：</p>

<script type="math/tex; mode=display">

g(\lambda^{\star}, \nu^{\star}) =  f_{0}(x^{\star})  

</script>

<p>則須滿足：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{aligned}

& \sum_{i=1}^{m} \lambda_{i}^{\star} f_{i}(x^{\star})  = 0 \mspace{40mu} \text{(a)} \\

& \sum_{j=1}^{p} \nu_{j}^{\star} h_{j}(x^{\star}) = 0 \mspace{40mu} \text{(b)}

\end{aligned}

 %]]&gt;</script>

<p>由於 <script type="math/tex">h_{j}(x^{\star}) = 0 </script> 為原本問題的等式條件限制，故 <strong>(b)</strong> 必會成立，若 <strong>(a)</strong> 要成立的話，須滿足：</p>

<script type="math/tex; mode=display">

\lambda_{i}^{\star} f_{i}(x^{\star})  = 0  \mspace{20mu} \textbf{for } i=1,2,...m

</script>

<p>也就是說，<script type="math/tex">f_{i}(x^{\star}) </script> 和  <script type="math/tex">\lambda_{i}^{\star}</script> 的其中一項必須為零，不可以兩項都不為零。這種情形稱為 <strong>complementary slackness</strong> ，也就是林軒田教授在<a href="https://www.coursera.org/course/ntumltwo">機器學習技法</a>課程中所提到的：</p>

<blockquote>
  <p>哈利波特和佛地魔，其中一個必須死掉。</p>
</blockquote>

<h2 id="karush-kuhn-tucker-kkt-conditions">Karush-Kuhn-Tucker (KKT) conditions</h2>

<p><em>KKT conditions</em> 即為下四個條件：</p>

<ol>
  <li>
    <p>Primal Feasibility：即滿足原本問題<a href="#eq1">公式一</a>的限制條件 <script type="math/tex">f_{i}(x) \leq 0</script> 和 <script type="math/tex">h_{j}(x) = 0 </script>  ，使原本問題有解。</p>
  </li>
  <li>
    <p>Dual Feasibility：即滿足對偶問題<a href="#eq2">公式二</a>的限制條件： <script type="math/tex"> \lambda \succeq 0 </script> ，使對偶問題有解。</p>
  </li>
  <li>
    <p>Complementary Slackness：即滿足 <em>strong duality</em> ：<script type="math/tex"> g(\lambda^{\star}, \nu^{\star}) =  f_{0}(x^{\star})  </script> ，使原本問題和對偶問題有相同解。</p>
  </li>
  <li>
    <p>Stationarity（gradient of Lagrangian with respect to x vanishes）：</p>
  </li>
</ol>

<script type="math/tex; mode=display">

\nabla f_{0}(x^{\star}) + \sum_{i=1}^{m} \lambda_{i} \nabla f_{i}(x^{\star}) + \sum_{j=1}^{p} \nu_{j} \nabla h_{j}(x^{\star}) = 0

</script>

<p>第四項雖然前面沒提到，但很容易理解，也就是說，在得出最佳解 <script type="math/tex">x^{\star}</script> 的時候， <em>Lagrangian</em> 對於 <script type="math/tex">x^{\star}</script> 的 <em>gradient</em> 要等於 0 。</p>

<h2 id="reference">Reference</h2>

<p>本文參考至以下教科書，本文中的圖片也取自於以下教科書：</p>

<p>Stephen Boyd &amp; Lieven Vandenberghe. Convex Optimization. Chapter 5 Duality.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[類神經網路 -- Neural Turing Machine]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/10/26/neural-network-neural-turing-machine/"/>
    <updated>2015-10-26T16:25:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/10/26/neural-network-neural-turing-machine</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p><em>Recurrent Neural Network</em> 在進行 <em>Gradient Descent</em> 的時候，會遇到所謂的 <em>Vanishing Gradient Problem</em> ，也就是說，在後面時間點的所算出的修正量，要回傳去修正較前面時間的參數值，此修正量會隨著時間傳遞而衰減。</p>

<p>為了改善此問題，可以用類神經網路模擬記憶體的構造，把前面神經元所算出的值，儲存起來。例如： <em>Long Short-term Memory (LSTM)</em> 即是模擬記憶體讀寫的構造，將某個時間點算出的值給儲存起來，等需要用它的時候再讀出來。</p>

<p>除了模擬單一記憶體的儲存與讀寫功能之外，也可以用類神經網路的構造來模擬 <em>Turing Machine</em> ，也就是說，有個 <em>Controller</em> ，可以更精確地控制，要將什麼值寫入哪一個記憶體區塊，或讀取哪一個記憶體區塊的值，這種類神經網路模型，稱為 <em>Neural Turing Machine</em> 。</p>

<p>如果可以模擬 <em>Turing Machine</em> ，即表示可以學會電腦能做的事。也就是說，這種機器學習模型可以學會電腦程式的邏輯控制與運算。</p>

<h2 id="neural-turing-machine">Neural Turing Machine</h2>

<p><em>Neural Turing Machine</em> 的架構如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00100.jpeg" alt="Neural Turing Machine" /></p>

<!--more-->

<p>可分為幾個部分：</p>

<p><strong>Input:</strong> 從外部輸入的值。</p>

<p><strong>Output:</strong> 輸出到外部的值。</p>

<p><strong>Controller:</strong> 相當於電腦的IO和CPU，可以從外部輸入值，或從記憶體讀取值，經過運算，再將算出的結果輸出去，或寫入記憶體， <em>Controller</em> 可以用 <em>feed forward neural network</em> 或者 <em>recurrent neural network</em> （相當於有register的CPU）來模擬。</p>

<p><strong>Read/Write Head:</strong> 記憶體的讀寫頭，相當於pointer ，是要被讀取或被寫入的記憶體的address。</p>

<p><strong>Memory:</strong> 記憶體，相當於電腦的RAM，同一個地址可對應到一整排的記憶體單位，就像電腦一樣，用8個bit組成的一個byte，具有同一個memory address。</p>

<p>以下細講每一部份的數學模型。</p>

<h3 id="memory">Memory</h3>

<p><em>memory</em> 是一個二維陣列。如下圖，一個 <em>memory block</em> 是由數個 <em>memory cell</em> 所構成。同一個 <em>block</em> 中的 <em>cell</em> 有相同的 <em>address</em> 。如下圖中，共有 <script type="math/tex">n</script> 個 <em>block</em> ， 每個 <em>block</em> 有 <script type="math/tex">m</script> 個 <em>cell</em> 。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00101.jpeg" alt="" /></p>

<p>操作 <em>Memory</em> 的動作有三種：即 <em>Read</em> ， <em>Erase</em> 和 <em>Add</em> 。</p>

<h4 id="read">Read</h4>

<p><em>Read</em> 是將記憶體裡面的值，讀出來，並傳給 <em>controller</em> 。由於記憶體有很多個 <em>memory block</em> ，至於要讀取哪個，由讀寫頭（ <em>Read/Write Head</em> ）來控制，讀寫頭為一個向量 <script type="math/tex">\textbf{w}</script> ，其數值表示要讀取記憶體位置的權重，滿足以下條件：</p>

<script type="math/tex; mode=display">

\sum_{i}w(i) = 1 \\

 0 \leq w(i) \leq 1, \forall i 

</script>

<p>讀寫頭內部各元素 <script type="math/tex">w_{i}</script> 的值介於 0 到 1 之間，且加起來的和為 1 ，這可解釋為，讀寫頭存在的位置，是用機率來表示。而讀出來的值，為記憶體區塊所儲存的值，乘上讀寫頭在此區塊 <script type="math/tex">i</script> 的機率 <script type="math/tex">w(i)</script> ，所得出之期望值，如下：</p>

<script type="math/tex; mode=display">

\textbf{r} \leftarrow \sum_{i}w(i)\textbf{M}(i) \mspace{40mu} \text{(1)}

</script>

<p>其中，<script type="math/tex">\textbf{r}</script> 為 <em>Read vector</em> ，即從記憶體讀出來的值，  <script type="math/tex">\textbf{M(i)}</script> 為記憶體 <script type="math/tex">i</script> 區塊的值， 而  <script type="math/tex">w(i)</script> 為讀寫頭 <script type="math/tex">w</script> 在區塊 <script type="math/tex">i</script> 的機率。</p>

<p>例如下圖中， <script type="math/tex">w(0) = 0.9</script> ， <script type="math/tex">w(1) = 0.1</script> ，即表示，讀寫頭在位置 0 的機率為 0.9，在位置 1 的機率為 0.1 。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00102.jpeg" alt="Read" /></p>

<p>將上圖中記憶體內部的值 <script type="math/tex">\textbf{M(i)}</script> ，以及讀寫頭位置的值 <script type="math/tex">w(i)</script> ，代入公式(1)，即可得出</p>

<script type="math/tex; mode=display">

\begin{bmatrix}

      r_{0}  \\[0.3em]

      r_{1}  \\[0.3em]

      r_{2}  \\[0.3em]

    \end{bmatrix}

=\begin{bmatrix}

      1*0.9+2*0.1  \\[0.3em]

      1*0.9+1*0.1  \\[0.3em]

      2*0.9+4*0.1  \\[0.3em]

    \end{bmatrix}

＝

\begin{bmatrix}

      1.1  \\[0.3em]

      1.0  \\[0.3em]

      2.2  \\[0.3em]

    \end{bmatrix}

</script>

<h4 id="erase">Erase</h4>

<p>如果要刪除記憶體內部的值，則要進行 <em>Erase</em> ，過程跟 <em>Read</em> 類似，都需要用讀寫頭 <em>w</em> 來控制。但刪除的動作，需要控制去刪除掉哪個 <em>memory cell</em> 的值，而不是一次就把整個 <em>memory block</em> 的值都刪除。所以需要另一個 <em>erase vector</em> <script type="math/tex">\textbf{e}</script> 來選擇要被刪除的 <em>cell</em> 。 <em>erase vector</em> 為一向量，如下：</p>

<script type="math/tex; mode=display">

 0 \leq e(j) \leq 1,  \mspace{10mu} 0 \leq j \leq m-1 , \mspace{10mu} \forall j 

</script>

<p>其中， <script type="math/tex">j</script> 為一個介於 0~m-1 之間的數， m 為 <em>block size</em> 。向量元素的值 <script type="math/tex">e(j)</script> 介於 0~1 之間。如果值為1，則表示要清空這個 <em>cell</em> 的值，若為 0 則表示保留 <em>cell</em> 原本的值， <em>erase</em> 的公式如下：</p>

<script type="math/tex; mode=display">

\textbf{M}(i) \leftarrow (1-w(i) \textbf{e} ) \textbf{M}(i) \mspace{40mu} \text{(2)}


</script>

<p>其中， <script type="math/tex">\textbf{w}</script> 是用來控制要清除哪個 <em>memory block</em> 而 <script type="math/tex">\textbf{e}</script> 是要控制清除這個 <em>block</em> 裡面的哪些 <em>cell</em> ，如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00103.jpeg" alt="Erase" /></p>

<p>上圖中，根據 <script type="math/tex">\textbf{w}</script> 和 <script type="math/tex">\textbf{e}</script> 這兩個向量所選擇的結果， 在 <script type="math/tex">\textbf{M}</script> 中，共有四個 <em>cell</em> 的值被削減了，分別位於左上角和左下角，用較明亮的背景色表示其位置。</p>

<p>將上圖中 <script type="math/tex">\textbf{w}</script> 、 <script type="math/tex">\textbf{e}</script> 和 <script type="math/tex">\textbf{M}</script> 中的值，代入公式(2) ，可得出此結果，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


M= 

\begin{bmatrix}

      1(1-0.9) & 2(1-0.1) & 3 & ...  \\[0.3em]

      1 & 1 & 2 & ...  \\[0.3em]

      2(1-0.9) & 4(1-0.1) & 1 & ...  \\[0.3em]

    \end{bmatrix}

=\begin{bmatrix}

      0.1 & 1.8 & 3 & ...  \\[0.3em]

      1 & 1 & 2 & ...  \\[0.3em]

      0.2 & 3.6 & 1 & ...  \\[0.3em]

    \end{bmatrix}


 %]]&gt;</script>

<h4 id="add">Add</h4>

<p>將新的值寫入記憶體的動作為 <em>add</em> 。之所以稱為 <em>add</em> （而非 <em>write</em> ）因為這個動作是會把記憶體內原本的值，再「加上」要寫入的值。至於要把哪些值加到記憶體，則需要有一個 <em>add vector</em> ，其維度和 <em>memory block</em> 的大小 <script type="math/tex">m</script> 相同。 <em>Add</em> 的公式如下：</p>

<script type="math/tex; mode=display">

\textbf{M}(i) \leftarrow  \textbf{M}(i)  + w(i) \textbf{a} \mspace{40mu} \text{(3)}

</script>

<p>過程如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00104.jpeg" alt="Add" /></p>

<p>上圖中，位於 <script type="math/tex">M</script> 的左上角，共有四個 <em>cell</em> 的值被增加了。</p>

<p>將上圖中 <script type="math/tex">\textbf{w}</script> 、 <script type="math/tex">\textbf{a}</script> 和 <script type="math/tex">\textbf{M}</script> 中的值，代入公式(3) ，可得出此結果，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


M= 

\begin{bmatrix}

      0.1+0.9 & 1.8+0.1 & 3 & ...  \\[0.3em]

      1.0+0.9 & 1.0+0.1 & 2 & ...  \\[0.3em]

      0.2 & 3.6 & 1 & ...  \\[0.3em]

    \end{bmatrix}

=\begin{bmatrix}

      1.0 & 1.9 & 3 & ...  \\[0.3em]

      1.9 & 1.1 & 2 & ...  \\[0.3em]

      0.2 & 3.6 & 1 & ...  \\[0.3em]

    \end{bmatrix}

 %]]&gt;</script>

<h3 id="controller">Controller</h3>

<p><em>Controller</em> 為控制器，它可以用類神經網路之類的機器學習模型來代替，但其實可以把它當成是黑盒子，只要可以符合下圖中所要求的 <em>input</em> 、 <em>output</em> 以及各種參數的值，就可以當 <em>controller</em> 。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00105.jpeg" alt="Controller" /></p>

<p>上圖中， <em>controller</em> 根據外部環境的輸入值 <em>input</em>，以及 <em>read vector</em> <script type="math/tex">\textbf{r}</script> ，經過其內部運算，會輸出 <em>output</em> 值到外在環境，還有 <em>erase vector</em> <script type="math/tex">\textbf{e}</script> 和 <em>add vector</em> <script type="math/tex">\textbf{a}</script> ，來控制記憶體的清除與寫入。但還缺少了讀寫頭向量 <script type="math/tex">\textbf{w}</script> 。</p>

<p>如果要產生讀寫頭向量 <script type="math/tex">\textbf{w}</script> ， 需要透過一連串的 <em>Addressing Mechanisms</em> 的運算，最後即可得出讀寫頭位置。而 <em>controller</em> 則負責產生出 <em>Addressing Mechanisms</em> 所需的參數。</p>

<h3 id="addressing-mechanism">Addressing Mechanism</h3>

<p><em>controller</em> 會產生五個參數來進行 <em>addressing mechanisms</em> ，這些參數分別為： <script type="math/tex">\textbf{k}, \beta, g , \textbf{s}, \gamma </script> 。其中， <script type="math/tex">\textbf{k}</script> 和 <script type="math/tex">\textbf{s}</script> 為向量，其餘參數為純量，這些參數的意義，在以下篇章會解釋，整個 <em>addressing mechanisms</em>  的過程如下圖所示。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00106.jpeg" alt="Addressing Mechanism" /></p>

<p>上圖中，總共有四個步驟，這四個步驟共需要用到這五種參數，經過了這一連串的過程之後，最後所產生出的 <script type="math/tex">\textbf{w}</script> 即為讀寫頭位置，如上圖左下角所示。以下細講每個步驟在做什麼。</p>

<h4 id="content-addressing">Content Addressing</h4>

<p>首先，是找出記憶體中跟參數 <em>memory key</em> <script type="math/tex">\textbf{k}</script> 值最相近的記憶體區塊。</p>

<p>讀寫頭的位置 <script type="math/tex">w</script> ，就先根據記憶體區塊中，跟 <script type="math/tex">\textbf{k}</script> 的相似度來決定，公示如下：</p>

<script type="math/tex; mode=display">

w(i) \leftarrow \frac{e^{\beta K[\textbf{k},\textbf{M}(i)] } }{ \sum_{j} e^{ \beta K[\textbf{k},\textbf{M}(j)] } }

</script>

<p>其中， <script type="math/tex">K[\textbf{k},\textbf{M}(i)]</script> 表示 <em>memory key</em> <script type="math/tex">\textbf{k}</script> 跟記憶體區塊 <script type="math/tex">M(i)</script> 的 <em>cosine similarity</em> ，即兩向量的夾角，如果 <script type="math/tex">\textbf{k}</script> 跟 <script type="math/tex">M(i)</script> 的內容越接近的話，則 <script type="math/tex"> K[\textbf{k},\textbf{M}(i)]</script> 算出來的值會越大。 最後算出來的值 <script type="math/tex">w(i)</script> ，即是 <script type="math/tex">\textbf{k}</script> 跟 <script type="math/tex">M(i)</script> 的相似度，除以記憶體內所有區塊相似度，標準化的結果。</p>

<p><em>cosine similarity</em> 的公式如下：</p>

<script type="math/tex; mode=display">

K[\textbf{u},\textbf{v} ] = \frac{ \textbf{u} \cdot \textbf{v} }{ |\textbf{u}| \cdot |\textbf{v}| } 

</script>

<p>經過了 <em>cosine similarity</em> 後，越相似的向量，值會越大，而參數 <script type="math/tex">\beta</script> 是個大於0的參數，可用來控制 <script type="math/tex">\textbf{w}</script> 內的元素值，集中與分散程度，如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00107.jpeg" alt="Content Addressing" /></p>

<p>上圖中，向量 <script type="math/tex">\textbf{k}</script> 中的值，與記憶體中第三行區塊的值最相似（用較淺色的背景表示）。但如果 <script type="math/tex">\beta</script> 很大（例如： <script type="math/tex">\beta=50</script>），算出來的 <script type="math/tex">\textbf{w}</script> 值會集中在第三個位置，也就是說，只有第三個位置的值是1，其他都是0（用較淺色的背景表示），如上圖的左下方。如果 <script type="math/tex">\beta</script> 很小（例如： <script type="math/tex">\beta=0</script>），則算出來的 <script type="math/tex">\textbf{w}</script> 值會平均分散到每個元素之中，如上圖的右下方。 </p>

<h4 id="interpolation">Interpolation</h4>

<p>讀寫頭其實也是有「記憶」的，也就是說，目前時間點的 <script type="math/tex">\textbf{w}_{t} </script> ，也可能會受到上個時間點 <script type="math/tex">\textbf{w}_{t-1}</script> 的影響，要達到這樣的效果，就是用 <em>content addressing</em> 所算出的值 <script type="math/tex">\textbf{w}_{t} </script> ，和上個時間點的讀寫頭位置 <script type="math/tex">\textbf{w}_{t-1}</script> 做 <em>interpolation</em> ，公式如下：</p>

<script type="math/tex; mode=display">

\textbf{w}_{t} \leftarrow g \textbf{w}_{t} + (1-g) \textbf{w}_{t-1} 

</script>

<p>其中，參數 <script type="math/tex">g</script> 用來表示 <script type="math/tex"> \textbf{w} </script> 有多少比例是這個時間點 <em>content addressing</em> 所算出的值，還是上個時間點的值。如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00108.jpeg" alt="Interpolation" /></p>

<p>如果 <script type="math/tex">g=1</script> ，則 <script type="math/tex">\textbf{w}</script> 的值會完全取決於這個時間點 <em>content addressing</em> 所算出的值，如上圖的左下方，若 <script type="math/tex">g=0</script> ，  <script type="math/tex">\textbf{w}</script> 會完全取決於上個時間點的值，如上圖的右下方。</p>

<h4 id="convolutional-shift">Convolutional Shift</h4>

<p>如果要讓讀寫頭的位置可以稍微往左或往右移動，這就要用 <em>Convolutional Shift</em> 來做調整。 參數 <script type="math/tex">\textbf{s}</script> 是一個向量，用 <em>convolutional shift</em> ，來將 <script type="math/tex">\textbf{w}</script> 的值往左或往右平移，公式如下：</p>

<script type="math/tex; mode=display">

w(i) \leftarrow \sum_{j} w(j) s(i-j)

</script>

<p>舉個例子，如果 <script type="math/tex">\textbf{s}</script> 中有三個元素：<script type="math/tex">s_{-1}, s_{0}, s_{1}</script> ，則 <script type="math/tex">w(i)</script> 經過了以上公式後，結果如下：</p>

<script type="math/tex; mode=display">

w(i) \leftarrow w(i+1) s(-1) + w(i)s(0) + w(i-1)s(1)

</script>

<p>根據此公式， <script type="math/tex">w(i)</script> 的值，如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00109.jpeg" alt="Convolutional" /></p>

<p>也就是說， <script type="math/tex">s_{-1}=1</script> 時，可讓原本的 <script type="math/tex">w_{i+1}</script> 往左移一格，移到 <script type="math/tex">w_{i}</script> ，若 <script type="math/tex">s_{1}=1</script> 時，可讓原本的 <script type="math/tex">w_{i-1}</script> 往右移一格，移到 <script type="math/tex">w_{i}</script> 。</p>

<p>舉個例子，如果 <script type="math/tex">s_{-1} = 1, s_{0}=0, s_{1} = 0</script> ，則  <script type="math/tex">\textbf{w}</script> 中的值，全部往左移動一格，若碰到邊界則再循環到最右邊，如下圖左方所示。 如果 <script type="math/tex">s_{-1} = 0, s_{0}=0, s_{1} = 1</script> ，則  <script type="math/tex">\textbf{w}</script> 中的值，全部往右移動一格。若 <script type="math/tex">s_{-1} = 0.5, s_{0}=0, s_{1} = 0.5</script> ，則 <script type="math/tex">\textbf{w}</script> 為往左和往右移動後的平均，如下圖右方所示。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00110.jpeg" alt="Convolutional Shift" /></p>

<h4 id="sharpening">Sharpening</h4>

<p>此過程是再一次調整 <script type="math/tex">\textbf{w}</script> 的集中與分散程度，公示如下：</p>

<script type="math/tex; mode=display">

w(i) \leftarrow \frac{w(i)^{\gamma}}{\sum_{j}w(j)^{\gamma}}

</script>

<p>其中， <script type="math/tex">\gamma</script> 的功能和 <em>Content Addressing</em> 中的 <script type="math/tex">\beta</script> 是一樣的，但是經過了接下來的 <em>Interpolation</em> 跟 <em>Convolutional Shift</em> 之後，<script type="math/tex">\textbf{w}</script> 裡面的集中度又會改變，所以要再重新調整一次。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00111.jpeg" alt="Sharpening" /></p>

<h2 id="experiment-repeat-copy">Experiment: Repeat Copy</h2>

<p>關於 <em>Neural Turing Machine</em> 的學習能力，可以參考以下例子。</p>

<p>在訓練資料中，給定一個區塊的 <em>data</em> （如下圖左上角紅色區塊）做為 <em>input data</em> ，將這個區塊複製成七份，做為 <em>output data</em> 。則 <em>Neural Turing Machine</em> 有辦法學會這個「複製」過程所需的運算程序，也就是重複跑七次輸出一樣的東西。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00112.png" alt="Experiment" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00113.png" alt="Experiment" /></p>

<p>從上圖中，可看到讀寫頭的移動，重複走了相同的路徑，走了七次，依序將記憶體中儲存的 <em>input data</em> 的值，讀出來並輸出到 <em>output</em> 。</p>

<p>有個完整的  <em>Neural Turing Machine</em> 套件，以及此實驗的相關程式碼於：https://github.com/fumin/ntm</p>

<h2 id="reference">Reference</h2>

<p><a href="http://arxiv.org/abs/1410.5401">Alex Graves, Greg Wayne, Ivo Danihelka. Neural Turing Machines. 2014</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[類神經網路 -- Recurrent Neural Network]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/06/06/neural-network-recurrent-neural-network/"/>
    <updated>2015-06-06T09:32:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/06/06/neural-network-recurrent-neural-network</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>在數位電路裡面，如果一個電路沒有 <em>latch</em> 或 <em>flip flop</em> 這類的元件，它的輸出值只會取決於目前的輸入值，和上個時間點的輸入值是無關的，這種的電路叫作 <em>combinational circuit</em> 。</p>

<p>對於類神經網路而言，如果它的值只是從輸入端一層層地依序傳到輸出端，不會再把值從輸出端傳回輸入端，這種神經元就相當於 <em>combinational circuit</em> ，也就是說它的輸出值只取決於目前時刻的輸入值，這樣的類神經網路稱為 <em>feedforward neural network</em> 。</p>

<p>如果一個電路有 <em>latch</em> 或 <em>flip flop</em> 這類的元件，它的輸出值就跟上個時間點的輸入值有關，這種的電路它稱為 <em>sequential circuit</em> 。</p>

<p>所謂的 <em>Recurrent Neural Network</em> ，是一種把輸出端再接回輸入端的類神經網路，這樣可以把上個時間點的輸出值再傳回來，記錄在神經元中，達成和 <em>latch</em> 類似的效果，使得下個時間點的輸出值，跟上個時間點有關，也就是說，這樣的神經網路是有 <em>記憶</em> 的。</p>

<h2 id="recurrent-neural-network">Recurrent Neural Network</h2>

<p>由一個簡單神經元所構成的 <em>Recurrent Neural Network</em> ，構造如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00095.png" alt="" /></p>

<p>這個神經元在 <script type="math/tex">t</script> 時間，訓練資料的輸入值為 <script type="math/tex">x_{t}</script> ，訓練資料的答案為 <script type="math/tex">y_{t}</script> ，神經元 <script type="math/tex">n</script> 的輸出值 <script type="math/tex">n_{out,t}</script> ，可用以下公式表示：</p>

<!--more-->

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{aligned}

& n_{in,t} = w_{c}x_{t}+ w_{p}n_{out,t-1} + w_{b} \\

& n_{out,t} = \frac{1}{1+e^{-n_{in,t}}} \\

\end{aligned}

 %]]&gt;</script>

<p>其中， <script type="math/tex">n_{in,t}</script> 為輸入神經元 <script type="math/tex">n</script> 的值， <script type="math/tex">w_{c}</script> 是給目前的時間(current)時，輸入值 <script type="math/tex">x_{t}</script> 的權重， <script type="math/tex">w_{p}</script> 是給上個時間點(previous)時，輸出值 <script type="math/tex">n_{out,t-1}</script> 的權重，而 <script type="math/tex">w_{b}</script> 為 <em>bias</em> 。從上圖可看出，紫色的線將神經網路的輸出端 <script type="math/tex">n_{out}</script> 連回輸入端 <script type="math/tex">n_{in}</script> ，使得於時間 <script type="math/tex">t</script> 的輸出值跟上個時間點 <script type="math/tex">t-1</script> 的輸出值有關。</p>

<p>可以把這個神經元從時間點 <script type="math/tex">0</script> 到時間點 <script type="math/tex">t</script> 的運算，展開成下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00096.png" alt="" /></p>

<p>從上圖，最左邊開始，依序將 <script type="math/tex">x_{0},x_{1},...,x_{t}</script> 輸入神經元 <script type="math/tex">n</script> ，而依序得出的值為 <script type="math/tex">n_{out,0},n_{out,1},...,n_{out,t}</script> 。神經元 <script type="math/tex">n</script> 在時間點 <script type="math/tex">t-1</script> 的輸出值 <script type="math/tex">n_{out,t-1}</script> ，會接到時間點 <script type="math/tex">t</script> 時的輸入值 <script type="math/tex">n_{in,t}</script> 。 </p>

<h2 id="training-recurrent-neural-network">Training Recurrent Neural Network</h2>

<p>訓練 <em>recurrent neural network</em> 的方法，和訓練 <em>feedforward neural network</em> 的方法一樣，都可以用 <em>back propagation</em> 。但是在 <em>recurrent neural network</em> 中，要依據時間順序，將值從最後一個時間點，回傳到第一個時間點。</p>

<p>在時間點 <script type="math/tex">t</script> 時的 <em>cost function</em> 為：</p>

<script type="math/tex; mode=display">

J=−y_{t}log(n_{out,t})−(1−y_{t})log(1−n_{out,t})

</script>

<p>計算 <em>recurrent neural network</em> 的 <em>back propagation</em> 要分為兩部分來算，先算好時間點位於 <script type="math/tex">t</script> 的偏微分 <script type="math/tex">\dfrac{\partial J}{ \partial n_{in} } </script> 值，再依序往前算出時間點 <script type="math/tex">t</script> 之前的偏微分值，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\dfrac{\partial J}{ \partial n_{in,s} } = 

\begin{cases} 

(\dfrac{\partial J}{ \partial n_{out,s} } )

(\dfrac{ \partial n_{out,s}}{\partial n_{in,s} } ) & \text{if } s = t \\

(\dfrac{\partial J}{ \partial n_{in,s+1} } )

(\dfrac{ \partial n_{in,s+1}}{\partial n_{out,s} } )

(\dfrac{ \partial n_{out,s}}{\partial n_{in,s} } )

  & \text{otherwise}

\end{cases}

 %]]&gt;</script>

<p>其中， <script type="math/tex">s</script> 為 <script type="math/tex">0</script> 到 <script type="math/tex">t</script> 中的其中一個時間點。用 <a href="http://ckmarkoh.github.io/blog/2015/05/28/neural-network-backward-propagation">Backward Propagation 詳細推導過程</a> 所提到的推導方法，可推導出 <script type="math/tex"> (\dfrac{\partial J}{ \partial n_{out,s} } )(\dfrac{ \partial n_{out,s}}{\partial n_{in,s} } )</script> 、 <script type="math/tex">(\dfrac{ \partial n_{in,s+1}}{\partial n_{out,s} } )</script> 與 <script type="math/tex">(\dfrac{ \partial n_{out,s}}{\partial n_{in,s} } )</script> 的值，並令 <script type="math/tex"> \delta_{in,s} = \dfrac{\partial J}{ \partial n_{in,s} } </script> 代入以上公式，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

\delta_{in,s}  = 

\begin{cases} 

n_{out,s} - y_{s}  & \text{if } s = t \\

\delta_{in,s+1} w_{p} n_{out,s} (1-n_{out,s})  & \text{otherwise}

\end{cases}

\end{align}

 %]]&gt;</script>

<p>此公是可分為兩部分，當 <script type="math/tex">s = t</script> 時，與  <script type="math/tex">s\neq t</script> 時。計算 <script type="math/tex">\delta_{s}</script> 的方式不同。</p>

<p>在 <script type="math/tex">s = t</script> 時， <script type="math/tex">\delta_{s}</script> 的傳遞過程就如同 <em>feedforward neural network</em> ，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00097.png" alt="" /></p>

<p>若 <script type="math/tex">s\neq t</script> 時， 要算 <script type="math/tex">\delta_{in,s}</script> 之前，要先從 <script type="math/tex">s+1</script> 時間點將 <script type="math/tex">\delta_{in,s+1}</script> 傳遞過來，傳遞過程如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00098.png" alt="" /></p>

<p>因為需要把 <script type="math/tex">\delta</script> 從後面的時間點往前面傳，故這個過程又稱為 <em>back propagation through time</em> 。</p>

<p>於時間點 <script type="math/tex">s</script> 計算完 <script type="math/tex">\delta</script> 後，用以下公式將 <script type="math/tex">s</script> 時間點算出的偏微分值，更新到神經元的權重：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{c} \leftarrow w_{c} - 

\eta \dfrac{\partial J}{ \partial n_{in,s} } \dfrac{\partial n_{in,s}}{\partial w_{c}} \\ 

& w_{b} \leftarrow w_{b} - 

\eta \dfrac{\partial J}{ \partial n_{in,s} } \dfrac{\partial n_{in,s}}{\partial w_{b}} \\

& w_{p} \leftarrow w_{p} - 

\eta \dfrac{\partial J}{ \partial n_{in,s} } \dfrac{\partial n_{in,s}}{\partial w_{p}}  \\

\end{align}

 %]]&gt;</script>

<p>用 <a href="http://ckmarkoh.github.io/blog/2015/05/28/neural-network-backward-propagation">Backward Propagation 詳細推導過程</a> ，求出 <script type="math/tex">\dfrac{\partial n_{in,s}}{\partial w_{c}}</script> 、 <script type="math/tex">\dfrac{\partial n_{in,s}}{\partial w_{b}} </script> 和 <script type="math/tex">\dfrac{\partial n_{in,s}}{\partial w_{p}}</script> 的值，並換成用 <script type="math/tex">\delta_{in,s}</script> 代替 <script type="math/tex">\dfrac{\partial J}{ \partial n_{in,s} } </script> 代入以上公式，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{c} \leftarrow w_{c} - \eta \delta_{in,s} x_{s} \\ 

& w_{b} \leftarrow w_{b} - \eta \delta_{in,s} \\

& w_{p} \leftarrow w_{p} - \eta \delta_{in,s} n_{out,s-1} \\

\end{align}

 %]]&gt;</script>

<p>此過程如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00099.png" alt="" /></p>

<h2 id="implementation">Implementation</h2>

<p>再來是實作的部分，以下是個簡單的應用，用 <em>Recurrent Neural Network</em> 來預測一個字串序列中，下一個可能出現的字是什麼。例如，給定以下字串：</p>

<figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">001001001
</span></code></pre></td></tr></table></div></figure>

<p>根據這個字串的特徵，如果連續出現了兩個 <em>0</em> ，可以預測下個出現的為 <em>1</em> ，若前面兩個字為 <em>10</em> 則可預測下個出現的自為 <em>0</em> ，以此類推。</p>

<p>以下為實作部分：</p>

<figure class="code"><figcaption><span>simple_rnn.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">e</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">random</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span><span class="line">    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">e</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">x</span><span class="p">))</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">my_rand</span><span class="p">():</span>
</span><span class="line">    <span class="k">return</span> <span class="n">random</span><span class="p">()</span><span class="o">-</span><span class="mf">0.5</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">rnn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span><span class="line">    <span class="n">r</span> <span class="o">=</span> <span class="mf">0.05</span>
</span><span class="line">    <span class="n">w_p</span><span class="p">,</span> <span class="n">w_c</span><span class="p">,</span> <span class="n">w_b</span> <span class="o">=</span> <span class="n">my_rand</span><span class="p">(),</span><span class="n">my_rand</span><span class="p">(),</span><span class="n">my_rand</span><span class="p">()</span>
</span><span class="line">    <span class="n">l</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line">    <span class="n">n_in</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">l</span>
</span><span class="line">    <span class="n">n_out</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">l</span>
</span><span class="line">    <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
</span><span class="line">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
</span><span class="line">            <span class="n">n_in</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">w_c</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">w_p</span> <span class="o">*</span> <span class="n">n_out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">w_b</span>
</span><span class="line">            <span class="n">n_out</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">n_in</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span><span class="line">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
</span><span class="line">            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
</span><span class="line">                <span class="n">k</span> <span class="o">=</span>  <span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="n">j</span><span class="p">)</span>
</span><span class="line">                <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">                    <span class="n">d_c</span> <span class="o">=</span> <span class="n">n_out</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
</span><span class="line">                <span class="k">else</span><span class="p">:</span>
</span><span class="line">                    <span class="n">d_c</span> <span class="o">=</span> <span class="n">w_p</span> <span class="o">*</span> <span class="n">n_out</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">n_out</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">d_c</span>
</span><span class="line">                <span class="n">w_c</span> <span class="o">=</span> <span class="n">w_c</span> <span class="o">-</span> <span class="n">r</span> <span class="o">*</span> <span class="n">d_c</span> <span class="o">*</span> <span class="n">x</span> <span class="p">[</span><span class="n">k</span><span class="p">]</span>
</span><span class="line">                <span class="n">w_b</span> <span class="o">=</span> <span class="n">w_b</span> <span class="o">-</span> <span class="n">r</span> <span class="o">*</span> <span class="n">d_c</span>
</span><span class="line">                <span class="n">w_p</span> <span class="o">=</span> <span class="n">w_p</span> <span class="o">-</span> <span class="n">r</span> <span class="o">*</span> <span class="n">d_c</span> <span class="o">*</span> <span class="n">n_out</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
</span><span class="line">        <span class="n">n_in</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">w_c</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">w_p</span> <span class="o">*</span> <span class="n">n_out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">w_b</span>
</span><span class="line">        <span class="n">n_out</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">n_in</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span><span class="line">    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">n_out</span><span class="p">):</span>
</span><span class="line">        <span class="k">print</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>其中， <code>x</code> 為輸入的序列， <code>n_out</code> 為神經元預測的結果。進行這個演算法之前，首先，先給權重 <code>w_p, w_c, w_b</code> 的初始值用介於 <em>-0.5~0.5</em> 之間的隨機值。再來是進行訓練過程，用 <em>for loop</em> 進行了 <em>10000</em> 次的訓練，在每次的訓練過程中，先進行 <em>forward propagation</em> 依時間順序，算出每個時間點的 <code>n_out</code> 。再來是用 <em>back propagation through time</em> 來更新 <code>w_p, w_c, w_b</code> 的值。訓練完後，進行一次 <em>forward propogation</em> 用訓練過程得出的權重來預測序列的下一個字，並將預測結果印出。</p>

<p>到 <em>interactive mode</em> 執行以下程式，輸入序列 <em>001001001</em> 。</p>

<figure class="code"><figcaption><span>simple_rnn.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">smallrnn</span>
</span><span class="line"><span class="o">&gt;&gt;&gt;</span> <span class="n">smallrnn</span><span class="o">.</span><span class="n">rnn</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line"><span class="mi">0</span> <span class="mi">0</span>
</span><span class="line"><span class="mi">0</span> <span class="mf">0.203697155215</span>
</span><span class="line"><span class="mi">1</span> <span class="mf">0.93315712478</span>
</span><span class="line"><span class="mi">0</span> <span class="mf">0.00354811782422</span>
</span><span class="line"><span class="mi">0</span> <span class="mf">0.215230877663</span>
</span><span class="line"><span class="mi">1</span> <span class="mf">0.945971073339</span>
</span><span class="line"><span class="mi">0</span> <span class="mf">0.00455854449755</span>
</span><span class="line"><span class="mi">0</span> <span class="mf">0.218600850027</span>
</span><span class="line"><span class="mi">1</span> <span class="mf">0.949254861129</span>
</span></code></pre></td></tr></table></div></figure>

<p>左側為輸入序列，右側為預測的結果，可以發現 <em>recurrent neural network</em> 可以預測出下個字可能會是 <em>0</em> 還是 <em>1</em> 。當左側為 <em>1</em> 時，右側的數字會接近於 <em>1</em> 。</p>

<h2 id="further-reading">Further Reading</h2>

<p>關於 <em>recurrent neural network</em> 可參考 coursera 課程 Geoffrey Hinton. Neural Networks for Machine Learning</p>

<p>https://www.coursera.org/course/neuralnets</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[類神經網路 -- Backward Propagation 詳細推導過程]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/05/28/neural-network-backward-propagation/"/>
    <updated>2015-05-28T07:47:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/05/28/neural-network-backward-propagation</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>在做 <a href="http://ckmarkoh.github.io/blog/2014/03/15/logisti-regression-model">Logistic Regression</a>的時候，可以用 <em>gradient descent</em> 來做訓練，而類神經網路本身即是很多層的 <em>Logistic Regression</em> 所構成，也可以用同樣方法來做訓練。</p>

<p>但類神經網路在訓練過程時，需要分為兩個步驟，為： <em>Forward Phase</em> 與 <em>Backward Phase</em> 。 也就是要先從 <em>input</em> 把值傳到 <em>output</em>，再從 <em>output</em> 往回傳遞 <em>error</em> 到每一層的神經元，去更新層與層之間權重的參數。</p>

<h2 id="forward-phase">Forward Phase</h2>

<p>在 <em>Forward Phase</em> 時，先從 <em>input</em> 將值一層層傳遞到 <em>output</em>。</p>

<p>對於一個簡單的神經元 <script type="math/tex">n</script> ，如下圖 <a name="pic1">＜圖一＞</a>：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00086.png" alt="" /></p>

<p>將一筆訓練資料 <script type="math/tex">x_{1},x_{2}</script> 和 <em>bias</em> <script type="math/tex">b</script> 輸入到神經元 <script type="math/tex">n</script> 到輸出的過程，分成兩步，分別為 <script type="math/tex">n_{in}</script>， <script type="math/tex">n_{out}</script> ，過程如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& n_{in} = w_{1} x_{1}+w_{2}  x_{2}+w_{b} \\

& n_{out} = \frac{1} {1+e^{-n_{in} } }

\end{align}

 %]]&gt;</script>

<!--more-->

<p>在輸入神經元時， <script type="math/tex">n_{in}</script> 先將 <em>input</em> 值和其權重作乘積。</p>

<p>在輸出神經元時， <script type="math/tex">n_{out}</script> 將 <script type="math/tex">n_{in}</script> 的值用 <em>sigmoid function</em> 轉成值範圍從 <em>0</em> 到 <em>1</em> 的函數。</p>

<p>傳遞到 <script type="math/tex">n_{out}</script> 後，可與訓練資料的答案 <script type="math/tex">y</script> 用 <em>cost function</em> 來計算其差值，並用 <em>backward propagation</em> 修正權重 <script type="math/tex">w_{1}</script> 、 <script type="math/tex">w_{2}</script> 和 <script type="math/tex">w_{b}</script> 。</p>

<p>對於一個簡單的類神經網路，共有兩層，四個神經元，如下圖<a name="pic2">＜圖二＞</a>：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00087.png" alt="" /></p>

<p>其值傳遞的過程如下：</p>

<p>1.把 <script type="math/tex">x</script> 和 <script type="math/tex">y</script> 和 <em>bias</em> <script type="math/tex">b</script> 傳入到第一層神經元 <script type="math/tex">n_{11}</script> 及 <script type="math/tex">n_{12}</script> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& n_{11(in)} = w_{11,x} x+w_{11,y} y+w_{11,b} \\

& n_{12(in)} = w_{12,x} x+w_{12,y} y+w_{12,b} \\

& n_{11(out)} = \frac{1} {1+e^{-n_{11(in)} } } \\

& n_{12(out)} = \frac{1} {1+e^{-n_{12(in)} } } \\

\end{align}

 %]]&gt;</script>

<p>其中，<script type="math/tex">n_{11(in)}</script> 表示傳入神經元 <script type="math/tex">n_{11}</script> 的值，而 <script type="math/tex">n_{11(out)}</script> 表示傳出神經元 <script type="math/tex">n_{11}</script> 的值，而 <script type="math/tex">w_{11,x}</script> 表示值從 <script type="math/tex">x</script> 傳入 <script type="math/tex">n_{11}</script> 時，所乘上的權重</p>

<p>2.第一層神經元將其輸出值 <script type="math/tex"> n_{11(out)}</script> 和 <script type="math/tex">n_{12(out)}</script> 傳到第二層神經元 <script type="math/tex"> n_{21}</script> 和 <script type="math/tex"> n_{22}</script>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& n_{21(in)} = w_{21,11} n_{11(out)} + w_{21,12} n_{12(out)}+w_{21,b} \\

& n_{22(in)} = w_{22,11} n_{11(out)} + w_{22,12} n_{12(out)}+w_{22,b} \\

& n_{21(out)} = \frac{1} {1+e^{-n_{21(in)} } } \\

& n_{22(out)} = \frac{1} {1+e^{-n_{22(in)} } } \\

\end{align}

 %]]&gt;</script>

<p>傳遞完後，可與訓練資料的答案 <script type="math/tex">z_{1}</script> 和 <script type="math/tex">z_{2}</script> 用 <em>cost function</em> 來計算其差值，並用 <em>backward propagation</em> 修正權重。</p>

<h2 id="derivation-of-gradient-descent">Derivation of Gradient Descent</h2>

<p>在講解 <em>backward Phase</em> 之前，先推導類神經網路的 <em>gradient descent</em> 公式和 <em>backward propagation</em> 的原理：</p>

<p>對於<a href="#pic1">＜圖一＞</a>中的一個簡單的神經元 <script type="math/tex">n</script>，將一筆訓練資料 <script type="math/tex">x_{1},x_{2}</script> 傳遞到 <script type="math/tex">n_{out}</script> 所得出的值和 <script type="math/tex">y</script> 的值做比較，我們可用以下的 <em>cost function</em> 來計算：</p>

<script type="math/tex; mode=display">

J =- y\times log(n_{out}) - (1-y)\times log (1 -n_{out} ) 

</script>

<p>從以上 <em>cost function</em> 可得知，如果 <script type="math/tex">n_{out}</script> 和 <script type="math/tex">y</script> 都等於 <em>0</em> ，或者都等於 <em>1</em> ，則 <em>cost</em> 會是 <em>0</em> ，若 <script type="math/tex">n_{out}</script> 和 <script type="math/tex">y</script> 其中有一個是 <em>1</em> ，而另一個是 <em>0</em> ，則 <em>cost</em> 會趨近於無限大。</p>

<p>用 <em>gradient Descent</em> 調整 <script type="math/tex">w_{1}</script> 、 <script type="math/tex">w_{2} </script> 和 <script type="math/tex">w_{b}</script> 來做訓練時，可用以下公式<a name="eq1">＜公式一＞</a>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{1} \leftarrow w_{1} - \eta \dfrac{\partial J}{\partial w_{1}} \\

& w_{2} \leftarrow w_{2} - \eta \dfrac{\partial J}{\partial w_{2}} \\

& w_{b} \leftarrow w_{b} - \eta \dfrac{\partial J}{\partial w_{b}} \\

\end{align}

 %]]&gt;</script>

<p>其中， <script type="math/tex">\eta</script> 為 <em>learning rate</em> ，用來控制訓練的速度。</p>

<p>接著要推導這個公式怎麼算，首先，將 <script type="math/tex">\dfrac{\partial J}{\partial w_{1}}</script> 的微分用 <em>chain rule</em> 展開，如下 <a name="eq2">＜公式二＞</a>：</p>

<script type="math/tex; mode=display">

\dfrac{\partial J}{\partial w_{1}}

= \dfrac{\partial J}{\partial n_{out}} 

 \dfrac{\partial n_{out}}{\partial n_{in}}

 \dfrac{\partial n_{in}}{\partial w_{1}}

</script>

<p>以上公式，總共有 <script type="math/tex">\dfrac{\partial J}{\partial n_{out}} </script> 、 <script type="math/tex">\dfrac{\partial n_{out}}{\partial n_{in}}</script> 與 <script type="math/tex">\dfrac{\partial n_{in}}{\partial w_{1}}</script> 三個部份的微分要算。</p>

<p>1.<script type="math/tex">\dfrac{\partial J}{\partial n_{out}} </script>：</p>

<script type="math/tex; mode=display">

 \dfrac{\partial J}{\partial n_{out}} 

 = -y \dfrac{\partial log(n_{out})}{\partial n_{out}}- (1-y) \dfrac{\partial log (1 -n_{out} )}{\partial n_{out}} \\

 = -\frac{y}{n_{out}}+\frac{1-y}{1-n_{out}}

</script>

<p>2.<script type="math/tex">\dfrac{\partial n_{out}}{\partial n_{in}}</script>：</p>

<script type="math/tex; mode=display">

\dfrac{\partial n_{out}}{\partial n_{in}} = \dfrac{\partial}{\partial n_{in}}(\frac{1}{1+e^{-n_{in}}})

=\frac{e^{-n_{in}}}{(1+e^{-n_{in}})^{2}} 

= \frac{1}{1+e^{-n_{in}}}  \frac{e^{-n_{in}}}{1+e^{-n_{in}}} \\

= n_{out}(1- n_{out})


</script>

<p>3.<script type="math/tex">\dfrac{\partial n_{in}}{\partial w_{1}}</script>：</p>

<script type="math/tex; mode=display">

\dfrac{\partial n_{in}}{\partial w_{1}} = \dfrac{\partial}{\partial w_{1}} (w_{1} x_{1}+w_{2} x_{2}+w_{b}) \\

= x_{1}

</script>

<p>代入以上三個結果到<a href="#eq2">＜公式二＞</a>，可得出 <script type="math/tex">\dfrac{\partial J}{\partial w_{1}}</script> 的值，如下：</p>

<script type="math/tex; mode=display">

\dfrac{\partial J}{\partial w_{1}} 

= \dfrac{\partial J}{\partial n_{out}} 

 \dfrac{\partial n_{out}}{\partial n_{in}}

 \dfrac{\partial n_{in}}{\partial w_{1}} \\

 = (-\frac{y}{n_{out}}+\frac{1-y}{1-n_{out}})  n_{out}(1- n_{out})  x_{1} \\

 = (n_{out}-y) x_{1}

</script>

<p>同理可得出 <script type="math/tex"> \dfrac{\partial J}{\partial w_{2}} </script> 與 <script type="math/tex">\dfrac{\partial J}{\partial w_{b}} </script> 的值，分別為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \dfrac{\partial J}{\partial w_{2}} 

= \dfrac{\partial J}{\partial n_{out}} 

 \dfrac{\partial n_{out}}{\partial n_{in}}

 \dfrac{\partial n_{in}}{\partial w_{2}}

=(n_{out}-y) x_{2} \\

& \dfrac{\partial J}{\partial w_{b}} 

= \dfrac{\partial J}{\partial n_{out}} 

 \dfrac{\partial n_{out}}{\partial n_{in}}

 \dfrac{\partial n_{in}}{\partial w_{b}}

= (n_{out}-y)

\end{align}

 %]]&gt;</script>

<p>其中， <script type="math/tex">\dfrac{\partial n_{in}}{\partial w_{b}}</script> 的結果為：</p>

<script type="math/tex; mode=display">

\dfrac{\partial n_{in}}{\partial w_{b}} = \dfrac{\partial}{\partial w_{b}} (w_{1} x_{1}+w_{2} x_{2}+w_{b}) = 1


</script>

<p>將 <script type="math/tex"> \dfrac{\partial J}{\partial w_{1}}</script> 、 <script type="math/tex"> \dfrac{\partial J}{\partial w_{2}} </script> 和 <script type="math/tex"> \dfrac{\partial J}{\partial w_{b}} </script> 的結果代入<a href="#eq1">＜公式一＞</a>，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{1} \leftarrow w_{1} - \eta (n_{out}-y) x_{1} \\

& w_{2} \leftarrow w_{2} - \eta (n_{out}-y) x_{2}  \\

& w_{b} \leftarrow w_{b} - \eta (n_{out}-y) \\

\end{align}

 %]]&gt;</script>

<h2 id="derivation-of-backward-propagation">Derivation of Backward Propagation</h2>

<p>若要推導超過一層的類神經網路的 <em>gradient descent</em> 公式，就要用到 <em>backward propagation</em> 。</p>

<p>對於<a href="#pic2">＜圖二＞</a>中的一個簡單的類神經網路，它的 <em>cost function</em> 如下：</p>

<script type="math/tex; mode=display">

J =-( z_{1}\times log(n_{21(out)}) + (1-z_{1})\times log (1 -n_{21(out)} ) + z_{2}\times log(n_{22(out)}) + (1-z_{2})\times log (1 -n_{22(out)} ))

</script>

<p>對於最後一層與倒數第二層之間的權重改變，可用 <em>gradient descent</em> ，如下<a name="eq3">＜公式三＞</a>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{21,11} \leftarrow w_{21,11} - \eta \dfrac{\partial J}{\partial w_{21,11}} \\

& w_{21,12} \leftarrow w_{21,12} - \eta \dfrac{\partial J}{\partial w_{21,12}} \\

& w_{21,b} \leftarrow w_{21,b} - \eta \dfrac{\partial J}{\partial w_{21,b}} \\

\end{align}

 %]]&gt;</script>

<p>可用先前推導出單一神經元時的微分結果，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}


& \dfrac{\partial J}{\partial w_{21,11}} 

= \dfrac{\partial J}{\partial n_{21(out)}} 

 \dfrac{\partial n_{21(out)}}{\partial n_{21(in)}}

 \dfrac{\partial n_{21(in)}}{\partial w_{21,11}}

= (n_{21(out)}-z_{1}) n_{11(out)} \\


& \dfrac{\partial J}{\partial w_{21,12}} 

= \dfrac{\partial J}{\partial n_{21(out)}} 

 \dfrac{\partial n_{21(out)}}{\partial n_{21(in)}}

 \dfrac{\partial n_{21(in)}}{\partial w_{21,12}}

= (n_{21(out)}-z_{1})n_{12(out)} \\


& \dfrac{\partial J}{\partial w_{21,b}} 

= \dfrac{\partial J}{\partial n_{21(out)}} 

 \dfrac{\partial n_{21(out)}}{\partial n_{21(in)}}

 \dfrac{\partial n_{21(in)}}{\partial w_{21,b}}

= (n_{21(out)}-z_{1})


\end{align}

 %]]&gt;</script>

<p>同理可求出 <script type="math/tex">w_{22,11}</script> 、 <script type="math/tex">w_{22,12}</script> 和 <script type="math/tex">w_{22,b}</script> 相對應的公式。</p>

<p>在要推導更往前一層的權重變化公式之前，先觀察以上公式，發現它們有共同的部分： <script type="math/tex">n_{21(out)}-z_{1}</script> ，可以用 <script type="math/tex">\delta_{21(in)}</script> 來表示這個值，即：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \delta_{21(in)} = \dfrac{\partial J}{\partial n_{21(out)}} 

 \dfrac{\partial n_{21(out)}}{\partial n_{21(in)}}

= n_{21(out)}-z_{1} \\

& \dfrac{\partial J}{\partial w_{21,11}} = \delta_{21(in)}  n_{11(out)}\\

& \dfrac{\partial J}{\partial w_{21,12}} = \delta_{21(in)}  n_{12(out)}\\

& \dfrac{\partial J}{\partial w_{21,b}} = \delta_{21(in)}


\end{align}

 %]]&gt;</script>

<p><script type="math/tex">\delta_{21(in)}</script> 的物理意義如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00088.png" alt="" /></p>

<p>圖中， <script type="math/tex">\delta_{21(out)}</script> 是 <script type="math/tex">J</script> 在神經元 <script type="math/tex">n_{21}</script> 輸出點的微分值，可以把 <script type="math/tex">\delta_{21(in)}</script> 看成是 <script type="math/tex">\delta_{21(out)}</script> <strong>從神經元</strong> <script type="math/tex">n_{21}</script> <strong>的輸出點往回傳到輸入點</strong>，即乘上 <script type="math/tex">\dfrac{\partial n_{21(out)}}{\partial n_{21(in)}}</script> 。因此，這過程又稱為 <em>backward propagation</em> 。</p>

<p>將 <script type="math/tex">\delta_{21(in)}</script> 置換到<a href="#eq3">＜公式三＞</a>，得出這一層推導的最後結果：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{21,11} \leftarrow w_{21,11} - \eta  \delta_{21(in)}  n_{11(out)} \\

& w_{21,12} \leftarrow w_{21,12} - \eta  \delta_{21(in)}  n_{12(out)} \\

& w_{21,b} \leftarrow w_{21,b} - \eta  \delta_{21(in)} \\

\end{align}

 %]]&gt;</script>

<p>同理， <script type="math/tex">w_{22,11},w_{22,12}, w_{22,b} </script> 的 <em>gradient descent</em> 公式，也可用相同方法推導出來：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{22,11} \leftarrow w_{22,11} - \eta  \delta_{22(in)}  n_{11(out)} \\

& w_{22,12} \leftarrow w_{22,12} - \eta  \delta_{22(in)}  n_{12(out)} \\

& w_{22,b} \leftarrow w_{22,b} - \eta  \delta_{22(in)} \\

\end{align}

 %]]&gt;</script>

<p>再來，要推導更往前一層的權重變化公式，要用 <em>gradient descent</em> <a name="eq4">＜公式四＞</a>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{11,x} \leftarrow w_{11,x} - \eta \dfrac{\partial J}{\partial w_{11,x}} \\

& w_{11,y} \leftarrow w_{11,y} - \eta \dfrac{\partial J}{\partial w_{11,y}} \\

& w_{11,b} \leftarrow w_{11,b} - \eta \dfrac{\partial J}{\partial w_{11,b}} \\

\end{align}

 %]]&gt;</script>

<p>舉 <script type="math/tex">w_{11,x}</script> 為例，用 <em>chain rule</em> 求出 <script type="math/tex"> \dfrac{\partial J}{\partial w_{11,x}}</script> 的值，如下<a name="eq5">＜公式五＞</a>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&\dfrac{\partial J}{\partial w_{11,x}}

=\dfrac{\partial J}{\partial n_{21(out)}}  \dfrac{\partial n_{21(out)}}{\partial w_{11,x}}

+ \dfrac{\partial J}{\partial n_{22(out)}}  \dfrac{\partial n_{22(out)}}{\partial w_{11,x}}

\\

&= \dfrac{\partial J}{\partial n_{21(out)}} 

\dfrac{\partial n_{21(out)}}{\partial n_{21(in)}} 

\dfrac{\partial n_{21(in)}}{\partial n_{11(out)}} 

\dfrac{\partial n_{11(out)}}{\partial n_{11(in)}} 

\dfrac{\partial n_{11(in)}}{\partial w_{11,x}} 

+ \dfrac{\partial J_{2}}{\partial n_{22(out)}} 

\dfrac{\partial n_{22(out)}}{\partial n_{22(in)}} 

\dfrac{\partial n_{22(in)}}{\partial n_{11(out)}} 

\dfrac{\partial n_{11(out)}}{\partial n_{11(in)}} 

\dfrac{\partial n_{11(in)}}{\partial w_{11,x}} \\


& = (

\dfrac{\partial J}{\partial n_{21(out)}} 

\dfrac{\partial n_{21(out)}}{\partial n_{21(in)}} 

\dfrac{\partial n_{21(in)}}{\partial n_{11(out)}} 

+ \dfrac{\partial J}{\partial n_{22(out)}} 

\dfrac{\partial n_{22(out)}}{\partial n_{22(in)}} 

\dfrac{\partial n_{22(in)}}{\partial n_{11(out)}} 

)  

\dfrac{\partial n_{11(out)}}{\partial n_{11(in)}} 

\dfrac{\partial n_{11(in)}}{\partial w_{11,x}} 

\end{align}

 %]]&gt;</script>

<p>其中， <script type="math/tex">\dfrac{\partial n_{21(in)}}{\partial n_{11(out)}} </script> 、 <script type="math/tex">\dfrac{\partial n_{22(in)}}{\partial n_{11(out)}} </script> 、 <script type="math/tex">\dfrac{\partial n_{11(out)}}{\partial n_{11(in)}} </script> 和 <script type="math/tex">\dfrac{\partial n_{11(in)}}{\partial w_{11,x}}</script> 這四項的值分別為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&\dfrac{\partial n_{21(in)}}{\partial n_{11(out)}} 

= \dfrac{\partial }{\partial n_{11(out)}}( w_{21,11}n_{11(out)} + w_{21,12}n_{12(out)}+w_{21,b} )

= w_{21,11} \\

&\dfrac{\partial n_{22(in)}}{\partial n_{11(out)}} 

= \dfrac{\partial }{\partial n_{11(out)}}( w_{22,11} n_{11(out)} + w_{22,12} n_{12(out)}+w_{22,b} )

= w_{22,11} \\

& \dfrac{\partial n_{11(out)}}{\partial n_{11(in)}} 

= \dfrac{\partial} {\partial n_{11(in)}}( \frac{1} {1+e^{-n_{11(in)} } } )

= n_{11(out)}(1-n_{11(out)}) \\

& \dfrac{\partial n_{11(in)}}{\partial w_{11,x}}

= \dfrac{\partial}{\partial w_{11,x}} (w_{11,x} x+w_{11,y} y+w_{11,b} ) 

= x \\

\end{align}

 %]]&gt;</script>

<p>再代入這些值與之前推導出的 <script type="math/tex">\dfrac{\partial J}{\partial n_{21(out)}} \dfrac{\partial n_{21(out)}}{\partial n_{21(in)}} </script> 和 <script type="math/tex">\dfrac{\partial J}{\partial n_{22(out)}} \dfrac{\partial n_{22(out)}}{\partial n_{22(in)}} </script> 的值到<a href="#eq5">＜公式五＞</a>，可求出 <script type="math/tex"> \dfrac{\partial J}{\partial w_{11,x}}</script> 為：</p>

<script type="math/tex; mode=display">

 \dfrac{\partial J}{\partial w_{11,x}}

 = ( (n_{21(out)}-z_{1})  w_{21,11} + (n_{22(out)}-z_{2})  w_{22,11} )  n_{11(out)}(1-n_{11(out)})  x

</script>

<p>同理，可求出 <script type="math/tex"> \dfrac{\partial J}{\partial w_{11,y}}</script> 和  <script type="math/tex"> \dfrac{\partial J}{\partial w_{11,b}}</script> 的值分別為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&\dfrac{\partial J}{\partial w_{11,y}} = ( (n_{21(out)}-z_{1})  w_{21,11} + (n_{22(out)}-z_{2})  w_{22,11} ) n_{11(out)}(1-n_{11(out)})  y \\

&\dfrac{\partial J}{\partial w_{11,b}} = ( (n_{21(out)}-z_{1})  w_{21,11} + (n_{22(out)}-z_{2})  w_{22,11} )  n_{11(out)}(1-n_{11(out)}) \\

\end{align}

 %]]&gt;</script>

<p>如同前一層所推導的，以上公式也有相同部分，也可以用 <script type="math/tex">\delta_{11(in)}</script> 來簡化它們，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&\delta_{11(in)} = ( (n_{21(out)}-z_{1})  w_{21,11} + (n_{22(out)}-z_{2})  w_{22,11} )  n_{11(out)}(1-n_{11(out)}) \\

& \dfrac{\partial J}{\partial w_{11,x}} =\delta_{11(in)}  x \\

&\dfrac{\partial J}{\partial w_{11,y}} = \delta_{11(in)}  y \\

&\dfrac{\partial J}{\partial w_{11,b}} =\delta_{11(in)} \\

\end{align}

 %]]&gt;</script>

<p>可把 <script type="math/tex">\delta_{11(in)}</script> 用後面層傳回來的的 <script type="math/tex">\delta</script> 來表示，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&\delta_{11(out)} = w_{21,11}  \delta_{21(in)} + w_{22,11}  \delta_{22(in)} \\

&\delta_{11(in)} = \delta_{11(out)}  n_{11(out)}(1-n_{11(out)}) = \delta_{11(out)}  \dfrac{\partial n_{11(out)}}{\partial n_{11(in)}}

\end{align}

 %]]&gt;</script>

<p>這些 <script type="math/tex">\delta</script> 的物理意義如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00089.png" alt="" /></p>

<p>從圖中可以看到， <script type="math/tex">\delta_{11(out)}</script> 是 <strong>由</strong> <script type="math/tex">\delta_{21(in)}</script> <strong>和</strong> <script type="math/tex">\delta_{22(in)}</script> <strong>往反方向傳遞，再乘上其權重</strong> <script type="math/tex">w_{21,11}</script> <strong>與</strong> <script type="math/tex">w_{22,11}</script> <strong>所得出的</strong> 。 </p>

<p>將 <script type="math/tex">\delta_{11(in)}</script> 置換到<a href="#eq4">＜公式四＞</a>，得出這一層推導的最後結果：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{11,x} \leftarrow w_{11,x} - \eta  \delta_{11(in)}  x \\

& w_{11,y} \leftarrow w_{11,y} - \eta  \delta_{11(in)}  y \\

& w_{11,b} \leftarrow w_{11,b} - \eta  \delta_{11(in)} \\

\end{align}

 %]]&gt;</script>

<p>同理， <script type="math/tex">w_{12,x},w_{12,y}, w_{12,b} </script> 的 <em>gradient descent</em> 的公式，也可用相同方法推導出來：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{12,x} \leftarrow w_{12,x} - \eta  \delta_{12(in)}  x \\

& w_{12,y} \leftarrow w_{12,y} - \eta  \delta_{12(in)}  y \\

& w_{12,b} \leftarrow w_{12,b} - \eta  \delta_{12(in)} \\

\end{align}

 %]]&gt;</script>

<h2 id="backward-phase">Backward Phase</h2>

<p><em>backward phase</em> 要做的即是 <em>backward propagation</em> ，也就是從 <em>output</em> 把 <script type="math/tex">\delta</script> 算出來，並更新權重 <script type="math/tex">w</script> ，再把 <script type="math/tex">\delta</script> 往回傳一層，再更新那層的權重 <script type="math/tex">w</script>，這樣一直傳下去直到 <em>input</em> 。</p>

<p>首先，把 <script type="math/tex">\delta_{21(in)}</script> 和 <script type="math/tex">\delta_{22(in)}</script> 算出來：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \delta_{21(in)} = n_{21(out)} - z_{1} \\

& \delta_{22(in)} = n_{22(out)} - z_{2} \\

\end{align}

 %]]&gt;</script>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00090.png" alt="" /></p>

<p>再來，用 <script type="math/tex">\delta_{21(in)}</script> 和 <script type="math/tex">\delta_{22(in)}</script> 更新以下權重的值：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{21,11} \leftarrow w_{21,11} - \eta  \delta_{21(in)}  n_{11(out)} \\

& w_{21,12} \leftarrow w_{21,12} - \eta  \delta_{21(in)}  n_{12(out)} \\

& w_{21,b} \leftarrow w_{21,b} - \eta  \delta_{21(in)} \\

& w_{22,11} \leftarrow w_{22,11} - \eta  \delta_{22(in)}  n_{11(out)} \\

& w_{22,12} \leftarrow w_{22,12} - \eta  \delta_{22(in)}  n_{12(out)} \\

& w_{22,b} \leftarrow w_{22,b} - \eta  \delta_{22(in)} \\

\end{align}

 %]]&gt;</script>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00091.png" alt="" /></p>

<p>再來，把 <script type="math/tex">\delta_{21(in)}</script> 和 <script type="math/tex">\delta_{22(in)}</script> 乘上權重，算出 <script type="math/tex">\delta_{11(in)}</script> 和 <script type="math/tex">\delta_{12(in)}</script>的值：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&\delta_{11(in)} = ( w_{21,11}  \delta_{21(in)} + w_{22,11}  \delta_{22(in)})  n_{11(out)}(1-n_{11(out)}) \\

&\delta_{12(in)} = ( w_{21,12}  \delta_{21(in)} + w_{22,12}  \delta_{22(in)})  n_{12(out)}(1-n_{12(out)}) \\

\end{align}

 %]]&gt;</script>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00092.png" alt="" /></p>

<p>最後，用 <script type="math/tex">\delta_{11(in)}</script> 和 <script type="math/tex">\delta_{12(in)}</script> 更新以下權重的值：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{11,x} \leftarrow w_{11,x} - \eta  \delta_{11(in)}  x \\

& w_{11,y} \leftarrow w_{11,y} - \eta  \delta_{11(in)}  y \\

& w_{11,b} \leftarrow w_{11,b} - \eta  \delta_{11(in)} \\

& w_{12,x} \leftarrow w_{12,x} - \eta  \delta_{12(in)}  x \\

& w_{12,y} \leftarrow w_{12,y} - \eta  \delta_{12(in)}  y \\

& w_{12,b} \leftarrow w_{12,b} - \eta  \delta_{12(in)} \\

\end{align}

 %]]&gt;</script>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00093.png" alt="" /></p>

<p>更新完後，即結束了在資料 <script type="math/tex">x,y</script> 上的這一輪訓練。</p>

<p>以下為整個過程的動畫版：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00094.gif" alt="" /></p>

<h2 id="reference">Reference</h2>

<p>本文參考 coursera 課程 Andrew Ng. Machine Learning</p>

<p>https://www.coursera.org/course/ml</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[類神經網路 -- Hierarchical Probabilistic Neural Network Language Model (Hierarchical Softmax)]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/05/23/-hierarchical-probabilistic-neural-networks-neural-network-language-model/"/>
    <updated>2015-05-23T15:33:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/05/23/-hierarchical-probabilistic-neural-networks-neural-network-language-model</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>將類神經網路應用在自然語言處理領域的模型有<a href="http://ckmarkoh.github.io/blog/2015/05/15/neural-network-neural-probabilistic-language-model">Neural Probabilistic Language Model(NPLM)</a>，但在實際應用時，運算瓶頸在於 <em>output layer</em> 的神經元個數，等同於總字彙量 <script type="math/tex">\mid V\mid </script> 。</p>

<p>每訓練一個字時，要讓 <em>output layer</em> 在那個字所對應的神經元輸出值為 <script type="math/tex">1</script> ，而其他 <script type="math/tex">\mid V\mid -1</script> 個神經元的輸出為 <script type="math/tex">0</script> ， 這樣總共要計算 <script type="math/tex">\mid V\mid </script> 次，會使得訓練變得沒效率。</p>

<p>若要減少於 <em>output layer</em> 的訓練時間，可以把 <em>output layer</em> 的字作分類階層，先判別輸出的字是屬於哪類，再判斷其子類別，最後再判斷是哪個字。 </p>

<h2 id="hierarchical-softmax">Hierarchical Softmax</h2>

<p>給定訓練資料為<script type="math/tex">X</script> ，輸出字的集合為 <script type="math/tex">Y</script> 。當輸入的字串為 <script type="math/tex">x</script> ，輸出的字為 <script type="math/tex">y</script> 時，訓練的演算法要將機率 <script type="math/tex">P (Y=y \mid  X=x) </script> 最佳化。</p>

<p>如果 <script type="math/tex">Y</script> 有 <em>10000</em> 種字，若沒有分類階層，訓練時就要直接對 <script type="math/tex">P (Y = y \mid  X = x) </script> 做計算，即是對這 <em>10000</em>種字做計算，使 <script type="math/tex">y</script> 所對應的神經元輸出為 <em>1</em> ，其它 <em>9999</em> 個神經元輸出 <em>0</em> ，這樣要計算 <em>10000</em> 次，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00079.png" alt="" /></p>

<p>若在訓練前，就事先把 <script type="math/tex">Y</script> 中的字彙分類好，以 <script type="math/tex">C(y)</script> 代表字 <script type="math/tex">y</script> 的類別，則可以改成用以下機率做最佳化：</p>

<!--more-->

<script type="math/tex; mode=display">

P (Y = y | X = x) =

P (Y = y | C = c(y), X) \times P (C = c(y) | X = x)


</script>

<p>根據以上公式，先判斷 <script type="math/tex">y</script> 是在哪類，要算 <script type="math/tex">P (C = c(y) \mid  X = x)</script> 的值，再判斷 <script type="math/tex">y</script> 是 <script type="math/tex">c(y)</script> 中的哪個字，要算 <script type="math/tex">P (Y = y \mid  C = c(y), X) </script> 的值。</p>

<p>若把它們分成 <em>100</em> 類，則每個類別有 <em>100</em> 種字。訓練時需要分成兩步來計算，第一步先計算 <script type="math/tex">P (C = c(y) \mid  X = x)</script> ，此時只要對 <em>100</em> 種分類做計算，使 <script type="math/tex">C(y)</script> 所對應的神經元輸出 <em>1</em> ，其它 <em>99</em> 個神經元輸出 <em>0</em> 。第二步是算 <script type="math/tex">P (Y = y \mid  C = c(y), X)</script>，即是讓 <script type="math/tex">C(y)</script> 底下對應到 <script type="math/tex">y</script> 的神經元輸出 <em>1</em> ，其它<em>99</em> 個神經元輸出 <em>0</em> ，這樣只需要計算 <em>200</em> 次即可，所做的計算只有原本的 <script type="math/tex">\frac{1}{50}</script> ，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00080.png" alt="" /></p>

<p>更進一步地，可以將 <em>output layer</em> 分成更多層，也就是說，將它用成樹狀結構，每一個節點代表一次分類，這樣一層層分下去，直到葉子節點，可分出是哪個字彙。分越多層，訓練時要計算的次數就會越小，分到最細的情況下，就變成二元分類樹，這樣所需的計算總量只有原本的 <script type="math/tex">\frac{log_{2}\mid V\mid }{\mid V\mid }</script> 倍。</p>

<p>根據這個二元分類樹，可以把 <script type="math/tex">Y</script> 中的每一個字，對應到一個由 <em>0</em> 和 <em>1</em> 所組成的二元字串 <script type="math/tex">(b_{1}(y), . . . b_{m}(y))</script> ，若字串中第一個數字為 <em>0</em> ，則表示這個字在第一層神經元所計算出的值為 <em>0</em>，以此類推。若要用這個二元分類樹計算 <script type="math/tex">P (Y = y \mid  X = x) </script> 可用以下公式來計算：</p>

<script type="math/tex; mode=display">

P( Y = y | X = x) =

\prod_{j=1}^{m}P(b_{j}(y) | b_{j-1}(y),b_{j-2}(y),...,b_{1}(y), X=x)

</script>

<p>其中， <script type="math/tex">（b_{j−1}(y),b_{j−2}(y),...,b_{1}(y))</script> 為長度小於 <script type="math/tex">m</script> 的二元字串，即是在二元分類樹中不是葉子也不是根的節點。</p>

<p>例如字串 <script type="math/tex">( b_{1}=0, b_{2}=1 )</script>即表示先從根節點往 <em>0</em> 的分支走到子節點，再從這個子節點走到 <em>1</em> 的分支的子節點，此子節點可用字串 <em>01</em> 來表示。若要計算以上公式的條件機率，方法為，先給這些子節點有它們自己的語意向量，再用 <em>NPLM</em> 把子節點的語意向量和 <script type="math/tex">x</script> 當成輸入值一起輸入 <em>NPLM</em> 來計算。</p>

<p>例如，假設詞庫裡總共有 <em>8</em> 個字， 則 <script type="math/tex">m=3</script> ，若字彙 <script type="math/tex">y</script> 的二元字串值 <script type="math/tex">b_{1} = 1, b_{2} =0, b_{3}=0 </script> ，則它在二元分類樹的位置如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00081.png" alt="" /></p>

<p>則 <script type="math/tex">P( Y = y \mid  X = x) </script> 為：</p>

<script type="math/tex; mode=display">

P(b_{1} = 1 | X=x) \times

P(b_{2} = 0 | b_{1} = 1, X=x) \times 

P(b_{3} = 0| b_{2} = 0, b_{1} = 1, X=x)

</script>

<p>訓練時，將訓練資料 <script type="math/tex">x</script> 輸入到 <em>NPLM</em> 的類神經網路中，並以二元分類樹的節點來代替它的 <em>output layer</em> 。</p>

<p>首先，先從根節點的神經元開始訓練，這一步是要算 <script type="math/tex">P(b_{1} = 1 \mid  , X=x) </script> 。由於 <script type="math/tex">b_{1} = 1 </script>，所以要以根節點的神經元輸出為 <em>1</em> 來調整 <em>NPLM</em> 模型的參數，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00082.png" alt="" /></p>

<p>再來，從根節點往左走，選 <em>1</em> 分支的子神經元來訓練，這一步是要算 <script type="math/tex">P(b_{2} = 0 \mid  b_{1} = 1, X=x) </script> 。因為這是基於 <script type="math/tex">b_{1} = 1</script> 的條件機率，所以要用字串 <em>1</em> 所對應它的語意向量和 <script type="math/tex">x</script> 一起輸入到 <em>NPLM</em> 。另外，由於 <script type="math/tex">b_{2} = 0 </script>，要以輸出為 <em>0</em> 來調整 <em>NPLM</em> 模型的參數，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00083.png" alt="" /></p>

<p>然後，從剛剛那個神經元的節點開始，選 <em>0</em> 分支的子神經元來訓練，這一步是要算 <script type="math/tex">P(b_{3} = 0\mid  b_{2} = 0, b_{1} = 1, X=x) </script> 。因為這是基於 <script type="math/tex">b_{1} = 1 , b_{2} = 0 </script> 的條件機率，所以要先將字串 <em>10</em> 所對應的語意向量和 <script type="math/tex">x</script> 一起輸入到 <em>NPLM</em> 。另外，由於 <script type="math/tex">b_{3} = 0 </script>，要以輸出為 <em>0</em> 來調整 <em>NPLM</em> 模型的參數，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00084.png" alt="" /></p>

<p>最後，選擇 <em>0</em> 的分支往下走，即可走到 <script type="math/tex">y</script> ，結束了針對 <script type="math/tex">x</script> 這筆資料的訓練過程。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00085.png" alt="" /></p>

<h2 id="hierarchical-clustering">Hierarchical Clustering</h2>

<p>在訓練之前，要先把字彙的階層分類給建立好。要建立這個分類階層，有很多種方式，例如，使用 <em>WordNet</em> 所建立的上下位關係，來建立階層，或者可以用 <em>Hierarchical Clustering</em> ，像是<a href="http://ckmarkoh.github.io/blog/2014/10/25/natural-language-processing-brown-clustering">Brown Clustering</a>之類的，自動從訓練資料中建立分類階層。</p>

<h2 id="conclusion">Conclusion</h2>

<p>用 <em>Hierarchical Softmax</em> 來置換掉原本 <em>NPLM</em> 的 <em>output layer</em> ，可以使得原本要計算 <script type="math/tex">\mid V\mid </script> 次的訓練，縮減為 <script type="math/tex">log_{2}\mid V\mid </script> 次，大幅提升了訓練速度。因此， <em>Hierarchical Softmax</em> 被日後眾多種類神經網路相關的模型所採用，包括近年來很熱門的 <em>word2vec</em> 也是。</p>

<h2 id="reference">Reference</h2>

<p>Frederic Morin , Yoshua Bengio. Hierarchical probabilistic neural network language model. 2005</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[類神經網路 -- Neural Probabilistic Language Model]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/05/15/neural-network-neural-probabilistic-language-model/"/>
    <updated>2015-05-15T02:38:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/05/15/neural-network-neural-probabilistic-language-model</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>傳統的語言模型 <em>Ngram</em> ，只考慮一個句子中的前面幾個字，來預測下個字是什麼。</p>

<p>當使用較長的 <em>Ngram</em> 的時候，則在測試資料中，找到和訓練資料相同的機率，就會大幅降低，甚至使機率為0，此種現象稱為 <em>curse of dimensionality</em> 。這類問題，傳統上可用 <a href="http://ckmarkoh.github.io/blog/2014/03/28/equations-for-nlp-ngram-smoothing"><em>Smoothing, Interpolation</em> 或 <em>Backoff</em></a>來處理。</p>

<p>另一種對付 <em>curse of dimensionality</em> 的方法，可以用 <em>Distributional Semantics</em> 的概念，即把詞彙的語意用向量來表示。在訓練資料中，根據這個詞和鄰近周圍的詞的關係，計算出向量中各個維度的值，得出來的值即可表示這個詞的語意。例如，假設在訓練資料中出現 <em>The cat is walking in the bedroom</em> 和 <em>A dog was running in a room</em> 這兩個句子，計算出 <em>dog</em> 和 <em>cat</em> 語意向量中的詞，可得出這兩個詞在語意上是相近的。</p>

<p>所謂的 <em>Neural Probabilistic Language Model</em> ，即是用類神經網路，從語料庫中計算出各個詞彙的語意向量值。</p>

<h2 id="neural-probabilistic-language-model">Neural Probabilistic Language Model</h2>

<p>給定訓練資料為 <script type="math/tex"> w_{1} \cdots w_{T} </script> 一連串的字，每個字都可包含於詞庫 <script type="math/tex">V</script> 中，即 <script type="math/tex">w_{t} \in V</script> ，然後，用此資料訓練出語言模型：</p>

<script type="math/tex; mode=display">


f(w_{t},...,w_{t-n+1}) = P(w_{t} \mid w_{1}^{t-1} )


</script>

<!--more-->

<p>其中， <script type="math/tex"> P(w_{t} \mid w_{1}^{t-1} ) </script> 為，給定第 <script type="math/tex">w_{1}</script> 到 <script type="math/tex">w_{t-1}</script> 的條件下，出現 <script type="math/tex">w_{t}</script> 的機率，為了簡化計算，通常只考慮 <script type="math/tex">w_{t}</script> 前面 <script type="math/tex">n-1</script> 個字，故語言模型的函數為 <script type="math/tex">f(w_{t},...,w_{t-n+1})</script> 。</p>

<p>可用以下兩部分來組成這個式子：</p>

<p>1.建立一個矩陣 <script type="math/tex">C</script> 其維度為 <script type="math/tex">\mid V\mid \times m</script> 。此矩陣中的每一列，即代表詞庫 <script type="math/tex">V</script> 中的每個字所對應之維度為 <script type="math/tex">m</script> 的語意向量 ，例如， <script type="math/tex">w_{t}</script> 的語意向量為 <script type="math/tex">C(w_{t}) </script> 。</p>

<p>2.語料庫中句子的某個字，跟其前面 <script type="math/tex">n-1</script> 個字的語意向量 <script type="math/tex">( C(w_{t-1}), ..., C(w_{t-n+1}) )</script> 有關， 用函數 <script type="math/tex">g</script> 來表示字 <script type="math/tex">w_{t}</script> 與前面 <script type="math/tex">n-1</script> 個字的關係，公式如下：</p>

<script type="math/tex; mode=display">

P(w_{t} \mid w_{1}^{t-1} ) = g(w_{t},w_{t-1}, ..., w_{t-n+1})


</script>

<p>用類神經網路，將 <script type="math/tex">( C(w_{t-1}), ..., C(w_{t-n+1}) )</script> 當成 <em>input</em> ， <script type="math/tex">w_{t}</script> 當成 <em>output</em> ，若 <script type="math/tex">w_{t}=i</script> 時，求 <script type="math/tex">P(w_{t}=i \mid w_{1}^{t-1} )</script> 的機率，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00076.png" alt="Neural Network of Neural Probabilistic Language Model" /></p>

<p>其中，此類神經網路有個 <em>tanh</em> 函數的 <em>hidden layer</em> ， <em>input layer</em> 和 <em>hidden layer</em> 都連結到 <em>output layer</em> ，而 <em>output layer</em> 為一個 <em>softmax</em> 函數，將輸出做正規化，即是將所有 <em>output</em> 的值轉化後，其和為 <em>1</em> ， <em>softmax</em> 函數如下：</p>

<script type="math/tex; mode=display">

P(w_{t} \mid w_{t-1} , ... , w_{t-n+1} ) = \frac{e^{y_{w_{t}}}}{\sum_{i} e^{y_{i}}}  

</script>

<p><script type="math/tex">y</script> 為此類神經網路在 <em>softmax</em> 做正規化之前，所算出的值：</p>

<script type="math/tex; mode=display">


y = b+ W\times x + U \times tanh(d+H\times x)

</script>

<p>其中， <script type="math/tex">b</script> 和 <script type="math/tex">d</script> 為 <em>bias</em> ， <script type="math/tex">x</script> 為 <script type="math/tex">( C(w_{t-1}), ..., C(w_{t-n+1}) )</script>，而 <script type="math/tex">H</script> 、 <script type="math/tex">U</script> 與 <script type="math/tex">W</script> 為類神經網路從上一層傳到下一層時，各個值所佔的權重比例。此公式中各矩陣（或向量）維度如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00077.png" alt="" /></p>

<p>在類神經網路中所對應的位置如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00078.png" alt="" /></p>

<p>於此類神經網路中，可經由訓練而調整的參數為 <script type="math/tex">\theta </script> ，如下：</p>

<script type="math/tex; mode=display">

\theta = (b,d,W,U,H,C)

</script>

<p>從 <script type="math/tex">b,d,W,U,H,C</script> 的維度，可得出共有 <script type="math/tex">\mid V\mid (1 + nm + h) + h(1 + (n − 1)m)</script> 個參數可調整。</p>

<h2 id="training-the-neural-probabilistic-language-model">Training the Neural Probabilistic Language Model</h2>

<p>要訓練此類神經網路，即是對 <script type="math/tex">P(w_{t} \mid w_{1}^{t-1} )</script> 做最佳化，並作 <em>regularization</em> 以防 <em>overfitting</em> ，即讓以下公式 <script type="math/tex">L</script> 為最佳解：</p>

<script type="math/tex; mode=display">

L = \frac{1}{T}\sum_{t} log(w_{t},w_{t-1},...,w_{t-n+1},\theta) +R(\theta)

</script>

<p>其中，<script type="math/tex">T</script> 為訓練資料的總字數，而 <script type="math/tex">R(\theta)</script> 作為 <em>regularization</em> 的用途。</p>

<p>可用 <em>Stochastic Gradient Descent</em> 的方式來求得最佳解，其中 <script type="math/tex">\epsilon</script> 為 <em>learning rate</em> ：</p>

<script type="math/tex; mode=display">

\theta \leftarrow \theta + \epsilon \dfrac{\partial log P(w_{t} \mid w_{t-1} , ... , w_{t-n+1} )}{\partial \theta}

</script>

<p>一般而言，類神經網路都是以 <em>Backward Propogation</em> 的方法來訓練，此方法分為兩個階段： <em>Forward Phase</em> 與 <em>Backward Phase</em> 。即是先計算從 <em>input</em> 透過中間的每一層把值傳遞到 <em>output</em> ，再從 <em>output</em> 往回傳遞 <em>error</em> 到中間的每一層，去更新層與層之間權重的參數 。</p>

<p>在此類神經網路的訓練過程中，運算的瓶頸在於從 <em>hidden layer</em> 到 <em>output layer</em> 這段，需要計算 <script type="math/tex">\mid V\mid (1+(n−1)m+h)</script> 個參數，因此通常採取平行化的運算，將此步驟的運算分配到不同的 <em>CPU</em> 。</p>

<p>由於 <em>output layer</em>  的大小為 <script type="math/tex">\mid V\mid </script> ，若總共有 <script type="math/tex">M</script> 個 <em>CPU</em> ，則可將 <em>output layer</em> 分成 <script type="math/tex">M</script> 個區塊，則每個 <em>CPU</em> 負責計算一個區塊，也就是 <script type="math/tex"> ⌈\mid V\mid /M⌉ </script> 個 <em>output unit</em> ，而第 <script type="math/tex">i</script> 個 <em>CPU</em> 則從第 <script type="math/tex">i × ⌈\mid V \mid /M⌉ </script> 個 <em>output unit</em> 開始計算，詳細過程如下：</p>

<p><strong>1.Forward Phase</strong></p>

<p>(a) 根據 <em>input</em> 的詞彙，從矩陣 <script type="math/tex">C</script> 中取出相對應的語意向量：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x(k) \leftarrow C(w_{t-k}) \\

& x = (x(1),x(2),...,x(n-1))

\end{align}

 %]]&gt;</script>

<p>(b) 將 <em>input</em> 的值傳遞到 <em>hidden layer</em> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& o \leftarrow d + H \times x \\

& a \leftarrow tanh(o)

\end{align}

 %]]&gt;</script>

<p>(c) 將 <em>hidden layer</em> 的值傳到 <em>output layer</em> 。</p>

<p>這步驟要平行運算，即第 <script type="math/tex">i</script> 個 <em>CPU</em> 負責運算第 <script type="math/tex">i</script> 個區塊的 <em>output layer</em> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& s_{i} \leftarrow 0 \\ 

& \text{Loop over } j \text{ in the i-th block }  \\

& 1. \mspace{20mu}	y_{j} \leftarrow b_{j} + U_{j} \times a + W_{j} \times x  \\

& 2. \mspace{20mu}	p_{j} \leftarrow e^{y_{j}} \\

& 3. \mspace{20mu} s_{i} \leftarrow s_{i} + p_{j} \\


\end{align}

 %]]&gt;</script>

<p>(d) 合併各個 <em>CPU</em> 所算出來的 <script type="math/tex">s_{i}</script>：</p>

<script type="math/tex; mode=display">

\begin{align}

S = \Sigma_{i}s_{i}

\end{align}

</script>

<p>(e) 將每個 <script type="math/tex">P_{w_{t}}</script> 正規化後取 <em>log</em> ，得出它們的 <script type="math/tex">L</script> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& p_{w_{t}}  \leftarrow \frac{ p_{w_{t}} }{ S }\\

& L = log(p_{w_{t}}) 

\end{align}

 %]]&gt;</script>

<p><em>註：本文所寫的演算法省略掉 Regularization 的過程。</em></p>

<p><strong>2.Backward Phase</strong></p>

<p>(a) 從 <em>output layer</em> 執行 <em>back propogation</em> 。</p>

<p>在此過程中，第 <script type="math/tex">i</script> 個 <em>CPU</em> 負責計算 <em>output layer</em> 中第 <script type="math/tex">i</script> 個區塊的 <em>gradient</em> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&\dfrac{ \partial L} { \partial a} \leftarrow 0 \\

&\dfrac{ \partial L} { \partial x} \leftarrow 0 \\

& \text{Loop over } j \text{ in the i-th block }  \\

& 1. \mspace{20mu}	\dfrac{ \partial L} { \partial y_{j}}  

\leftarrow 1_{j == w_{t}} - p_{j} \\

& 2. \mspace{20mu}	b_{j} \leftarrow b_{j} + \epsilon \dfrac{\partial L }{\partial y_{j}} \\

& 3. \mspace{20mu} \dfrac{ \partial L} { \partial x} \leftarrow 

\dfrac{ \partial L} { \partial x} + \dfrac{ \partial L} { \partial y_{j}}W_{j} \\

& 4. \mspace{20mu} \dfrac{ \partial L} { \partial a} \leftarrow 

\dfrac{ \partial L} { \partial a} + \dfrac{ \partial L} { \partial y_{j}}U_{j} \\

& 5. \mspace{20mu} W_{j} \leftarrow W_{j} + \epsilon\dfrac{ \partial L} { \partial y_{j}}x

\\

& 6. \mspace{20mu} U_{j} \leftarrow U_{j} + \epsilon\dfrac{ \partial L} { \partial y_{j}}a

\end{align}

 %]]&gt;</script>

<p>(b) 把每個 <em>CPU</em> 算出來的 <script type="math/tex">\dfrac{ \partial L} { \partial a}</script> 與 <script type="math/tex">\dfrac{ \partial L} { \partial x}</script> 都加起來。</p>

<p>(c) 用 <em>back propogation</em> 更新 <em>hidden layer</em> 到 <em>input layer</em> 之間的參數：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \text{Loop over } k \text{ between } 1 \text{ and } h  \\

& \mspace{20mu} \dfrac{\partial L}{\partial o_{k}} \leftarrow (1-a_{k}^{2}) \dfrac{\partial L}{\partial a_{k}} \\

& \dfrac{\partial L}{\partial x} \leftarrow \dfrac{\partial L}{\partial x} + H'\dfrac{\partial  L}{\partial o} \\

& d \leftarrow d + \epsilon \dfrac{\partial L}{\partial o} \\

& H \leftarrow H + \epsilon \dfrac{\partial L}{\partial o} x'

\end{align}

 %]]&gt;</script>

<p>(d) 更新 <em>input layer</em> 的語意向量值（從第一個字到第 <script type="math/tex">n-1</script> 個字）：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \text{Loop over } k \text{ between } 1 \text{ and } n-1  \\

& \mspace{20mu} C(w_{t-k}) \leftarrow C(w_{t-k}) + \epsilon \dfrac{\partial L}{\partial x(k)}

\end{align}

 %]]&gt;</script>

<h2 id="conclusion">Conclusion</h2>

<p><em>Neural Probabilistic Language Model</em> 是2003年期間所提出的語言模型，但受限於當時的電腦運算能力，這個模型的複雜度實在太高，難以實際應用。</p>

<p>近幾年來由於平行運算和 <em>GPU</em> 的發展，大幅提升了矩陣運算的效率，促成類神經網路與深度學習的發展。因此，建構在 <em>Neural Probabilistic Language Model</em> 的基礎之上，發展出了各種類神經網路相關的語言模型，一再突破了以往自然語言處理的瓶頸，包括近兩年來很熱門的 <em>word2vec</em> ，也是從它發展而來。</p>

<h2 id="reference">Reference</h2>

<p>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. J. Mach. Learn. Res. 3 (March 2003), 1137-1155.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[機器翻譯 -- IBM Model 1]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/02/21/mt-ibm-model-1/"/>
    <updated>2015-02-21T09:31:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/02/21/mt-ibm-model-1</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>繼續之前<a href="http://ckmarkoh.github.io/blog/http://cpmarkchang.logdown.com/posts/239855-machine-translation-statistical-machine-translation">統計機器翻譯</a>所提到的，要計算 <em>Translation Model</em> <script type="math/tex">p(F∣E)</script> ，即給定某個英文句子 <script type="math/tex">E</script> ，則它被翻自成中文句子 <script type="math/tex">F</script> 的機率是多少？</p>

<p>計算 <em>Translation Model</em> 需要用到較複雜的模型，例如 <em>IBM Model</em> 。而 <em>IBM Model 1</em>  是最基本的一種 <em>IBM Model</em> 。</p>

<h2 id="alignment">Alignment</h2>

<p>在講 <em>IBM Model 1</em> 之前，要先介紹 <em>alignment</em> 是什麼。因為，要將英文翻譯成中文，則中文字詞的順序可能跟英文字詞的順序，不太一樣。例如：</p>

<blockquote>
  <p>這不是一個蘋果
This is not an apple </p>
</blockquote>

<p>這兩個句子，在中文中的 <strong>“不是”</strong> 英文為 <strong>“is not（是不）”</strong> 。為了考慮到字詞順序會變，因此要定義 <em>alignment</em> ，來記錄中文句子中的哪個字，對應到英文句子中的哪個字。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00075.tiff" alt="" /></p>

<p>此例中， <em>alignment</em> 為 <script type="math/tex">\{1\rightarrow 1,3\rightarrow 2,2\rightarrow 3,4 \rightarrow 4,5\rightarrow 5 \}</script>，表示第一個中文字對應到第一個英文字，第二個中文字對應到第三個英文字，以此類推。</p>

<!--more-->

<h2 id="ibm-model-1">IBM Model 1</h2>

<p><em>IBM Model 1</em> 的公式如下：</p>

<script type="math/tex; mode=display">

P(F\mid E) = \sum_{A} P(F,A \mid E) = \sum_{A} P(F \mid E, A) \times P(A \mid E)

</script>

<p>此公式可分為 <script type="math/tex">P(F \mid E, A)</script> 和 <script type="math/tex">P(A \mid E)</script> 兩部分。</p>

<p>第一部分為 <script type="math/tex">P(F \mid E, A)</script> ，給定一個 <em>alignment</em> <script type="math/tex">A</script> ，和英文句子 <script type="math/tex">E = (e_{1},e_{2}...,e_{I})</script> ，定義中文句子 <script type="math/tex">F = (f_{1},f_{2},...,f_{J})</script> 的出現機率為：</p>

<script type="math/tex; mode=display">

  P(F \mid E, A) = \prod_{j=1}^{J} t(f_{j} \mid e_{A_{j}})


</script>

<p>其中，<script type="math/tex"> t(f_{j} \mid e_{A_{j}}) </script> 表示在 <em>alignment</em> <script type="math/tex">A</script> 之下，對應到中文字 <script type="math/tex">f_{j}</script>的英文字 <script type="math/tex">e_{A_{j}}</script> ，翻譯成中文字 <script type="math/tex">f_{j}</script> 的機率。</p>

<p>舉例，用先前提到的例句，以此公式算出的機率為：</p>

<script type="math/tex; mode=display">

  P(F \mid E, A) =  

  t(\text{這} \mid \text{this}) \times

  t(\text{不} \mid \text{not}) \times

  t(\text{是} \mid \text{is}) \times

  t(\text{一個} \mid \text{an}) \times

  t(\text{蘋果} \mid \text{apple}) 

</script>

<p>其中， <script type="math/tex">t(\text{這} \mid \text{this})</script> 表示英文字 <strong>“this”</strong> 翻譯成中文字 <strong>“這”</strong> 的機率。 </p>

<p>第二部分為<script type="math/tex">P(A \mid E)</script>，給定長度為 <script type="math/tex">I</script> 的英文句 <script type="math/tex">E</script>，將翻譯成長度為 <script type="math/tex">J</script> 的中文句，則 <em>alignment</em> 為 <script type="math/tex">A</script> 的機率為：</p>

<script type="math/tex; mode=display">

  P(A \mid E) = \frac{\epsilon}{(I+1)^{J}}


</script>

<p><em>IBM Model 1</em> 採用最簡單的假設：<strong>假設每個alignment所發生的機率都相同</strong> 。如果中文字長度為 <script type="math/tex">J</script> ，英文長度為 <script type="math/tex">I</script> ，則共有 <script type="math/tex">I^{J}</script> 種組合。但實際上，有些中文字（例如：的），沒有相對應的英文字，所以共有 <script type="math/tex">(I+1)^{J}</script> 種可能。另外，公式中的 <script type="math/tex">\epsilon</script> 為一常數。</p>

<p>綜合以上兩項，可以得出 <em>IBM Model 1</em> 的公式為：</p>

<script type="math/tex; mode=display">

\begin{equation}

 P(F\mid E) = \sum_{A} P(F,A \mid E) = \sum_{A} P(F \mid E, A) \times P(A \mid E) \\

 = \sum_{A}  \prod_{j=1}^{J} t(f_{j} \mid e_{A_{j}}) \times  \frac{\epsilon}{(I+1)^{J}} \\

\end{equation}

</script>

<p>通常，在求解的時候，不會把每一種 <em>alignment</em> 的結果都加起來，而是求出機率最大的 <em>alignment</em> ，並採用它來計算 <em>IBM Model</em> 的值，如下：</p>

<script type="math/tex; mode=display">

\mathop{\arg\,\max}\limits_{A} P(F,A \mid E)

= \mathop{\arg\,\max}\limits_{A} \prod_{j=1}^{J} t(f_{j} \mid e_{A_{j}}) \times  \frac{\epsilon}{(I+1)^{J}} 


</script>

<h2 id="em-algorithm">EM algorithm</h2>

<p>在進行<a href="http://ckmarkoh.github.io/blog/http://cpmarkchang.logdown.com/posts/239855-machine-translation-statistical-machine-translation">統計機器翻譯</a>的演算法時，需從平行語料庫中訓練出模型。但通常平行語料庫中不會標記 <em>alignment</em> ，可是必須要先有 <em>alignment</em> ，才能得出 <script type="math/tex">t(f_{j} \mid e_{A_{j}})</script> 的值，這樣才能用 <em>IBM Model 1</em> 來計算。</p>

<p>這時就需要用到 <em>EM algorithm</em> 從平行語料庫中，計算出 <script type="math/tex">t(f_{j} \mid e_{A_{j}})</script> 的值，以得出機率最大的 <em>alignment</em>。</p>

<p>設 <script type="math/tex">(F^{(k)}, E^{(k)})</script> 為共 <script type="math/tex">k</script> 個句子的 <em>中-英</em> 平行語料庫，其中，<script type="math/tex"> F^{(k)} = (f_{1}^{(k)}, f_{2}^{(k)},...,f_{m_{k}}^{(k)} )</script> 為第 <script type="math/tex">k</script> 個中文句子， <script type="math/tex">f_{1}^{(k)}</script> 為此句第一個中文字，  <script type="math/tex">m_{k}</script> 為句子長度。<script type="math/tex"> E^{(k)} = (e_{1}^{(k)}, e_{2}^{(k)},...,e_{l_{k}}^{(k)} )</script> 為第 <script type="math/tex">k</script> 個英文句子。演算法如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

\text{1} & \text{For all e , f, set } t(f\mid e)  = 1 \mathbin{/} V \\

\text{2} & \textbf{for } s = 1 ... S \\ 

\text{3} & \mspace{20mu} \text{Set all counts } c = 0 \\

\text{4}& \mspace{20mu} \textbf{for } k = 1...N \\

\text{5}& \mspace{40mu} \textbf{for } i = 1...m_{k} \\

\text{6}& \mspace{60mu} \textbf{for } j = 0...l_{k} \\

\text{7}& \mspace{80mu} \delta (k,i,j) = t( f_{i}^{(k)} \mid e_{j}^{(k)}) \mathbin{/}  \Sigma_{j=0}^{l_{k}} t( f_{i}^{(k)} \mid e_{j}^{(k)}) \\

\text{8}& \mspace{80mu} c(e_{j}^{(k)}, f_{i}^{(k)}) \leftarrow c(e_{j}^{(k)}, f_{i}^{(k)})  + \delta (k,i,j) \\

\text{9}& \mspace{80mu} c(e_{j}^{(k)}) \leftarrow c(e_{j}^{(k)})  + \delta (k,i,j) \\

\text{10}& \mspace{20mu} \text{Set all } t(f\mid e) = c(e,f) \mathbin{/}  c(e) \\

\text{11}& \textbf{return } t


\end{align}

 %]]&gt;</script>

<p>第1行先將所有的<script type="math/tex">t(f \mid e)</script> 初始化為 <script type="math/tex">\frac{1}{V}</script> ，其中，<script type="math/tex">V</script> 為語料庫中的中文詞類數量。</p>

<p>第2行的 <script type="math/tex">S</script> 表示，總共進行了 <script type="math/tex">S</script> 次的運算。</p>

<p>第3行的 <script type="math/tex">c</script> 用來計 <script type="math/tex">t(f \mid e)</script> 累加的值，初始化先設定為 <script type="math/tex">0</script> 。</p>

<p>第4行用迴圈跑平行語料庫中所有的句子</p>

<p>第5到第9行迴圈用來逐一跑中文和英文句中的每一個字。</p>

<p>第5行跑中文句中的中文字。</p>

<p>第6行跑英文字。由於有些中文字，沒有相對應的英文字，故英文句的迴圈從 <em>0</em> （沒有對應到任何字）開始跑。</p>

<p>第7行算出 <script type="math/tex">\delta (k,i,j)</script> 的值，並於第8,9行累加到 <script type="math/tex">c</script> 中。</p>

<p>第10行，算完所有的句子之後，用 <script type="math/tex">c(e,f)</script> 和 <script type="math/tex">c(e)</script> 來更新 <script type="math/tex">t(f\mid e)</script> 的值，再回到第3行，用新的 <script type="math/tex">t</script> 值來計算新的 <script type="math/tex">c</script> 值。</p>

<p>第11行，以上迴圈重複跑過 <script type="math/tex">S</script> 次以後，回傳 <script type="math/tex">t</script> 的值。</p>

<h2 id="implementation">Implementation</h2>

<p>給定以下的平行語料庫</p>

<blockquote>
  <p>一本書 : a book
一本雜誌 : a magazine
這本書 : this book
這本雜誌 : this magazine </p>
</blockquote>

<p>可用以下的程式碼 <em>EM Algorithm</em> 算出 <script type="math/tex">t</script> 的值。</p>

<figure class="code"><figcaption><span>ibm1.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="c">#-*- encoding:utf-8 -*-</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">operator</span>
</span><span class="line"><span class="n">CORPUS_CH</span> <span class="o">=</span> <span class="p">[</span> <span class="p">[</span><span class="s">&#39;一本&#39;</span><span class="p">,</span><span class="s">&#39;書&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s">&#39;一本&#39;</span><span class="p">,</span><span class="s">&#39;雜誌&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s">&#39;這本&#39;</span><span class="p">,</span><span class="s">&#39;書&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s">&#39;這本&#39;</span><span class="p">,</span><span class="s">&#39;雜誌&#39;</span><span class="p">],</span> <span class="p">]</span>
</span><span class="line"><span class="n">CORPUS_EN</span> <span class="o">=</span> <span class="p">[</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">,</span><span class="s">&#39;book&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s">&#39;a&#39;</span><span class="p">,</span><span class="s">&#39;magazine&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s">&#39;this&#39;</span><span class="p">,</span><span class="s">&#39;book&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s">&#39;this&#39;</span><span class="p">,</span><span class="s">&#39;magazine&#39;</span><span class="p">],</span> <span class="p">]</span>
</span><span class="line"><span class="n">f_word</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="nb">reduce</span><span class="p">(</span><span class="n">operator</span><span class="o">.</span><span class="n">add</span><span class="p">,</span><span class="n">CORPUS_CH</span><span class="p">)))</span>
</span><span class="line"><span class="n">e_word</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="nb">reduce</span><span class="p">(</span><span class="n">operator</span><span class="o">.</span><span class="n">add</span><span class="p">,</span><span class="n">CORPUS_EN</span><span class="p">)))</span>
</span><span class="line"><span class="n">T</span> <span class="o">=</span> <span class="p">{}</span>
</span><span class="line">
</span><span class="line"><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="p">:</span>
</span><span class="line">  <span class="n">C</span> <span class="o">=</span> <span class="p">{}</span>
</span><span class="line">  <span class="k">for</span> <span class="n">m</span><span class="p">,</span><span class="n">l</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">CORPUS_CH</span><span class="p">,</span><span class="n">CORPUS_EN</span><span class="p">):</span>
</span><span class="line">    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">      <span class="k">for</span> <span class="n">fi</span> <span class="ow">in</span> <span class="n">m</span><span class="p">:</span>
</span><span class="line">        <span class="k">for</span> <span class="n">ej</span> <span class="ow">in</span> <span class="n">l</span><span class="p">:</span>
</span><span class="line">          <span class="k">if</span> <span class="s">&quot;</span><span class="si">%s</span><span class="s">|</span><span class="si">%s</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">fi</span><span class="p">,</span><span class="n">ej</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">T</span><span class="p">:</span>
</span><span class="line">            <span class="n">T</span><span class="p">[</span><span class="s">&quot;</span><span class="si">%s</span><span class="s">|</span><span class="si">%s</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">fi</span><span class="p">,</span><span class="n">ej</span><span class="p">)]</span>  <span class="o">=</span>  <span class="mf">1.0</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">e_word</span><span class="p">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">fi</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span><span class="line">      <span class="n">sum_t</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span> <span class="n">T</span><span class="p">[</span><span class="s">&quot;</span><span class="si">%s</span><span class="s">|</span><span class="si">%s</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">fi</span><span class="p">,</span><span class="n">ej</span><span class="p">)]</span> <span class="k">for</span> <span class="n">ej</span> <span class="ow">in</span> <span class="n">l</span><span class="p">])</span> <span class="o">*</span> <span class="mf">1.0</span>
</span><span class="line">      <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">ej</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">l</span><span class="p">):</span>
</span><span class="line">        <span class="n">delta</span> <span class="o">=</span> <span class="n">T</span><span class="p">[</span><span class="s">&quot;</span><span class="si">%s</span><span class="s">|</span><span class="si">%s</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">fi</span><span class="p">,</span><span class="n">ej</span><span class="p">)]</span> <span class="o">/</span> <span class="n">sum_t</span>
</span><span class="line">        <span class="n">C</span><span class="p">[</span><span class="s">&quot;</span><span class="si">%s</span><span class="s"> </span><span class="si">%s</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">ej</span><span class="p">,</span> <span class="n">fi</span><span class="p">)]</span> <span class="o">=</span>  <span class="n">C</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&quot;</span><span class="si">%s</span><span class="s"> </span><span class="si">%s</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">ej</span><span class="p">,</span> <span class="n">fi</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">delta</span>
</span><span class="line">        <span class="n">C</span><span class="p">[</span><span class="s">&quot;</span><span class="si">%s</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">ej</span><span class="p">)]</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&quot;</span><span class="si">%s</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">ej</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">delta</span>
</span><span class="line">  <span class="k">print</span> <span class="s">&quot;---iteration:</span><span class="si">%s</span><span class="s">---&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
</span><span class="line">  <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">T</span><span class="p">:</span>
</span><span class="line">    <span class="k">print</span> <span class="n">key</span><span class="p">,</span><span class="s">&quot;:&quot;</span><span class="p">,</span><span class="n">T</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
</span><span class="line">  <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">f_word</span><span class="p">:</span>
</span><span class="line">    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">e_word</span><span class="p">:</span>
</span><span class="line">      <span class="k">if</span> <span class="s">&quot;</span><span class="si">%s</span><span class="s"> </span><span class="si">%s</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span> <span class="ow">in</span> <span class="n">C</span> <span class="ow">and</span> <span class="s">&quot;</span><span class="si">%s</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="ow">in</span> <span class="n">C</span><span class="p">:</span>
</span><span class="line">        <span class="n">T</span><span class="p">[</span><span class="s">&quot;</span><span class="si">%s</span><span class="s">|</span><span class="si">%s</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">e</span><span class="p">)]</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="s">&quot;</span><span class="si">%s</span><span class="s"> </span><span class="si">%s</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">f</span><span class="p">)]</span> <span class="o">/</span> <span class="n">C</span><span class="p">[</span><span class="s">&quot;</span><span class="si">%s</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">e</span><span class="p">)]</span>
</span><span class="line">
</span><span class="line"><span class="k">print</span> <span class="s">&quot;---iteration:</span><span class="si">%s</span><span class="s">---&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
</span><span class="line"><span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">T</span><span class="p">:</span>
</span><span class="line">  <span class="k">print</span> <span class="n">key</span><span class="p">,</span><span class="s">&quot;:&quot;</span><span class="p">,</span><span class="n">T</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>執行 <em>python ibm1.py</em> 計算結果：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">&gt; python ibm1.py
</span><span class="line">---iteration:0---
</span><span class="line">雜誌<span class="p">|</span>magazine : 0.25
</span><span class="line">書<span class="p">|</span>this : 0.25
</span><span class="line">這本<span class="p">|</span>book : 0.25
</span><span class="line">這本<span class="p">|</span>this : 0.25
</span><span class="line">一本<span class="p">|</span>magazine : 0.25
</span><span class="line">雜誌<span class="p">|</span>a : 0.25
</span><span class="line">雜誌<span class="p">|</span>this : 0.25
</span><span class="line">書<span class="p">|</span>book : 0.25
</span><span class="line">一本<span class="p">|</span>a : 0.25
</span><span class="line">這本<span class="p">|</span>magazine : 0.25
</span><span class="line">一本<span class="p">|</span>book : 0.25
</span><span class="line">書<span class="p">|</span>a : 0.25
</span><span class="line">...
</span><span class="line">---iteration:5---
</span><span class="line">雜誌<span class="p">|</span>magazine : 0.941176470588
</span><span class="line">書<span class="p">|</span>this : 0.0294117647059
</span><span class="line">這本<span class="p">|</span>book : 0.0294117647059
</span><span class="line">這本<span class="p">|</span>this : 0.941176470588
</span><span class="line">一本<span class="p">|</span>magazine : 0.0294117647059
</span><span class="line">雜誌<span class="p">|</span>a : 0.0294117647059
</span><span class="line">雜誌<span class="p">|</span>this : 0.0294117647059
</span><span class="line">書<span class="p">|</span>book : 0.941176470588
</span><span class="line">一本<span class="p">|</span>a : 0.941176470588
</span><span class="line">這本<span class="p">|</span>magazine : 0.0294117647059
</span><span class="line">一本<span class="p">|</span>book : 0.0294117647059
</span><span class="line">書<span class="p">|</span>a : 0.0294117647059
</span></code></pre></td></tr></table></div></figure>

<p>其中，初始值的 <script type="math/tex">t</script> ，不管是哪個中文字對應到哪個英文字，都是 <em>0.25</em> 。經過了五次的迴圈運算後，  <script type="math/tex">t</script> 值會逐漸收斂，漸與訓練資料吻合。例如 <script type="math/tex">t(\text{雜誌} \mid \text{magazine}) = 0.941</script> ，大於 <script type="math/tex">t(\text{一本} \mid \text{magazine}) = 0.029</script> 。表示英文字 <em>magazine</em> 較可能翻譯成中文字詞 <strong>“雜誌”</strong> ，而不是 <strong>“一本”</strong> 。</p>

<h2 id="reference">Reference</h2>

<p>本文參考至coursera線上課程</p>

<h4 id="michael-collins-natural-language-processing">Michael Collins. Natural Language Processing</h4>

<p>https://www.coursera.org/course/nlangp</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[資訊檢索 -- Boolean Retrieval]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/02/09/-information-retrieval-boolean-retrieval/"/>
    <updated>2015-02-09T00:27:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/02/09/-information-retrieval-boolean-retrieval</id>
    <content type="html"><![CDATA[<h2 id="information-retrieval">Information Retrieval</h2>

<p>所謂的資訊檢索（ <em>Information Retrieval</em> )，就是從大量非結構的資料，例如網頁，根據某些關鍵字，找出具有此關鍵字的文件。例如，搜尋引擎，即是一種資訊檢索的應用。</p>

<p>資訊檢索的演算法，其實跟我們要在某本書中，找尋某個字的方法差不多。</p>

<p>例如我們想在 <em>Introduction to Information Retrieval</em> 這本教科書中，找到 <em>informational queries</em> 這個詞在哪一頁，如果從第一頁開始，一個字一個字慢慢找，要比對成千上萬個字才能找到。</p>

<p>但如果我們翻到書本後面的 <em>Index</em> （如下） ，即可很快找到 <em>informational queries</em> 這個字是在第 <em>432</em> 頁。</p>

<figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class=""><span class="line">...
</span><span class="line">information gain, 285 
</span><span class="line">information need, 5, 152 
</span><span class="line">information retrieval, 1 
</span><span class="line">informational queries, 432 
</span><span class="line">inner product, 121 
</span><span class="line">...
</span></code></pre></td></tr></table></div></figure>

<p>所以，資訊檢索，最核心的概念就是建立 <em>Index</em> ，這樣就可以快速找到哪些文件中具有某個關鍵字。</p>

<!--more-->

<h2 id="boolean-retrieval">Boolean Retrieval</h2>

<p>最基本的資訊檢索方法，就是 <em>Boolean Retrieval</em> 。它用 <em>boolean logic</em> 的概念，來建立 <em>index</em> 和執行 <em>query</em> 。</p>

<p>舉個例子，假設有三篇文劍，內容分別如下：</p>

<table>
  <thead>
    <tr>
      <th>文件</th>
      <th>內容</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>D1</td>
      <td>The way to avoid linearly scanning is to index the documents in advance.</td>
    </tr>
    <tr>
      <td>D2</td>
      <td>The model views each document as just a set of words.</td>
    </tr>
    <tr>
      <td>D3</td>
      <td>We will discuss and model these size assumption.</td>
    </tr>
  </tbody>
</table>

<p>根據這些文字，我們可以建立 <em>Term-Document Matrix</em> ，記錄哪個字出現在哪篇文章，如下：</p>

<table>
  <thead>
    <tr>
      <th>Term</th>
      <th>D1</th>
      <th>D2</th>
      <th>D3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>way</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>avoid</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>linear</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>scan</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>index</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>document</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>advance</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>model</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>view</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>set</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>word</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>discuss</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <td>size</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <td>assumption</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>此表格用 <em>boolean</em> 值來表示某個字出現在哪幾篇文章，例如， <em>way</em> 出現在文件 <em>D1</em> ，而沒出現在 <em>D2</em> 和 <em>D3</em> ，所以它的值為 <em>1 0 0</em> 。</p>

<p>為了避免此表格過大，建立過多無意義的 <em>index</em> ，因此運用了以下兩個原則：</p>

<pre><code>1. 忽略 *the* , *a* 這些高頻字（ 這種字稱為 *stop word* ）

2. 將有形態變化的字轉為原型，例如把 *views* 轉為 *view* （ 此過程稱為 *stemming* ） 
</code></pre>

<p>建立了 <em>Term-Document Matrix</em> 之後，就可以用關鍵字來查詢</p>

<p><strong>1.查詢 <em>way</em> 這個字，可得出 <em>way</em> 的值為：</strong></p>

<script type="math/tex; mode=display">

way = [1,0,0] 

</script>

<p>所以 <em>way</em> 出現在 <em>D1</em> 。</p>

<p><strong>2.查詢沒有 <em>way</em> 這個字的，可用以下的 <em>boolean</em> 運算得出結果：</strong></p>

<script type="math/tex; mode=display">

\neg way = \neg ([1,0,0]) = [0 ,1, 1]

</script>

<p>所以沒有 <em>way</em> 的文件為 <em>D2</em> 、 <em>D3</em> 。</p>

<p><strong>3.查詢包含 <em>document</em> 和 <em>model</em> 這兩個字都有文件：</strong></p>

<script type="math/tex; mode=display">

document \wedge model  = [1,1,0] \wedge [0,1,1] = [0 ,1, 0]

</script>

<p>得出結果為 <em>D2</em> 。</p>

<p><strong>4.查詢有 <em>avoid</em> 或 <em>view</em> 其中一個字的文件：</strong></p>

<script type="math/tex; mode=display">

avoid \vee view  = [1,0,0] \vee [0,1,0] = [1 ,1, 0]

</script>

<p>得出結果為 <em>D1</em> 和 <em>D2</em> 。</p>

<h2 id="reference">Reference</h2>

<p>本文參考教科書 <em>Introduction to Information Retrieval</em></p>

<p>http://nlp.stanford.edu/IR-book/</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[機器翻譯 -- Statistical Machine Translation]]></title>
    <link href="http://ckmarkoh.github.io/blog/2014/10/27/machine-translation-statistical-machine-translation/"/>
    <updated>2014-10-27T16:49:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2014/10/27/machine-translation-statistical-machine-translation</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p><em>Statistical Machine Translation</em> （統計機器翻譯）是一種機器翻譯的演算法，這種方法藉由從 <em>parallel corpus</em>（平行語料庫）語料庫，當成訓練資料，訓練出機器學習的模型，以此將句子翻譯成另一個句子。</p>

<p>平行語料庫中包含大量句子，這些句子意思一樣，但分別用兩種語言寫成，例如：</p>

<figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class=""><span class="line">這是一個蘋果。
</span><span class="line">This is an apple.
</span><span class="line">
</span><span class="line">桌上有一本書。
</span><span class="line">There is a book on the table．
</span><span class="line">
</span><span class="line">...... 
</span></code></pre></td></tr></table></div></figure>

<p>藉由這種平行語料庫，就可以用統計的方式，讓機器學會如何將一種語言，翻譯成另一種語言。</p>

<h2 id="noisy-channel-model">Noisy Channel Model</h2>

<p><em>Noicy Channel Model</em> 是一種常用的統計機器翻譯的模型。</p>

<!--more-->

<p>如果，要把中文翻譯成英文，則可用計算某個中文句子翻譯成英文句子的機率，來找出哪個翻譯結果，是比較好的翻譯。給定中文句子 <em>c</em> ，它翻譯成某個英文句子 <em>e</em> 的機率，為 <script type="math/tex">p(e \mid c)</script> ，即為，在中文句子為 <em>c</em> 的條件下，出現英文句子 <em>e</em> 的條件機率。</p>

<p>在 <em>Noicy Channel Model</em> 中，有兩個成分：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& p(e)  & \text{The Language Model}   \\

& p(c \mid e)  & \text{The Translation Model}  

\end{align}

 %]]&gt;</script>

<p>其中， <em>Language Model</em> 表示英文句子 <em>e</em> 在語料庫中的機率。要建立此模型，用單一一種語言的語料庫做訓練即可。</p>

<p>而 <em>Translation Model</em> 表示給定英文句子 <em>e</em> ，在此條件下，翻譯成中文句子 <em>c</em> 的機率。要建立此模型，需要用平行語料庫做訓練。</p>

<p>用貝氏定理，得出某個中文句子 <em>c</em> 翻譯成某個英文句子 <em>e</em> 的機率，為：</p>

<script type="math/tex; mode=display">

p(e \mid c) = \frac{p(e,c)}{p(c)} = \frac{p(e)\times p(c \mid e)}{\sum_{e} p(e)\times p(c \mid e)}

</script>

<p>若要得出翻譯最佳的英文句子，則可以求機率最大者，如下：</p>

<script type="math/tex; mode=display">

\mathop{\arg\,\max}\limits_{e}  p(e \mid c) = \mathop{\arg\,\max}\limits_{e} p(e)\times p(c \mid e)

</script>

<h2 id="example">Example</h2>

<p>舉個例子，若要將中文句 <em>「我肚子餓了」</em> 翻譯成英文。假設已經先從從語料庫中，計算出可能翻譯出的英文句子，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{array}{c | c c}

English & p(c \mid e) & p (e) \\ \hline

\text{I am hungry} & 0.00019 & 0.0084\\

\text{My belly hungry} & 0.00031 & 0.0000031\\

\text{I starve} &  0.00045 & 0.0000012\\

\end{array}

 %]]&gt;</script>

<p>則這句中文最有可能的翻譯為 <em>“I am hungry”</em> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \mathop{\arg\,\max}\limits_{e}  p(e \mid c) \\

& = \mathop{\arg\,\max}\limits_{e} p(e)\times p(c \mid e) \\

& = p(e = \text{"I am hungry"} )\times p(c \mid e = \text{"I am hungry"} ) \\

& =  0.00019 \times 0.0084 = 0.000001596 

\end{align}

 %]]&gt;</script>

<p>註：本例子已先假設 <em>Language Model</em> <script type="math/tex">p(e)</script> 和 <em>Translation Model</em> <script type="math/tex">p(c \mid e)</script> 都已經先計算好。若要從頭計算 <em>Language Model</em> 可以用 <em>n-gram</em> ，但計算 <em>Translation Model</em> 則需要更複雜的模型，例如 <em>IBM Model</em> 或 <em>Phrase-based Model</em> 。</p>

<h2 id="reference">Reference</h2>

<p>本文參考至coursera線上課程</p>

<h4 id="michael-collins-natural-language-processing">Michael Collins. Natural Language Processing</h4>

<p>https://www.coursera.org/course/nlangp</p>
]]></content>
  </entry>
  
</feed>
