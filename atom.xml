<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Mark Chang's Blog]]></title>
  <link href="http://ckmarkoh.github.io/atom.xml" rel="self"/>
  <link href="http://ckmarkoh.github.io/"/>
  <updated>2017-05-31T23:07:29+08:00</updated>
  <id>http://ckmarkoh.github.io/</id>
  <author>
    <name><![CDATA[Mark Chang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Linear Programming 1: Standard Form of Linear Programming]]></title>
    <link href="http://ckmarkoh.github.io/blog/2017/05/30/optimization-linear-programming-standard-form/"/>
    <updated>2017-05-30T09:05:30+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2017/05/30/optimization-linear-programming-standard-form</id>
    <content type="html"><![CDATA[<h2 id="what-is-linear-programming">What is Linear Programming?</h2>

<p>Linear Programming 是一種最佳化問題，在此最佳化問題中。Objective Function 和Constraints 都是線性的。</p>

<p>舉個例子，以下為一個 Linear Programming 的問題<a name="eq1">（公式一）</a>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}
&\text{Objective Function} \\
&\mspace{20mu}\text{maximize: }  2x_1+3x_2  \\
&\text{Constraints} \\
&\mspace{20mu} x_1 + x_2  \leq 2 \\
&\mspace{20mu} x_1 + 2x_2 \leq 3 \\
&\mspace{20mu} x_1 , x_2  \geq 0  \\
\end{align}

 %]]&gt;</script>

<p>將 Constraints 畫在平面上，得出下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/new/linearprog1.png" alt="" /></p>

<!--more-->

<p>其中，淺藍色區域 <script type="math/tex">\mathbf{P}</script> 是 <script type="math/tex">(x_1,x_2)</script> 滿足 Constraints 的範圍，稱為 Feasible Domain。</p>

<script type="math/tex; mode=display">

P = \{(x_1,x_2) \mid x_1+x_2 \leq 2 , x_1+2x_2 \leq 3, x_1,x_2 \geq 0\}

</script>

<p>而從 Feasible Domain 中，可以使 Objective Function 得到最大值的 <script type="math/tex">(x_1,x_2)</script> 為最佳解。 
此問題的最佳解為 <script type="math/tex">(x_1,x_2)=(1,1)</script> ，因為它使 Objective Function 得最大值：</p>

<script type="math/tex; mode=display">
2x_1+3x_2 = 2\times 1 + 3\times 1 = 5
</script>

<p>Linear Programming 的解法主要有兩種：分別是Simplex Method 和 Interior Point Method。本文先不講解這兩種方法，而會在後續文章講解。</p>

<h2 id="formulate-a-linear-programming-problem">Formulate A Linear Programming Problem</h2>

<p>在現實生活的應用中，有許多問題是可以用 Linear Programming 來解，本段舉例，如何將有限的資源下將利潤最大化的問題，寫成 Linear Programming 的形式，例如：</p>

<p>某工廠生產兩種產品X和Y ，這兩種產品都需要用到兩種原料 A和B，生產1公噸X需要1公噸的A和1公噸的B，生產1公噸Y需要1公噸的A和2公噸的B。工廠存有原料A有2公噸，B有3公噸。而1公噸X可賣2百萬元，1公噸Y可賣3百萬元。 以工廠的現有的資源，要生產多少X和Y怎樣才能獲得最大利潤?</p>

<p>設工廠要生產 <script type="math/tex">x_1</script> 公噸的 X ， <script type="math/tex">y_1</script> 公噸的 Y ，才能獲得最大利潤。</p>

<p>目標是要將利潤最大化，由於而1公噸X可賣2百萬元，1公噸Y可賣3百萬元，所以 Objective Function 如下（省略百萬元）：</p>

<script type="math/tex; mode=display">
\begin{align}
\text{maximize: }  2x_1+3x_2  \\
\end{align}
</script>

<p>但是 X 和 Y 的產量受限於目前工廠僅存的原料量， <script type="math/tex">x_1 + x_2</script> 為原料A的用量，不可多餘2公噸，以此類推，<script type="math/tex">x_1 + 2x_2</script> 為原料 B的用量，不可多餘3公噸。又由於產量不可能為負，因此 <script type="math/tex">x_1,x_2</script> 都不能為負， 所以 Constraints 如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\mspace{40mu} x_1 + x_2  \leq 2 \\
&\mspace{40mu} x_1 + 2x_2 \leq 3 \\
&\mspace{40mu} x_1 , x_2  \geq 0  \\
\end{align}
 %]]&gt;</script>

<p>結合以上的 Objective Function 和 Constraints，即為<a href="#eq1">（公式一）</a>的 Linear Programming 問題。</p>

<h2 id="standard-form-of-linear-programming">Standard Form of Linear Programming</h2>

<p>在解 Linear Programming 的問題時，會先把問題轉成 Standard Form，使得不同的 Linear Programming 問題能有一致性，以便之後能用相同的演算法來求解。
Linear Programming 的 Standard Form 如下<a name="eq2">（公式二）</a>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}
&\text{1.Objective Function} \\
&\mspace{20mu}\text{minimize: } z = c_1x_1+c_2x_2+...+c_nx_n \\
&\text{2.Equality Constraints} \\
&\mspace{20mu} a_{11}x_1 + a_{12}x_2 +...+a_{1n}x_n = b_1 \\
&\mspace{20mu} a_{21}x_1 + a_{22}x_2 +...+a_{2n}x_n = b_2 \\
&\mspace{20mu} \vdots \\
&\mspace{20mu} a_{m1}x_1 + a_{m2}x_2 +...+a_{mn}x_n = b_m \\
&\text{3.Non-Negative Variables} \\
&\mspace{20mu} x_1 , x_2,...,x_n  \geq 0  \\
\end{align}

 %]]&gt;</script>

<p>其中，$n$ 為 Variables 的個數， $m$ 為 Constraints 的個數。通常 <script type="math/tex">n >  m</script> ，這樣才需要從 Feasible Domain 中挑選出最佳解，不然 Feasible Domain 可能會只有一個點 ， 或者根本無解。</p>

<p>而 Standard Form 須滿足以下三個條件：</p>

<ol>
  <li>目標是將 Objective Function 最小化，而非最大化。</li>
  <li>Constraints 皆為 Equality Constraints ，沒有 Inequality Constraints。</li>
  <li>Variables 都必須是非負實數。</li>
</ol>

<p>為了以矩陣運算來加速，也可以把<a href="#eq2">（公式二）</a> 的 Standard Form 寫成矩陣的形式<a name="eq3">（公式三）</a>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\mathbf{c} = \begin{bmatrix}
c_1 \\
c_2 \\
\vdots \\
c_n \\
\end{bmatrix}
\mathbf{x} = \begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n \\
\end{bmatrix}
\mathbf{b} = \begin{bmatrix}
b_1 \\
b_2 \\
\vdots \\
b_m \\
\end{bmatrix} \\
&\mathbf{A} = \begin{bmatrix}
a_{11} &a_{12} &\cdots &a_{1n} \\
a_{21} &a_{22} &\cdots &a_{2n} \\
\vdots &\vdots &\vdots &\vdots \\
a_{m1} &a_{m2} &\cdots &a_{mn} \\
\end{bmatrix} \\
\end{align}
 %]]&gt;</script>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\text{Objective Function} \\
&\mspace{40mu} \text{minimize: } z =\mathbf{c}^T\mathbf{x}  \\
&\text{Equality Constraints} \\
&\mspace{40mu} \mathbf{A}\mathbf{x} = \mathbf{b} \\
&\text{Non-Negative Variables} \\
&\mspace{40mu} \mathbf{x}\geq 0 \\
\end{align}
 %]]&gt;</script>

<h2 id="converting-into-standard-form">Converting into Standard Form</h2>

<p>我們可以把前例<a href="#eq1">（公式一）</a>轉換成<a href="#eq2">（公式二）</a> 的 standard form 。</p>

<p>首先，為了滿足條件 1.， 「目標是將 Objective Function 最小化。」
必須將 Objective Function 改為 Minimize。這步驟很簡單，把原本的 Objective Function 乘上負號就可以了：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\text{maximize: }  2x_1+3x_2  \\
&\Rightarrow \text{minimize: }  -2x_1-3x_2  \\
\end{align}
 %]]&gt;</script>

<p>下一步，為了滿足條件 2.，「 Constraints 皆為 Equality Constraints 」，則須將 Inequality Constraints 轉換成 Equality Constraints 。 這可以藉由加上 Slack Variable 來達成。</p>

<p>例如 Inequalitn Constraint：</p>

<script type="math/tex; mode=display">
x_1 + x_2  \leq 2 
</script>

<p>可以將它加上 Slack Variable ，<script type="math/tex">x_3</script> 。它可以補足原本不等式少於2的部分，使之等於2。如下：</p>

<script type="math/tex; mode=display">
\Rightarrow  x_1 + x_2 + x_3  = 2 , \mspace{20mu} x_3 \geq 0 
</script>

<p>同理，另一個 Constraints 也可以加上另一個 Slack Variable ，<script type="math/tex">x_4</script> 。</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&x_1 + 2x_2 \leq 3 \\
&\Rightarrow  x_1 + 2x_2 + x_4  = 3 , \mspace{20mu} x_4 \geq 0 \\
\end{align}
 %]]&gt;</script>

<p>經過這些轉換後，<a href="#eq1">（公式一）</a> 變成 Standard Form ，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\text{Objective Function} \\
&\mspace{20mu}\text{minimize: } -2x_1-3x_2  \\
&\text{Constraints} \\
&\mspace{20mu} x_1 + x_2 + x_3 =  2 \\
&\mspace{20mu} x_1 + 2x_2 + x_4 = 3 \\
&\mspace{20mu} x_1 , x_2 , x_3 , x_4 \geq 0  \\
\end{align}
 %]]&gt;</script>

<p>可以再將它寫成矩陣形式：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\mathbf{c} = \begin{bmatrix}
-2 \\
-3 \\
0 \\
0 \\
\end{bmatrix}
\mathbf{x} = \begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
\end{bmatrix}
\mathbf{b} = \begin{bmatrix}
2 \\
3 \\
\end{bmatrix} \\
&\mathbf{A} = \begin{bmatrix}
1 &1 &1 &0 \\
1 &2 &0 &1 \\
\end{bmatrix} \\
\end{align}
 %]]&gt;</script>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\text{minimize: } z =\mathbf{c}^T\mathbf{x}  \\
&\text{subject to: } \mathbf{A}\mathbf{x} = \mathbf{b} \\
&\mathbf{x}\geq 0 \\
\end{align}
 %]]&gt;</script>

<p>至於如何得出 Linear Programming 的最佳解？會在後續文章講解。</p>

<h2 id="reference">Reference</h2>

<p>本文參考交通大學開放式課程：線性規劃</p>

<p>http://ocw.nctu.edu.tw/course_detail.php?bgid=3&amp;gid=0&amp;nid=245#.WS2AasklFYc</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Torch NN Tutorial 6 : Backward Propagation ( Part 3 : nn.Module )]]></title>
    <link href="http://ckmarkoh.github.io/blog/2017/01/01/torch-nn-tutorial-6-backward-propagation/"/>
    <updated>2017-01-01T18:36:08+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2017/01/01/torch-nn-tutorial-6-backward-propagation</id>
    <content type="html"><![CDATA[<h2 id="backward-propagation-in-nnmodule">Backward Propagation in nn.Module</h2>

<p>本文接續 <a href="http://ckmarkoh.github.io/blog/2017/01/01/torch-nn-tutorial-4-backward-propagation/">Torch NN Tutorial 4: Backward Propagation (part 1)</a> ，與 <a href="http://ckmarkoh.github.io/blog/2017/01/01/torch-nn-tutorial-5-backward-propagation/">Torch NN Tutorial 5: Backward Propagation (part 2)</a> 講解 <code>nn.Module</code> 中， 與 backward propagation 有關的程式碼。</p>

<p><code>nn.Module</code> 程式碼：<a href="https://github.com/torch/nn/blob/master/Module.lua">https://github.com/torch/nn/blob/master/Module.lua</a></p>

<p>與 backward propagation 有關的，主要有以下三個運算 ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\text{1.} \mspace{20mu} \Delta_\textbf{W} = 0 \\
&   \mspace{40mu} \Delta_\textbf{b} = 0 \\
&\text{2.} \mspace{20mu} \Delta_\textbf{W} =  \frac{\partial J}{\partial \textbf{W}} =  \frac{\partial J}{\partial \textbf{y}}  \frac{\partial \textbf{y}}{\partial \textbf{W}}  \\ 
&   \mspace{40mu} \Delta_\textbf{b} =  \frac{\partial J}{\partial \textbf{b}} =  \frac{\partial J}{\partial \textbf{y}}  \frac{\partial \textbf{y}}{\partial \textbf{b}}  \\
&\text{3.} \mspace{20mu} \textbf{W} \leftarrow \textbf{W}  - \eta \Delta_\textbf{W} \\ 
&   \mspace{40mu} \textbf{b} \leftarrow \textbf{b} - \eta \Delta_\textbf{b} \\
\end{align}
 %]]&gt;</script>

<p>而這三個運算，分別對應到以下三個 function ：</p>

<ol>
  <li>
    <p>zeroGradParameters</p>
  </li>
  <li>
    <p>backward</p>
  </li>
  <li>
    <p>updateParameters</p>
  </li>
</ol>

<p>本文將詳細講解這三個 function 的程式碼內容。</p>

<p>註：</p>

<p>1.本文的 <script type="math/tex">\textbf{y}</script> 同前兩篇文（Torch NN Tutorial 4~5）中的 <script type="math/tex">\bar{y}</script> 。</p>

<p>2.本文中的 <script type="math/tex">\textbf{W}</script> 為矩陣，而 <script type="math/tex">\textbf{x}</script> ，<script type="math/tex">\textbf{y}</script> 為向量。</p>

<!--more-->

<h2 id="zerogradparameters">1.zeroGradParameters</h2>

<p>此步驟是將 <script type="math/tex">\Delta_\textbf{W}</script> 和 <script type="math/tex">\Delta_\textbf{b}</script> 歸零。</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \Delta_\textbf{W} = 0 \\
& \Delta_\textbf{b} = 0 \\
\end{align}
 %]]&gt;</script>

<p>在程式中， <script type="math/tex">\Delta_\textbf{W}</script> 和 <script type="math/tex">\Delta_\textbf{b}</script> 的變數是 <code>gradWeight</code> 和 <code>gradBias</code> 。可用 <code>zeroGradParameters</code> 將它們歸零，程式碼如下：</p>

<figure class="code"><figcaption><span>nn/Module.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">Module</span><span class="p">:</span><span class="n">zeroGradParameters</span><span class="p">()</span>
</span><span class="line">   <span class="kd">local</span> <span class="n">_</span><span class="p">,</span><span class="n">gradParams</span> <span class="o">=</span> <span class="n">self</span><span class="p">:</span><span class="n">parameters</span><span class="p">()</span>
</span><span class="line">   <span class="k">if</span> <span class="n">gradParams</span> <span class="k">then</span>
</span><span class="line">      <span class="k">for</span> <span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="o">#</span><span class="n">gradParams</span> <span class="k">do</span>
</span><span class="line">         <span class="n">gradParams</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span><span class="n">zero</span><span class="p">()</span>
</span><span class="line">      <span class="k">end</span>
</span><span class="line">   <span class="k">end</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>在第五行中，可看到 <code>gradParams[i]:zero()</code> ，即是將所有的 <code>gradParams</code> ，包含 <code>gradWeight</code> 及 <code>gradBias</code> 歸零。</p>

<p>由於 <code>nn.Module</code> 本身沒有 <code>gradWeight</code> 和 <code>gradBias</code> ，如果要取得它們，就要透過 <code>self:parameters()</code> 來取得。
<code>parameters</code> 的程式碼如下：</p>

<figure class="code"><figcaption><span>nn/Module.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">Module</span><span class="p">:</span><span class="n">parameters</span><span class="p">()</span>
</span><span class="line">   <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">weight</span> <span class="ow">and</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="k">then</span>
</span><span class="line">      <span class="k">return</span> <span class="p">{</span><span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="p">},</span> <span class="p">{</span><span class="n">self</span><span class="p">.</span><span class="n">gradWeight</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">gradBias</span><span class="p">}</span>
</span><span class="line">   <span class="k">elseif</span> <span class="n">self</span><span class="p">.</span><span class="n">weight</span> <span class="k">then</span>
</span><span class="line">      <span class="k">return</span> <span class="p">{</span><span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="p">},</span> <span class="p">{</span><span class="n">self</span><span class="p">.</span><span class="n">gradWeight</span><span class="p">}</span>
</span><span class="line">   <span class="k">elseif</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="k">then</span>
</span><span class="line">      <span class="k">return</span> <span class="p">{</span><span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="p">},</span> <span class="p">{</span><span class="n">self</span><span class="p">.</span><span class="n">gradBias</span><span class="p">}</span>
</span><span class="line">   <span class="k">else</span>
</span><span class="line">      <span class="k">return</span>
</span><span class="line">   <span class="k">end</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>從第二行開始，可看到，如果繼承 <code>nn.Module</code> 的類別有實作 <code>weight</code> ，則回傳 <code>weight</code> 和 <code>gradWeight</code> ，以此類推。</p>

<p>至於歸零的作用，因為在建立 Module 的時候， <code>gradWeight</code> 和 <code>gradBias</code> 並沒有被設為什麼值，它有可能是任意數。舉個例子，建立一個 <code>nn.Linear</code> ，命名為 <code>l1</code> ， input size 為 2， output size 為 3，一開始什麼都沒做，就直接印出  <code>gradWeight</code> 和 <code>gradBias</code> ，程式如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">gradWeight</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">gradBias</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>得出結果如下，它們可能是任意數：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">-1.2882e-231 -1.2882e-231
</span><span class="line"> 2.1403e+161  3.9845e+252
</span><span class="line">  7.7075e-43   4.5713e-71
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3x2<span class="o">]</span>
</span><span class="line">
</span><span class="line">-1.2882e-231
</span><span class="line">-1.2882e-231
</span><span class="line">  1.1659e-28
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>如果用 <code>zeroGradParameters</code> 將它們歸零，程式碼如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">l1</span><span class="p">:</span><span class="n">zeroGradParameters</span><span class="p">()</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">gradWeight</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">gradBias</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下，可看到它們都歸零了：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">  <span class="m">0</span>  0
</span><span class="line">  <span class="m">0</span>  0
</span><span class="line">  <span class="m">0</span>  0
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3x2<span class="o">]</span>
</span><span class="line">
</span><span class="line">  0
</span><span class="line">  0
</span><span class="line">  0
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<h2 id="backward">2.backward</h2>

<p>再來看到 <code>backward</code> 的部分， <code>backward</code> 個功能由兩個 <code>function</code> 來執行，分別是 <code>updateGradInput</code> 及 <code>accGradParameters</code> 。</p>

<figure class="code"><figcaption><span>nn/Module.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">Module</span><span class="p">:</span><span class="n">backward</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">gradOutput</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
</span><span class="line">   <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span> <span class="ow">or</span> <span class="mi">1</span>
</span><span class="line">   <span class="n">self</span><span class="p">:</span><span class="n">updateGradInput</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">gradOutput</span><span class="p">)</span>
</span><span class="line">   <span class="n">self</span><span class="p">:</span><span class="n">accGradParameters</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">gradOutput</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
</span><span class="line">   <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">gradInput</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>為何要分這兩部分？從數學公式上來看，如果 <code>input</code> 是 <script type="math/tex">\textbf{x}</script> ， <code>output</code> 是 <script type="math/tex">\textbf{y}</script> ，loss function 為 <script type="math/tex">J</script>   ，則：</p>

<script type="math/tex; mode=display">
 \textbf{y}=\textbf{W}\textbf{x}+\textbf{b}
</script>

<p><code>accGradParameters</code> 是負責計算以下兩項，也就是 <code>gradWeight</code> 和 <code>gradBias</code> 的值。</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \Delta_\textbf{W} =  \frac{\partial J}{\partial \textbf{W}} =  \frac{\partial J}{\partial \textbf{y}}  \frac{\partial \textbf{y}}{\partial \textbf{W}}  \\ 
& \Delta_\textbf{b} =  \frac{\partial J}{\partial \textbf{b}} =  \frac{\partial J}{\partial \textbf{y}}  \frac{\partial \textbf{y}}{\partial \textbf{b}}  \\
\end{align}
 %]]&gt;</script>

<p>除了以上兩項以外，如果 input 前面還有其他 layer 的話，就需計算 <script type="math/tex"> \frac{\partial J}{\partial \textbf{x}}</script> ，並將此值往前傳遞。</p>

<p><code>updateGradInput</code> 是負責計算 <script type="math/tex">\frac{\partial J}{\partial \textbf{x}}</script> 的值，也就是 <code>gradInput</code> 的值，如下：</p>

<script type="math/tex; mode=display">

\frac{\partial J}{\partial \textbf{x}} = \frac{\partial J}{\partial \textbf{y}}\frac{\partial \textbf{y}}{\partial \textbf{x}}

</script>

<p>在 <code>nn.Module</code> 中，這兩個 function 皆沒實作，需由繼承 <code>nn.Module</code> 的類別來實作。</p>

<figure class="code"><figcaption><span>nn/Module.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">Module</span><span class="p">:</span><span class="n">updateGradInput</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">gradOutput</span><span class="p">)</span>
</span><span class="line">   <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">gradInput</span>
</span><span class="line"><span class="k">end</span>
</span><span class="line">
</span><span class="line"><span class="k">function</span> <span class="nf">Module</span><span class="p">:</span><span class="n">accGradParameters</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">gradOutput</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p><code>nn.Linear</code> 則實作了這兩個 function。</p>

<p><code>nn.Linear</code> 程式碼：<a href="https://github.com/torch/nn/blob/master/Linear.lua">https://github.com/torch/nn/blob/master/Linear.lua</a></p>

<h3 id="updategradinput">updateGradInput</h3>

<p>先看 <code>updateGradInput</code> 的程式碼：</p>

<figure class="code"><figcaption><span>nn/Linear.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">Linear</span><span class="p">:</span><span class="n">updateGradInput</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">gradOutput</span><span class="p">)</span>
</span><span class="line">   <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">gradInput</span> <span class="k">then</span>
</span><span class="line">
</span><span class="line">      <span class="kd">local</span> <span class="n">nElement</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">gradInput</span><span class="p">:</span><span class="n">nElement</span><span class="p">()</span>
</span><span class="line">      <span class="n">self</span><span class="p">.</span><span class="n">gradInput</span><span class="p">:</span><span class="n">resizeAs</span><span class="p">(</span><span class="n">input</span><span class="p">)</span>
</span><span class="line">      <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">gradInput</span><span class="p">:</span><span class="n">nElement</span><span class="p">()</span> <span class="o">~=</span> <span class="n">nElement</span> <span class="k">then</span>
</span><span class="line">         <span class="n">self</span><span class="p">.</span><span class="n">gradInput</span><span class="p">:</span><span class="n">zero</span><span class="p">()</span>
</span><span class="line">      <span class="k">end</span>
</span><span class="line">      <span class="k">if</span> <span class="n">input</span><span class="p">:</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">then</span>
</span><span class="line">         <span class="n">self</span><span class="p">.</span><span class="n">gradInput</span><span class="p">:</span><span class="n">addmv</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="p">:</span><span class="n">t</span><span class="p">(),</span> <span class="n">gradOutput</span><span class="p">)</span>
</span><span class="line">      <span class="k">elseif</span> <span class="n">input</span><span class="p">:</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">then</span>
</span><span class="line">         <span class="n">self</span><span class="p">.</span><span class="n">gradInput</span><span class="p">:</span><span class="n">addmm</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">gradOutput</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
</span><span class="line">      <span class="k">end</span>
</span><span class="line">
</span><span class="line">      <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">gradInput</span>
</span><span class="line">   <span class="k">end</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>此處可分為兩部分來看，在 <a href="http://ckmarkoh.github.io/blog/2016/12/19/torch-nn-tutorial-1-nn-module/">Torch NN Tutorial 1 : NN.Module &amp; NN.Linear</a> 曾講到，當 <code>input:dim() == 1</code> 時，是一次輸入一筆資料。</p>

<p>計算 <code>gradInput</code> 公式如下：</p>

<script type="math/tex; mode=display">

\frac{\partial J}{\partial \textbf{x}} = \frac{\partial J}{\partial \textbf{y}}\frac{\partial \textbf{y}}{\partial \textbf{x}}

</script>

<p>公式中的 <script type="math/tex">\frac{\partial J}{\partial \textbf{y}}</script> ，在程式中是 loss function 的 <code>gradInput</code> ，並從 <code>updateGradInput</code> 的參數 <code>gradOutput</code> 輸入。</p>

<p>假設 output size 為 3， input size 為 2，則在 forward propagation 時，<script type="math/tex">\textbf{y}</script> 是由以下公式算出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \textbf{y} = \textbf{W}\textbf{x} + \textbf{b} \\
& \Rightarrow
\begin{bmatrix}
y_{1} \\
y_{2} \\ 
y_{3} \\
\end{bmatrix}
= 
\begin{bmatrix}
w_{11} & w_{12} \\
w_{21} & w_{22} \\
w_{31} & w_{32} \\
\end{bmatrix}
\begin{bmatrix}
x_{1}
x_{2}
\end{bmatrix}
+
\begin{bmatrix}
b_{1} \\
b_{2} \\
b_{3} \\
\end{bmatrix} \\
= 
&\begin{bmatrix}
w_{11}x_{1}+w_{12}x_{2}+b_{1} \\
w_{21}x_{1}+w_{22}x_{2}+b_{2} \\
w_{31}x_{1}+w_{32}x_{2}+b_{3} \\
\end{bmatrix} \\
\end{align}
 %]]&gt;</script>

<p>而 <script type="math/tex"> \frac{\partial \textbf{y}}{\partial \textbf{x}} </script>  的結果如下，</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}
& \frac{\partial \textbf{y}}{\partial \textbf{x}} 
 = 
\begin{bmatrix}
\frac{\partial y_{1}}{\partial x_{1}} & \frac{\partial y_{1}}{\partial x_{2}} \\
\frac{\partial y_{2}}{\partial x_{1}} & \frac{\partial y_{2}}{\partial x_{2}} \\
\frac{\partial y_{3}}{\partial x_{1}} & \frac{\partial y_{3}}{\partial x_{2}} \\
\end{bmatrix} \\
&= 
\begin{bmatrix}
\frac{w_{11}x_{1}+w_{12}x_{2}+b_{1}}{\partial x_{1}} & \frac{w_{11}x_{1}+w_{12}x_{2}+b_{1}}{\partial x_{2}} \\
\frac{w_{21}x_{1}+w_{22}x_{2}+b_{2}}{\partial x_{1}} & \frac{w_{21}x_{1}+w_{22}x_{2}+b_{2}}{\partial x_{2}} \\
\frac{w_{31}x_{1}+w_{32}x_{2}+b_{3}}{\partial x_{1}} & \frac{w_{31}x_{1}+w_{32}x_{2}+b_{3}}{\partial x_{2}} \\
\end{bmatrix} \\
&= 
\begin{bmatrix}
w_{11} & w_{12} \\
w_{21} & w_{22} \\
w_{31} & w_{32} \\
\end{bmatrix}\\
&= \textbf{W}
\end{align}
 %]]&gt;</script>

<p>因此， <script type="math/tex">\frac{\partial J}{\partial \textbf{x}}</script> 的值為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\frac{\partial J}{\partial \textbf{x}} 
=\begin{bmatrix}
\frac{\partial J}{\partial x_{1}} \\
\frac{\partial J}{\partial x_{2}}
\end{bmatrix} \\
&=\begin{bmatrix}
\frac{\partial J}{\partial y_{1}}\frac{\partial y_{1}}{\partial x_{1}} +
\frac{\partial J}{\partial y_{2}}\frac{\partial y_{2}}{\partial x_{1}} +
\frac{\partial J}{\partial y_{3}}\frac{\partial y_{3}}{\partial x_{1}} 
\\
\frac{\partial J}{\partial y_{1}}\frac{\partial y_{1}}{\partial x_{2}} +
\frac{\partial J}{\partial y_{2}}\frac{\partial y_{2}}{\partial x_{2}} +
\frac{\partial J}{\partial y_{3}}\frac{\partial y_{3}}{\partial x_{2}} 
\\
\end{bmatrix} \\

&=
\begin{bmatrix}
\frac{\partial y_{1}}{\partial x_{1}} & \frac{\partial y_{1}}{\partial x_{2}} \\
\frac{\partial y_{2}}{\partial x_{1}} & \frac{\partial y_{2}}{\partial x_{2}} \\
\frac{\partial y_{3}}{\partial x_{1}} & \frac{\partial y_{3}}{\partial x_{2}} \\
\end{bmatrix} ^{T}
\begin{bmatrix}
\frac{\partial J}{\partial y_{1}} \\
\frac{\partial J}{\partial y_{2}} \\ 
\frac{\partial J}{\partial y_{3}} \\
\end{bmatrix}\\

&=
\textbf{W}^{T}\frac{\partial J}{\partial \textbf{y}}
\end{align}\\
 %]]&gt;</script>

<p>從以上結果得知，要先將 <code>weight</code> 轉置，再和 <code>gradOutput</code> 做 矩陣-向量 乘積。如同程式碼中所寫 <code>self.gradInput:addmv(0, 1, self.weight:t(), gradOutput)</code></p>

<p>如果 <script type="math/tex">W</script> 和 <script type="math/tex">\frac{\partial J}{\partial \textbf{y}}</script> 的值分別如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \textbf{W} =
\begin{bmatrix}
 0.1453 & 0.5062 \\
 0.0635 & 0.4911 \\
-0.1080 & 0.1747 \\
\end{bmatrix} \\
& \frac{\partial J}{\partial \textbf{y}} = 
\begin{bmatrix}
1 \\
2 \\
3 \\ 
\end{bmatrix}
\end{align}
 %]]&gt;</script>

<p>則， <script type="math/tex">\frac{\partial J}{\partial \textbf{x}}</script> 的值為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\frac{\partial J}{\partial \textbf{x}} 
=\textbf{W}^{T}\frac{\partial J}{\partial \textbf{y}}\\
&=
\begin{bmatrix}
 0.1453 & 0.0635 & -0.1080 \\ 
 0.5062 & 0.4911 & 0.1747 \\
 \end{bmatrix}
\begin{bmatrix}
1 \\
2 \\
3 \\ 
\end{bmatrix}\\
&=
\begin{bmatrix}
 0.1453 \times 1 & 0.0635 \times 2  & -0.1080 \times 3  \\ 
 0.5062 \times 1 & 0.4911 \times 2  & 0.1747 \times 3  \\
\end{bmatrix} \\
&=
\begin{bmatrix}
-0.0516 \\
 2.0126 \\
\end{bmatrix}\\
\end{align}
 %]]&gt;</script>

<p>執行 <code>updateGradInput</code>  ，程式碼如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">dj_dy</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">}</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">{</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">}</span>
</span><span class="line"><span class="n">l1</span><span class="p">:</span><span class="n">updateGradInput</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span><span class="n">dj_dy</span> <span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">gradInput</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">-0.0516
</span><span class="line"> 2.0126
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 2<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>以上程式碼，輸入 <code>x</code> 只是為了滿足 <code>input:dim() == 1</code> 的條件， 而 <code>x</code> 的數值是不會影響結果的。</p>

<p>至於 <code>input:dim() == 2</code> 的情況，推導方式同上，在此不詳細推導。</p>

<p>再來看 <code>accGradParameters</code> 的程式碼：</p>

<h3 id="accgradparameters">accGradParameters</h3>

<figure class="code"><figcaption><span>nn/Linear.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">Linear</span><span class="p">:</span><span class="n">accGradParameters</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">gradOutput</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
</span><span class="line">   <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span> <span class="ow">or</span> <span class="mi">1</span>
</span><span class="line">   <span class="k">if</span> <span class="n">input</span><span class="p">:</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">then</span>
</span><span class="line">      <span class="n">self</span><span class="p">.</span><span class="n">gradWeight</span><span class="p">:</span><span class="n">addr</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">gradOutput</span><span class="p">,</span> <span class="n">input</span><span class="p">)</span>
</span><span class="line">      <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="k">then</span> <span class="n">self</span><span class="p">.</span><span class="n">gradBias</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">gradOutput</span><span class="p">)</span> <span class="k">end</span>
</span><span class="line">   <span class="k">elseif</span> <span class="n">input</span><span class="p">:</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">then</span>
</span><span class="line">      <span class="n">self</span><span class="p">.</span><span class="n">gradWeight</span><span class="p">:</span><span class="n">addmm</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">gradOutput</span><span class="p">:</span><span class="n">t</span><span class="p">(),</span> <span class="n">input</span><span class="p">)</span>
</span><span class="line">      <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="k">then</span>
</span><span class="line">         <span class="c1">-- update the size of addBuffer if the input is not the same size as the one we had in last updateGradInput</span>
</span><span class="line">         <span class="n">updateAddBuffer</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input</span><span class="p">)</span>
</span><span class="line">         <span class="n">self</span><span class="p">.</span><span class="n">gradBias</span><span class="p">:</span><span class="n">addmv</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">gradOutput</span><span class="p">:</span><span class="n">t</span><span class="p">(),</span> <span class="n">self</span><span class="p">.</span><span class="n">addBuffer</span><span class="p">)</span>
</span><span class="line">      <span class="k">end</span>
</span><span class="line">   <span class="k">end</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>先看到 <code>input:dim() == 1</code> 的情形，計算 <code>gradWeight</code> 的公式如下：</p>

<script type="math/tex; mode=display">

 \Delta_\textbf{W} =  \frac{\partial J}{\partial \textbf{W}} =  \frac{\partial J}{\partial \textbf{y}}  \frac{\partial \textbf{y}}{\partial \textbf{W}}  

</script>

<p>公式中的 <script type="math/tex">\frac{\partial J}{\partial \textbf{y}}</script> ，在程式中是 loss function 的 <code>gradInput</code> ，並從 <code>updateGradInput</code> 的參數 <code>gradOutput</code> 輸入。</p>

<p>假設 output size 為 3， input size 為 2，則 <script type="math/tex"> \frac{\partial \textbf{y}}{\partial \textbf{W}} </script>  的結果如下，</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}
& \frac{\partial \textbf{y}}{\partial \textbf{W}} 
 = 
\begin{bmatrix}
\frac{\partial y_{1}}{\partial w_{11}} & \frac{\partial y_{1}}{\partial w_{12}} \\
\frac{\partial y_{2}}{\partial w_{21}} & \frac{\partial y_{2}}{\partial w_{22}} \\
\frac{\partial y_{3}}{\partial w_{31}} & \frac{\partial y_{3}}{\partial w_{32}} \\
\end{bmatrix} \\
&= 
\begin{bmatrix}
\frac{w_{11}x_{1}+w_{12}x_{2}+b_{1}}{\partial w_{11}} & \frac{w_{11}x_{1}+w_{12}x_{2}+b_{1}}{\partial w_{12}} \\
\frac{w_{21}x_{1}+w_{22}x_{2}+b_{2}}{\partial w_{21}} & \frac{w_{21}x_{1}+w_{22}x_{2}+b_{2}}{\partial w_{22}} \\
\frac{w_{31}x_{1}+w_{32}x_{2}+b_{3}}{\partial w_{31}} & \frac{w_{31}x_{1}+w_{32}x_{2}+b_{3}}{\partial w_{32}} \\
\end{bmatrix} \\
&= 
\begin{bmatrix}
x_{1} & x_{2} \\
x_{1} & x_{2} \\
x_{1} & x_{2} \\
\end{bmatrix}\\
\end{align}
 %]]&gt;</script>

<p>因此， <script type="math/tex"> \frac{\partial J}{\partial \textbf{W}} </script> 的值為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \frac{\partial J}{\partial \textbf{W}} 
 = 
\begin{bmatrix}
\frac{\partial J}{\partial y_{1}} \frac{\partial y_{1}}{\partial w_{11}} &
\frac{\partial J}{\partial y_{1}} \frac{\partial y_{1}}{\partial w_{12}} \\
\frac{\partial J}{\partial y_{2}} \frac{\partial y_{2}}{\partial w_{21}} &
\frac{\partial J}{\partial y_{2}} \frac{\partial y_{2}}{\partial w_{22}} \\
\frac{\partial J}{\partial y_{3}} \frac{\partial y_{3}}{\partial w_{31}} &
\frac{\partial J}{\partial y_{3}} \frac{\partial y_{3}}{\partial w_{32}} \\
\end{bmatrix} \\
&= 
\begin{bmatrix}
\frac{\partial J}{\partial y_{1}} x_{1} &
\frac{\partial J}{\partial y_{1}} x_{2} \\
\frac{\partial J}{\partial y_{2}} x_{1} &
\frac{\partial J}{\partial y_{2}} x_{2} \\
\frac{\partial J}{\partial y_{3}} x_{1} &
\frac{\partial J}{\partial y_{3}} x_{2} \\
\end{bmatrix} \\
&= 
\begin{bmatrix}
\frac{\partial J}{\partial y_{1}} \\
\frac{\partial J}{\partial y_{2}} \\
\frac{\partial J}{\partial y_{3}} \\
\end{bmatrix}
\begin{bmatrix}
x_{1} &  x_{2} \\
\end{bmatrix}\\
&= \frac{\partial J}{\partial \textbf{y}} \textbf{x}^{T}
\end{align}
 %]]&gt;</script>

<p>如果 <script type="math/tex">\frac{\partial J}{\partial \textbf{y}}</script> 的值同前例， <script type="math/tex">\textbf{x}</script> 的值如下：</p>

<script type="math/tex; mode=display">
\textbf{x} =
\begin{bmatrix}
4 \\
5 \\
\end{bmatrix} 
</script>

<p>則， <script type="math/tex">\frac{\partial J}{\partial \textbf{W}}</script> 的值為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}
& \frac{\partial J}{\partial \textbf{W}} 
= \frac{\partial J}{\partial \textbf{y}}\textbf{x}^{T}\\
& = 
\begin{bmatrix}
1 \\
2 \\
3 \\ 
\end{bmatrix}
\begin{bmatrix}
4 & 5 
\end{bmatrix}\\ 
&=
\begin{bmatrix}
1 \times 4 & 1 \times 5 \\
2 \times 4 & 2 \times 5 \\
3 \times 4 & 3 \times 5 \\ 
\end{bmatrix}\\
&=
\begin{bmatrix}
4 & 5 \\
8 & 10 \\
12 & 15 \\ 
\end{bmatrix}\\
\end{align}
 %]]&gt;</script>

<p>計算 <code>gradBias</code> 的公式如下：</p>

<script type="math/tex; mode=display">

 \Delta_\textbf{W} =  \frac{\partial J}{\partial \textbf{b}} =  \frac{\partial J}{\partial \textbf{y}}  \frac{\partial \textbf{y}}{\partial \textbf{b}}  

</script>

<p>而 <script type="math/tex"> \frac{\partial \textbf{y}}{\partial \textbf{b}} </script>  這項的結果如下，</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \frac{\partial \textbf{y}}{\partial \textbf{b}} 
= 
\begin{bmatrix}
\frac{\partial y_{1}}{\partial b_{1}} \\
\frac{\partial y_{2}}{\partial b_{2}} \\
\frac{\partial y_{3}}{\partial b_{3}} \\
\end{bmatrix} \\
&= 
\begin{bmatrix}
\frac{w_{11}x_{1}+w_{12}x_{2}+b_{1}}{\partial b_{1}} \\
\frac{w_{21}x_{1}+w_{22}x_{2}+b_{2}}{\partial b_{2}} \\
\frac{w_{31}x_{1}+w_{32}x_{2}+b_{3}}{\partial b_{3}} \\
\end{bmatrix} \\
&= 
\begin{bmatrix}
1 \\
1 \\
1 \\
\end{bmatrix}\\
\end{align}
 %]]&gt;</script>

<p>因此， <script type="math/tex"> \frac{\partial J}{\partial \textbf{b}} </script> 的值為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \frac{\partial J}{\partial \textbf{b}} 
 = 
\begin{bmatrix}
\frac{\partial J}{\partial y_{1}} \frac{\partial y_{1}}{\partial b_{1}} \\
\frac{\partial J}{\partial y_{1}} \frac{\partial y_{1}}{\partial b_{2}} \\
\frac{\partial J}{\partial y_{2}} \frac{\partial y_{2}}{\partial b_{3}} \\
\end{bmatrix} \\
&= 
\begin{bmatrix}
\frac{\partial J}{\partial y_{1}} \\
\frac{\partial J}{\partial y_{2}} \\
\frac{\partial J}{\partial y_{3}} \\
\end{bmatrix} \\
&= \frac{\partial J}{\partial \textbf{y}} \\
\end{align}
 %]]&gt;</script>

<p>因此， <script type="math/tex"> \frac{\partial J}{\partial \textbf{b}} </script> 的值，和 <script type="math/tex">\frac{\partial J}{\partial \textbf{y}}</script> 的值相同。</p>

<p>執行 <code>accGradParameters</code>  ，程式碼如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">dj_dy</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">}</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">{</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">}</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span> <span class="n">l1</span><span class="p">:</span><span class="n">accGradParameters</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span><span class="n">dj_dy</span> <span class="p">))</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">gradWeight</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">gradBias</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">  <span class="m">4</span>   5
</span><span class="line">  <span class="m">8</span>  10
</span><span class="line"> <span class="m">12</span>  15
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3x2<span class="o">]</span>
</span><span class="line">
</span><span class="line"> 1
</span><span class="line"> 2
</span><span class="line"> 3
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>至於 <code>input:dim() == 2</code> 的情況，推導方式同上，在此不詳細推導。</p>

<h2 id="updateparameters">3.updateParameters</h2>

<p>此函數在 <code>nn.Module</code> 中即有實作其內容，它所進行的運算相當簡單，就是更新 <script type="math/tex">\textbf{W}</script> 和 <script type="math/tex">\textbf{b}</script> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \textbf{W} \leftarrow \textbf{W}  - \eta \Delta_\textbf{W} \\ 
& \textbf{b} \leftarrow \textbf{b} - \eta \Delta_\textbf{b} \\
\end{align}
 %]]&gt;</script>

<p>程式碼如下：</p>

<figure class="code"><figcaption><span>nn/Module.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">Module</span><span class="p">:</span><span class="n">updateParameters</span><span class="p">(</span><span class="n">learningRate</span><span class="p">)</span>
</span><span class="line">   <span class="kd">local</span> <span class="n">params</span><span class="p">,</span> <span class="n">gradParams</span> <span class="o">=</span> <span class="n">self</span><span class="p">:</span><span class="n">parameters</span><span class="p">()</span>
</span><span class="line">   <span class="k">if</span> <span class="n">params</span> <span class="k">then</span>
</span><span class="line">      <span class="k">for</span> <span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="o">#</span><span class="n">params</span> <span class="k">do</span>
</span><span class="line">         <span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span><span class="n">add</span><span class="p">(</span><span class="o">-</span><span class="n">learningRate</span><span class="p">,</span> <span class="n">gradParams</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span><span class="line">      <span class="k">end</span>
</span><span class="line">   <span class="k">end</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>其中，公式中的 <script type="math/tex">\eta</script> 即為程式碼中的 <code>learningRate</code> ，而程式中的第五行，則是負責將 <code>weight</code> 和 <code>bias</code> 更新。 其中， <code>weight</code> 和 <code>bias</code> 由第二行的 <code>parameters</code> 函數所取得，回傳到 <code>param</code> 中，而 <code>gradWeight</code> 和 <code>gradBias</code> 也用同樣方式取得，回傳到 <code>gradParams</code> 中。</p>

<p>假設 <script type="math/tex">\eta = 0.02</script> ，<script type="math/tex">\textbf{W} , \textbf{b} , \frac{\partial J}{\partial \textbf{W}} ,  \frac{\partial J}{\partial \textbf{b}}</script> 的值分別如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}
&\textbf{W}=
\begin{bmatrix}
 0.1453 & 0.5062 \\
 0.0635 & 0.4911 \\
-0.1080 & 0.1747 \\
\end{bmatrix} \\
&\textbf{b}=
\begin{bmatrix}
 0.2063 \\
-0.1635 \\
-0.0883 \\
\end{bmatrix} \\
& \frac{\partial J}{\partial \textbf{W}} = 
\begin{bmatrix}
4 & 5 \\
8 & 10 \\
12 & 15 \\ 
\end{bmatrix}\\
& \frac{\partial J}{\partial \textbf{b}} = 
\begin{bmatrix}
1 \\
2 \\
3 \\ 
\end{bmatrix}\\
\end{align}

 %]]&gt;</script>

<p>更新 <script type="math/tex">\textbf{W}</script> 和 <script type="math/tex">\textbf{b}</script> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \textbf{W}  - \eta \Delta_\textbf{W} \\
& = 
\begin{bmatrix}
 0.1453 & 0.5062 \\
 0.0635 & 0.4911 \\
-0.1080 & 0.1747 \\
\end{bmatrix} -
0.02
\begin{bmatrix}
4 & 5 \\
8 & 10 \\
12 & 15 \\ 
\end{bmatrix}\\
& = 
\begin{bmatrix}
 0.0653 & 0.4062 \\
-0.0965 & 0.2911 \\
-0.3480 & -0.1253 \\
\end{bmatrix} \\

& \textbf{b}  - \eta \Delta_\textbf{b} \\
& = 
\begin{bmatrix}
 0.2063 \\
-0.1635 \\
-0.0883 \\
\end{bmatrix} -
0.02
\begin{bmatrix}
1 \\
2 \\
3 \\ 
\end{bmatrix}\\
&=
\begin{bmatrix}
 0.1863 \\
-0.2035 \\ 
-0.1483 \\
\end{bmatrix} 
\end{align}
 %]]&gt;</script>

<p>印出 <code>l1</code> 的 <code>weight</code> 和 <code>bias</code> ，執行 <code>updateParameters</code> ，比較執行前後的 <code>weight</code> 和 <code>bias</code> 差異，程式如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
</span><span class="line"><span class="n">l1</span><span class="p">:</span><span class="n">updateParameters</span><span class="p">(</span><span class="mf">0.02</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"> 0.1453  0.5062
</span><span class="line"> 0.0635  0.4911
</span><span class="line">-0.1080  0.1747
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3x2<span class="o">]</span>
</span><span class="line">
</span><span class="line"> 0.2063
</span><span class="line">-0.1635
</span><span class="line">-0.0883
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3<span class="o">]</span>
</span><span class="line">
</span><span class="line"> 0.0653  0.4062
</span><span class="line">-0.0965  0.2911
</span><span class="line">-0.3480 -0.1253
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3x2<span class="o">]</span>
</span><span class="line">
</span><span class="line"> 0.1863
</span><span class="line">-0.2035
</span><span class="line">-0.1483
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<h2 id="materials">Materials</h2>

<p>本次教學的完整程式碼於此：</p>

<p><a href="https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/6_backward_propagation_part_3.ipynb">https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/6_backward_propagation_part_3.ipynb
</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Torch NN Tutorial 5 : Backward Propagation ( Part 2 : nn.Criterion )]]></title>
    <link href="http://ckmarkoh.github.io/blog/2017/01/01/torch-nn-tutorial-5-backward-propagation/"/>
    <updated>2017-01-01T15:17:57+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2017/01/01/torch-nn-tutorial-5-backward-propagation</id>
    <content type="html"><![CDATA[<h2 id="backward-propagation-in-nncriterion">Backward Propagation in nn.Criterion</h2>

<p>本文接續 <a href="http://ckmarkoh.github.io/blog/2017/01/01/torch-nn-tutorial-4-backward-propagation/">Torch NN Tutorial 4: Backward Propagation (part 1)</a> ，講解 <code>nn.Criterion</code> 中，與 backward propagation 相關的程式碼。</p>

<p>Criterion 的 <code>forward</code>  是負責計算 loss funciton <script type="math/tex">J</script> 的值，而 <code>backward</code> 則是計算 <script type="math/tex">\frac{\partial J}{\partial \bar{y}}</script> 的值。其中， <script type="math/tex">\bar{y}</script> 為模型預測出的結果。</p>

<p><code>nn.Criterion</code> 程式碼：<a href="https://github.com/torch/nn/blob/master/Criterion.lua">https://github.com/torch/nn/blob/master/Criterion.lua</a></p>

<p><code>backward</code> 的部分，如下：</p>

<figure class="code"><figcaption><span>nn/Criterion.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">Criterion</span><span class="p">:</span><span class="n">backward</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</span><span class="line">   <span class="k">return</span> <span class="n">self</span><span class="p">:</span><span class="n">updateGradInput</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</span><span class="line"><span class="k">end</span>
</span><span class="line">
</span><span class="line"><span class="k">function</span> <span class="nf">Criterion</span><span class="p">:</span><span class="n">updateGradInput</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p><code>nn.Criterion</code> 的 <code>backward</code> 只做一件事，也就是 <code>updateGradInput</code> ，而  <code>updateGradInput</code> 的運算內容則由繼承它的類別來實作。</p>

<!--more-->

<h2 id="backward-propagation-in-nnmsecriterion">Backward Propagation in nn.MSECriterion</h2>

<p>本文舉 <code>nn.MSECriterion</code> 為例。</p>

<p><code>nn.MSECriterion</code> 程式碼：<a href="https://github.com/torch/nn/blob/master/MSECriterion.lua">https://github.com/torch/nn/blob/master/MSECriterion.lua</a></p>

<p><code>updateGradInput</code> 的部分，如下：</p>

<figure class="code"><figcaption><span>nn/MSECriterion.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">MSECriterion</span><span class="p">:</span><span class="n">updateGradInput</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</span><span class="line">   <span class="n">input</span><span class="p">.</span><span class="n">THNN</span><span class="p">.</span><span class="n">MSECriterion_updateGradInput</span><span class="p">(</span>
</span><span class="line">      <span class="n">input</span><span class="p">:</span><span class="n">cdata</span><span class="p">(),</span>
</span><span class="line">      <span class="n">target</span><span class="p">:</span><span class="n">cdata</span><span class="p">(),</span>
</span><span class="line">      <span class="n">self</span><span class="p">.</span><span class="n">gradInput</span><span class="p">:</span><span class="n">cdata</span><span class="p">(),</span>
</span><span class="line">      <span class="n">self</span><span class="p">.</span><span class="n">sizeAverage</span>
</span><span class="line">   <span class="p">)</span>
</span><span class="line">   <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">gradInput</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>此部分的運算由 C 來實作，運算完後，回傳結果 <code>gradInput</code> 。</p>

<p>C 程式碼： <a href="https://github.com/torch/nn/blob/master/lib/THNN/generic/MSECriterion.c">https://github.com/torch/nn/blob/master/lib/THNN/generic/MSECriterion.c</a></p>

<figure class="code"><figcaption><span>nn/lib/THNN/generic/MSECriterion.c</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span class="kt">void</span> <span class="nf">THNN_</span><span class="p">(</span><span class="n">MSECriterion_updateGradInput</span><span class="p">)(</span>
</span><span class="line">          <span class="n">THNNState</span> <span class="o">*</span><span class="n">state</span><span class="p">,</span>
</span><span class="line">          <span class="n">THTensor</span> <span class="o">*</span><span class="n">input</span><span class="p">,</span>
</span><span class="line">          <span class="n">THTensor</span> <span class="o">*</span><span class="n">target</span><span class="p">,</span>
</span><span class="line">          <span class="n">THTensor</span> <span class="o">*</span><span class="n">gradInput</span><span class="p">,</span>
</span><span class="line">          <span class="kt">bool</span> <span class="n">sizeAverage</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">  <span class="n">THNN_CHECK_NELEMENT</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">target</span><span class="p">);</span>
</span><span class="line">
</span><span class="line">  <span class="n">real</span> <span class="n">norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">sizeAverage</span> <span class="o">?</span> <span class="mf">2.</span><span class="o">/</span><span class="p">((</span><span class="n">real</span><span class="p">)</span><span class="n">THTensor_</span><span class="p">(</span><span class="n">nElement</span><span class="p">)(</span><span class="n">input</span><span class="p">))</span> <span class="o">:</span> <span class="mf">2.</span><span class="p">);</span>
</span><span class="line">
</span><span class="line">  <span class="n">THTensor_</span><span class="p">(</span><span class="n">resizeAs</span><span class="p">)(</span><span class="n">gradInput</span><span class="p">,</span> <span class="n">input</span><span class="p">);</span>
</span><span class="line">  <span class="n">TH_TENSOR_APPLY3</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">gradInput</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span> <span class="n">input</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span>
</span><span class="line">    <span class="o">*</span><span class="n">gradInput_data</span> <span class="o">=</span> <span class="n">norm</span> <span class="o">*</span> <span class="p">(</span><span class="o">*</span><span class="n">input_data</span> <span class="o">-</span> <span class="o">*</span><span class="n">target_data</span><span class="p">);</span>
</span><span class="line">  <span class="p">);</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>

<p>主要的運算是由第13行的 <code>TH_TENSOR_APPLY3</code> 來執行，這個 function 的意思是，執行某個 function 在三個 tensor 上。而要執行的 function 是 <code>*gradInput_data = norm * (*input_data - *target_data)</code> ，它需要 tensor ，分別是 <code>input_data</code> ， <code>target_data</code> 和 <code>gradInput_data</code> 。</p>

<p>從數學公式上來看，公式如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \frac{\partial J}{\partial \bar{y} } = \frac{ \partial (\bar{y} -y)^2  }{\partial \bar{y}} \\
& = 2(\bar{y} -y)
\end{align}
 %]]&gt;</script>

<p>其中， <script type="math/tex"> \frac{\partial J}{\partial \bar{y} } </script> 為 <code>gradInput_data</code> ， <script type="math/tex">\bar{y}</script> 為 <code>input_data</code> ，而 <script type="math/tex">y</script> 為 <code>target_data</code> 。  <script type="math/tex">2</script> 為常數 <code>norm</code> 。</p>

<p>如果 <script type="math/tex"> \bar{y} </script> 和 <script type="math/tex">y</script> 皆為向量，則 <script type="math/tex"> \frac{\partial J}{\partial \bar{y} } </script> 則是針對 <script type="math/tex"> \bar{y} </script> 中的每個元素來分別計算，如果 <code>sizeAverage</code> 為 true 的話，<code>norm</code> 還要除以向量的長度。舉以下例子，<script type="math/tex"> \bar{y} = [1,1,1], y=[1,2,3] </script>  則 ：</p>

<script type="math/tex; mode=display">
\frac{\partial J}{\partial \bar{y} } = 
\begin{bmatrix}
\frac{\partial J}{\partial \bar{y}_{1} } \\
\frac{\partial J}{\partial \bar{y}_{2} } \\ 
\frac{\partial J}{\partial \bar{y}_{3} } \\
\end{bmatrix} 
=
\frac{2}{3}
\begin{bmatrix}
\bar{y}_{1} - y_{1} \\
\bar{y}_{2} - y_{2} \\
\bar{y}_{3} - y_{3} \\
\end{bmatrix} 
=
\frac{2}{3}
\begin{bmatrix}
1 - 1 \\
1 - 2 \\
1 - 3 \\
\end{bmatrix} 
=
\begin{bmatrix}
0.0000 \\
-0.6667 \\
-1.3333 \\
\end{bmatrix}
</script>

<p>其中， <script type="math/tex">\bar{y}_{1}</script> 代表 <script type="math/tex">\bar{y}</script> 中的第一的元素，而前面乘上 <script type="math/tex">\frac{2}{3}</script> 是因為 <code>sizeAverage</code> 為 <code>true</code> ，所以微分的結果要除以向量長度 3。</p>

<p>以 <code>backward</code> 執行以上運算，程式如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">y_</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">{</span>
</span><span class="line">    <span class="p">{</span><span class="mi">1</span><span class="p">},{</span><span class="mi">1</span><span class="p">},{</span><span class="mi">1</span><span class="p">}</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line">
</span><span class="line"><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">{</span>
</span><span class="line">    <span class="p">{</span><span class="mi">1</span><span class="p">},{</span><span class="mi">2</span><span class="p">},{</span><span class="mi">3</span><span class="p">}</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line">
</span><span class="line"><span class="n">c1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSECriterion</span><span class="p">()</span>
</span><span class="line"><span class="n">c1</span><span class="p">:</span><span class="n">backward</span><span class="p">(</span><span class="n">y_</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">c1</span><span class="p">.</span><span class="n">gradInput</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"> 0.0000
</span><span class="line">-0.6667
</span><span class="line">-1.3333
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3x1<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>如果 <code>sizeAverage</code> 為 <code>false</code> 的話， 則不需再除以 <code>input</code> 的大小， <script type="math/tex"> \frac{\partial J}{\partial \bar{y} } </script> 的結果如下：</p>

<script type="math/tex; mode=display">
\frac{\partial J}{\partial \bar{y} } = 
\begin{bmatrix}
\frac{\partial J}{\partial \bar{y}_{1} } \\
\frac{\partial J}{\partial \bar{y}_{2} } \\ 
\frac{\partial J}{\partial \bar{y}_{3} } \\
\end{bmatrix} 
=
2
\begin{bmatrix}
\bar{y}_{1} - y_{1} \\
\bar{y}_{2} - y_{2} \\
\bar{y}_{3} - y_{3} \\
\end{bmatrix} 
=
2
\begin{bmatrix}
1 - 1 \\
1 - 2 \\
1 - 3 \\
\end{bmatrix} 
=
\begin{bmatrix}
0 \\
-2 \\
-4 \\
\end{bmatrix}
</script>

<p>將 <code>sizeAverage</code> 設為 <code>false</code> 的方法，即是在建立 <code>nn.MSECriterion</code> 時，輸入 <code>false</code> ，方法如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">c2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSECriterion</span><span class="p">(</span><span class="kc">false</span><span class="p">)</span>
</span><span class="line"><span class="n">c2</span><span class="p">:</span><span class="n">backward</span><span class="p">(</span><span class="n">y_</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">c2</span><span class="p">.</span><span class="n">gradInput</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>以 <code>backward</code> 執行運算，結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"> 0
</span><span class="line">-2
</span><span class="line">-4
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3x1<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<h2 id="materials">Materials</h2>

<p>本次教學的完整程式碼於此：</p>

<p><a href="https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/5_backward_propagation_part_2.ipynb">https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/5_backward_propagation_part_2.ipynb
</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Torch NN Tutorial 4: Backward Propagation ( Part 1 : Overview & Linear Regression )]]></title>
    <link href="http://ckmarkoh.github.io/blog/2017/01/01/torch-nn-tutorial-4-backward-propagation/"/>
    <updated>2017-01-01T14:00:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2017/01/01/torch-nn-tutorial-4-backward-propagation</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>本文以 Linear Regression 為例，介紹 Torch nn 如何進行 Backward Propagation。</p>

<p>Linear Regression 是以機器學習的方式，學出以下函數：</p>

<script type="math/tex; mode=display">

y = wx+b

</script>

<p>以 Gradient Descent 的方式來進行 Linear regression 的流程如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\text{1.} \mspace{20mu} \text{initialize } w \text{ and } b \\
&\text{2.} \mspace{20mu} \text{for i=0 ; i < max_epoch; i++} \\
&\text{3.} \mspace{40mu} \bar{y} = wx+b \\
&\text{4.} \mspace{40mu} J = (\bar{y} - y )^2 \\
&\text{5.} \mspace{40mu} \Delta_w = 0 \\
&   \mspace{60mu} \Delta_b = 0 \\
&\text{6.} \mspace{40mu} \frac{\partial J}{\partial \bar{y}} = 2(\bar{y} - y) \\
&\text{7.} \mspace{40mu} \Delta_w =  \frac{\partial J}{\partial w} =  \frac{\partial J}{\partial \bar{y}}  \frac{\partial y}{\partial w} =  \frac{\partial J}{\partial \bar{y}} \times x  \\ 
&   \mspace{60mu} \Delta_b =  \frac{\partial J}{\partial b} =  \frac{\partial J}{\partial \bar{y}}  \frac{\partial y}{\partial b}  = \frac{\partial J}{\partial \bar{y}} \times 1 \\
&\text{8.} \mspace{40mu} w \leftarrow w  - \eta \Delta_w \\ 
&   \mspace{60mu} b \leftarrow b - \eta \Delta_b \\
\end{align}
 %]]&gt;</script>

<p>其中 <script type="math/tex">x</script> 為 training data， <script type="math/tex">y</script> 為 golden value ， <script type="math/tex">\bar{y}</script> 為 predicted value， <script type="math/tex">w</script> 和 <script type="math/tex">b</script> 分別為 weight 和 bias 。 max_epoch 為 for 迴圈執行次數。</p>

<p>以下詳細講解整個流程，並實作之。</p>

<!--more-->

<h2 id="linear-regression-by-gradient-descent">Linear Regression by Gradient Descent</h2>

<p>首先，載入 <code>nn</code> 套件，並產生 Training Data <script type="math/tex">x</script> 及 <script type="math/tex">y</script> ：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="nb">require</span> <span class="s1">&#39;</span><span class="s">nn&#39;</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">):</span><span class="n">resize</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</span><span class="line"><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">*</span><span class="mi">2</span><span class="o">+</span><span class="mi">1</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>以上程式，假設 <script type="math/tex">y</script> 是由 <script type="math/tex">y = 2 x + 1 </script> 產生出來的。而訓練的目標，是要讓 <script type="math/tex">w=2, b=1</script> 。</p>

<p>產生出來的 <code>x</code> 為 <code>[1,2,3]</code> ， <code>y</code> 為 <code>[3,5,7]</code> 如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"> 1
</span><span class="line"> 2
</span><span class="line"> 3
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3x1<span class="o">]</span>
</span><span class="line">
</span><span class="line"> 3
</span><span class="line"> 5
</span><span class="line"> 7
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3x1<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>為了方便以 batch 計算，將 <code>x</code> 及 <code>y</code> 調整成 3x1 的大小。</p>

<p>建立完 training data 之後，可以開始進行 Linear Regression。</p>

<p>第一步，將 <script type="math/tex">w</script> 和 <script type="math/tex">b</script> 以隨機值初始化。</p>

<script type="math/tex; mode=display"> \text{1.}  \mspace{20mu} \text{initialize } w \text{ and } b </script>

<p>建立 <code>nn.Linear</code> ，命名為 <code>l1</code> ，如下： </p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>一開始， <code>weight</code> 和 <code>bias</code> 會以隨機值初始化，結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"> 0.2055
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 1x1<span class="o">]</span>
</span><span class="line">
</span><span class="line"> 0.7159
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 1<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>再來是建立 loss function，命名為 <code>c1</code> ：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"><span class="nv">c1</span> <span class="o">=</span> nn.MSECriterion<span class="o">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>由於 loss function 為 Mimimum Square Error，所以 criterion 採用 <code>nn.MSECriterion</code></p>

<p>再來，這裡先講解 for 迴圈中進行的運算。</p>

<p>第三步算 linear 的 forward propagation ：</p>

<script type="math/tex; mode=display">\text{3.} \mspace{20mu} \bar{y} = wx+b </script>

<p>將 <script type="math/tex">x,w,b</script> 的數值帶入以上公式，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \bar{y} = wx+b\\
& =
0.2055
\begin{bmatrix}
1 \\
2 \\
3 \\
\end{bmatrix}
+ 0.7159 \\
& =
\begin{bmatrix}
 0.2055 \times 1 + 0.7159 \\
 0.2055 \times 2 + 0.7159 \\
 0.2055 \times 3 + 0.7159 \\
\end{bmatrix} \\
& =
\begin{bmatrix}
 0.9214 \\
 1.1269 \\
 1.3325 \\
\end{bmatrix}
\end{align}
 %]]&gt;</script>

<p>程式碼如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">y_</span> <span class="o">=</span> <span class="n">l1</span><span class="p">:</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">y_</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"> 0.9214
</span><span class="line"> 1.1269
</span><span class="line"> 1.3325
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3x1<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>第四步，計算 coss function 的 forward propagation ：</p>

<script type="math/tex; mode=display">\text{4.} \mspace{20mu} J = (\bar{y} - y )^2 </script>

<p>將 <script type="math/tex">y,\bar{y}</script> 的數值帶入以上公式，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& J = (\bar{y} - y)^2\\
& =
\begin{bmatrix}
(0.9214 - 3)^2 \\
(1.1269 - 5)^2 \\
(1.3325 - 7)^2 \\
\end{bmatrix} \\
& =
\begin{bmatrix}
 4.3206 \\
 15.0009 \\
 32.1206 \\
\end{bmatrix}
\end{align}
 %]]&gt;</script>

<p>如果輸入的值有多個維度，則 loss function 會將個維度的值，加起來平均，結果如下：</p>

<script type="math/tex; mode=display">

J = \frac{4.3206 + 15.0009 + 32.1206}{3} = 17.147313377985 

</script>

<p>計算 <script type="math/tex">J</script> 值的程式碼如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">j</span> <span class="o">=</span> <span class="n">c1</span><span class="p">:</span><span class="n">forward</span><span class="p">(</span><span class="n">y_</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">j</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>loss 值 <code>j</code> 如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">17.147313377985
</span></code></pre></td></tr></table></div></figure>

<p>再來，要進行 backward propagation。</p>

<p>第五步，先將 <script type="math/tex"> \Delta_w </script> 和 <script type="math/tex"> \Delta_b </script> 歸零：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\text{5.} \mspace{40mu} \Delta_w = 0 \\
&   \mspace{60mu} \Delta_b = 0 \\
\end{align}
 %]]&gt;</script>

<p><script type="math/tex"> \Delta_w </script> 和 <script type="math/tex"> \Delta_b </script> 在程式中所對應的值，是 <code>l1</code> 的 <code>gradWeight</code> 和 <code>gradBias</code> 。在還沒歸零之前，這兩變數可能是任意值，印出這兩數的值，程式如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">gradWeight</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">gradBias</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">1e-154 *
</span><span class="line"> -1.4917
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 1x1<span class="o">]</span>
</span><span class="line">
</span><span class="line">1e-154 *
</span><span class="line">-1.4917
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 1<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>可用函式 <code>zeroGradParameters</code> 將這兩個值歸零，程式如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">l1</span><span class="p">:</span><span class="n">zeroGradParameters</span><span class="p">()</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">gradWeight</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">gradBias</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果為0，表示已歸零：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"> 0
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 1x1<span class="o">]</span>
</span><span class="line">
</span><span class="line"> 0
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 1<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>第六步，計算 <script type="math/tex">\frac{\partial J}{\partial \bar{y}}</script> 的值。</p>

<script type="math/tex; mode=display"> \text{6.} \mspace{40mu} \frac{\partial J}{\partial \bar{y}} = 2(\bar{y} - y) </script>

<p>將 <script type="math/tex">y,\bar{y}</script> 的數值帶入以上公式，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \frac{\partial J}{\partial \bar{y}} = 2(\bar{y} - y)\\
& =
\begin{bmatrix}
2(0.9214 - 3) \\
2(1.1269 - 5) \\
2(1.3325 - 7) \\
\end{bmatrix} \\
& =
\begin{bmatrix}
-1.3857 \\
-2.5820 \\
-3.7784 \\
\end{bmatrix}
\end{align}
 %]]&gt;</script>

<p>由於 <script type="math/tex">\bar{y}</script> 有三筆資料，每筆資料都會各算出一個微分結果。
程式中，計算此值的方式即是呼叫 <code>c1</code> 的 <code>backward</code> ，如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">dj_dy_</span> <span class="o">=</span><span class="n">c1</span><span class="p">:</span><span class="n">backward</span><span class="p">(</span><span class="n">y_</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">dj_dy_</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>得出的結果即是 <script type="math/tex">\frac{\partial J}{\partial \bar{y} } </script> ，結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">-1.3857
</span><span class="line">-2.5820
</span><span class="line">-3.7784
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3x1<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>第七步，計算 <script type="math/tex"> \Delta_w </script> 和 <script type="math/tex"> \Delta_b </script> 的值：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\text{7.} \mspace{40mu} \Delta_w =  \frac{\partial J}{\partial w} =  \frac{\partial J}{\partial \bar{y}}  \frac{\partial y}{\partial w} =  \frac{\partial J}{\partial \bar{y}} \times x  \\ 
&   \mspace{60mu} \Delta_b =  \frac{\partial J}{\partial b} =  \frac{\partial J}{\partial \bar{y}}  \frac{\partial y}{\partial b}  = \frac{\partial J}{\partial \bar{y}} \times 1 \\
\end{align}
 %]]&gt;</script>

<p>先看 <script type="math/tex">\Delta_w </script> 的數值，將 <script type="math/tex">x</script> 的值，以及先前算出的 <script type="math/tex"> \frac{\partial J}{\partial \bar{y}}</script> 值代入，即可算出它：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \Delta_w =  \frac{\partial J}{\partial \bar{y}} \times x \\
&=
\begin{bmatrix}
-1.3857 \times 1\\
-2.5820 \times 2\\
-3.7784 \times 3\\
\end{bmatrix}  \\
&=
\begin{bmatrix}
-1.3857 \\
-5.1640 \\
-11.3352 \\
\end{bmatrix}
\end{align}
 %]]&gt;</script>

<p>如果輸值為有多筆資料，則 <script type="math/tex">\Delta_w</script> 的最終結果會將每筆的結果累加起來，如下：</p>

<script type="math/tex; mode=display">
\Delta_w = -1.3857 + -5.1640 + -11.3352 = -17.8849
</script>

<p>而 <script type="math/tex">\Delta_b</script> 的值是把 <script type="math/tex"> \frac{\partial J}{\partial \bar{y}}</script> 中的每筆資料結果累積起來。</p>

<script type="math/tex; mode=display">
\Delta_b = -1.3857 + -2.5820 + -3.7784 = -7.7461
</script>

<p>以程式來計算此兩值。呼叫 <code>l1</code> 的 <code>backward</code> ，輸入 <script type="math/tex">\frac{\partial J}{\partial \bar{y} }</script> 到 <code>backward</code> 後，它與  <script type="math/tex">\frac{\partial \bar{y}}{\partial w}</script> 和 <script type="math/tex">\frac{\partial \bar{y}}{\partial b}</script> 相乘後結果分別為 <script type="math/tex">\Delta_w</script> 和 <script type="math/tex">\Delta_b</script> ，此兩數分別儲存於 <code>gradWeight</code> 和 <code>gradBias</code> 。</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">l1</span><span class="p">:</span><span class="n">backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dj_dy_</span> <span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">gradWeight</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">gradBias</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">-17.8849
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 1x1<span class="o">]</span>
</span><span class="line">
</span><span class="line">-7.7461
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 1<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>第八步，更新 weight 和 bias ，公式如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}
&\text{8.} \mspace{40mu} w \leftarrow w  - \eta \Delta_w \\ 
&   \mspace{60mu} b \leftarrow b - \eta \Delta_b \\
\end{align}

 %]]&gt;</script>

<p>其中， <script type="math/tex">eta</script>  是指 learning rate 。令 <script type="math/tex">eta=0.2</script> ，更新 <script type="math/tex">w</script> 和 <script type="math/tex">b</script> ，數值如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& w  - \eta \Delta_w 
= 0.2055 - 0.02 \times (- 17.8849) = 0.5632 \\
& b - \eta \Delta_b 
= 0.7159 - 0.02 \times (-7.7461) = 0.8708 \\
\end{align}
 %]]&gt;</script>

<p>程式中，更新 <code>l1</code> 的 <code>weight</code> 和 <code>bias</code> 可以用 <code>updateParameters</code> ，它的輸入即是 <script type="math/tex">eta</script> 。令 <script type="math/tex">eta=0.2</script> ，程式如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">l1</span><span class="p">:</span><span class="n">updateParameters</span><span class="p">(</span><span class="mf">0.02</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>更新完後印出 <code>weight</code> 和 <code>bias</code> 的值，結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"> 0.5632
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 1x1<span class="o">]</span>
</span><span class="line">
</span><span class="line"> 0.8708
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 1<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>把第三到第八步的 for 迴圈，串連起來。此 for 迴圈連續跑 500 次，每 50 次印出一次結果，程式碼如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span><span class="mi">500</span> <span class="k">do</span>
</span><span class="line">    <span class="n">y_</span> <span class="o">=</span> <span class="n">l1</span><span class="p">:</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line">    <span class="n">j</span> <span class="o">=</span> <span class="n">c1</span><span class="p">:</span><span class="n">forward</span><span class="p">(</span><span class="n">y_</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</span><span class="line">    <span class="n">l1</span><span class="p">:</span><span class="n">zeroGradParameters</span><span class="p">()</span>
</span><span class="line">    <span class="n">dj_dy_</span> <span class="o">=</span><span class="n">c1</span><span class="p">:</span><span class="n">backward</span><span class="p">(</span><span class="n">y_</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</span><span class="line">    <span class="n">l1</span><span class="p">:</span><span class="n">backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">dj_dy_</span><span class="p">)</span>
</span><span class="line">    <span class="n">l1</span><span class="p">:</span><span class="n">updateParameters</span><span class="p">(</span><span class="mf">0.02</span><span class="p">)</span>
</span><span class="line">    <span class="k">if</span> <span class="n">i</span><span class="o">%</span><span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">then</span>
</span><span class="line">        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="s">i:&quot;</span> <span class="o">..</span> <span class="nb">tostring</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</span><span class="line">           <span class="o">..</span> <span class="s2">&quot;</span><span class="s"> loss:&quot;</span> <span class="o">..</span> <span class="nb">tostring</span><span class="p">(</span><span class="n">j</span><span class="p">)</span>
</span><span class="line">           <span class="o">..</span> <span class="s2">&quot;</span><span class="s"> weight:&quot;</span> <span class="o">..</span> <span class="nb">tostring</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">weight</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line">           <span class="o">..</span> <span class="s2">&quot;</span><span class="s"> bias:&quot;</span> <span class="o">..</span> <span class="nb">tostring</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">bias</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">..</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">)</span>
</span><span class="line">    <span class="k">end</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>執行此程式，在訓練完 500 次以後，<code>loss</code> 會接近 0 ，而 <code>weight</code> 和 <code>bias</code> 會接近 2 和 1 了。結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">i:50 loss:0.015879173659737 weight:1.8543434015475 bias:1.3310995564975
</span><span class="line">
</span><span class="line">
</span><span class="line">i:100 loss:0.0098066687579948 weight:1.885537390716 bias:1.2602004152583
</span><span class="line">
</span><span class="line">
</span><span class="line">...
</span><span class="line">
</span><span class="line">i:450 loss:0.00033602903432248 weight:1.9788119352928 bias:1.0481654513272
</span><span class="line">
</span><span class="line">
</span><span class="line">i:500 loss:0.00020752499774989 weight:1.9833490852405 bias:1.0378514430405
</span><span class="line">
</span></code></pre></td></tr></table></div></figure>

<h2 id="materials">Materials</h2>

<p>本次教學的完整程式碼於此：</p>

<p><a href="https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/4_backward_propagation_part_1.ipynb">https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/4_backward_propagation_part_1.ipynb
</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Torch NN Tutorial 3 : NN.Criterion & NN.MSECriterion]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/12/25/torch-nn-tutorial-3-nn-dot-criterion/"/>
    <updated>2016-12-25T18:48:14+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/12/25/torch-nn-tutorial-3-nn-dot-criterion</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>torch 的 <code>nn.Module</code> 是組成 neural network 的部分，但是要訓練一個 neural network ，就需要有 loss function 。而 <code>nn.Criterion</code> 就是用來計算 loss function 的值。<code>nn.Criterion</code> 是個抽象類別，所有種類的 loss function 都繼承於它。</p>

<p>例如， loss funciton 用 Minimum Square Error 時， <script type="math/tex">\bar{y}</script> 為模型預測出來的數值， <script type="math/tex">y</script> 為正確的數值 ，則 loss function <script type="math/tex"> L (y, \bar{y}) </script> 的公式如下：</p>

<script type="math/tex; mode=display">

l( y , \bar{y}) = (y  - \bar{y} )^2 

</script>

<p>在 torch 中，負責計算 Minimum Square Error 的 criterion 為 <code>nn.MSECriterion</code> 。</p>

<!-- more -->

<p>實作部份如下，首先，載入 <code>nn</code> 模組：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="nb">require</span> <span class="s1">&#39;</span><span class="s">nn&#39;</span>
</span></code></pre></td></tr></table></div></figure>

<p>建立 <code>nn.MSECriterion</code> ，命名為 <code>c1</code> ，如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">c1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSECriterion</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>假設模型預測出的數值 <script type="math/tex"> \bar{y} = 5</script> ，正確數值 <script type="math/tex">y=3</script> ，則：</p>

<script type="math/tex; mode=display">

l( y , \bar{y}) = (y  - \bar{y} )^2  = (3-5)^2 = 4

</script>

<p>實作如下，將 <code>y_ = 5</code> 與 <code>y = 3</code> 兩數值傳入 <code>c1</code> ，進行 forward propagation ，得出 loss function <code>l</code> 的數值。</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">y_</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">{</span><span class="mi">5</span><span class="p">}</span>
</span><span class="line"><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">{</span><span class="mi">3</span><span class="p">}</span>
</span><span class="line"><span class="n">l</span> <span class="o">=</span> <span class="n">c1</span><span class="p">:</span><span class="n">forward</span><span class="p">(</span><span class="n">y_</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>執行結果， <code>l = 4</code> ，如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">4	
</span></code></pre></td></tr></table></div></figure>

<h2 id="nncriterion--nnmsecriterion">nn.Criterion &amp; nn.MSECriterion</h2>

<p>這邊講解 <code>nn.Criterion</code> 以及 <code>nn.MSECriterion</code> 的程式碼：</p>

<p><code>nn.Criterion</code> 程式碼：<a href="https://github.com/torch/nn/blob/master/Criterion.lua">https://github.com/torch/nn/blob/master/Criterion.lua</a></p>

<p><code>nn.MSECriterion</code> 程式碼： <a href="https://github.com/torch/nn/blob/master/MSECriterion.lua">https://github.com/torch/nn/blob/master/MSECriterion.lua</a></p>

<p>首先看到 <code>nn.Criterion</code> 的部分，先從 <code>init</code> 開始。</p>

<figure class="code"><figcaption><span>nn/Criterion.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="kd">local</span> <span class="n">Criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">class</span><span class="p">(</span><span class="s1">&#39;</span><span class="s">nn.Criterion&#39;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">function</span> <span class="nf">Criterion</span><span class="p">:</span><span class="n">__init</span><span class="p">()</span>
</span><span class="line">   <span class="n">self</span><span class="p">.</span><span class="n">gradInput</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">()</span>
</span><span class="line">   <span class="n">self</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>從以上程式碼得知， <code>nn.Criterion</code> 雖然有 <code>output</code> 和 <code>gradInput</code> 這兩個變量，但它並沒有繼承 <code>nn.Module</code> 。</p>

<p>再來看到 forward propagation 的部分， <code>nn.Criterion</code> 的 <code>forward</code> 和 <code>nn.Module</code> 的不一樣，因為它的輸入有 <code>input</code> 和 <code>target</code> 這兩個變量，如下：</p>

<p>因為 loss function 的 forward propagation 需要有 input 和 target 這兩個數值才算得出來，而 module 中的 forward propagation 部分，通常只需要 input 的數值即可。</p>

<figure class="code"><figcaption><span>nn/Criterion.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">Criterion</span><span class="p">:</span><span class="n">forward</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</span><span class="line">   <span class="k">return</span> <span class="n">self</span><span class="p">:</span><span class="n">updateOutput</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>再來看到 <code>nn.MSECriterion</code> 的程式碼。</p>

<figure class="code"><figcaption><span>nn/MSECriterion.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">MSECriterion</span><span class="p">:</span><span class="n">__init</span><span class="p">(</span><span class="n">sizeAverage</span><span class="p">)</span>
</span><span class="line">   <span class="n">parent</span><span class="p">.</span><span class="n">__init</span><span class="p">(</span><span class="n">self</span><span class="p">)</span>
</span><span class="line">   <span class="k">if</span> <span class="n">sizeAverage</span> <span class="o">~=</span> <span class="kc">nil</span> <span class="k">then</span>
</span><span class="line">     <span class="n">self</span><span class="p">.</span><span class="n">sizeAverage</span> <span class="o">=</span> <span class="n">sizeAverage</span>
</span><span class="line">   <span class="k">else</span>
</span><span class="line">     <span class="n">self</span><span class="p">.</span><span class="n">sizeAverage</span> <span class="o">=</span> <span class="kc">true</span>
</span><span class="line">   <span class="k">end</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>其中， <code>sizeAverage</code> 是設定要不要將 output 根據 input 的維度來做平均，預設值為 <code>true</code> 。</p>

<p>舉個例子，<code>y2_</code> 和 <code>y2</code> 分別為 input 與 target ，分別建立 <code>c2</code> 與 <code>c3</code> 兩個 <code>nn.MSECriterion</code> ， <code>c2</code> 的 <code>sizeAverage</code> 為 <code>true</code> ，而 <code>c3</code> 的 <code>sizeAverage</code> 為 <code>false</code> 。</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">y2_</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">{</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">}</span>
</span><span class="line"><span class="n">y2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">{</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">}</span>
</span><span class="line"><span class="n">c2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSECriterion</span><span class="p">(</span><span class="kc">true</span><span class="p">)</span>
</span><span class="line"><span class="n">c3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSECriterion</span><span class="p">(</span><span class="kc">false</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>將 <code>y2_</code> 和 <code>y2</code> 都輸入 <code>c2</code> 和 <code>c3</code> ，進行 forward propagation ，比較其結果差異，程式碼如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">c2</span><span class="p">:</span><span class="n">forward</span><span class="p">(</span><span class="n">y2_</span><span class="p">,</span><span class="n">y2</span><span class="p">))</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">c3</span><span class="p">:</span><span class="n">forward</span><span class="p">(</span><span class="n">y2_</span><span class="p">,</span><span class="n">y2</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>

<p>則輸出結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">4	
</span><span class="line">8
</span></code></pre></td></tr></table></div></figure>

<p>以上， 4 是 <code>sizeAverage</code> 為 <code>true</code> 的結果，也就是將 <code>y2_</code> 和 <code>y2</code> 的每個元素相減後再平均，如下：</p>

<script type="math/tex; mode=display">

\frac{(5-3)^2 + (5-3)^2 }{2} = 4

</script>

<p>而 8 是 <code>sizeAverage</code> 為 <code>false</code> 的結果，也就是將 <code>y2_</code> 和 <code>y2</code> 的每個元素相減，但沒平均，如下：</p>

<script type="math/tex; mode=display">

(5-3)^2 + (5-3)^2  = 8

</script>

<p>這裡要注意的是，不管 <code>sizeAverage</code> 是 <code>ture</code> 或 <code>false</code> ，也不論 input 的維度有多少，output 都是一維的純量。</p>

<p>再來看到 <code>updateOutput</code> 的程式碼：</p>

<figure class="code"><figcaption><span>nn/Criterion.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">MSECriterion</span><span class="p">:</span><span class="n">updateOutput</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</span><span class="line">   <span class="n">self</span><span class="p">.</span><span class="n">output_tensor</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">output_tensor</span> <span class="ow">or</span> <span class="n">input</span><span class="p">.</span><span class="n">new</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span class="line">   <span class="n">input</span><span class="p">.</span><span class="n">THNN</span><span class="p">.</span><span class="n">MSECriterion_updateOutput</span><span class="p">(</span>
</span><span class="line">      <span class="n">input</span><span class="p">:</span><span class="n">cdata</span><span class="p">(),</span>
</span><span class="line">      <span class="n">target</span><span class="p">:</span><span class="n">cdata</span><span class="p">(),</span>
</span><span class="line">      <span class="n">self</span><span class="p">.</span><span class="n">output_tensor</span><span class="p">:</span><span class="n">cdata</span><span class="p">(),</span>
</span><span class="line">      <span class="n">self</span><span class="p">.</span><span class="n">sizeAverage</span>
</span><span class="line">   <span class="p">)</span>
</span><span class="line">   <span class="n">self</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">output_tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span class="line">   <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">output</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>此部分的核心運算是用 C 來加速，在第三行會呼叫 C 的函數 <code>MSECriterion_updateOutput</code> ，再將結果存在 <code>output_tensor</code> ，而 lua 則從 <code>output_tensor</code> 中取出結果，傳到 <code>output</code> 。</p>

<p>C 的程式碼在 <a href="https://github.com/torch/nn/blob/master/lib/THNN/generic/MSECriterion.c">MSECriterion.c</a> 檔案中：</p>

<figure class="code"><figcaption><span>nn/lib/THNN/generic/MSECriterion.c</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span class="kt">void</span> <span class="nf">THNN_</span><span class="p">(</span><span class="n">MSECriterion_updateOutput</span><span class="p">)(</span>
</span><span class="line">          <span class="n">THNNState</span> <span class="o">*</span><span class="n">state</span><span class="p">,</span>
</span><span class="line">          <span class="n">THTensor</span> <span class="o">*</span><span class="n">input</span><span class="p">,</span>
</span><span class="line">          <span class="n">THTensor</span> <span class="o">*</span><span class="n">target</span><span class="p">,</span>
</span><span class="line">          <span class="n">THTensor</span> <span class="o">*</span><span class="n">output</span><span class="p">,</span>
</span><span class="line">          <span class="kt">bool</span> <span class="n">sizeAverage</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">  <span class="n">THNN_CHECK_NELEMENT</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">target</span><span class="p">);</span>
</span><span class="line">  <span class="n">THNN_CHECK_DIM_SIZE</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</span><span class="line">
</span><span class="line">  <span class="n">real</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">  <span class="n">TH_TENSOR_APPLY2</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">input</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span>
</span><span class="line">    <span class="n">real</span> <span class="n">z</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">input_data</span> <span class="o">-</span> <span class="o">*</span><span class="n">target_data</span><span class="p">);</span>
</span><span class="line">    <span class="n">sum</span> <span class="o">+=</span> <span class="n">z</span><span class="o">*</span><span class="n">z</span><span class="p">;</span>
</span><span class="line">  <span class="p">);</span>
</span><span class="line">
</span><span class="line">  <span class="k">if</span> <span class="p">(</span><span class="n">sizeAverage</span><span class="p">)</span>
</span><span class="line">    <span class="n">sum</span> <span class="o">/=</span> <span class="n">THTensor_</span><span class="p">(</span><span class="n">nElement</span><span class="p">)(</span><span class="n">input</span><span class="p">);</span>
</span><span class="line">
</span><span class="line">  <span class="n">THTensor_</span><span class="p">(</span><span class="n">set1d</span><span class="p">)(</span><span class="n">output</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>

<p>在第 14 ~ 15 行中可看到將 <code>input_data</code> 與 <code>target_data</code> 相減後得出 <code>z</code> ，並將 <code>z</code> 平方後累加，得到 <code>sum</code> ，而在 18 ~ 19 行中， <code>sum</code> 會根依序 <code>sizeAverage</code> 的值，來決定要不要除以所有元素的個數。</p>

<p>最後一行則將 <code>sum</code> 的值存到 <code>output</code> 中，這個 <code>output</code> 所對應到 lua 中的 <code>output_tensor</code> 。</p>

<p>關於 <code>updateGradInput</code> 以及 backward propagation 的部分，將在下回介紹。</p>

<h2 id="materials">Materials</h2>

<p>本次教學的完整程式碼於此：</p>

<p><a href="https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/3_nn_criterion_and_msecriterion.ipynb">https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/3_nn_criterion_and_msecriterion.ipynb</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Torch NN Tutorial 2 : NN.Container & NN.Sequential]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/12/24/torch-nn-tutorial-2-nn-container/"/>
    <updated>2016-12-24T00:24:25+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/12/24/torch-nn-tutorial-2-nn-container</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>在<a href="http://ckmarkoh.github.io/blog/2016/12/19/torch-nn-tutorial-1-nn-module/">Torch NN tutorial 1 : NN.Module &amp; NN.Linear</a>中，介紹了構成 neural network 的最基本的單位，也就是 <code>nn.Module</code> ，並介紹了 <code>nn.Linear</code> 可以進行一次線性運算，但通常一個 neural network 是經由多個線性和非線性的運算構成的。要把多個 Module 串接起來，就需要用到 <code>nn.Contaner</code> 。</p>

<p>例如，有個 neural network 的輸入為 x ，輸出為 z ，中間經過了一次線性運算與一個sigmoid function ，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\textbf{y} = \textbf{W}\textbf{x} + \textbf{b}  \\
&\textbf{z} = \frac{1}{1+e^{-\textbf{y}}}
\end{align}
 %]]&gt;</script>

<p>假設 <script type="math/tex">\textbf{x}</script> 為 2 維的向量，而 <script type="math/tex">\textbf{y}</script> 為 3 維向量， 
<script type="math/tex">\textbf{W},\textbf{b}</script> 分別為 weight 和 bias ，此兩參數皆以隨機值進行初始化，
<script type="math/tex">\textbf{z}</script>  是 <script type="math/tex">\textbf{y}</script> 經過 sigmoid 非線性運算的輸出結果。</p>

<!--more-->

<p>使用 torch 實作此運算：</p>

<p>首先，載入 <code>nn</code> 套件：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="nb">require</span> <span class="s1">&#39;</span><span class="s">nn&#39;</span>
</span></code></pre></td></tr></table></div></figure>

<p>建立以上兩個運算所需的Module，分別為 <code>nn.Linear</code> 和 <code>nn.Sigmoid</code> ，並將它們分別命名為 <code>l1</code> 和 <code>s1</code> ，如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</span><span class="line"><span class="n">s1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sigmoid</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>其中， <code>nn.Sigmoid</code> 為進行 sigmoid 運算所需的 module 。</p>

<p>如果要進行運算，方法如下：</p>

<p>假設 x 為一個二維向量 <script type="math/tex">[0,1]</script> ，先輸入 <code>l1</code> ，進行 forward propagation ，將運算結果傳給 <code>y</code> ，如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">{</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">}</span>
</span><span class="line"><span class="n">y</span> <span class="o">=</span> <span class="n">l1</span><span class="p">:</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>執行完後會印出 <code>l1</code> 的輸出值，也就是 <code>y</code> 的值，是個三維的向量，
y 的值會與 <code>l1.weight</code> 和 <code>l1.bias</code> 的初始值值關， <code>y</code> 值如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"> 0.1948
</span><span class="line"> 0.9780
</span><span class="line"> 0.3982
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>再來將 <code>y</code> 輸入到 <code>s1</code> 中， 進行 forward propagation，將運算結果傳給 <code>z</code> ，如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">z</span> <span class="o">=</span> <span class="n">s1</span><span class="p">:</span><span class="n">forward</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>執行完後會印出 <code>z</code> 值，此值是由 <code>y</code> 的值經過 sigmoid function 的運算所得出來的， <code>z</code> 值如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"> 0.5485
</span><span class="line"> 0.7267
</span><span class="line"> 0.5983
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>以上過程中，給定了 <code>x</code> 值，經過每一個 module 都要呼叫一次 <code>forward</code> ，最後才能取得 <code>z</code> 的值，
如果一個 network 是由很多個 module 所組成的，這樣要呼叫很多次 <code>forward</code> ，會很麻煩。</p>

<p>而 <code>nn.Container</code> 則是一個容器，它可以把多個 module 放進去，並自動處理這些 module 輸入與輸出值的傳遞。 <code>nn.Container</code> 也是個抽象個類別，泛指能夠把 module 放進去的容器。</p>

<p>其中一種 container 可將前一個 module 的輸出，傳給下一個 module 的輸入，這種 contanier 為 <code>nn.Sequential()</code> 。</p>

<p>例如，如果要將 <code>l1</code> 的輸出，傳入 <code>s1</code> ，則可以用 <code>nn.Sequential</code> 來達成。建立一個命名為 <code>n1</code> 的 sequential container ，將 <code>l1</code> 和 <code>s1</code> 串起來，方法如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">n1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
</span><span class="line"><span class="n">n1</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">l1</span><span class="p">)</span>
</span><span class="line"><span class="n">n1</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">s1</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>其中，<code>add</code> 是將 module 加入 container 的 function。</p>

<p>其實， <code>nn.Container</code> 也是從 <code>nn.Module</code> 繼承而來的，所以它也有 <code>forward</code> ，可以進行 forward propagation ，只要呼叫了 <code>nn.Container</code> 的 <code>forward</code> ，它就會自動將裡面的 module 進行 forward ，並輸出結果。</p>

<p>如此給定 <code>x</code> 之後，只要進行一次 forward 即可取得 <code>z</code> 的結果，如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">{</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">}</span>
</span><span class="line"><span class="n">z</span> <span class="o">=</span> <span class="n">n1</span><span class="p">:</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>執行完後，會印出 <code>z</code> 的結果，如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"> 0.5485
</span><span class="line"> 0.7267
</span><span class="line"> 0.5983
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<h2 id="nncontainer--nnsequential">nn.Container &amp; nn.Sequential</h2>

<p>這邊要介紹 <code>nn.Container</code> 和 <code>nn.Sequential</code> 的程式碼。</p>

<p><code>nn.Container</code> 程式碼：<a href="https://github.com/torch/nn/blob/master/Container.lua">https://github.com/torch/nn/blob/master/Container.lua</a></p>

<p><code>nn.Sequential</code> 程式碼： <a href="https://github.com/torch/nn/blob/master/Sequential.lua">https://github.com/torch/nn/blob/master/Sequential.lua</a></p>

<p>首先，介紹 <code>nn.Container</code> ，先看 <code>init()</code> 的部分：</p>

<figure class="code"><figcaption><span>nn/Container.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="kd">local</span> <span class="n">Container</span><span class="p">,</span> <span class="n">parent</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">class</span><span class="p">(</span><span class="s1">&#39;</span><span class="s">nn.Container&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="s">nn.Module&#39;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">function</span> <span class="nf">Container</span><span class="p">:</span><span class="n">__init</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</span><span class="line">    <span class="n">parent</span><span class="p">.</span><span class="n">__init</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</span><span class="line">    <span class="n">self</span><span class="p">.</span><span class="n">modules</span> <span class="o">=</span> <span class="p">{}</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>第1行處，看到 <code>nn.Container</code> 是從 <code>nn.Module</code> 繼承而來的，因此它具有 <code>nn.Module</code> 的 variable 及 function 。</p>

<p>第5行中，container 的變量 <code>modules</code> ，它是一個 lua table ，是用來存放加到 container 中的  module 。</p>

<p>至於，要如何將 module 加到 container 中？可以用 <code>add</code> 加入。 <code>add</code> 的程式碼如下： </p>

<figure class="code"><figcaption><span>nn/Container.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">Container</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="nb">module</span><span class="p">)</span>
</span><span class="line">    <span class="nb">table.insert</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">modules</span><span class="p">,</span> <span class="nb">module</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">self</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>在第2行可看到， <code>add</code> 可以把新的 module 加到 <code>modules</code> 中。</p>

<figure class="code"><figcaption><span>nn/Container.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">Container</span><span class="p">:</span><span class="n">get</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">modules</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>除了加進去之外，也可以用 <code>get</code> 取得 <code>modules</code> 中的某個元素，或是用 <code>size</code> 取得 <code>modules</code> 的大小，程式碼如下：</p>

<figure class="code"><figcaption><span>nn/Container.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">Container</span><span class="p">:</span><span class="n">get</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">modules</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
</span><span class="line"><span class="k">end</span>
</span><span class="line">
</span><span class="line"><span class="k">function</span> <span class="nf">Container</span><span class="p">:</span><span class="n">size</span><span class="p">()</span>
</span><span class="line">    <span class="k">return</span> <span class="o">#</span><span class="n">self</span><span class="p">.</span><span class="n">modules</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>再來是實作的部分，首先我們建立一個 sequential 的 container ，命名為 <code>n2</code> ，並以 <code>size</code> 取得它所含的 module 個數，如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">n2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">n2</span><span class="p">:</span><span class="n">size</span><span class="p">())</span>
</span></code></pre></td></tr></table></div></figure>

<p>剛建立時， 由於 <code>n2</code> 的 <code>modules</code> 是空的，所以 <code>size</code> 是 0。執行結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">0	
</span></code></pre></td></tr></table></div></figure>

<p>再來將 <code>l1</code> 和 <code>s1</code> 依序加入，再看看 <code>size</code> 的變化：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">n2</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">l1</span><span class="p">)</span>
</span><span class="line"><span class="n">n2</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">s1</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">n2</span><span class="p">:</span><span class="n">size</span><span class="p">())</span>
</span></code></pre></td></tr></table></div></figure>

<p>此時已經加入了兩個 module 進去了，所以 <code>size</code> 會是 2 ，執行結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">2	
</span></code></pre></td></tr></table></div></figure>

<p>可以用 <code>get</code> 取得加進去的 module ，例如，想取得第一個加進去的，作法如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">n2</span><span class="p">:</span><span class="n">get</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>

<p>第一個加進去的為 <code>nn.Linear</code> ，執行結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">nn.Linear<span class="o">(</span><span class="m">2</span> -&gt; 3<span class="o">)</span>
</span><span class="line"><span class="o">{</span>
</span><span class="line">  gradBias : DoubleTensor - size: 3
</span><span class="line">  weight : DoubleTensor - size: 3x2
</span><span class="line">  _type : torch.DoubleTensor
</span><span class="line">  output : DoubleTensor - size: 3
</span><span class="line">  gradInput : DoubleTensor - empty
</span><span class="line">  bias : DoubleTensor - size: 3
</span><span class="line">  gradWeight : DoubleTensor - size: 3x2
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>

<p>再來看到 <code>nn.Sequential()</code> 的程式碼。</p>

<figure class="code"><figcaption><span>nn/Sequential.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="kd">local</span> <span class="n">Sequential</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">class</span><span class="p">(</span><span class="s1">&#39;</span><span class="s">nn.Sequential&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="s">nn.Container&#39;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>第一行顯示 <code>nn.Sequential</code> 繼承了 <code>nn.Container</code> 。它的 <code>init</code> 也與 <code>nn.Container</code> 共用，沒有再另外實作。</p>

<p>再來看 <code>add</code> 的程式碼。</p>

<figure class="code"><figcaption><span>nn/Sequential.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">Sequential</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="nb">module</span><span class="p">)</span>
</span><span class="line">   <span class="k">if</span> <span class="o">#</span><span class="n">self</span><span class="p">.</span><span class="n">modules</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">then</span>
</span><span class="line">      <span class="n">self</span><span class="p">.</span><span class="n">gradInput</span> <span class="o">=</span> <span class="n">module</span><span class="p">.</span><span class="n">gradInput</span>
</span><span class="line">   <span class="k">end</span>
</span><span class="line">   <span class="nb">table.insert</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">modules</span><span class="p">,</span> <span class="nb">module</span><span class="p">)</span>
</span><span class="line">   <span class="n">self</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">module</span><span class="p">.</span><span class="n">output</span>
</span><span class="line">   <span class="k">return</span> <span class="n">self</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>在 <code>nn.Sequential</code> 中， <code>add</code> 除了會將 module 加到 <code>modules</code> 之外，它的 <code>output</code> 會是最後一個加進去的 module 的 <code>output</code> 。</p>

<p>以下實作，將 <code>n2</code> 的 <code>output</code> 和它裡面的兩個 module 的 <code>output</code> 都印出來看看：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">n2</span><span class="p">:</span><span class="n">get</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">output</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">n2</span><span class="p">:</span><span class="n">get</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">output</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">n2</span><span class="p">.</span><span class="n">output</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下， <code>n2</code> 的 output 是第二個 module 的 output ：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"> 0.1948
</span><span class="line"> 0.9780
</span><span class="line"> 0.3982
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3<span class="o">]</span>
</span><span class="line">
</span><span class="line"> 0.5485
</span><span class="line"> 0.7267
</span><span class="line"> 0.5983
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3<span class="o">]</span>
</span><span class="line">
</span><span class="line"> 0.5485
</span><span class="line"> 0.7267
</span><span class="line"> 0.5983
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>再來看到 <code>updateOutput</code> 的部分：</p>

<figure class="code"><figcaption><span>nn/Sequential.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">Sequential</span><span class="p">:</span><span class="n">updateOutput</span><span class="p">(</span><span class="n">input</span><span class="p">)</span>
</span><span class="line">   <span class="kd">local</span> <span class="n">currentOutput</span> <span class="o">=</span> <span class="n">input</span>
</span><span class="line">   <span class="k">for</span> <span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="o">#</span><span class="n">self</span><span class="p">.</span><span class="n">modules</span> <span class="k">do</span>
</span><span class="line">      <span class="n">currentOutput</span> <span class="o">=</span> <span class="n">self</span><span class="p">:</span><span class="n">rethrowErrors</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">i</span><span class="p">,</span> <span class="s1">&#39;</span><span class="s">updateOutput&#39;</span><span class="p">,</span> <span class="n">currentOutput</span><span class="p">)</span>
</span><span class="line">   <span class="k">end</span>
</span><span class="line">   <span class="n">self</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">currentOutput</span>
</span><span class="line">   <span class="k">return</span> <span class="n">currentOutput</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>以上可知，在第3行的 for 迴圈中， <code>modules</code> 中的 <code>module</code> 會依序進行  <code>updateOutput</code> ，並將其 <code>output</code> 傳遞到下個 <code>module</code> 的 <code>input</code> ，而 <code>nn.Sequential</code> 的 <code>output</code> 會是最後一個 <code>module</code> 的 <code>output</code></p>

<p>假設輸入 <code>n2</code> 的 <code>x</code> 為 <script type="math/tex">[1,0]</script> ，則進行 forward propagation ，將輸出結果存到 <code>z</code> ，實作如下 ：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">}</span>
</span><span class="line"><span class="n">z</span> <span class="o">=</span> <span class="n">n2</span><span class="p">:</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>透過 updateOutput 中的 for 迴圈，它會依序跑完內部所有的 module，並得出最後結果，結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"> 0.5331
</span><span class="line"> 0.6890
</span><span class="line"> 0.5093
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>如果想看 <code>n2</code> 中有哪些 module ，以及它們的順序，也可以直接用 <code>print</code> 的方式，作法如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">n2</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下：</p>

<figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">nn.Sequential <span class="o">{</span>
</span><span class="line">  <span class="o">[</span>input -&gt; <span class="o">(</span>1<span class="o">)</span> -&gt; <span class="o">(</span>2<span class="o">)</span> -&gt; output<span class="o">]</span>
</span><span class="line">  <span class="o">(</span>1<span class="o">)</span>: nn.Linear<span class="o">(</span><span class="m">2</span> -&gt; 3<span class="o">)</span>
</span><span class="line">  <span class="o">(</span>2<span class="o">)</span>: nn.Sigmoid
</span><span class="line"><span class="o">}</span>
</span><span class="line"><span class="o">{</span>
</span><span class="line">  gradInput : DoubleTensor - empty
</span><span class="line">  modules :
</span><span class="line">    <span class="o">{</span>
</span><span class="line">      <span class="m">1</span> :
</span><span class="line">        nn.Linear<span class="o">(</span><span class="m">2</span> -&gt; 3<span class="o">)</span>
</span><span class="line">        <span class="o">{</span>
</span><span class="line">          gradBias : DoubleTensor - size: 3
</span><span class="line">          weight : DoubleTensor - size: 3x2
</span><span class="line">          _type : torch.DoubleTensor
</span><span class="line">          output : DoubleTensor - size: 3
</span><span class="line">          gradInput : DoubleTensor - empty
</span><span class="line">          bias : DoubleTensor - size: 3
</span><span class="line">          gradWeight : DoubleTensor - size: 3x2
</span><span class="line">        <span class="o">}</span>
</span><span class="line">      <span class="m">2</span> :
</span><span class="line">        nn.Sigmoid
</span><span class="line">        <span class="o">{</span>
</span><span class="line">          gradInput : DoubleTensor - empty
</span><span class="line">          _type : torch.DoubleTensor
</span><span class="line">          output : DoubleTensor - size: 3
</span><span class="line">        <span class="o">}</span>
</span><span class="line">    <span class="o">}</span>
</span><span class="line">  _type : torch.DoubleTensor
</span><span class="line">  output : DoubleTensor - size: 3
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>

<p>以上結果可分成兩部分來看，第一部分是 <code>nn.Sequential</code> 裡面每個 module 的順序，從 input 到 output 之間，依次進行了哪些運算。</p>

<p>第二部份是詳細印出 <code>nn.Sequential</code> 中，有哪些成員 ，它有從 <code>nn.Module</code> 繼承而來的 <code>gradInput</code> 即 <code>output</code> ，也有從 <code>nn.Container</code> 繼承而來的 <code>modules</code> 。而 <code>modules</code> 的部分會詳細列出裡面每個 module 中有哪些 variable。</p>

<p>這部分的程式碼於 <code>tostring</code> 函式中，程式碼如下：</p>

<figure class="code"><figcaption><span>nn/Sequential.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">Sequential</span><span class="p">:</span><span class="n">__tostring__</span><span class="p">()</span>
</span><span class="line">   <span class="kd">local</span> <span class="n">tab</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="s">  &#39;</span>
</span><span class="line">   <span class="kd">local</span> <span class="n">line</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s">&#39;</span>
</span><span class="line">   <span class="kd">local</span> <span class="nb">next</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="s"> -&gt; &#39;</span>
</span><span class="line">   <span class="kd">local</span> <span class="n">str</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="s">nn.Sequential&#39;</span>
</span><span class="line">   <span class="n">str</span> <span class="o">=</span> <span class="n">str</span> <span class="o">..</span> <span class="s1">&#39;</span><span class="s"> {&#39;</span> <span class="o">..</span> <span class="n">line</span> <span class="o">..</span> <span class="n">tab</span> <span class="o">..</span> <span class="s1">&#39;</span><span class="s">[input&#39;</span>
</span><span class="line">   <span class="k">for</span> <span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="o">#</span><span class="n">self</span><span class="p">.</span><span class="n">modules</span> <span class="k">do</span>
</span><span class="line">      <span class="n">str</span> <span class="o">=</span> <span class="n">str</span> <span class="o">..</span> <span class="nb">next</span> <span class="o">..</span> <span class="s1">&#39;</span><span class="s">(&#39;</span> <span class="o">..</span> <span class="n">i</span> <span class="o">..</span> <span class="s1">&#39;</span><span class="s">)&#39;</span>
</span><span class="line">   <span class="k">end</span>
</span><span class="line">   <span class="n">str</span> <span class="o">=</span> <span class="n">str</span> <span class="o">..</span> <span class="nb">next</span> <span class="o">..</span> <span class="s1">&#39;</span><span class="s">output]&#39;</span>
</span><span class="line">   <span class="k">for</span> <span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="o">#</span><span class="n">self</span><span class="p">.</span><span class="n">modules</span> <span class="k">do</span>
</span><span class="line">      <span class="n">str</span> <span class="o">=</span> <span class="n">str</span> <span class="o">..</span> <span class="n">line</span> <span class="o">..</span> <span class="n">tab</span> <span class="o">..</span> <span class="s1">&#39;</span><span class="s">(&#39;</span> <span class="o">..</span> <span class="n">i</span> <span class="o">..</span> <span class="s1">&#39;</span><span class="s">): &#39;</span> <span class="o">..</span> <span class="nb">tostring</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">]):</span><span class="n">gsub</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">line</span> <span class="o">..</span> <span class="n">tab</span><span class="p">)</span>
</span><span class="line">   <span class="k">end</span>
</span><span class="line">   <span class="n">str</span> <span class="o">=</span> <span class="n">str</span> <span class="o">..</span> <span class="n">line</span> <span class="o">..</span> <span class="s1">&#39;</span><span class="s">}&#39;</span>
</span><span class="line">   <span class="k">return</span> <span class="n">str</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<h2 id="materials">Materials</h2>

<p>本次教學的完整程式碼於此：</p>

<p><a href="https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/2_nn_container_and_sequential.ipynb">https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/2_nn_container_and_sequential.ipynb</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Torch NN Tutorial 1 : NN.Module & NN.Linear]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/12/19/torch-nn-tutorial-1-nn-module/"/>
    <updated>2016-12-19T22:36:47+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/12/19/torch-nn-tutorial-1-nn-module</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>此系列講解如何用 torch 實作 neural network 。</p>

<p>本系列不講解如何安裝 torch 及 lua 的基本語法，假設讀者都已具備這些基礎知識。</p>

<p>以 torch 實作 neural network 時，最常用的套件為 <a href="https://github.com/torch/nn">nn</a>，而在 <code>nn</code> 中，建構 neural network 最基本的單位為 <a href="https://github.com/torch/nn/blob/master/Module.lua">nn.Module</a> 。 <code>nn.Module</code> 是一個抽象類別，所有建構 neural network 本身有關的 module ，都是從 <code>nn.Module</code> 所繼承而來。</p>

<p>舉個例子，如果要實作以下運算：</p>

<script type="math/tex; mode=display">

\textbf{y} = \textbf{W}\textbf{x} + \textbf{b} 

</script>

<p>假設 <script type="math/tex">\textbf{x}</script> 為 2 維的 input ，而 <script type="math/tex">\textbf{y}</script> 為 3 維的output， <script type="math/tex">\textbf{W},\textbf{b}</script> 分別為 weight 和 bias ，此兩參數皆以隨機值進行初始化。</p>

<p>使用 torch 實作此運算的方法如下：</p>

<p>首先，載入 nn 套件：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="nb">require</span> <span class="s1">&#39;</span><span class="s">nn&#39;</span>
</span></code></pre></td></tr></table></div></figure>

<p>建立一個 Linear Module：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<!--more-->

<p>其中， <a href="https://github.com/torch/nn/blob/master/Linear.lua">nn.Linear</a> 即是用來進行 <script type="math/tex">\textbf{y} = \textbf{W}\textbf{x} + \textbf{b} </script> 這類的線性運算所用的模組，它繼承了 <code>nn.Module</code> 。 而 2 和 3 分別代表了 <script type="math/tex">\textbf{x}</script> 和 <script type="math/tex">\textbf{y}</script> 的維度。 當它被建構出來時， weight 和 bias 的值會以隨機值來初始化。 </p>

<p>以上程式中，建立一個命名為 <code>l1</code> 的 module ，如果要取得它的 weight 和 bias ，可以用 <code>l1.weight</code> 和 <code>l1.bias</code> 取得，方法如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l1</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>執行結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"> 0.1453  0.5062
</span><span class="line"> 0.0635  0.4911
</span><span class="line">-0.1080  0.1747
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3x2<span class="o">]</span>
</span><span class="line">
</span><span class="line"> 0.2063
</span><span class="line">-0.1635
</span><span class="line">-0.0883
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>其中， size 3x2 的 tensor 為 weight, size 3 的 tensor 為 bias。</p>

<p>用此 module 可以執行運算，令 x 為一個二維向量 <script type="math/tex">[0,1]</script> ，輸入此 module ，進行 forward propagation ，也就是說，執行 <script type="math/tex">\textbf{y} = \textbf{W}\textbf{x} + \textbf{b} </script> 的運算， 並輸出結果為 <script type="math/tex">\textbf{y}</script>  ，實作如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">{</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">}</span>
</span><span class="line"><span class="n">y</span> <span class="o">=</span> <span class="n">l1</span><span class="p">:</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>輸出結果 <code>y</code> 為一個三維向量，如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"> 0.7125
</span><span class="line"> 0.3276
</span><span class="line"> 0.0865
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<h2 id="nnmodule--nnlinear">nn.Module &amp; nn.Linear</h2>

<p>這邊要更進一步介紹 <code>nn.Module</code> 和 <code>nn.Linear</code> 的內容是什麼。由於 torch 的程式碼相當簡潔易懂，可以直接看程式碼來了解它的功能是什麼。</p>

<p><code>nn.Module</code> 程式碼： <a href="https://github.com/torch/nn/blob/master/Module.lua">https://github.com/torch/nn/blob/master/Module.lua</a></p>

<p><code>nn.Linear</code> 程式碼： <a href="https://github.com/torch/nn/blob/master/Linear.lua">https://github.com/torch/nn/blob/master/Linear.lua</a></p>

<p>首先，介紹 <code>nn.Module</code> ，先看 <code>init()</code> 的部分：</p>

<figure class="code"><figcaption><span>nn/Module.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">Module</span><span class="p">:</span><span class="n">__init</span><span class="p">()</span>
</span><span class="line">   <span class="n">self</span><span class="p">.</span><span class="n">gradInput</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">()</span>
</span><span class="line">   <span class="n">self</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">()</span>
</span><span class="line">   <span class="n">self</span><span class="p">.</span><span class="n">_type</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">output</span><span class="p">:</span><span class="nb">type</span><span class="p">()</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p><code>Module</code> 中最基本的成員有 <code>output</code> 和 <code>gradInput</code> 。
<code>output</code> 為此 <code>Module</code> 的 forward propagation 結果，而 <code>gradInput</code> 為 backward propagation 的運算結果。
這些變量一開始都會被初始化為 空的 tensor 。</p>

<p>註：本文先不講解 backward propagation 與 <code>gradInput</code> 的部分，交由之後的教學文章來解釋。</p>

<p>在 <code>Module:forward</code> 的部分，是用來進行 forward propagation的，如下：</p>

<figure class="code"><figcaption><span>nn/Module.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">Module</span><span class="p">:</span><span class="n">updateOutput</span><span class="p">(</span><span class="n">input</span><span class="p">)</span>
</span><span class="line">   <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">output</span>
</span><span class="line"><span class="k">end</span>
</span><span class="line">
</span><span class="line"><span class="k">function</span> <span class="nf">Module</span><span class="p">:</span><span class="n">forward</span><span class="p">(</span><span class="n">input</span><span class="p">)</span>
</span><span class="line">   <span class="k">return</span> <span class="n">self</span><span class="p">:</span><span class="n">updateOutput</span><span class="p">(</span><span class="n">input</span><span class="p">)</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>先看 forward 的部分， Module 沒有運算的實作，僅單純輸出 <code>output</code> 值。如果呼叫了 forward propagation ，則從 <code>Module:updateOutput</code> 就直接輸出了 <code>output</code> 。</p>

<p>而 <code>nn.Linear</code> 則實作了 forward propagation。</p>

<p>所謂的 Linear，即是指 <script type="math/tex">\textbf{y} = \textbf{W}\textbf{x} + \textbf{b} </script> 的線性運算。</p>

<p>再來看 <code>nn.Linear</code> 的程式碼，先看 <code>init()</code> 的部分：</p>

<figure class="code"><figcaption><span>nn/Linear.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="kd">local</span> <span class="n">Linear</span><span class="p">,</span> <span class="n">parent</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">class</span><span class="p">(</span><span class="s1">&#39;</span><span class="s">nn.Linear&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="s">nn.Module&#39;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">function</span> <span class="nf">Linear</span><span class="p">:</span><span class="n">__init</span><span class="p">(</span><span class="n">inputSize</span><span class="p">,</span> <span class="n">outputSize</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
</span><span class="line">   <span class="n">parent</span><span class="p">.</span><span class="n">__init</span><span class="p">(</span><span class="n">self</span><span class="p">)</span>
</span><span class="line">   <span class="kd">local</span> <span class="n">bias</span> <span class="o">=</span> <span class="p">((</span><span class="n">bias</span> <span class="o">==</span> <span class="kc">nil</span><span class="p">)</span> <span class="ow">and</span> <span class="kc">true</span><span class="p">)</span> <span class="ow">or</span> <span class="n">bias</span>
</span><span class="line">   <span class="n">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">outputSize</span><span class="p">,</span> <span class="n">inputSize</span><span class="p">)</span>
</span><span class="line">   <span class="n">self</span><span class="p">.</span><span class="n">gradWeight</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">outputSize</span><span class="p">,</span> <span class="n">inputSize</span><span class="p">)</span>
</span><span class="line">   <span class="k">if</span> <span class="n">bias</span> <span class="k">then</span>
</span><span class="line">      <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">outputSize</span><span class="p">)</span>
</span><span class="line">      <span class="n">self</span><span class="p">.</span><span class="n">gradBias</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">outputSize</span><span class="p">)</span>
</span><span class="line">   <span class="k">end</span>
</span><span class="line">   <span class="n">self</span><span class="p">:</span><span class="n">reset</span><span class="p">()</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>在第1行， <code>nn.Linear</code> 繼承了 <code>nn.Module</code> 。</p>

<p>在第3行開始可以看到，建構 Linear 所需的參數有 <code>inputSize</code> , <code>outputSize</code> 和 <code>bias</code> 。 <code>bias</code>  不一定要給，如果沒有給，則預設值會讓它是隨機的。除非 <code>bias=false</code> ，則此 Linear Module 就不會有 <code>bias</code> 。
從6~10行中，它比 <code>nn.Module</code> 多了 <code>weigt</code> 和 <code>bias</code> 這兩個變量，而 <code>reset()</code> 則是將它們初始化。</p>

<p>如果要建立一個 Linear Module，則要給定 <code>inputSize</code> 和 <code>outputSize</code> ，也就是 <script type="math/tex">\textbf{x}</script> 和 <script type="math/tex">\textbf{y}</script> 的維度。</p>

<p>假設  <script type="math/tex">\textbf{x}</script> 是二維， <script type="math/tex">\textbf{y}</script> 是三維，建立一個命名為 <code>l2</code> 的 Linear 模組：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">l2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>用以下方法印出 l2 的 <code>weight</code> , <code>bias</code> 和 <code>output</code> ：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l2</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l2</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l2</span><span class="p">.</span><span class="n">output</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>輸出結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">-0.2863  0.5541
</span><span class="line">-0.6269  0.6557
</span><span class="line">-0.3215 -0.1648
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3x2<span class="o">]</span>
</span><span class="line">
</span><span class="line">-0.0316
</span><span class="line"> 0.4126
</span><span class="line"> 0.4415
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3<span class="o">]</span>
</span><span class="line">
</span><span class="line"><span class="o">[</span>torch.DoubleTensor with no dimension<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>其中，<code>weight</code> 和 <code>bias</code> 會被初始化隨機成 size 3x2 和 size 3 的 double tensor ，而最後一行顯示出 <code>output</code> 還是空的（with no dimension）。</p>

<p>如果想知道各個 variable 的 size ，還有個方式，就是直接用 print 的方式把它印出來，作法如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l2</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>執行結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">nn.Linear<span class="o">(</span><span class="m">2</span> -&gt; 3<span class="o">)</span>
</span><span class="line"><span class="o">{</span>
</span><span class="line">  gradBias : DoubleTensor - size: 3
</span><span class="line">  weight : DoubleTensor - size: 3x2
</span><span class="line">  _type : torch.DoubleTensor
</span><span class="line">  output : DoubleTensor - empty
</span><span class="line">  gradInput : DoubleTensor - empty
</span><span class="line">  bias : DoubleTensor - size: 3
</span><span class="line">  gradWeight : DoubleTensor - size: 3x2
</span><span class="line"><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>

<p>要讓 <code>output</code> 有值，就要進行 forward propagation 。而 <code>Linear:updateOutput</code> 則是實作了 <code>Module:updateOutput</code> 中， forward propagation 運算的實際內容，程式碼如下：</p>

<figure class="code"><figcaption><span>nn/Linear.lua</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="k">function</span> <span class="nf">Linear</span><span class="p">:</span><span class="n">updateOutput</span><span class="p">(</span><span class="n">input</span><span class="p">)</span>
</span><span class="line">   <span class="k">if</span> <span class="n">input</span><span class="p">:</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">then</span>
</span><span class="line">      <span class="n">self</span><span class="p">.</span><span class="n">output</span><span class="p">:</span><span class="n">resize</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="p">:</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</span><span class="line">      <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="k">then</span> <span class="n">self</span><span class="p">.</span><span class="n">output</span><span class="p">:</span><span class="n">copy</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span> <span class="k">else</span> <span class="n">self</span><span class="p">.</span><span class="n">output</span><span class="p">:</span><span class="n">zero</span><span class="p">()</span> <span class="k">end</span>
</span><span class="line">      <span class="n">self</span><span class="p">.</span><span class="n">output</span><span class="p">:</span><span class="n">addmv</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">input</span><span class="p">)</span>
</span><span class="line">   <span class="k">elseif</span> <span class="n">input</span><span class="p">:</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">then</span>
</span><span class="line">      <span class="kd">local</span> <span class="n">nframe</span> <span class="o">=</span> <span class="n">input</span><span class="p">:</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span class="line">      <span class="kd">local</span> <span class="n">nElement</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">output</span><span class="p">:</span><span class="n">nElement</span><span class="p">()</span>
</span><span class="line">      <span class="n">self</span><span class="p">.</span><span class="n">output</span><span class="p">:</span><span class="n">resize</span><span class="p">(</span><span class="n">nframe</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="p">:</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</span><span class="line">      <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">output</span><span class="p">:</span><span class="n">nElement</span><span class="p">()</span> <span class="o">~=</span> <span class="n">nElement</span> <span class="k">then</span>
</span><span class="line">         <span class="n">self</span><span class="p">.</span><span class="n">output</span><span class="p">:</span><span class="n">zero</span><span class="p">()</span>
</span><span class="line">      <span class="k">end</span>
</span><span class="line">      <span class="n">updateAddBuffer</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input</span><span class="p">)</span>
</span><span class="line">      <span class="n">self</span><span class="p">.</span><span class="n">output</span><span class="p">:</span><span class="n">addmm</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">input</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="p">:</span><span class="n">t</span><span class="p">())</span>
</span><span class="line">      <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="k">then</span> <span class="n">self</span><span class="p">.</span><span class="n">output</span><span class="p">:</span><span class="n">addr</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">addBuffer</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span> <span class="k">end</span>
</span><span class="line">   <span class="k">else</span>
</span><span class="line">      <span class="nb">error</span><span class="p">(</span><span class="s1">&#39;</span><span class="s">input must be vector or matrix&#39;</span><span class="p">)</span>
</span><span class="line">   <span class="k">end</span>
</span><span class="line">
</span><span class="line">   <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">output</span>
</span><span class="line"><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>

<p>以上可以分為兩部分來看，首先是當 <code>input:dim() ==1</code> 時，也就是 <code>input</code> 的維度為 1 ，也就是一次只輸入單筆資料的時候。第一步，會先調整 <code>output</code> 的 <code>size</code> 為適當的大小，再將 <code>bias</code> 複製到 <code>output</code> ，再讓 <code>input</code> 和 <code>weight</code> 進行矩陣相乘，並和 <code>output</code> 相加，再輸出結果。</p>

<p>從數學公式上來看，輸入值 <script type="math/tex">\textbf{x}</script> 是一維的向量，進行的運算如下：</p>

<script type="math/tex; mode=display">
\textbf{W}\textbf{x} + \textbf{b}
</script>

<p>此時， <script type="math/tex">\textbf{W}</script> 放前面，而 <script type="math/tex">\textbf{x}</script> 放後面。</p>

<p>例如當輸入值向量 <script type="math/tex">[0,1]</script> 時，則矩陣運算的結果為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\begin{bmatrix}
-0.2863 & 0.5541 \\
-0.6269 & 0.6557 \\
-0.3215 & -0.1648 \\
\end{bmatrix}
\begin{bmatrix}
0 \\
1
\end{bmatrix}
+ 
\begin{bmatrix}
-0.0316 \\
 0.4126 \\
 0.4415 \\
\end{bmatrix}
=
\begin{bmatrix}
-0.2863 \times 0 + 0.5541 \times 1  -0.0316 \\
-0.6269 \times 0 + 0.6557 \times 1 + 0.4126\\
-0.3215 \times 0  -0.1648 \times 1 + 0.4415\\
\end{bmatrix}\\
&=
\begin{bmatrix}
0.5541 -0.0316 \\
0.6557 + 0.4126\\
-0.1648 + 0.4415\\
\end{bmatrix}
=
\begin{bmatrix}
  0.5225 \\
  1.0683  \\
  0.2766 \\
\end{bmatrix}
\end{align}

 %]]&gt;</script>

<p>實作以上算式，呼叫 <code>l2:forward</code> ，輸入 <code>torch.Tensor{0,1}</code> 並印出結果：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l2</span><span class="p">:</span><span class="n">forward</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">{</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">}))</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"> 0.5225
</span><span class="line"> 1.0683
</span><span class="line"> 0.2766
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>這時已經進行過了 forward 運算，而 <code>output</code> 有值了，所以可以印出它的值，方法如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l2</span><span class="p">.</span><span class="n">output</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p><code>output</code> 的值也是如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"> 0.5225
</span><span class="line"> 1.0683
</span><span class="line"> 0.2766
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 3<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>而當 <code>input:dim() ==2</code> 時，也就是 <code>input</code> 的維度為 2 ，也就是一次輸入多筆資料。這時需要先把 <code>weight</code> 進行轉置，再和 <code>input</code> ，並和 <code>bias</code> 相加，並將結果加到 <code>output</code> 並輸出。</p>

<p>一次輸入多筆資料的目的是為了加速運算，因為用矩陣對矩陣的相乘的方式就可以來加速。這樣同時輸入的一批資料，就稱為 batch 。</p>

<p>從公式上來看，輸入值 <script type="math/tex">\textbf{X}</script> 是二維的矩陣，則進行以下矩陣運算：</p>

<script type="math/tex; mode=display">
\textbf{X}\textbf{W}^{T} + \textbf{B}
</script>

<p>此時， <script type="math/tex">\textbf{X}</script> 放前面，而 <script type="math/tex">\textbf{W}</script> 進行轉置後放後面。</p>

<p>而 <script type="math/tex">\textbf{B}</script> 是將 <script type="math/tex">\textbf{b}</script> 轉置以後，再複製其橫排所形成的矩陣，以便和前面的矩陣相乘結果來相加。</p>

<p>例如當輸入資料有兩筆向量 <script type="math/tex">[0, 1]</script> 和 <script type="math/tex">[2, 1]</script> 時，則可以組成以下矩陣 (2x2) ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
0 &1 \\
2 &1 \\
\end{bmatrix}


 %]]&gt;</script>

<p>則矩陣運算的過程如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&
\begin{bmatrix}
0 &1 \\
2 &1 \\
\end{bmatrix}
\begin{bmatrix}
-0.2863 & -0.6269 & -0.3215 \\
 0.5541 & 0.6557 & -0.1648 \\
 \end{bmatrix}
+ 
\begin{bmatrix}
-0.0316 & 0.4126 & 0.4415 \\
-0.0316 & 0.4126 & 0.4415 
\end{bmatrix}\\
&
=
\begin{bmatrix}
   -0.2863 \times 0 +  0.5541 \times 1
&  -0.6269 \times 0 +  0.6557 \times 1
&  -0.3215 \times 0   -0.1648 \times 1\\

   -0.2863 \times 2 +  0.5541 \times 1
&  -0.6269 \times 2 +  0.6557 \times 1
&  -0.3215 \times 2   -0.1648 \times 1\\
\end{bmatrix}\\
&
+ 
\begin{bmatrix}
-0.0316 & 0.4126 & 0.4415 \\
-0.0316 & 0.4126 & 0.4415 
\end{bmatrix}\\
&
=
\begin{bmatrix}
0.5541 -0.0316
& 0.6557 +0.4126
& -0.1648 +0.4415 \\
-0.0186  -0.0316
& -0.5981 +0.4126
& -0.8079 +0.4415 \\
\end{bmatrix}\\
&
=
\begin{bmatrix}
 0.5225 & 1.0683  & 0.2766 \\
-0.0502 & -0.1855 & -0.3664 \\
\end{bmatrix}
\end{align}\\

 %]]&gt;</script>

<p>輸出結果為一個 2x3 的矩陣，每一個橫排為一個三維向量，代表著每一筆資料經過線性運算的結果。</p>

<p>實作以上算式，呼叫 <code>l2:forward</code> ，輸入由 <code>{0,1}</code> 和 <code>{2,1}</code> 這兩筆資料組成的 batch， 並印出結果：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="n">input</span><span class="o">=</span><span class="p">{</span>
</span><span class="line">   <span class="p">{</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">},</span>
</span><span class="line">   <span class="p">{</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">}</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l2</span><span class="p">:</span><span class="n">forward</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">input</span><span class="p">)))</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"> 0.5225  1.0683  0.2766
</span><span class="line">-0.0502 -0.1855 -0.3664
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 2x3<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>也可印出 <code>output</code> 的值：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="lua"><span class="line"><span class="nb">print</span><span class="p">(</span><span class="n">l2</span><span class="p">.</span><span class="n">output</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"> 0.5225  1.0683  0.2766
</span><span class="line">-0.0502 -0.1855 -0.3664
</span><span class="line"><span class="o">[</span>torch.DoubleTensor of size 2x3<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<h2 id="materials">Materials</h2>

<p>本次教學的完整程式碼於此：</p>

<p><a href="https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/1_nn_module_and_linear.ipynb">https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/1_nn_module_and_linear.ipynb</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Word2vec (Part 3 : Implementation)]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/08/29/neural-network-word2vec-part-3-implementation/"/>
    <updated>2016-08-29T11:17:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/08/29/neural-network-word2vec-part-3-implementation</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>本文接續 <a href="http://ckmarkoh.github.io/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation">word2vec (part2)</a> ，介紹如何根據推導出來的 <em>backward propagation</em> 公式，從頭到尾實作一個簡易版的 <em>word2vec</em> 。</p>

<p>本例的 input layer 採用 <em>skip-gram</em> ， output layer 採用 <em>negative sampling</em></p>

<p>本例用唐詩語料庫：https://github.com/ckmarkoh/coscup_nndl/blob/master/poem.txt</p>

<p>首先，載入所需的模組</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">json</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span><span class="p">,</span> <span class="n">OrderedDict</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">random</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">math</span>
</span></code></pre></td></tr></table></div></figure>

<!--more-->

<h2 id="build-dictionray">Build Dictionray</h2>

<p>再來是建立字典，即將每個字給一個id來對應。</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">LearnVocabFromTrainFile</span><span class="p">():</span>
</span><span class="line">		
</span><span class="line">    <span class="c"># 開啟唐詩語料庫</span>
</span><span class="line">    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">&quot;poem.txt&quot;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 統計唐詩語料庫中每個字出現的頻率</span>
</span><span class="line">    <span class="n">vcount</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
</span><span class="line">    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">():</span>
</span><span class="line">        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s">&quot;utf-8&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
</span><span class="line">            <span class="n">vcount</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 僅保留出現次數大於五的字，並按照出現次數排序</span>
</span><span class="line">    <span class="n">vcount_list</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">vcount</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
</span><span class="line">                         <span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 建立字典，將每個字給一個id ，字為 key, id 為 value</span>
</span><span class="line">    <span class="n">vocab_dict</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vcount_list</span><span class="p">)))</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 建立詞頻統計用的字典，給定某字，可查到其出現頻率</span>
</span><span class="line">    <span class="n">vocab_freq_dict</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">vcount_list</span><span class="p">))</span>
</span><span class="line">    <span class="k">return</span> <span class="n">vocab_dict</span><span class="p">,</span> <span class="n">vocab_freq_dict</span>
</span><span class="line">
</span><span class="line"><span class="n">vocab_dict</span><span class="p">,</span> <span class="n">vocab_freq_dict</span> <span class="o">=</span>  <span class="n">LearnVocabFromTrainFile</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>印出字典檔，每個字對應到一個id（編號）</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">for</span> <span class="n">w</span><span class="p">,</span><span class="n">wid</span> <span class="ow">in</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span class="line">    <span class="k">print</span> <span class="s">&quot;</span><span class="si">%s</span><span class="s"> : </span><span class="si">%s</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">wid</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">不 : 0
</span><span class="line">人 : 1
</span><span class="line">山 : 2
</span><span class="line">無 : 3
</span><span class="line">風 : 4
</span><span class="line">......
</span><span class="line">謏 : 5496
</span><span class="line">笮 : 5497
</span><span class="line">躠 : 5498
</span><span class="line">噆 : 5499
</span></code></pre></td></tr></table></div></figure>

<p>印出詞頻統計用的字典，給定某字，可查詢到其出現頻率：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">for</span> <span class="n">w</span><span class="p">,</span><span class="n">wfreq</span> <span class="ow">in</span> <span class="n">vocab_freq_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span class="line">    <span class="k">print</span> <span class="s">&quot;</span><span class="si">%s</span><span class="s"> : </span><span class="si">%s</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">wfreq</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">不 : 26426
</span><span class="line">人 : 20966
</span><span class="line">山 : 16056
</span><span class="line">無 : 15795
</span><span class="line">風 : 15618
</span><span class="line">...
</span><span class="line">謏 : 5
</span><span class="line">笮 : 5
</span><span class="line">躠 : 5
</span><span class="line">噆 : 5
</span></code></pre></td></tr></table></div></figure>

<h2 id="build-unigram-table">Build Unigram Table</h2>

<p>本例採用 <em>negative sampling</em> ，需要先建立 <em>unigram table</em> 以便進行 <em>negative sampling</em> 。</p>

<p>所謂的 <em>Unigram Table</em> 即是一個 <em>array</em> ，其中每個元素為某字的id，而某字的頻率，即為此id在此 <em>table</em> 中出現的次數的 0.75次方。</p>

<p>例如，id 為 5496 的字，詞頻為 5 ，則在此 <em>Unigram Table</em> 中，5496 的次數為：</p>

<script type="math/tex; mode=display">

5^{0.75} = 3.34 \approx 3

</script>

<p>由於 <em>array</em> 中的元素個數必須是整數，所以 5496 在 <em>Unigram Table</em> 中出現三次。</p>

<p>建立 <em>Unigram Table</em> 的程式碼如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">InitUnigramTable</span><span class="p">(</span><span class="n">vocab_freq_dict</span><span class="p">):</span>
</span><span class="line">    <span class="n">table_freq_list</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">0.75</span><span class="p">)),</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab_freq_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()))</span>
</span><span class="line">    <span class="n">table_size</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">table_freq_list</span><span class="p">])</span>
</span><span class="line">    <span class="n">table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">table_size</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
</span><span class="line">    <span class="n">offset</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">table_freq_list</span><span class="p">:</span>
</span><span class="line">        <span class="n">table</span><span class="p">[</span><span class="n">offset</span><span class="p">:</span><span class="n">offset</span> <span class="o">+</span> <span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">        <span class="n">offset</span> <span class="o">+=</span> <span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">    <span class="k">return</span> <span class="n">table</span>
</span><span class="line">
</span><span class="line"><span class="n">table</span> <span class="o">=</span> <span class="n">InitUnigramTable</span><span class="p">(</span><span class="n">vocab_freq_dict</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>得出的 <em>Unigram Table</em> 如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="p">[</span>   <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>
</span><span class="line"><span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>  <span class="o">...</span> <span class="p">,</span> <span class="mi">5495</span> <span class="mi">5495</span> <span class="mi">5495</span> <span class="mi">5496</span>  <span class="mi">5496</span> <span class="mi">5496</span> <span class="mi">5497</span> <span class="mi">5497</span> <span class="mi">5497</span> <span class="mi">5498</span>
</span><span class="line"><span class="mi">5498</span> <span class="mi">5498</span> <span class="mi">5499</span> <span class="mi">5499</span> <span class="mi">5499</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>

<h2 id="training-word2vec">Training word2vec</h2>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
<span class="line-number">61</span>
<span class="line-number">62</span>
<span class="line-number">63</span>
<span class="line-number">64</span>
<span class="line-number">65</span>
<span class="line-number">66</span>
<span class="line-number">67</span>
<span class="line-number">68</span>
<span class="line-number">69</span>
<span class="line-number">70</span>
<span class="line-number">71</span>
<span class="line-number">72</span>
<span class="line-number">73</span>
<span class="line-number">74</span>
<span class="line-number">75</span>
<span class="line-number">76</span>
<span class="line-number">77</span>
<span class="line-number">78</span>
<span class="line-number">79</span>
<span class="line-number">80</span>
<span class="line-number">81</span>
<span class="line-number">82</span>
<span class="line-number">83</span>
<span class="line-number">84</span>
<span class="line-number">85</span>
<span class="line-number">86</span>
<span class="line-number">87</span>
<span class="line-number">88</span>
<span class="line-number">89</span>
<span class="line-number">90</span>
<span class="line-number">91</span>
<span class="line-number">92</span>
<span class="line-number">93</span>
<span class="line-number">94</span>
<span class="line-number">95</span>
<span class="line-number">96</span>
<span class="line-number">97</span>
<span class="line-number">98</span>
<span class="line-number">99</span>
<span class="line-number">100</span>
<span class="line-number">101</span>
<span class="line-number">102</span>
<span class="line-number">103</span>
<span class="line-number">104</span>
<span class="line-number">105</span>
<span class="line-number">106</span>
<span class="line-number">107</span>
<span class="line-number">108</span>
<span class="line-number">109</span>
<span class="line-number">110</span>
<span class="line-number">111</span>
<span class="line-number">112</span>
<span class="line-number">113</span>
<span class="line-number">114</span>
<span class="line-number">115</span>
<span class="line-number">116</span>
<span class="line-number">117</span>
<span class="line-number">118</span>
<span class="line-number">119</span>
<span class="line-number">120</span>
<span class="line-number">121</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">vocab_dict</span><span class="p">,</span> <span class="n">vocab_freq_dict</span><span class="p">,</span> <span class="n">table</span><span class="p">):</span>
</span><span class="line">		
</span><span class="line">    <span class="n">total_words</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">vocab_freq_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
</span><span class="line">    <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab_dict</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 參數設定</span>
</span><span class="line">    <span class="n">layer1_size</span> <span class="o">=</span> <span class="mi">30</span> <span class="c"># hidden layer 的大小，即向量大小</span>
</span><span class="line">    <span class="n">window</span> <span class="o">=</span> <span class="mi">2</span> <span class="c"># 上下文寬度的上限</span>
</span><span class="line">    <span class="n">alpha_init</span> <span class="o">=</span> <span class="mf">0.025</span> <span class="c"># learning rate</span>
</span><span class="line">    <span class="n">sample</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="c"># 用來隨機丟棄高頻字用</span>
</span><span class="line">    <span class="n">negative</span> <span class="o">=</span> <span class="mi">10</span> <span class="c"># negative sampling 的數量</span>
</span><span class="line">    <span class="n">ite</span> <span class="o">=</span> <span class="mi">2</span> <span class="c"># iteration 次數</span>
</span><span class="line">
</span><span class="line">    <span class="c"># Weights 初始化</span>
</span><span class="line">    <span class="c"># syn0 : input layer 到 hidden layer 之間的 weights ，用隨機值初始化</span>
</span><span class="line">    <span class="c"># syn1 : hidden layer 到 output layer 之間的 weights ，用0初始化</span>
</span><span class="line">    <span class="n">syn0</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">layer1_size</span><span class="p">))</span> <span class="o">/</span> <span class="n">layer1_size</span>
</span><span class="line">    <span class="n">syn1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layer1_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 印出進度用</span>
</span><span class="line">    <span class="n">train_words</span> <span class="o">=</span> <span class="mi">0</span> <span class="c"># 總共訓練了幾個字</span>
</span><span class="line">    <span class="n">p_count</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">    <span class="n">avg_err</span> <span class="o">=</span> <span class="mf">0.</span>
</span><span class="line">    <span class="n">err_count</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">
</span><span class="line">    <span class="k">for</span> <span class="n">local_iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ite</span><span class="p">):</span>
</span><span class="line">        <span class="k">print</span> <span class="s">&quot;local_iter&quot;</span><span class="p">,</span> <span class="n">local_iter</span>
</span><span class="line">        <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">&quot;poem.txt&quot;</span><span class="p">)</span>
</span><span class="line">        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">():</span>
</span><span class="line">
</span><span class="line">            <span class="c">#用來暫存要訓練的字，一次訓練一個句子</span>
</span><span class="line">            <span class="n">sen</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">
</span><span class="line">            <span class="c"># 取出要被訓練的字</span>
</span><span class="line">            <span class="k">for</span> <span class="n">word_raw</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s">&quot;utf-8&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
</span><span class="line">                <span class="n">last_word</span> <span class="o">=</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word_raw</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">                <span class="c"># 丟棄字典中沒有的字（頻率太低）</span>
</span><span class="line">                <span class="k">if</span> <span class="n">last_word</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
</span><span class="line">                    <span class="k">continue</span>
</span><span class="line">                <span class="n">cn</span> <span class="o">=</span> <span class="n">vocab_freq_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word_raw</span><span class="p">)</span>
</span><span class="line">                <span class="n">ran</span> <span class="o">=</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cn</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">sample</span> <span class="o">*</span> <span class="n">total_words</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">*</span> <span class="p">(</span><span class="n">sample</span> <span class="o">*</span> <span class="n">total_words</span><span class="p">)</span> <span class="o">/</span> <span class="n">cn</span>
</span><span class="line">
</span><span class="line">                <span class="c"># 根據字的頻率，隨機丟棄，頻率越高的字，越有機會被丟棄</span>
</span><span class="line">                <span class="k">if</span> <span class="n">ran</span> <span class="o">&lt;</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">():</span>
</span><span class="line">                    <span class="k">continue</span>
</span><span class="line">                <span class="n">train_words</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">
</span><span class="line">                <span class="c"># 將要被訓練的字加到 sen</span>
</span><span class="line">                <span class="n">sen</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">last_word</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">            <span class="c"># 根據訓練過的字數，調整 learning rate</span>
</span><span class="line">            <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha_init</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">train_words</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">ite</span> <span class="o">*</span> <span class="n">total_words</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
</span><span class="line">            <span class="k">if</span> <span class="n">alpha</span> <span class="o">&lt;</span> <span class="n">alpha_init</span> <span class="o">*</span> <span class="mf">0.0001</span><span class="p">:</span>
</span><span class="line">                <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha_init</span> <span class="o">*</span> <span class="mf">0.0001</span>
</span><span class="line">
</span><span class="line">            <span class="c"># 逐一訓練 sen 中的字</span>
</span><span class="line">            <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sen</span><span class="p">):</span>
</span><span class="line">
</span><span class="line">            		<span class="c"># 隨機調整 window 大小</span>
</span><span class="line">                <span class="n">b</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">window</span><span class="p">)</span>
</span><span class="line">                <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span class="line">
</span><span class="line">                    <span class="c"># input 為 window 範圍中，上下文的某一字</span>
</span><span class="line">                    <span class="k">if</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">c</span> <span class="o">==</span> <span class="n">a</span> <span class="ow">or</span> <span class="n">c</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sen</span><span class="p">):</span>
</span><span class="line">                        <span class="k">continue</span>
</span><span class="line">                    <span class="n">last_word</span> <span class="o">=</span> <span class="n">sen</span><span class="p">[</span><span class="n">c</span><span class="p">]</span>
</span><span class="line">										
</span><span class="line">                    <span class="c"># h_err 暫存 hidden layer 的 error 用</span>
</span><span class="line">                    <span class="n">h_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layer1_size</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">                    <span class="c"># 進行 negative sampling</span>
</span><span class="line">                    <span class="k">for</span> <span class="n">negcount</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">negative</span><span class="p">):</span>
</span><span class="line">
</span><span class="line">                    		<span class="c"># positive example，從 sen 中取得，模型要輸出 1</span>
</span><span class="line">                        <span class="k">if</span> <span class="n">negcount</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">                            <span class="n">target_word</span> <span class="o">=</span> <span class="n">word</span>
</span><span class="line">                            <span class="n">label</span> <span class="o">=</span> <span class="mi">1</span>
</span><span class="line">
</span><span class="line">                        <span class="c"># negative example，從 table 中抽樣，模型要輸出 0 </span>
</span><span class="line">                        <span class="k">else</span><span class="p">:</span>
</span><span class="line">                            <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
</span><span class="line">                                <span class="n">target_word</span> <span class="o">=</span> <span class="n">table</span><span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">table</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
</span><span class="line">                                <span class="k">if</span> <span class="n">target_word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sen</span><span class="p">:</span>
</span><span class="line">                                    <span class="k">break</span>
</span><span class="line">                            <span class="n">label</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">
</span><span class="line">                        <span class="c"># 模型預測結果</span>
</span><span class="line">                        <span class="n">o_pred</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">syn0</span><span class="p">[</span><span class="n">last_word</span><span class="p">,</span> <span class="p">:],</span> <span class="n">syn1</span><span class="p">[:,</span> <span class="n">target_word</span><span class="p">])))</span>
</span><span class="line">
</span><span class="line">                        <span class="c"># 預測結果和標準答案的差距</span>
</span><span class="line">                        <span class="n">o_err</span> <span class="o">=</span> <span class="n">o_pred</span> <span class="o">-</span> <span class="n">label</span>
</span><span class="line">
</span><span class="line">                        <span class="c"># backward propagation</span>
</span><span class="line">                        <span class="c"># 此部分請參照 word2vec part2 的公式推導結果</span>
</span><span class="line">
</span><span class="line">                        <span class="c"># 1.將 error 傳遞到 hidden layer                        </span>
</span><span class="line">                        <span class="n">h_err</span> <span class="o">+=</span> <span class="n">o_err</span> <span class="o">*</span> <span class="n">syn1</span><span class="p">[:,</span> <span class="n">target_word</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">                        <span class="c"># 2.更新 syn1</span>
</span><span class="line">                        <span class="n">syn1</span><span class="p">[:,</span> <span class="n">target_word</span><span class="p">]</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">o_err</span> <span class="o">*</span> <span class="n">syn0</span><span class="p">[</span><span class="n">last_word</span><span class="p">]</span>
</span><span class="line">                        <span class="n">avg_err</span> <span class="o">+=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">o_err</span><span class="p">)</span>
</span><span class="line">                        <span class="n">err_count</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">
</span><span class="line">                    <span class="c"># 3.更新 syn0</span>
</span><span class="line">                    <span class="n">syn0</span><span class="p">[</span><span class="n">last_word</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">h_err</span>
</span><span class="line">
</span><span class="line">                    <span class="c"># 印出目前結果</span>
</span><span class="line">                    <span class="n">p_count</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">                    <span class="k">if</span> <span class="n">p_count</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">                        <span class="k">print</span> <span class="s">&quot;Iter: </span><span class="si">%s</span><span class="s">, Alpha </span><span class="si">%s</span><span class="s">, Train Words </span><span class="si">%s</span><span class="s">, Average Error: </span><span class="si">%s</span><span class="s">&quot;</span> \
</span><span class="line">                              <span class="o">%</span> <span class="p">(</span><span class="n">local_iter</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">train_words</span><span class="p">,</span> <span class="n">avg_err</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">err_count</span><span class="p">))</span>
</span><span class="line">                        <span class="n">avg_err</span> <span class="o">=</span> <span class="mf">0.</span>
</span><span class="line">                        <span class="n">err_count</span> <span class="o">==</span> <span class="mf">0.</span>
</span><span class="line">
</span><span class="line">        <span class="c"># 每一個 iteration 儲存一次訓練完的模型</span>
</span><span class="line">        <span class="n">model_name</span> <span class="o">=</span> <span class="s">&quot;w2v_model_blog_</span><span class="si">%s</span><span class="s">.json&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">local_iter</span><span class="p">)</span>
</span><span class="line">        <span class="k">print</span> <span class="s">&quot;save model: </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</span><span class="line">        <span class="n">fm</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="s">&quot;w&quot;</span><span class="p">)</span>
</span><span class="line">        <span class="n">fm</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">syn0</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>
</span><span class="line">        <span class="n">fm</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>開始訓練：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">train</span><span class="p">(</span><span class="n">vocab_dict</span><span class="p">,</span> <span class="n">vocab_freq_dict</span><span class="p">,</span> <span class="n">table</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>輸出結果如下，可以看到，當訓練過的字數增加時， Error 也跟著降低</p>

<p>大概要花幾十分鐘左右訓練完</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">Iter: 0, Alpha 0.0249923666923, Train Words 475200, Average Error: 0.499999254842
</span><span class="line">Iter: 0, Alpha 0.0249846739501, Train Words 954100, Average Error: 0.249998343836
</span><span class="line">Iter: 0, Alpha 0.0249771900316, Train Words 1420000, Average Error: 0.166660116256
</span><span class="line">Iter: 0, Alpha 0.0249693430813, Train Words 1908500, Average Error: 0.124949913475
</span><span class="line">Iter: 0, Alpha 0.024961329072, Train Words 2407400, Average Error: 0.0993522008349
</span><span class="line">Iter: 0, Alpha 0.0249531817368, Train Words 2914600, Average Error: 0.0787704454331
</span><span class="line">Iter: 0, Alpha 0.0249453540624, Train Words 3401900, Average Error: 0.06351951221
</span><span class="line">Iter: 0, Alpha 0.0249377801891, Train Words 3873400, Average Error: 0.0495117808015
</span><span class="line">..........
</span></code></pre></td></tr></table></div></figure>

<h2 id="show-result">Show Result</h2>

<p>檢視 word2vec 訓練結果的方法，即是看使用 <em>cosine similarity</em> 計算，是否能得出與某字語意相近的字。</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="c"># 讀取訓練好的模型</span>
</span><span class="line"><span class="n">f2</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">&quot;w2v_model_1.json&quot;</span><span class="p">,</span> <span class="s">&quot;r&quot;</span><span class="p">)</span>
</span><span class="line"><span class="n">w2v_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="s">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">f2</span><span class="o">.</span><span class="n">readlines</span><span class="p">())))</span>
</span><span class="line"><span class="n">f2</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span><span class="line">
</span><span class="line"><span class="n">vocab_dict_reversed</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">([(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
</span><span class="line">
</span><span class="line"><span class="c"># 計算 cosine similarity 最高的前五字</span>
</span><span class="line"><span class="k">def</span> <span class="nf">get_top</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
</span><span class="line">    <span class="n">wid</span> <span class="o">=</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 將某字與模型中所有的字向量做內積</span>
</span><span class="line">    <span class="n">dot_result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2v_model</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">w2v_model</span><span class="p">[</span><span class="n">wid</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</span><span class="line">    <span class="n">norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 計算 cosine similarity</span>
</span><span class="line">    <span class="n">cosine_result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">dot_result</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">norm</span><span class="o">*</span><span class="n">norm</span><span class="p">[</span><span class="n">wid</span><span class="p">])</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 根據 cosine similarity 的值排序</span>
</span><span class="line">    <span class="n">final_result</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">wid</span><span class="p">,</span>
</span><span class="line">                          <span class="p">[(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cosine_result</span><span class="p">)]),</span>
</span><span class="line">                          <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class="line">    <span class="k">print</span> <span class="n">word</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 印出語意最接近的前五字，以及其 cosine similarity</span>
</span><span class="line">    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">final_result</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
</span><span class="line">        <span class="k">print</span> <span class="n">vocab_dict_reversed</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>分別計算「山、峰、河、日」這四字語意最相近的字</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">get_top</span><span class="p">(</span><span class="s">u&quot;山&quot;</span><span class="p">)</span>
</span><span class="line"><span class="n">get_top</span><span class="p">(</span><span class="s">u&quot;峰&quot;</span><span class="p">)</span>
</span><span class="line"><span class="n">get_top</span><span class="p">(</span><span class="s">u&quot;河&quot;</span><span class="p">)</span>
</span><span class="line"><span class="n">get_top</span><span class="p">(</span><span class="s">u&quot;日&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下，可看出，計算所得出語意最相近的字，實際上，語意也相近，例如，山和峰、嶺的語意都很接近。</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">山
</span><span class="line">嶺 0.854901128361
</span><span class="line">嵩 0.846620438864
</span><span class="line">峰 0.842831270385
</span><span class="line">岡 0.838129842909
</span><span class="line">嶂 0.834701215189
</span><span class="line">峰
</span><span class="line">山 0.842831270385
</span><span class="line">嶽 0.83917452917
</span><span class="line">嶺 0.8219837161
</span><span class="line">頂 0.821088331571
</span><span class="line">嶂 0.809565794884
</span><span class="line">河
</span><span class="line">湟 0.787726187693
</span><span class="line">涇 0.770652269018
</span><span class="line">淮 0.751135710239
</span><span class="line">川 0.742243126005
</span><span class="line">汾 0.740643816278
</span><span class="line">日
</span><span class="line">旦 0.869047480855
</span><span class="line">又 0.842383624714
</span><span class="line">曛 0.830549707539
</span><span class="line">夕 0.826327222048
</span><span class="line">暉 0.82616774597
</span></code></pre></td></tr></table></div></figure>

<p>向量加減運算後的 <em>cosine similarity</em> ，例如： 女 + 父 - 男 = 母</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">get_calculated_top</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">w3</span><span class="p">):</span>
</span><span class="line">    <span class="n">wid1</span><span class="p">,</span> <span class="n">wid2</span><span class="p">,</span> <span class="n">wid3</span> <span class="o">=</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">w1</span><span class="p">),</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">w2</span><span class="p">),</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">w3</span><span class="p">)</span>
</span><span class="line">    <span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">,</span> <span class="n">v3</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="p">[</span><span class="n">wid1</span><span class="p">],</span> <span class="n">w2v_model</span><span class="p">[</span><span class="n">wid2</span><span class="p">],</span> <span class="n">w2v_model</span><span class="p">[</span><span class="n">wid3</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 得出加減運算後的向量</span>
</span><span class="line">    <span class="n">combined_vec</span> <span class="o">=</span> <span class="n">v1</span> <span class="o">+</span> <span class="p">(</span><span class="n">v2</span> <span class="o">-</span> <span class="n">v3</span><span class="p">)</span>
</span><span class="line">    <span class="n">dot_result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2v_model</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">combined_vec</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</span><span class="line">    <span class="n">norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</span><span class="line">    <span class="n">cvec_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">combined_vec</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
</span><span class="line">    <span class="n">cosine_result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">dot_result</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">norm</span> <span class="o">*</span> <span class="n">cvec_norm</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="n">final_result</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">wid1</span><span class="p">,</span> <span class="n">wid2</span><span class="p">,</span> <span class="n">wid3</span><span class="p">],</span>
</span><span class="line">                                 <span class="p">[(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cosine_result</span><span class="p">)]),</span>
</span><span class="line">                          <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class="line">    <span class="k">print</span> <span class="s">&quot;</span><span class="si">%s</span><span class="s"> + </span><span class="si">%s</span><span class="s"> - </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">w3</span><span class="p">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">final_result</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
</span><span class="line">        <span class="k">print</span> <span class="n">vocab_dict_reversed</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">get_calculated_top</span><span class="p">(</span><span class="s">u&quot;女&quot;</span><span class="p">,</span> <span class="s">u&quot;父&quot;</span><span class="p">,</span> <span class="s">u&quot;男&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下，如預期，運算結果的語意接近「母」：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">女 + 父 - 男
</span><span class="line">母 0.731002049447
</span><span class="line">娥 0.707469857054
</span><span class="line">客 0.69027387716
</span><span class="line">娃 0.687831493041
</span><span class="line">侶 0.681667240226
</span></code></pre></td></tr></table></div></figure>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Word2vec (Part 2 : Backward Propagation)]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation/"/>
    <updated>2016-07-12T09:21:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>本文接續 <a href="http://ckmarkoh.github.io/blog/2016/07/12/neural-network-word2vec-part-1-overview">word2vec (part1)</a> ，介紹 <em>word2vec</em> 訓練過程的 <em>backward propagation</em> 公式推導。</p>

<p><em>word2vec</em> 的訓練過程中，輸出的結果，跟上下文有關的字，在 <em>output layer</em> 輸出為 1 ，跟上下文無關的字，在 <em>output layer</em> 輸出為 0。 在此，把跟上下文有關的，稱為 <em>positive example</em> ，而跟上下文無關的，稱為 <em>negative example</em> 。</p>

<p>根據 <a href="http://ckmarkoh.github.io/blog/2016/07/12/neural-network-word2vec-part-1-overview">word2vec (part1)</a> 中提到的例子， <em>cat</em> 的向量為 <script type="math/tex">\textbf{v}_2</script> ， <em>run</em> 的向量為 <script type="math/tex">\textbf{w}_3</script> ， <em>fly</em> 的向量為 <script type="math/tex">\textbf{w}_4</script> ，由於 <em>cat</em> 的上下文有 <em>run</em> ，所以 <em>run</em> 為 <em>positive example</em> ，而 <em>cat</em> 的上下文沒有 <em>fly</em> ，所以 <em>fly</em> 為 <em>negative example</em> ，如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00187.png" alt="" /></p>

<!--more-->

<h2 id="objective-function">Objective Function</h2>

<p>訓練類神經網路需要有個目標函數，如果希望 <em>positive example</em> 輸出為 1 ， <em>negative example</em> 輸出為 0，則可以將以下函數 <script type="math/tex">J</script> 做最小化。</p>

<script type="math/tex; mode=display">

J = - \text{log}(\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{pos} }}) - \sum_{neg} \text{log} ( 1- \frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{neg} }} )

</script>

<p>其中 <script type="math/tex">\textbf{v}_I</script> 為輸入端的字向量，而 <script type="math/tex">\textbf{w}_{pos}</script> 和 <script type="math/tex">\textbf{w}_{neg}</script> 為輸出端的字向量。 <script type="math/tex">\textbf{w}_{pos}</script> 為 <em>positive example</em> ，而  <script type="math/tex">\textbf{w}_{neg}</script> 為 <em>negative example</em> 。通常，對於每筆 <script type="math/tex">\textbf{v}_I</script> 而言，會找一個 <em>positive example</em> 和多個 <em>negative example</em> ，因此用 <script type="math/tex">\sum</script> 將這些 <em>negative example</em> 算出的結果給加起來。</p>

<p>先看這公式前半部的部分：</p>

<script type="math/tex; mode=display">

-\text{log}(\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{pos} }})

</script>

<p>從以上公式得知，當 <script type="math/tex">\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{pos} }} = 0</script> 時，  <script type="math/tex">J</script> 會趨近無限大，而當 <script type="math/tex">\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{pos} }} = 1</script> 時 ，  <script type="math/tex">J</script> 會趨近 0 ，所以，降低 <script type="math/tex">J</script> 的值，會迫使  <script type="math/tex">\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{pos} }}</script> 接近 1 。</p>

<p>再來看另一部分：</p>

<script type="math/tex; mode=display">

-\text{log}(1 - \frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{neg} }})

</script>

<p>當 <script type="math/tex">\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{neg} }} = 1</script> 時 <script type="math/tex">J</script> 會趨近無限大，反之亦然，同理，降低 <script type="math/tex">J</script> 的值，會迫使  <script type="math/tex">1- \frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{neg} }}</script> 接近 0 。</p>

<h2 id="backward-propagation">Backward Propagation</h2>

<p>至於要怎麼調整 <script type="math/tex">\textbf{v}</script> 和  <script type="math/tex">\textbf{w}</script> 的值，才能讓 <script type="math/tex">J</script> 變小？ 就是要用到 <em>backward propagation</em> 。</p>

<h3 id="positive-example">Positive Example</h3>

<p>這邊先看 <em>positive example</em> 的部分， 輸入端為 <em>dog</em> ，輸出端為 <em>run</em> ，在進行偏微分公式推導之前，先定義一些符號，以便之後推導：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x_{13} = \textbf{v}_1 \cdot \textbf{w}_3 \\

& y_{13} = \dfrac{1}{1+e^{-x_{13}}}  \\

& J = - \text{log} (y_{13} )

\end{align}

 %]]&gt;</script>

<p>這些符號，如下圖所示， <script type="math/tex">x_{13}</script> 是 <em>output layer</em> 在 <em>sigmoid</em> 之前的值， <script type="math/tex">y_{13} </script> 是通過 <em>sigmoid</em> 後的值， <script type="math/tex">J</script> 是針對這筆 <em>positive example</em> 所算出來的 <em>cost function</em> 。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00188.png" alt="" /></p>

<p>如果要更新 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_3</script> 的值，讓 <script type="math/tex">J</script> 變小，就要用 <em>gradient descent</em> 的方式，過程如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{v}_1 \leftarrow \textbf{v}_1 - \eta \nabla_{\textbf{v}_1}  J  \\

& \textbf{w}_3 \leftarrow \textbf{w}_3 - \eta \nabla_{\textbf{w}_1}  J   

\end{align}

 %]]&gt;</script>

<p>想瞭解更多關於 <em>gradient descent</em> ，請參考：<a href="http://ckmarkoh.github.io/blog/2015/12/23/optimization-method-adagrad">Gradient Descent &amp; AdaGrad </a></p>

<p>其中， <script type="math/tex">\eta</script> 為 <em>learning rate</em> ，為一常數，就是決定每一步要走多大，至於 <script type="math/tex">\nabla_{\textbf{v}_1} J </script> 這項要怎麼算？</p>

<p>先看看它每個維度上的值：</p>

<script type="math/tex; mode=display">

\nabla_{\textbf{v}_1} J = 

\begin{bmatrix}

\frac{\partial J}{\partial v_{11}} \\

\frac{\partial J}{\partial v_{12}} \\

\frac{\partial J}{\partial v_{13}} \\

\end{bmatrix}

</script>

<p>先看 <script type="math/tex">\frac{\partial J}{\partial v_{11}}</script> 這項，可以用 <em>chain rule</em> 把它拆開：</p>

<script type="math/tex; mode=display">

\frac{\partial J}{\partial v_{11} } = \frac{\partial J}{\partial y_{13}} \times \frac{\partial y_{13}}{\partial x_{13}}  \times \frac{\partial x_{13}}{\partial v_{11}}

</script>

<p>將 <script type="math/tex">\frac{\partial J}{\partial v_{11} }</script> 拆成 <script type="math/tex">\frac{\partial J}{\partial y_{13}}</script> 、 <script type="math/tex">\frac{\partial y_{13}}{\partial x_{13}}</script> 和 <script type="math/tex">\frac{\partial x_{13}}{\partial v_{11}}</script> 這三項。而這三項的值可分別求出來：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \frac{\partial J}{\partial y_{13}} = \frac{\partial   - \text{log} (y_{13} )}{\partial y_{13}} = \frac{-1}{y_{13}} \\ 

& \frac{\partial y_{13}}{\partial x_{13}} = \frac{\partial (\frac{1}{1+e^{-x_{13}}})}{\partial x_{13}} = \frac{1}{1+e^{-x_{13}}}( 1- \frac{1}{1+e^{-x_{13}}}) = y_{13} ( 1- y_{13}) \\

& \frac{\partial x_{13}}{\partial v_{11}} = \frac{\partial \textbf{v}_{1} \cdot \textbf{w}_3} {\partial v_{11}} = w_{31}


\end{align}

 %]]&gt;</script>

<p>代回這三項的結果到 <script type="math/tex">\frac{\partial J}{\partial v_{11} }</script> ，得出：</p>

<script type="math/tex; mode=display">

\frac{\partial J}{\partial v_{11} } = \frac{-1}{y_{13}} \times y_{13} ( 1- y_{13}) \times  w_{31} = ( y_{13} - 1)  \times w_{31}

</script>

<p>而 <script type="math/tex">\frac{\partial J}{\partial v_{12}}</script> 和 <script type="math/tex">\frac{\partial J}{\partial v_{13}}</script> 也可用同樣方式得出其值， 如下：</p>

<script type="math/tex; mode=display">

\nabla_{\textbf{v}_1} J = 

\begin{bmatrix}

\frac{\partial J}{\partial v_{11}} \\

\frac{\partial J}{\partial v_{12}} \\

\frac{\partial J}{\partial v_{13}} \\

\end{bmatrix}

=

\begin{bmatrix}

 ( y_{13} - 1)  \times w_{31} \\

 ( y_{13} - 1)  \times w_{32} \\

 ( y_{13} - 1)  \times w_{33} \\

 \end{bmatrix}

 = 

  ( y_{13} - 1)  \textbf{w}_3

</script>

<p>因此，可得出 <script type="math/tex">\textbf{v}_1</script> 要調整的量：</p>

<script type="math/tex; mode=display">

\textbf{v}_1 \leftarrow \textbf{v}_1 - \eta  ( y_{13} - 1)  \textbf{w}_3

</script>

<p>同理， <script type="math/tex">\textbf{w}_3</script> 要調整的量：</p>

<script type="math/tex; mode=display">

\textbf{w}_3 \leftarrow \textbf{w}_3 - \eta  ( y_{13} - 1)   \textbf{v}_1

</script>

<p>其中，可以把 <script type="math/tex">( y_{13} - 1)</script> 看成是，模型預測出來的，和我們所想要的值，差距多少。因為在 <em>positive example</em> 的情況下，我們希望模型輸出結果 <script type="math/tex">y_{13}</script> 為 1 。如果  <script type="math/tex">y_{13} = 1</script> 表示模型預測對了，則不需要修正 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_3</script> ，如果  <script type="math/tex">y_{13} \neq 1</script> 時，才要修正 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_3</script> 。</p>

<p>還有，之所以把這過程，稱為 <em>backward propagation</em> ，是因為可以把 <em>chain rule</em> 拆解 <script type="math/tex">\nabla_{\textbf{v}_1} J </script> 的過程，看成是將 <script type="math/tex">\frac{\partial J}{\partial y_{13} }</script> 的值， 由 <em>output layer</em> 往前傳遞，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00189.png" alt="" /></p>

<p>想瞭解更多關於 <em>backward propagation</em> 的推導，請參考： <a href="http://ckmarkoh.github.io/blog/2015/05/28/neural-network-backward-propagation">Backward Propagation 詳細推導過程 </a></p>

<h3 id="negative-example">Negative Example</h3>

<p>再來看看 <em>negative example</em> 的部分， 輸入端為 <em>dog</em> ，輸出端為 <em>fly</em> ，在進行偏微分公式推導之前，先定義一些符號，以便之後推導：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x_{14} = \textbf{v}_1 \cdot \textbf{w}_4 \\

& y_{14} = \dfrac{1}{1+e^{-x_{14}}}  \\

& J = - \text{log} (1 - y_{14} )

\end{align}

 %]]&gt;</script>

<p>這些符號，如下圖所示， <script type="math/tex">x_{14}</script> 是 <em>output layer</em> 在 <em>sigmoid</em> 之前的值， <script type="math/tex">y_{14} </script> 是通過 <em>sigmoid</em> 後的值， <script type="math/tex">J</script> 是針對這筆 <em>positive example</em> 所算出來的 <em>cost function</em> 。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00190.png" alt="" /></p>

<p>同之前 <em>positive example</em> ，如果要更新 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_4</script> 的值，讓 <script type="math/tex">J</script> 變小，就要用 <em>gradient descent</em> 的方式：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{v}_1 \leftarrow \textbf{v}_1 - \eta \nabla_{\textbf{v}_1}  J  \\

& \textbf{w}_4 \leftarrow \textbf{w}_4 - \eta \nabla_{\textbf{w}_1}  J   

\end{align}

 %]]&gt;</script>

<p>剩下的推導和 <em>positive example</em> 時，幾乎一樣，只有 <script type="math/tex">J</script> 不一樣。此處只需推導 <script type="math/tex">\frac{\partial J}{\partial y_{14}}</script> 的結果。</p>

<script type="math/tex; mode=display">

\frac{\partial J}{\partial y_{14}} = \frac{\partial   - \text{log} (1 - y_{14} )}{\partial y_{14}} = \frac{1}{1 - y_{14}} 

</script>

<p>代回此結果到 <script type="math/tex">\frac{\partial J}{\partial v_{11} }</script> ，得出：</p>

<script type="math/tex; mode=display">

\frac{\partial J}{\partial v_{11} } = \frac{1}{ 1- y_{14}} \times y_{14} ( 1- y_{14}) \times  w_{41} = ( y_{14} - 0)  \times w_{41}

</script>

<p>於是可以得出要修正的量：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{v}_1 \leftarrow \textbf{v}_1 - \eta  ( y_{14} - 0)  \textbf{w}_4 \\ 

& \textbf{w}_4 \leftarrow \textbf{w}_4 - \eta  ( y_{14} - 0)   \textbf{v}_1

\end{align}

 %]]&gt;</script>

<p>其中，可以把 <script type="math/tex">( y_{14} - 0)</script> 看成是，模型預測出來的，和我們所想要的值，差距多少。因為在 <em>negative example</em> 的情況下，我們希望模型輸出結果 <script type="math/tex">y_{14}</script> 為 0 。如果  <script type="math/tex">y_{13} = 0</script> 表示模型預測對了，則不需要修正 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_4</script> ，如果  <script type="math/tex">y_{14} \neq 0</script> 時，才要修正 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_4</script> 。</p>

<h2 id="further-reading">Further Reading</h2>

<p>關於如何從無到有，不使用任何自動微分的套件，實作一個 <em>word2vec</em> ，請看：<a href="http://ckmarkoh.github.io/blog/2016/08/29/neural-network-word2vec-part-3-implementation">word2vec (part3)</a></p>

<p>註： 實際上， <em>word2vec</em> 的 <em>input layer</em> 和 <em>output layer</em> 各有兩種架構。<em>input layer</em> 有 <em>cbow</em> 和 <em>skip-gram</em> 兩種，<em>output layer</em> 有 <em>hierarchical sofrmax</em> 和 <em>negative sampling</em> 兩種，本文所寫的為 <em>skip-gram</em> 搭配 <em>negative sampling</em> 的架構。關於 <em>hierarchical softmax</em> 請參考<a href="http://ckmarkoh.github.io/blog/2015/05/23/-hierarchical-probabilistic-neural-networks-neural-network-language-model">hierarchical sofrmax</a> 。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Word2vec (Part 1 : Overview)]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/07/12/neural-network-word2vec-part-1-overview/"/>
    <updated>2016-07-12T09:19:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/07/12/neural-network-word2vec-part-1-overview</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>文字的語意可以用向量來表示，在上一篇 <a href="http://ckmarkoh.github.io/blog/2016/07/10/nlp-vector-space-semantics"> Vector Space of Semantics </a> 中提到，如果把每種字當成一個維度，假設總共有一萬總字，那向量就會有一萬個維度。有兩種方法可降低維度，分別是 <em>singular value decomposition</em> 和 <em>word2vec</em> 。</p>

<p>本文講解 <em>word2vec</em> 的原理。 <em>word2vec</em> 流程，總結如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00191.png" alt="" /></p>

<p>首先，將文字做 <em>one-hot encoding</em> ，然後再用 <em>word2vec</em> 類神經網路計算，求出壓縮後（維度降低後）的語意向量。</p>

<!--more-->

<h2 id="one-hot-encoding">One-Hot Encoding</h2>

<p>一開始，不知道哪個字和哪個字語意相近，所以就假設每個字的語意是不相干的。也就是說，每個字的向量都是互相垂直。</p>

<p>這邊舉個比較簡單的例子，假設總字彙量只有 4 個， 分別為 <em>dog, cat, run, fly</em> ，那麼，經過 <em>one-hot encoding</em> 的結果如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00192.png" alt="" /></p>

<p>如上圖， <em>dog</em> 的向量為 (1,0,0,0) ，只有在第一個維度是 1 ，其他維度是 0 ，而 <em>cat</em> 的向量為 (0,1,0,0) 只有在第一個維度是 1 ，其他維度是 0 。</p>

<p>也就是說，每個字都有一個代表它的維度，而它 <em>one hot encoding</em> 的結果，只有在那個維度上是 1 ，其他維度都是 0 。這樣的話，任意兩個 <em>one hot encoding</em> 的向量內積結果，都會是 0 ，內積結果為 0 ，表示兩向量是垂直的。</p>

<p>註：實際應用中，字彙量即是語料庫中的單字種類，通常會有幾千個甚至一萬個以上。</p>

<h2 id="word2vec">word2vec</h2>

<p><em>word2vec</em> 的神經網路架構如下，總共有三層， <em>input layer</em> 和 <em>output layer</em> 一樣大，中間的 <em>hidden layer</em> 比較小。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00193.png" alt="" /></p>

<p>如上圖，總字彙量有 4 個，那麼 <em>input layer</em> 和 <em>output layer</em> 的維度為 4， 每個維度分別代表一個字。 如果想要把向量維度降至三維， <em>hidden layer</em> 的維度為 3。</p>

<p>另外要注意的是， <em>hidden layer</em> 沒有非線性的 <em>activation funciton</em> ，而 <em>output layer</em> 的 <em>activation function</em> 是 <em>sigmoid</em> ，這兩點會有什麼影響，之後會提到。</p>

<p>其中，在 <em>input layer</em> 和 <em>hidden layer</em> 之間， 有 <script type="math/tex">4 \times 3</script> 個 <em>weight</em> ，在此命名為 <script type="math/tex">\textbf{V}</script> ，而在 <em>hidden layer</em> 到 <em>output layer</em> 之間， 有 <script type="math/tex">3 \times 4</script> 個 <em>weight</em> ，在此命名為 <script type="math/tex">\textbf{W}</script> 。</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\textbf{V}=

\begin{bmatrix}

    v_{11} & v_{12} & v_{13}  \\

    v_{21} & v_{22} & v_{23}  \\

    v_{31} & v_{32} & v_{33}  \\

    v_{41} & v_{42} & v_{43}  \\

\end{bmatrix}

\mspace{30mu}

\textbf{W}^T=

\begin{bmatrix}

    w_{11} & w_{12} & w_{13}  \\

    w_{21} & w_{22} & w_{23}  \\

    w_{31} & w_{32} & w_{33}  \\

    w_{41} & w_{42} & w_{43}  \\

\end{bmatrix}

 %]]&gt;</script>

<p>由於 <em>input</em> 是 <em>one hot encoding</em> 的向量，又因為 <em>hidden layer</em> 沒有 <em>sigmoid</em> 之類的非線性 <em>activation function</em>。 輸入到類神經網路後，在 <em>hidden layer</em> 所取得的值，即是 <script type="math/tex">\textbf{V}</script> 中某個橫排的值，如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00194.png" alt="" /></p>

<p>例如，輸入的是 <em>dog</em> 的 <em>one hot encoding</em> ，只有在第 1 個維度是 1 ，與 <script type="math/tex">\textbf{V}</script> 作矩陣相乘後，在 <em>hidden layer</em> 取得的值是 <script type="math/tex">\textbf{V}</script> 中的第一個橫排： <script type="math/tex">(v_{11}, v_{12}, v_{13})</script> ，這個向量就是 <em>dog</em> 壓縮後的語意向量。運算過程如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{bmatrix}

1 & 0 & 0 & 0 

\end{bmatrix}

\begin{bmatrix}

    v_{11} & v_{12} & v_{13}  \\

    v_{21} & v_{22} & v_{23}  \\

    v_{31} & v_{32} & v_{33}  \\

    v_{41} & v_{42} & v_{43}  \\

\end{bmatrix}

= 

\begin{bmatrix}

v_{11} & v_{12} & v_{13}

\end{bmatrix}

 %]]&gt;</script>

<p>因此， <script type="math/tex">\textbf{V}</script> 中的某個橫排，就是某個字的語意向量。從反方向來看，由於 <em>output layer</em> 也是對應到字彙的 <em>one hot encoding</em> 因此， <script type="math/tex">\textbf{W}^T</script> 中的某個橫排，就是某個字的語意向量。</p>

<p>所以，一個字分別在 <script type="math/tex">\textbf{V}</script> 和 <script type="math/tex">\textbf{W}</script> 中各有一個語意向量。但通常會選擇 <script type="math/tex">\textbf{V}</script> 中的語意向量，作為 <em>word2vec</em> 的輸出結果。</p>

<h2 id="initializing-word2vec">Initializing word2vec</h2>

<p>至於如何訓練這個類神經網路？ 訓練一個類神經網路的過程，第一步就是要先將 <em>weight</em> 作初始化。初始化即是隨機給每個 <em>weight</em> 不同的數值，這些數值介於 <script type="math/tex"> -N \sim N</script> 之間。</p>

<p>因此，在還沒開始訓練之前，這些向量的方向都是隨機的，跟語意無關。</p>

<p>舉 <em>dog</em> 和 <em>cat</em> 在 <script type="math/tex">\textbf{V}</script> 中的向量，為 <script type="math/tex">\textbf{V}_1,\textbf{V}_2</script> ，以及 <em>run</em> 和 <em>fly</em> 在 <script type="math/tex">\textbf{W}</script> 中的向量 為 <script type="math/tex">\textbf{W}_3,\textbf{W}_4</script> ，為例：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00195.png" alt="" /></p>

<p>由於 <script type="math/tex">\textbf{V}</script> 和 <script type="math/tex">\textbf{W}</script> 都是隨機初始化的，因此 <script type="math/tex">\textbf{V}_1, \textbf{V}_2, \textbf{W}_3, \textbf{W}_4 </script> 這些向量的方向都是隨機的，跟語意無關，如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00196.png" alt="" /></p>

<h2 id="training-word2vec">Training word2vec</h2>

<p>訓練 <em>word2vec</em> 的目的，是希望讓語意向量真的跟語意有關。，在上一篇 <a href="http://ckmarkoh.github.io/blog/2016/07/10/nlp-vector-space-semantics"> Vector Space of Semantics </a> 中提到，某字的語意，可從其上下文有哪些字來判斷。因此，可以用某字上下文的字，來做訓練，讓語意向量能抓到文字的語意。</p>

<p>若 <em>dog</em> 的上下文中有 <em>run</em> ， 令 <em>dog</em> 為 <em>word2vec</em> 的 <em>input</em> ， <em>run</em> 為 <em>output</em> 則輸入類神經網路後，在 <em>run</em> 的位置，在經過 <em>sigmoid</em> 之前，得到的結果是 <script type="math/tex">\textbf{V}_1</script> 和 <script type="math/tex">\textbf{W}_3</script> 的內積。經過了，<em>sigmoid</em> ，得到的值為：</p>

<script type="math/tex; mode=display">

\frac{1}{1+e^{- \textbf{V}_1 \cdot \textbf{W}_3}}

</script>

<p>由於 <em>run</em> 出現在 <em>dog</em> 的上下文中，所以要訓練類神經網路，在 <em>run</em> 位置可以輸出 1，如下：</p>

<script type="math/tex; mode=display">

\frac{1}{1+e^{- \textbf{V}_1 \cdot \textbf{W}_3}} \approx 1

</script>

<p>過程如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00197.png" alt="" /></p>

<p>根據上圖，如果要讓 <em>run</em> 的位置輸出為 1 ，則 <script type="math/tex">\textbf{V}_1</script> 和 <script type="math/tex">\textbf{W}_3</script> 的內積要越大越好。</p>

<p>內積要大，就是向量角度要越小，訓練過程中，會修正這兩個向量的角度，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00198.png" alt="" /></p>

<p>上圖左方為先正之前，各向量的方向，上圖右方為修正之後的方向，其中，深藍色為修正後的，淺藍色為修正前的，畫在一起以便作比較。修正完後， <script type="math/tex">\textbf{V}_1</script> 和 <script type="math/tex">\textbf{W}_3</script> 的角度又更接近了。</p>

<p>同理，若 <em>cat</em> 的上下文中有 <em>run</em> ，則用 <em>word2vec</em> 做同樣訓練，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00199.png" alt="" /></p>

<p>修正向量的角度後，<script type="math/tex">\textbf{V}_2</script> 和 <script type="math/tex">\textbf{W}_3</script> 的角度會更接近，結果如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00200.png" alt="" /></p>

<p>不過，以上訓練方法有個問題，就是訓練完後， <em>所有的向量都會位於同一條直線上，而無法分辨出每個字語意的差異</em> 。如果要讓 <em>word2vec</em> 學會分辨語意的差異，就需要加入反例，也就是 <em>不是出現在上下文的字</em> 。</p>

<p>如果 <em>dog</em> 的上下文中，不會出現 <em>fly</em> ， <em>fly</em> 的向量為 <script type="math/tex">\textbf{W}_4</script> ，將 <em>dog</em> 輸入類神經網路後，在 <em>fly</em> 的位置，訓練其輸出結果為 0 ，如下：</p>

<script type="math/tex; mode=display">

\frac{1}{1+e^{- \textbf{V}_1 \cdot \textbf{W}_3}} \approx 0

</script>

<p>如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00201.png" alt="" /></p>

<p>如果要讓輸出結果接近 0，則 <script type="math/tex">\textbf{V}_1</script> 和 <script type="math/tex">\textbf{W}_4</script> 的內積要越小越好，也就是說，它們之間的角度要越大越好。修正這兩個向量的角度，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00202.png" alt="" /></p>

<p>同理，若 <em>cat</em> 的上下文中沒有 <em>fly</em> ，則訓練其輸出 0 ：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00203.png" alt="" /></p>

<p>修正  <script type="math/tex">\textbf{V}_2</script> 和 <script type="math/tex">\textbf{W}_4</script> 的夾角，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00204.png" alt="" /></p>

<p>訓練後，得出的這些語意向量，語意相近的，夾角越小，語意相差越遠的，夾角越大，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00205.png" alt="" /></p>

<h2 id="further-reading">Further Reading</h2>

<ol>
  <li>
    <p><em>word2vec</em> 的 <em>backward propagation</em> 公式要怎麼推導，請看：<a href="http://ckmarkoh.github.io/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation">word2vec (part2 : Backward Propagation)</a></p>
  </li>
  <li>
    <p>如何從無到有，不使用任何自動微分的套件，實作一個 <em>word2vec</em> ，請看：<a href="http://ckmarkoh.github.io/blog/2016/08/29/neural-network-word2vec-part-3-implementation">word2vec (part3 : Implementation)</a></p>
  </li>
</ol>

<p>註： 實際上， <em>word2vec</em> 的 <em>input layer</em> 和 <em>output layer</em> 各有兩種架構。<em>input layer</em> 有 <em>cbow</em> 和 <em>skip-gram</em> 兩種，<em>output layer</em> 有 <em>hierarchical sofrmax</em> 和 <em>negative sampling</em> 兩種，本文所寫的為 <em>skip-gram</em> 搭配 <em>negative sampling</em> 的架構。關於 <em>hierarchical softmax</em> 請參考<a href="http://ckmarkoh.github.io/blog/2015/05/23/-hierarchical-probabilistic-neural-networks-neural-network-language-model">hierarchical sofrmax</a> 。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vector Space of Semantics]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/07/10/nlp-vector-space-semantics/"/>
    <updated>2016-07-10T14:06:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/07/10/nlp-vector-space-semantics</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>如果要判斷某個字的語意，可以用它鄰近的字來判斷，例如以下句子：</p>

<blockquote>
  <p>The dog run.
A cat run.
A dog sleep.
The cat sleep.
A dog bark.
The cat meows.
The bird fly.
A bird sleep.</p>
</blockquote>

<p>由於 <em>dog</em> 和 <em>cat</em> 這兩個字出現在類似的上下文情境中，因此，可以判斷出 <em>dog</em> 和 <em>cat</em> 語意相近。</p>

<p>如果要能夠用數學運算，來計算語意相近程度，可以把文字的語意用向量表示，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{array}{c|c c c c c c c c c }

     &a &bark &bird &cat &dog &fly &meow & run & sleep & the \\ \hline

 dog &2 &1 &0 &0 &0 &0 &0 &1 &1 &1 \\

 cat &1 &0 &0 &0 &0 &0 &1 &1 &1 &2 \\

 bird &1 &0 &0 &0 &0 &1 &0 &0 &1 &1 

\end{array}

 %]]&gt;</script>

<p>其中， <em>dog</em> 的向量為  ( 2, 1, 0, 0, 0, 0, 0, 1, 1, 1 ) ，第一個維度表示 <em>dog</em> 在 <em>a</em> 旁邊的次數有 2 次，第二個維度表示在 <em>bark</em> 旁邊的次數有 1 次，以此類推。</p>

<!--more-->

<h2 id="vector-space-of-semantics">Vector Space of Semantics</h2>

<p>有了向量就可以用 <em>cosine similarity</em> 來計算語意相近程度。</p>

<p>給定兩向量 <script type="math/tex">A = (a_1,a_2,...,a_n)</script> 和<script type="math/tex">B = (b_1,b_2,...,b_n)</script> ，則這兩向量的 <em>cosine similarity</em> 為：</p>

<script type="math/tex; mode=display">

\frac{A \cdot B}{|A||B|}=  \frac{\sum a_i b_i}{\sqrt{\sum_i a_i^2}\sqrt{\sum_i b_i^2}}

</script>

<p><em>cosine similarity</em> 算出來的值，即是計算 <script type="math/tex">A</script> 和 <script type="math/tex">B</script> 兩向量的夾角的 <em>cosine</em> 值。 <em>cosine</em> 值越接近 1 ，表示兩向量夾角越小，表示兩向量的語意越接近。</p>

<p>根據以上例子， <em>dog</em> ( 2, 1, 0, 0, 0, 0, 0, 1, 1, 1 ) 和 <em>cat</em> ( 1, 0, 0, 0, 0, 0, 1, 1, 1, 2 )  的 <em>cosine similarity</em> 為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \frac{ 2 \times 1 + 1 \times 0 + 0 \times 0 + 0 \times 0 + 0 \times 0 + 0 \times 0 + 0 \times 1 + 1 \times 1 + 1 \times 1 + 1 \times 2 }{ \sqrt{ 2^2 + 1^2 + 0^2 + 0^2 + 0^2 + 0^2 + 0^2 + 1^2 + 1^2 + 1^2 } \sqrt{ 1^2 + 0^2 + 0^2 + 0^2 + 0^2 + 0^2 + 1^2 + 1^2 + 1^2 + 2^2} } \\

& = \frac{ 6 }{ \sqrt{ 8 } \sqrt{ 8} } 

= 0.75 

\end{align}

 %]]&gt;</script>

<p>而 <em>bird</em> 的向量為：( 1, 0, 0, 0, 0, 1, 0, 0, 1, 1 )  ，計算 <em>dog</em>  和  <em>bird</em> 的 <em>cosine similarity</em> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \frac{ 2 \times 1 + 1 \times 0 + 0 \times 0 + 0 \times 0 + 0 \times 0 + 0 \times 1 + 0 \times 0 + 1 \times 0 + 1 \times 1 + 1 \times 1 }{ \sqrt{ 2^2 + 1^2 + 0^2 + 0^2 + 0^2 + 0^2 + 0^2 + 1^2 + 1^2 + 1^2 } \sqrt{ 1^2 + 0^2 + 0^2 + 0^2 + 0^2 + 1^2 + 0^2 + 0^2 + 1^2 + 1^2} }  \\

& = \frac{ 4 }{ \sqrt{ 8 } \sqrt{ 4} } 

\approx 0.707106781187 

\end{align}

 %]]&gt;</script>

<p>由於 0.75 &gt; 0.707 ，因此 <em>dog</em> 和 <em>cat</em> 語意比較接近， <em>dog</em> 和 <em>bird</em> 語意比較不同。</p>

<p>此種語意向量有個缺點，就是向量的維度等於總字彙量。例如英文單字種共有好幾萬種，那麼，向量就有好幾萬個維度，向量維度過大的缺點，會造成資料過度稀疏，以及占記憶體的空間。</p>

<p>降低向量維度的方式，有兩種方法，分別是：</p>

<ol>
  <li>
    <p><a href="https://ccjou.wordpress.com/2009/09/01/%E5%A5%87%E7%95%B0%E5%80%BC%E5%88%86%E8%A7%A3-svd/">singular value decompisition (SVD)</a></p>
  </li>
  <li>
    <p><a href="http://blog.csdn.net/itplus/article/details/37969519">word2vec</a></p>
  </li>
</ol>

<p>本文先不詳細介紹這部分。</p>

<h2 id="implementation">Implementation</h2>

<p>在此實作將文字轉成向量，並用 SVD 降為至二維，作視覺化</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</span><span class="line">
</span><span class="line"><span class="n">text</span> <span class="o">=</span> <span class="p">[</span>
</span><span class="line">    <span class="p">[</span><span class="s">&quot;the&quot;</span><span class="p">,</span> <span class="s">&quot;dog&quot;</span><span class="p">,</span> <span class="s">&quot;run&quot;</span><span class="p">,</span> <span class="p">],</span>
</span><span class="line">    <span class="p">[</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;cat&quot;</span><span class="p">,</span> <span class="s">&quot;run&quot;</span><span class="p">,</span> <span class="p">],</span>
</span><span class="line">    <span class="p">[</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;dog&quot;</span><span class="p">,</span> <span class="s">&quot;sleep&quot;</span><span class="p">,</span> <span class="p">],</span>
</span><span class="line">    <span class="p">[</span><span class="s">&quot;the&quot;</span><span class="p">,</span> <span class="s">&quot;cat&quot;</span><span class="p">,</span> <span class="s">&quot;sleep&quot;</span><span class="p">,</span> <span class="p">],</span>
</span><span class="line">    <span class="p">[</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;dog&quot;</span><span class="p">,</span> <span class="s">&quot;bark&quot;</span><span class="p">,</span> <span class="p">],</span>
</span><span class="line">    <span class="p">[</span><span class="s">&quot;the&quot;</span><span class="p">,</span> <span class="s">&quot;cat&quot;</span><span class="p">,</span> <span class="s">&quot;meows&quot;</span><span class="p">,</span> <span class="p">],</span>
</span><span class="line">    <span class="p">[</span><span class="s">&quot;the&quot;</span><span class="p">,</span> <span class="s">&quot;bird&quot;</span><span class="p">,</span> <span class="s">&quot;fly&quot;</span><span class="p">,</span> <span class="p">],</span>
</span><span class="line">    <span class="p">[</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;bird&quot;</span><span class="p">,</span> <span class="s">&quot;sleep&quot;</span><span class="p">,</span> <span class="p">],</span>
</span><span class="line"><span class="p">]</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">build_word_vector</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
</span><span class="line">    <span class="n">word2id</span> <span class="o">=</span> <span class="p">{</span><span class="n">w</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="nb">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">,</span> <span class="n">text</span><span class="p">)))))}</span>
</span><span class="line">    <span class="n">id2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">word2id</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</span><span class="line">    <span class="n">wvectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">word2id</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2id</span><span class="p">)))</span>
</span><span class="line">    <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
</span><span class="line">        <span class="k">for</span> <span class="n">word1</span><span class="p">,</span> <span class="n">word2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sentence</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">sentence</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
</span><span class="line">            <span class="n">id1</span><span class="p">,</span> <span class="n">id2</span> <span class="o">=</span> <span class="n">word2id</span><span class="p">[</span><span class="n">word1</span><span class="p">],</span> <span class="n">word2id</span><span class="p">[</span><span class="n">word2</span><span class="p">]</span>
</span><span class="line">            <span class="n">wvectors</span><span class="p">[</span><span class="n">id1</span><span class="p">,</span> <span class="n">id2</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">            <span class="n">wvectors</span><span class="p">[</span><span class="n">id2</span><span class="p">,</span> <span class="n">id1</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">    <span class="k">return</span> <span class="n">wvectors</span><span class="p">,</span> <span class="n">word2id</span><span class="p">,</span> <span class="n">id2word</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">cosine_sim</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">):</span>
</span><span class="line">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))))</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">visualize</span><span class="p">(</span><span class="n">wvectors</span><span class="p">,</span> <span class="n">id2word</span><span class="p">):</span>
</span><span class="line">    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</span><span class="line">    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
</span><span class="line">    <span class="n">U</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">Vh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">wvectors</span><span class="p">)</span>
</span><span class="line">    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
</span><span class="line">    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">id2word</span><span class="p">:</span>
</span><span class="line">        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">U</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">U</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">id2word</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>本程式中， <code>text</code> 是輸入的文字， <code>build_word_vector</code> 將文字轉成向量：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; wvectors, word2id, <span class="nv">id2word</span> <span class="o">=</span> build_word_vector<span class="o">(</span>text<span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>其中， <code>wvectors</code> 是根據前後文統計後，得出各文字的向量：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; print wvectors
</span><span class="line"><span class="o">[[</span> 0.  0.  1.  1.  2.  0.  0.  0.  0.  0.<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> 1.  0.  0.  0.  0.  1.  0.  0.  1.  1.<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> 1.  0.  0.  0.  0.  0.  1.  1.  1.  2.<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> 2.  1.  0.  0.  0.  0.  0.  1.  1.  1.<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> 0.  0.  0.  1.  1.  0.  0.  0.  0.  0.<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> 0.  0.  1.  1.  1.  0.  0.  0.  0.  0.<span class="o">]</span>
</span><span class="line"> <span class="o">[</span> 0.  0.  1.  2.  1.  0.  0.  0.  0.  0.<span class="o">]]</span>
</span></code></pre></td></tr></table></div></figure>

<p>每一橫排（或直排）代表著某個字的向量，但從它看不出是哪個字，所以 <code>word2id</code> 則是給定文字，找出是第幾個向量，而 <code>id2word</code> 則是給定第幾個向量，找出所對應的文字。</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; print word2id
</span><span class="line"><span class="o">{</span><span class="s1">&#39;a&#39;</span>: 0, <span class="s1">&#39;fly&#39;</span>: 5, <span class="s1">&#39;run&#39;</span>: 7, <span class="s1">&#39;the&#39;</span>: 9, <span class="s1">&#39;dog&#39;</span>: 4, <span class="s1">&#39;cat&#39;</span>: 3,
</span><span class="line"><span class="s1">&#39;meows&#39;</span>: 6, <span class="s1">&#39;sleep&#39;</span>: 8, <span class="s1">&#39;bark&#39;</span>: 1, <span class="s1">&#39;bird&#39;</span>: 2<span class="o">}</span>
</span><span class="line">
</span><span class="line">&gt;&gt;&gt; print id2word
</span><span class="line"><span class="o">{</span>0: <span class="s1">&#39;a&#39;</span>, 1: <span class="s1">&#39;bark&#39;</span>, 2: <span class="s1">&#39;bird&#39;</span>, 3: <span class="s1">&#39;cat&#39;</span>, 4: <span class="s1">&#39;dog&#39;</span>, 5: <span class="s1">&#39;fly&#39;</span>,
</span><span class="line">6: <span class="s1">&#39;meows&#39;</span>, 7: <span class="s1">&#39;run&#39;</span>, 8: <span class="s1">&#39;sleep&#39;</span>, 9: <span class="s1">&#39;the&#39;</span><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>

<p>例如 <em>dog</em> 是在 <code>wvectors</code> 中， 第 5 排的向量（註：index 從0開始算），用 <code>word2id</code> 可從 <code>wvector</code> 中，取出其對應向量：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; print wvectors<span class="o">[</span>word2id<span class="o">[</span><span class="s2">&quot;dog&quot;</span><span class="o">]]</span>
</span><span class="line"><span class="o">[</span> 2.  1.  0.  0.  0.  0.  0.  1.  1.  1.<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>程式碼中的 <code>cosine_sim</code> ，則可計算兩向量間的 <em>cosine similarity</em> ，例如，</p>

<p>計算 <em>dog</em> 和 <em>cat</em> 與 <em>dog</em> 和 <em>bird</em> 之間的  <em>cosine similarity</em>  ：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; print cosine_sim<span class="o">(</span>wvectors<span class="o">[</span>word2id<span class="o">[</span><span class="s2">&quot;dog&quot;</span><span class="o">]]</span>, wvectors<span class="o">[</span>word2id<span class="o">[</span><span class="s2">&quot;cat&quot;</span><span class="o">]])</span>
</span><span class="line">0.75
</span><span class="line">
</span><span class="line">&gt;&gt;&gt; print cosine_sim<span class="o">(</span>wvectors<span class="o">[</span>word2id<span class="o">[</span><span class="s2">&quot;dog&quot;</span><span class="o">]]</span>, wvectors<span class="o">[</span>word2id<span class="o">[</span><span class="s2">&quot;bird&quot;</span><span class="o">]])</span>
</span><span class="line">0.707106781187
</span></code></pre></td></tr></table></div></figure>

<p>再來是用 <code>visualize</code> 將這些語意向量在平面上作視覺化：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; visualize<span class="o">(</span>wvectors, id2word<span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>在平面上作視覺化的方法，是用 SVD 將語意向量降為至二維，就可以把這些向量所對應的字，畫在平面上，結果如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00186.png" alt="" /></p>

<p>此結果顯示，   <em>dog</em> 、 <em>cat</em> 和 <em>bird</em> 是語意相近的一群， <em>a</em> 和 <em>the</em> 語意相近，以此類推。</p>

<h2 id="further-reading">Further Reading</h2>

<p>Simple Word Vector representations</p>

<p>http://cs224d.stanford.edu/lectures/CS224d-Lecture2.pdf</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Gibbs Sampling]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/07/09/pgm-gibbs-sampling/"/>
    <updated>2016-07-09T08:36:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/07/09/pgm-gibbs-sampling</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p><em>Gibbs Sampling</em> 是一種類似於 <a href="http://ckmarkoh.github.io/blog/2016/07/07/pgm-metropolis-hasting">Metropolis Hasting</a> 的抽樣方式，也是根據機率分佈來建立 <em>Markov Chain</em> ，並在 <em>Markov Chain</em> 上行走，抽樣出機率分佈。</p>

<p>設一 <em>Markov Chain</em> ， 有 <em>a</em> 和 <em>b</em> 兩個 <em>state</em> ，它們的值分別為 <script type="math/tex">p(a)</script> 和 <script type="math/tex">p(b)</script> ，而它們之間的轉移機率，分別為 <script type="math/tex">q_1(a,b)</script> 和 <script type="math/tex">q_2(a,b)</script> ，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00179.png" alt="" /></p>

<p>達平衡時，會滿足以下條件：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&

\begin{bmatrix} 

p(a) \\

p(b)

\end{bmatrix}

=

\begin{bmatrix} 

1-q_1(a,b) & q_2(a,b) \\

q_1(a,b) & 1-q_2(a,b)

\end{bmatrix}

\begin{bmatrix} 

p(a) \\

p(b)

\end{bmatrix} \\

& \Rightarrow

\begin{cases}

p(a)(1-q_1(a,b) +p(b)q_2(a,b) = p(a)  \\

p(a)q_1(a,b) + p(b)(1-q_2(a,b)) = p(b) 

\end{cases}\\

& \Rightarrow p(a)q_1(a,b) = p(b)q_2(a,b)

\end{align}


 %]]&gt;</script>

<p>因此，達到平衡時，得出 <a name="eq1">（公式一）</a> ：</p>

<script type="math/tex; mode=display">

 p(a)q_1(a,b) = p(b)q_2(a,b)

</script>

<p>在  <a href="http://ckmarkoh.github.io/blog/2016/07/07/pgm-metropolis-hasting">Metropolis Hasting</a> 這篇有提到，可以利用 <em>Markov Chain</em> 最終會達到平衡的特性，來為某機率分佈 <script type="math/tex">p(x)</script> 抽樣。</p>

<p>但是 <em>Metropolis Hasting</em> 抽樣時，需要先用 <em>proposed distribution</em> 計算出下一個時間點可能的值，然後 <em>acceptance probability</em> 來拒絕它，因為計算出來的值會被拒絕，所以造成計算上的浪費。</p>

<p>而對於一高維度的機率分佈 <script type="math/tex">p(x_1,x_2,...,x_n)</script> ，可以用另一種方式來建立 <em>Markov Chain</em> ，則不會有這個問題。這種方法為 <em>Gibbs Sampling</em> 。</p>

<!--more-->

<h2 id="gibbs-sampling">Gibbs Sampling</h2>

<p>首先，先考慮二維的空間，設一機率分佈 <script type="math/tex">p(X,Y)</script> ，若此機率分佈的 <em>Random Variable</em> 有四個值，分別為 <script type="math/tex">A(x_1,y_1)</script> ， <script type="math/tex">B(x_2,y_1)</script> ， <script type="math/tex">C(x_1,y_2)</script> 和 <script type="math/tex">D(x_2,y_2)</script> 。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00180.png" alt="" /></p>

<p>首先，看看如何在  <em>A</em> 和 <em>B</em> 建立 <em>Markov Chain</em> 。由於 <em>A</em> 和 <em>B</em> 只有在 <script type="math/tex">x</script> 維度上的值不一樣，則：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& p(x_1,y_1)p(x_2|y_1)=p(y_1)p(x_1|y_1)p(x_2|y_1) \\

& p(x_2,y_1)p(x_1|y_1)=p(y_1)p(x_2|y_1)p(x_1|y_1)

\end{align}

 %]]&gt;</script>

<p>由於兩式的等號右邊一樣，則可得出：</p>

<script type="math/tex; mode=display">

\begin{align}

p(x_1,y_1)p(x_2|y_1)=p(x_2,y_1)p(x_1|y_1)

\end{align}

</script>

<p>此結果符合 <a href="#eq1">（公式一）</a> 的 <em>Markov Chain</em> 平衡狀態。根據此結果，建立以下的 <em>Markov Chain</em> ：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00181.png" alt="" /></p>

<p>其中， <script type="math/tex">p(x_1\mid y_1)+p(x_2\mid y_1) = 1</script> 。</p>

<p>若在此 <em>Markov Chain</em> 上行走，如果走得夠多次的話，走到 <em>A</em> 和走到 <em>B</em> 的次數比，會是 <script type="math/tex">p(x_1,y_1) : p(x_2,y_1)</script> 。因此，用此 <em>Markov Chain</em> 得出的次數比，就相當於從機率分佈 <script type="math/tex">p(X,Y)</script> 抽樣後，  <em>A</em> 和 <em>B</em>  所抽出的次數比。</p>

<p>也可用同樣方法，在 <em>A</em> 和 <em>C</em> 之間建立 <em>Markov Chain</em>：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00182.png" alt="" /></p>

<p>同理，<em>D</em> 和 <em>B</em> 或 <em>C</em> 之間，只有一個維度是不一樣的值，也可建立這樣的 <em>Markov Chain</em> 。</p>

<p>結合以上 <em>Markov Chain</em> ，若要對 <script type="math/tex">p(X,Y)</script> 進行抽樣，則可先固定 <script type="math/tex">y</script> 的值，先在 <script type="math/tex">x</script> 軸上抽樣， 再固定 <script type="math/tex">x</script> 的值，在  <script type="math/tex">y</script> 軸上抽樣，如此交替進行，即為 <em>Gibbs Sampling</em> 的方法。</p>

<p>舉個例子，假設有一聯合分佈 <script type="math/tex">p(X,Y)</script> ，其機率值如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{array}{c | c c}

p(X,Y) & Y=0  & Y=1  \\ \hline

X=0  & 0.5 & 0.2 \\

X=1  & 0.1 & 0.2 \\

\end{array}

 %]]&gt;</script>

<p>這些點在座標上的位置如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00183.png" alt="" /></p>

<p>先把 <script type="math/tex">p(X\mid Y)</script> 和 <script type="math/tex">p(Y\mid X)</script> 計算出來，以便之後使用。</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&

\begin{array}{c | c c}

p(X|Y) & Y=0  & Y=1  \\ \hline

X=0  & \frac{5}{6} & \frac{1}{2} \\

X=1  & \frac{1}{6} & \frac{1}{2} \\

\end{array} 

&

\begin{array}{c | c c}

p(Y|X) & Y=0  & Y=1  \\ \hline

X=0  & \frac{5}{7} & \frac{2}{7} \\

X=1  & \frac{1}{3} & \frac{2}{3} \\

\end{array}

\end{align}

 %]]&gt;</script>

<p>然後，進行 <em>gibbs sampling</em> ，首先，從 (0,0) 開始，固定 y 軸，在 x 軸上抽樣，用 <script type="math/tex">p(X\mid Y=0)</script> 的值，建立從 (0,0) 在 x 軸上轉移的 <em>Markov Chain</em> ，如下： </p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00184.png" alt="" /></p>

<p>根據此 <em>Marokv Chain</em> ， 從 <script type="math/tex">p(X\mid  Y=0)</script> 中，抽出 <script type="math/tex">X</script> ， 假設抽出的 <script type="math/tex">X</script> 值為 1，則抽出來的點為 (1,0) ，下一步則是要固定 x 軸，在 y 軸上抽樣。用 <script type="math/tex">p(Y\mid Y=1)</script> 的值，建立從 (1,0) 在 y 軸上轉移的 <em>Markov Chain</em> ，如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00185.png" alt="" /></p>

<p>根據此 <em>Marokv Chain</em> ， 從 <script type="math/tex">p(Y\mid  X=1)</script> 中，抽出 <script type="math/tex">Y</script>， 假設抽出的 <script type="math/tex">Y</script> 值為 1，則抽出來的點為 (1,1)，到目前為止，抽出來的樣本序列為這樣：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"><span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>1,0<span class="o">)</span> <span class="o">(</span>1,1<span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>然後再固定 y 軸，在 x 軸上抽樣，這樣持續抽樣下去，抽到最後，得出以下序列：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"><span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>1,0<span class="o">)</span> <span class="o">(</span>1,1<span class="o">)</span> <span class="o">(</span>0,1<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>1,0<span class="o">)</span>
</span><span class="line"><span class="o">(</span>1,1<span class="o">)</span> <span class="o">(</span>1,1<span class="o">)</span> <span class="o">(</span>1,1<span class="o">)</span> <span class="o">(</span>1,1<span class="o">)</span> <span class="o">(</span>1,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span>
</span><span class="line"><span class="o">(</span>0,1<span class="o">)</span> <span class="o">(</span>1,1<span class="o">)</span> <span class="o">(</span>1,1<span class="o">)</span> <span class="o">(</span>0,1<span class="o">)</span> <span class="o">(</span>0,1<span class="o">)</span> <span class="o">(</span>0,1<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> <span class="o">(</span>0,0<span class="o">)</span> ...
</span></code></pre></td></tr></table></div></figure>

<p>最後，統計抽樣序列中各個點的出現次數， (0,0) (1,0) (1,1) (0,1) 這四個點的次數比會接近 0.5 : 0.1 : 0.2 : 0.2</p>

<p><em>Gibbs Sampling</em> 也可用在高維度的機率分佈 <script type="math/tex">p(x_1,x_2,...,x_n)</script> 。抽樣時，先從第一個維度上抽樣，固定其他維度，用 <script type="math/tex">p(x_1\mid x_2,...,x_{n})</script> 抽出下一個樣本，然後再從第二個維度抽樣，固定其他維度，用 <script type="math/tex">p(x_2\mid x_1,x_3,...,x_{n})</script> 抽出下一個樣本，如此一直循環下去。</p>

<p>整個抽樣過程的 pseudo code 如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

1 & \text{set }t = 0 \\

2 & \text{generate an initial state } \textbf{x}_0 \in \text{domain of }p(x_1,x_2,...,x_n) \\

3 & \text{repeat until }t = M \\

4 & \mspace{30mu}\text{for each dimension }i = 1...n \\

5 & \mspace{60mu}\text{draw } \textbf{x}_t \text{ from } p(x_i|x_1,x_2,\dots,x_{i-1},x_{i+1},\dots,x_n) \\

6 & \mspace{60mu}\text{set }t = t+1

\end{align}

 %]]&gt;</script>

<p>此處流程大致上和 <em>Metropolis Hasting</em> 類似， 而 <script type="math/tex">p(x_i\mid x_1,x_2,\dots,x_{i-1},x_{i+1},\dots,x_n)</script> 就相當於是  <em>Metropolis Hasting</em> 中的 <em>proposed distribution</em> ，但在 <em>Gibbs Sampling</em> 中 ， <script type="math/tex">p(x_i\mid x_1,x_2,\dots,x_{i-1},x_{i+1},\dots,x_n)</script> 得出的值，就直接是抽樣結果了，不需要再算一個 <em>acceptance probability</em> 來判斷是否要接受或拒絕它。因此，不會造成計算上的浪費。</p>

<h2 id="implementation">Implementation</h2>

<p>此例實作先前提到的  <script type="math/tex">p(X,Y)</script> 抽樣：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{array}{c | c c}

p(X,Y) & Y=0  & Y=1  \\ \hline

X=0  & 0.5 & 0.2 \\

X=1  & 0.1 & 0.2 \\

\end{array}

 %]]&gt;</script>

<p>程式碼如下：</p>

<figure class="code"><figcaption><span>gibbssamp.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">gibbssamp</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span><span class="line">    <span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]])</span>
</span><span class="line">    <span class="n">condi</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span><span class="p">]</span>
</span><span class="line">    <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</span><span class="line">    <span class="n">samples</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span><span class="line">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
</span><span class="line">            <span class="n">one_prob</span> <span class="o">=</span> <span class="n">condi</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">[(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]]</span>
</span><span class="line">            <span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">one_prob</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">            <span class="n">samples</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
</span><span class="line">    <span class="k">return</span> <span class="n">samples</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">count</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
</span><span class="line">    <span class="n">c</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
</span><span class="line">    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">:</span>
</span><span class="line">        <span class="n">c</span><span class="o">.</span><span class="n">update</span><span class="p">([</span><span class="s">&quot;(</span><span class="si">%s</span><span class="s">,</span><span class="si">%s</span><span class="s">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">])])</span>
</span><span class="line">    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">c</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
</span><span class="line">        <span class="k">print</span> <span class="n">k</span><span class="p">,</span> <span class="n">c</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>其中，程式碼中的 <code>P</code> 為機率分佈 <script type="math/tex">p(X,Y)</script> ， <code>condi[0]</code> 為 <script type="math/tex">p(X\mid Y)</script> ， <code>condi[1]</code> 為 <script type="math/tex">p(Y\mid X)</script> 。 <code>gibbssamp</code> 為執行 <em>Gibbs Sampling</em> 的主要函數。</p>

<p>執行 <code>gibbssamp</code> 抽出 1000 * 2 個樣本，如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; <span class="nv">X</span> <span class="o">=</span> gibbssamp<span class="o">(</span>1000<span class="o">)</span>
</span><span class="line">&gt;&gt;&gt; print X
</span><span class="line"><span class="o">[[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>1, 0<span class="o">]</span>, <span class="o">[</span>1, 1<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>,
</span><span class="line"><span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>1, 1<span class="o">]</span>, <span class="o">[</span>1, 1<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>1, 1<span class="o">]</span>, <span class="o">[</span>1, 1<span class="o">]</span>,
</span><span class="line"><span class="o">[</span>1, 1<span class="o">]</span>, <span class="o">[</span>1, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>1, 0<span class="o">]</span>, <span class="o">[</span>1, 1<span class="o">]</span>,
</span><span class="line"><span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>,
</span><span class="line"><span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>1, 0<span class="o">]</span>, <span class="o">[</span>1, 1<span class="o">]</span>, <span class="o">[</span>1, 1<span class="o">]</span>, <span class="o">[</span>1, 0<span class="o">]</span>, <span class="o">[</span>0, 0<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span>, <span class="o">[</span>0, 1<span class="o">]</span> ...
</span></code></pre></td></tr></table></div></figure>

<p>用 <code>count</code> 用來統計抽樣結果：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; count<span class="o">(</span>X<span class="o">)</span>
</span><span class="line"><span class="o">(</span>0,0<span class="o">)</span> 1043
</span><span class="line"><span class="o">(</span>1,0<span class="o">)</span> 172
</span><span class="line"><span class="o">(</span>0,1<span class="o">)</span> 387
</span><span class="line"><span class="o">(</span>1,1<span class="o">)</span> 398
</span></code></pre></td></tr></table></div></figure>

<p>統計結果顯示， (0,0) (1,0) (0,1) (1,1) 的比率會接近 0.5 : 0.1 : 0.2 : 0.2</p>

<h2 id="further-reading">Further Reading</h2>

<p>The Gibbs Sampler</p>

<p>https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/</p>

<p>LDA-math-MCMC 和 Gibbs Sampling(1)</p>

<p>http://www.52nlp.cn/lda-math-mcmc-%e5%92%8c-gibbs-sampling1</p>

<p>LDA-math-MCMC 和 Gibbs Sampling(2)</p>

<p>http://www.52nlp.cn/lda-math-mcmc-%e5%92%8c-gibbs-sampling2</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Metropolis Hasting]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/07/07/pgm-metropolis-hasting/"/>
    <updated>2016-07-07T17:21:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/07/07/pgm-metropolis-hasting</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>對一個 <em>Markov Chain</em> 而言，不論起始狀態為多少，最後會達到一個穩定平衡的狀態。</p>

<p>舉個例子，以下為一 <em>Markov Chain</em></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00172.png" alt="" /></p>

<p>則此 <em>Markov Chain</em> 達平衡狀態時， <script type="math/tex">A,B</script> 的比率為<script type="math/tex"> 5:6 </script>，也就是說：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&

\begin{bmatrix} 

A \\

B

\end{bmatrix}

=

\begin{bmatrix} 

0.4 & 0.5 \\

0.6 & 0.5 

\end{bmatrix}

\begin{bmatrix} 

A \\

B

\end{bmatrix} \\

& \Rightarrow

\begin{cases}

4A+5B = 10A \\

6A+5B = 10B 

\end{cases}\\

& \Rightarrow 6A = 5B

\end{align}

 %]]&gt;</script>

<p>不管此 <em>Markov Chain</em> 的起始狀態如何，最後達平衡狀態時， <script type="math/tex">A,B</script> 的比率一定為<script type="math/tex"> 5:6 </script>。因此，如果在這 <em>Markov Chain</em> 的  <script type="math/tex">A</script> 和 <script type="math/tex">B</script> 任一一個點開始走，假設走的次數夠多的話，走到 <script type="math/tex">A</script> 和走到 <script type="math/tex">B</script> 的比例。會是 <script type="math/tex">5 : 6</script> 。因此，可利用 <em>Markov Chain</em> 最後會收斂到一穩定狀態的特性，來進行抽樣。</p>

<p><em>Metropolis Sampler</em> 即是給定一機率分佈函數，從這機率函數建立 <em>Markov Chain</em> ，然後再用建立出來的 <em>Markov Chain</em> 來進行抽樣。</p>

<!--more-->

<h2 id="metropolis-sampler">Metropolis Sampler</h2>

<p>設某一機率分佈 <script type="math/tex">p(X)</script> ，起始值為 <script type="math/tex">x_{0}</script> 。隨機變數的值為 <script type="math/tex"> X = \{ a_{1}, a_{2}, ..., a_{n}\} </script>。</p>

<p>抽樣過程如下：</p>

<p>設目前時間點 <script type="math/tex">t</script> 抽出的值為 <script type="math/tex">x_{t}=a_{i}</script> 然後，從一機率分佈（稱為 <em>proposal distribution</em>），<script type="math/tex">q(x_{t+1}\mid x_{t})</script> 中，抽出一個值，為 <script type="math/tex">a_{j}</script> ，其中 <script type="math/tex">q(x_{t+1}\mid x_{t})</script> 滿足以下對稱性即可：</p>

<script type="math/tex; mode=display">

q(x_{t+1}|x_{t}) = q(x_{t}|x_{t+1})

</script>

<p><script type="math/tex">q(x_{t+1}\mid x_{t})</script> 可以是比較好計算的函數，用於快速產生下一個可能的樣本 <script type="math/tex">a_{j}</script> ，  <script type="math/tex">a_{j}</script> 為 <em>proposed state</em> ，意思就是說，想從 <script type="math/tex">q(x_{t+1}\mid x_{t})</script> 「提出」一個值，看看這個值能不能成為 <script type="math/tex">x_{t+1}</script> 的值，但實際上，這時還不知道 <script type="math/tex">a_{j}</script> 適不適合為<script type="math/tex">x_{t+1}</script> 的值，所以要做以下運算。</p>

<p>如果 <script type="math/tex">p(a_{j}) \geq p(a_{i})</script> ，則：</p>

<script type="math/tex; mode=display">

x_{t+1} = a_{j}

</script>

<p>如果 <script type="math/tex">% &lt;![CDATA[
p(a_{j}) < p(a_{i}) %]]&gt;</script> 則：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x_{t+1} = \begin{cases}

   a_{j} & \text{with probability of } \alpha  \\

   a_{i} & \text{with probability of } 1- \alpha 

\end{cases}\\

& \text{where }  \alpha = \frac{p(a_j)}{p(a_{i})}

\end{align}


 %]]&gt;</script>

<p>其中， <script type="math/tex">\alpha</script> 稱為 <em>acceptance probability</em> ，意思是，接受 <script type="math/tex">x_{t+1}</script> 的值為 <script type="math/tex">a_{j}</script> 的機率有多少？</p>

<p>此方法可以看成是在 <script type="math/tex">a_{i}</script> 與 <script type="math/tex">a_{j}</script> 之間，建立 <em>Markov Chain</em> ：</p>

<p>如果想從 <script type="math/tex">a_{i}</script> 轉移到 <script type="math/tex">a_{j}</script> ，且， <script type="math/tex">p(a_{j}) \geq p(a_{i})</script> ，則建立出的 <em>Markov Chain</em> ，如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00173.png" alt="" /></p>

<p>由於 <script type="math/tex">p(a_{j}) \geq p(a_{i})</script> ，所以一定會接受 從 <script type="math/tex">a_{i}</script> 到 <script type="math/tex">a_{j}</script> 的轉移。</p>

<p>呈上，如果想從 <script type="math/tex">a_{j}</script> 轉移回 <script type="math/tex">a_{i}</script> ，則建立出的 <em>Markov Chain</em> ，如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00174.png" alt="" /></p>

<p>由於 <script type="math/tex">% &lt;![CDATA[
p(a_{i}) < p(a_{j}) %]]&gt;</script>  ，所以從 <script type="math/tex">a_{j}</script> 轉到 <script type="math/tex">a_{i}</script> 的轉移，不一定會成立，而是由 <em>acceptance probability</em> 來決定。</p>

<p>把以上兩個合起來，得出 <script type="math/tex">a_{i}</script> 與 <script type="math/tex">a_{j}</script> 之間的  <em>Markov Chain</em> ，如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00175.png" alt="" /></p>

<p>假設在這 <em>Markov Chain</em> 上，隨機走動，設走到 <script type="math/tex">a_i</script> 的機率為 <script type="math/tex">A</script> ，走到 <script type="math/tex">a_j</script> 的機率為 <script type="math/tex">B</script> ，則達平衡時：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&

\begin{bmatrix} 

A \\

B

\end{bmatrix}

=

\begin{bmatrix} 

0 & \frac{p(a_i)}{p(a_j)} \\

1 & 1 -  \frac{p(a_i)}{p(a_j)}

\end{bmatrix}

\begin{bmatrix} 

A \\

B

\end{bmatrix} \\

& \Rightarrow

\begin{cases}

\frac{p(a_i)}{p(a_j)} B = A \\

A + (1 -  \frac{p(a_i)}{p(a_j)})B = B 

\end{cases}\\

& \Rightarrow A : B = p(a_i) : p(a_j)

\end{align}

 %]]&gt;</script>

<p>因此 <em>Markov Chain</em> 達到平衡狀態時， 走到 <script type="math/tex">a_i</script> 和走到 <script type="math/tex">a_j</script> 的次數比，會是 <script type="math/tex">p(a_i):p(a_j)</script> 。</p>

<p>舉個簡單的例子，假設 <script type="math/tex">p(x)</script> 如下， <script type="math/tex">X</script> 有0.8的機率為 0 ，有0.2的機率為 1 ，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


p(x) = \begin{cases}

0.2 & X=0 \\

0.8 & X=1

\end{cases}\\

 %]]&gt;</script>

<p>則建立出來的 <em>Markov Chain</em> 如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00176.png" alt="" /></p>

<p>從這 <em>Markov Chain</em> 上，從 0 開始走，會先轉移到 1 ，走過的序列如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">01
</span></code></pre></td></tr></table></div></figure>

<p>到 1 以後，有 0.75的機率會走回 1，有0.25會再走到 0 ，走到 1 和走到 0 的比率是 3:1 ， 假設走回 1 三次以後才走回0 ，則走過的序列如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">011110
</span></code></pre></td></tr></table></div></figure>

<p>這樣一直走下去，則</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">01111011110111101111...
</span></code></pre></td></tr></table></div></figure>

<p>序列中任取一個數，則有0.8的機率為 0 ，有0.2的機率為 1 ，和 <script type="math/tex">p(x)</script> 的機率分佈一樣。</p>

<p>因此，建立了  <script type="math/tex">a_{i}</script> 與 <script type="math/tex">a_{j}</script> 之間的 <em>Markov Chain</em> 之後，就可以在這兩個點上面來回行走，抽出適當比例的樣本。</p>

<p>可以把 <script type="math/tex">X</script> 只有兩個值的機率分佈，推廣到多個值，在 <script type="math/tex"> X = \{ a_{1}, a_{2}, ..., a_{n}\} </script> 中的任兩個值都可以建立這樣的 <em>Markov Chain</em> ，在這些 <em>Markov Chain</em> 中，來回行走，最後達平衡時，抽出來的序列就等同於從 <script type="math/tex">p(X)</script> 的機率分佈中抽樣。</p>

<p>整個抽樣過程的 <em>pseudo code</em> 如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

1 & \text{set }t = 0 \\

2 & \text{generate an initial state } x_0 \in \{ a_{1}, a_{2}, ..., a_{n}\} \\

3 & \text{repeat until }t = M \\

4 & \mspace{30mu} \text{generate a proposal state } a_{j} \text{ from } q(x_{t+1} | x_t ) \\

5 & \mspace{30mu} \text{calculate the acceptance probability }\alpha = \text{min} \left(1,\frac{p(a_{j})}{p(x_t)} \right) \\

6 & \mspace{30mu} \text{draw a random number } u \text{ from } \text{Unif}(0,1) \\

7 & \mspace{30mu} \text{if }u \leq \alpha \\

8 & \mspace{60mu} \text{ accept the proposal state } a_{j} \text{ and set } x_{t+1}= a_{j} \\

9 & \mspace{30mu} \text{else set } \\

10 & \mspace{60mu} x_{t+1} = x_{t}

\end{align}

 %]]&gt;</script>

<p>其中， <script type="math/tex">M</script> 為抽出的樣本數，而從 <em>Unif(0,1)</em> 抽出的 <script type="math/tex">u</script> ，目的是要讓 <script type="math/tex">x_{t+1}= a_{j}</script> 成立的機率為 <script type="math/tex">\alpha</script> 。</p>

<h2 id="metropolis-hasting">Metropolis Hasting</h2>

<p>由於 <em>Metropolis Sampler</em> 的 <em>proposed distribution</em> 一定要滿足對稱性，如果 <script type="math/tex">p(x)</script> 為非對稱性的機率分佈，例如 <em>Gamma distribution</em> ，就不太適合用對稱性的 <em>proposed distribution</em> 來進行抽樣。</p>

<p><em>Metropolis Hasting</em> 是一種更 <em>General</em> 的抽樣方式，它可用於以下情形：</p>

<script type="math/tex; mode=display">

q(x_{t+1}|x_{t}) \neq q(x_{t}|x_{t+1})

</script>

<p>如果 <script type="math/tex">q(x_{t+1}\mid x_{t})</script> 不對稱，例如 <script type="math/tex">q(x_{t+1}\mid x_{t}) > q(x_{t}\mid x_{t+1}) </script> 這種情況，表示從 <script type="math/tex">x_{t}</script> 的位置「提出」走到 <script type="math/tex">x_{t+1}</script> 的位置，機率是比從 <script type="math/tex">x_{t+1}</script> 「提出」走回 <script type="math/tex">x_{t}</script> 還高。所以也要把這個機率的差異考慮到 <em>Markov Chain</em> 裡面，因此要加入 <em>correction factor</em> <script type="math/tex">c</script> ：</p>

<script type="math/tex; mode=display">

c = \frac{q(x_{t}|x_{t+1})}{q(x_{t+1}|x_{t})}

</script>

<p>將 <script type="math/tex">c</script> 乘上 <em>proposed probability</em>  ，來修改 <em>acceptance probability</em> ：</p>

<script type="math/tex; mode=display">

\alpha = \frac{p(a_j)}{p(a_{i})} \times c = \frac{p(a_j)}{p(a_{i})} \times \frac{q(a_i|a_j)}{q(a_j|a_i)}

</script>

<p>如果要在 <script type="math/tex">a_i</script> 與 <script type="math/tex">a_j</script> 之間，建立 <em>Markov Chain</em> ，且 <script type="math/tex">p(a_j)q(a_i\mid a_j) > p(a_{i})q(a_j\mid a_i)</script>，則建立出的 Markov Chain ，如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00177.png" alt="" /></p>

<p>除了 <em>acceptance probability</em> 要加上 <em>correction factor</em> 之外， <em>Metropolis Hasting</em> 的抽樣過程，幾乎和 <em>Metropolis Sampler</em> 一樣。整個抽樣過程的 <em>pseudo code</em> 如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

1 & \text{set }t = 0 \\

2 & \text{generate an initial state } x_0 \in \{ a_{1}, a_{2}, ..., a_{n}\} \\

3 & \text{repeat until }t = M \\

4 & \mspace{30mu} \text{generate a proposal state } a_{j} \text{ from } q(x_{t+1} | x_t ) \\

5 & \mspace{30mu} \text{calculate the proposal correction factor }c = \frac{q(x_{t-1} | a_j) }{q(a_j|x_{t-1})} \\

6 & \mspace{30mu} \text{calculate the acceptance probability }\alpha = \text{min} \left(1,\frac{p(a_{j})}{p(x_t)} \times c \right) \\

7 & \mspace{30mu} \text{draw a random number } u \text{ from } \text{Unif}(0,1) \\

8 & \mspace{30mu} \text{if }u \leq \alpha \\

9 & \mspace{60mu} \text{ accept the proposal state } a_{j} \text{ and set } x_{t+1}= a_{j} \\

10 & \mspace{30mu} \text{else set } \\

11 & \mspace{60mu} x_{t+1} = x_{t}

\end{align}

 %]]&gt;</script>

<h2 id="implementation">Implementation</h2>

<p>再來，進入實作的部分</p>

<h4 id="metropolis-sampler-1">Metropolis Sampler</h4>

<p>首先，從 <em>Metropolis Sampler</em> 開始，用 <em>Metropolis Sampler</em> 對以下 <script type="math/tex">p(x)</script> 進行抽樣：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


p(x) = \begin{cases}

0.2 & X=0 \\

0.8 & X=1

\end{cases}\\

 %]]&gt;</script>

<p>程式碼如下：</p>

<figure class="code"><figcaption><span>matrosamp.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">random</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">collections</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">metrosamp</span><span class="p">(</span><span class="n">ITER</span><span class="p">):</span>
</span><span class="line">    <span class="n">P</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">}</span>
</span><span class="line">    <span class="n">Q</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
</span><span class="line">    <span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">    <span class="n">xt</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ITER</span><span class="p">):</span>
</span><span class="line">        <span class="n">xtp1</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="n">xt</span><span class="p">]</span>
</span><span class="line">        <span class="n">alpha</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">P</span><span class="p">[</span><span class="n">xtp1</span><span class="p">]</span> <span class="o">/</span> <span class="n">P</span><span class="p">[</span><span class="n">xt</span><span class="p">])</span>
</span><span class="line">        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">alpha</span><span class="p">:</span>
</span><span class="line">            <span class="n">xt</span> <span class="o">=</span> <span class="n">xtp1</span>
</span><span class="line">        <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">X</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">count</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
</span><span class="line">    <span class="n">counter</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">counter</span><span class="p">:</span>
</span><span class="line">        <span class="k">print</span> <span class="n">key</span><span class="p">,</span> <span class="n">counter</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>程式碼中的 <code>P</code> 即 <script type="math/tex">p(x)</script> ， 而 <code>Q</code> 則是 <em>proposed distribution</em> ， <script type="math/tex">q(x_{t}\mid x_{t-1})</script> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& q(x_{t} = 0 |x_{t-1}=1) = 1 \\

& q(x_{t} = 1|x_{t-1}=0) = 1 \\

\end{align}

 %]]&gt;</script>

<p>也就是說，位於 0 時， <em>proposed distribution</em> 會「提出」走到 1 ，位於 1 時會提出走到 0 。</p>

<p>而進行 <em>Metropolis Sampler</em> 抽樣的演算法為程式碼中的 <code>metrosamp</code> ，抽樣結果儲存於 <code>X</code> ：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; <span class="nv">X</span> <span class="o">=</span> metrosamp<span class="o">(</span>10000<span class="o">)</span>
</span><span class="line">&gt;&gt;&gt; print X
</span><span class="line"><span class="o">[</span>1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,....<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>用count` 則是將結果做統計：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; count<span class="o">(</span>X<span class="o">)</span>
</span><span class="line"><span class="m">0</span> 2027
</span><span class="line"><span class="m">1</span> 7973
</span></code></pre></td></tr></table></div></figure>

<p>0 和 1 的比例，接近 0.2 和 0.8 。</p>

<h4 id="metropolis-hasting-1">Metropolis Hasting</h4>

<p>再來舉個 <em>Metropolis Hasting</em> 的例子。</p>

<p><em>Metropolis Hasting</em> 也可以用在連續的機率分佈，此例用 <em>Metropolis Hasting</em> 來對 <em>gamma distribution</em> 進行抽樣，程式碼如下：</p>

<figure class="code"><figcaption><span>metrohast.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">random</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">collections</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">math</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">scipy.stats</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">p_func_raw</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
</span><span class="line">    <span class="n">S1</span> <span class="o">=</span> <span class="p">((</span><span class="n">b</span> <span class="o">**</span> <span class="n">a</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
</span><span class="line">    <span class="n">S2</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span><span class="line">    <span class="n">S3</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">b</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">S1</span> <span class="o">*</span> <span class="n">S2</span> <span class="o">*</span> <span class="n">S3</span>  <span class="c"># * S4</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">p_func</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
</span><span class="line">    <span class="k">return</span> <span class="n">p_func_raw</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">q_func</span><span class="p">(</span><span class="n">beta</span><span class="p">):</span>
</span><span class="line">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">q_func_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
</span><span class="line">    <span class="k">return</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">metrohast</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
</span><span class="line">    <span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">    <span class="n">beta</span> <span class="o">=</span> <span class="mf">5.</span>
</span><span class="line">    <span class="n">xt</span> <span class="o">=</span> <span class="n">beta</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
</span><span class="line">        <span class="n">aj</span> <span class="o">=</span> <span class="n">q_func</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
</span><span class="line">        <span class="n">c</span> <span class="o">=</span> <span class="n">q_func_pdf</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">/</span> <span class="n">q_func_pdf</span><span class="p">(</span><span class="n">aj</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
</span><span class="line">        <span class="n">alpha</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="p">(</span><span class="n">p_func</span><span class="p">(</span><span class="n">aj</span><span class="p">)</span> <span class="o">/</span> <span class="n">p_func</span><span class="p">(</span><span class="n">xt</span><span class="p">))</span> <span class="o">*</span> <span class="n">c</span><span class="p">)</span>
</span><span class="line">        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">alpha</span><span class="p">:</span>
</span><span class="line">            <span class="n">xt</span> <span class="o">=</span> <span class="n">aj</span>
</span><span class="line">        <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">X</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">draw</span><span class="p">(</span><span class="n">S</span><span class="p">):</span>
</span><span class="line">    <span class="n">n</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">patches</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">normed</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s">&#39;b&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bins</span><span class="p">,</span> <span class="p">[</span><span class="n">p_func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">bins</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>本例實作一個 <em>gamma distribution</em> 的抽樣：</p>

<script type="math/tex; mode=display">

Gamma(x,a,b) = \frac{ b^a }{\Gamma(a)}x^{a-1}e^{-bx}

</script>

<p>在本例中，令 a=2, b=1</p>

<script type="math/tex; mode=display">

p(x) = Gamma(x,2,1)

</script>

<p>程式碼中， <code>p_func</code> 為 <script type="math/tex">p(x)</script></p>

<p>而抽樣所用的 <em>proposed distribution</em> 為 <em>exponantial distribution</em> ：</p>

<script type="math/tex; mode=display">

exp(x,\beta) =  \frac{1}{\beta} e^{-\frac{x}{\beta}}

</script>

<p>在本例中，令 <script type="math/tex">\beta=5</script> </p>

<script type="math/tex; mode=display">

q(x) = exp(x,5) 

</script>

<p>此例的 <script type="math/tex">q(x)</script> 跟上一時間點的 <script type="math/tex">x</script> 值無關，即： <script type="math/tex">q(x_{t+1}) = q(x_{t+1}\mid x_{t})</script> 。</p>

<p>程式碼中， <code>q_func</code> 和 <code>q_func_pdf</code> 為 <script type="math/tex">q(x)</script> 。其中， <code>q_func</code> 用於產生 <script type="math/tex">a_j</script> ，而 <code>q_func_pdf</code> 用於計算 <em>correction factor</em> 。</p>

<p>而進行 <em>Metropolis Hasting</em> 抽樣的演算法為程式碼中的 <code>metrohast</code> ，抽樣結果儲存於 <code>X</code> 。<code>draw</code> 則是將結果畫成 <em>Histogram</em> 。</p>

<p>用 <code>metrohast</code> 執行抽樣，並用 <code>draw</code> 畫出結果：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; <span class="nv">X</span> <span class="o">=</span> metrohast<span class="o">(</span>10000<span class="o">)</span>
</span><span class="line">&gt;&gt;&gt; draw<span class="o">(</span>X<span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00178.png" alt="" /></p>

<p>其中，紅色的線為 <em>Gamma Distribution</em> 實際上的值，藍色的線為 <em>Metropolis Hasting</em> 的抽樣結果。</p>

<h2 id="further-reading">Further Reading</h2>

<p>Metropolis Sampler</p>

<p>https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/</p>

<p>Metropolis Hasting</p>

<p>https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/</p>

<p>LDA-math-MCMC 和 Gibbs Sampling(1)</p>

<p>http://www.52nlp.cn/lda-math-mcmc-%e5%92%8c-gibbs-sampling1</p>

<p>LDA-math-MCMC 和 Gibbs Sampling(2)</p>

<p>http://www.52nlp.cn/lda-math-mcmc-%e5%92%8c-gibbs-sampling2</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Variational Inference]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/06/20/pgm-variational-inference/"/>
    <updated>2016-06-20T02:42:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/06/20/pgm-variational-inference</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>若某個有 <script type="math/tex">m</script> 個維度的 data <script type="math/tex">\textbf{x} = \{x_1,x_2,...,x_m\}</script> 的產生，是跟某個有 <script type="math/tex">n</script>個維度的 hidden variable <script type="math/tex">\textbf{z} = \{z_1,z_2,...,z_n\} </script> 有關，在機率圖模型，表示成：  </p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00171.png" alt="" /></p>

<p>從這機率圖形，可得出 hidden variable <script type="math/tex">\textbf{z}</script> 和 <script type="math/tex">\textbf{x}</script> 的聯合分佈機率：</p>

<script type="math/tex; mode=display">

p(\textbf{x},\textbf{z})  = p(\textbf{z})p(\textbf{x}|\textbf{z})

</script>

<p>若是給定 hidden variable <script type="math/tex">\textbf{z}</script> 的值，則可產生 data <script type="math/tex">\textbf{x}</script> ，如下：</p>

<script type="math/tex; mode=display">

p(\textbf{x}|\textbf{z}) 

</script>

<p>但如果給定 data <script type="math/tex">\textbf{x}</script> 的值，這組 data 所對應到的 hidden variable 的值，如下：</p>

<script type="math/tex; mode=display">

p(\textbf{z}|\textbf{x}) = \frac{p(\textbf{x} , \textbf{z})}{p(\textbf{x})}  =\frac{p(\textbf{x} | \textbf{z}) p(\textbf{z}) }{\int p(\textbf{x} | \textbf{z})p(\textbf{z}) \text{d}\textbf{z}} 


</script>

<p>其中，分母 <script type="math/tex">\int p(\textbf{x} \mid  \textbf{z})p(\textbf{z}) \text{d}\textbf{z}</script> 積分如果無法算出來的時候，就無法直接算出 <script type="math/tex">p(\textbf{z}\mid \textbf{x})</script> 的值，則要用估計的方法來計算 <script type="math/tex">p(\textbf{z}\mid \textbf{x})</script> 的值。</p>

<p>Variational Inference 用來估計 <script type="math/tex">p(\textbf{z}\mid \textbf{x})</script> 的值 。</p>

<!--more-->

<p><em>Variational Inference</em> 的做法是，不直接把 <script type="math/tex">p(\textbf{z}\mid \textbf{x})</script> 求出，而是用一個較好算的 <script type="math/tex">q(\textbf{z}\mid \mathbf{\theta})</script> 來求出近似解。其中， <script type="math/tex">\mathbf{\theta}</script> 為參數，調整此參數可以讓 <script type="math/tex">q(\textbf{z}\mid \mathbf{\theta})</script>。較接近 <script type="math/tex">p(\textbf{z}\mid \textbf{x})</script> 。</p>

<p>求近似解的方法，是讓 <script type="math/tex">q(\textbf{z}\mid \mathbf{\theta})</script> 和 <script type="math/tex">p(\textbf{z}\mid \textbf{x})</script> 這兩個機率分佈的 <em>KL Divergence</em> 越小越好：<a name="eq1">（公式一）</a></p>

<script type="math/tex; mode=display">

\min_{\theta} D_{KL}[  q(\textbf{z}| \mathbf{\theta} )  ||   p(\textbf{z}|\textbf{x})   ]

</script>

<h2 id="evidence-lower-bound-elob">Evidence Lower Bound (ELOB)</h2>

<p>由於<a href="#eq1">（公式一）</a>中的 <script type="math/tex">p(\textbf{z}\mid \textbf{x})</script> 無法直接計算出，因此這個</p>

<p><em>KL Divergence</em> 的值也無法算出來。但可以把它稍微整理一下，如下<a name="eq2">（公式二）</a>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}


&

D_{KL}[q(\textbf{z})||p(\textbf{z}|\textbf{x})] = \int

q(\textbf{z}) \text{log}  \frac{q(\textbf{z})}{p(\textbf{z}|\textbf{x})} \text{d}\textbf{z}

\\&


=  \int

q(\textbf{z}) \text{log} \frac{  q(\textbf{z}) p(\textbf{x}) }{ p(\textbf{x},\textbf{z})  } \text{d}\textbf{z}

\\

&


=  \int

q(\textbf{z}) \text{log} \frac{  q(\textbf{z})  }{ p(\textbf{x},\textbf{z})  } \text{d}\textbf{z}

+ \int q(\textbf{z})\text{log} {p(\textbf{x})} \text{d}\textbf{z}

\\


&

=  \int 

q(\textbf{z})

\bigg(\text{log}   q(\textbf{z})

-\text{log} p(\textbf{z},\textbf{x} )\bigg)  \text{d} \textbf{z}

+ \text{log} p(\textbf{x})

\\

&

= -\bigg( E_{q(\textbf{z}) }[\text{log}p(\textbf{x},\textbf{z})] - E_{ q(\textbf{z} ) }[\text{log}q(\textbf{z})]\bigg) + \text{log}p(\textbf{x})


\end{align}

 %]]&gt;</script>

<p>註：以上省略 <script type="math/tex">\theta</script></p>

<p>定義 Evidence Lower Bound (ELOB) <script type="math/tex">L[q(\textbf{z})]</script> 為：</p>

<script type="math/tex; mode=display">

L[q(\textbf{z})]

   =  E_{q(\textbf{z}) }[\text{log}p(\textbf{x},\textbf{z})] - E_{ q(\textbf{z} ) }[\text{log}q(\textbf{z})]


</script>

<p>將 <script type="math/tex">L[q(\textbf{z})]</script> 代入<a href="#eq2">（公式二）</a>的推導結果，得出：</p>

<script type="math/tex; mode=display">

D_{KL}[q(\textbf{z})||p(\textbf{z}|\textbf{x})]  = -L[q(\textbf{z})]

 + \text{log}p(\textbf{x})

</script>

<p>因此：</p>

<script type="math/tex; mode=display">

\text{log}p(\textbf{x})  = D_{KL}[q(\textbf{z})||p(\textbf{z}|\textbf{x})] +  L[q(\textbf{z})]

</script>

<p>由於 <script type="math/tex">p(\textbf{x})</script> 為一個固定的機率分佈，因此 <script type="math/tex">\text{log}p(\textbf{x})</script> 為常數。</p>

<p>因此，只要將 <script type="math/tex">L[q(\textbf{z})]</script> 的最大化，即可將 <script type="math/tex">D_{KL}[q(\textbf{z})\mid \mid p(\textbf{z}\mid \textbf{x})] </script> 最小化。</p>

<p>而 <script type="math/tex">L[q(\textbf{z})]</script> 中的 <script type="math/tex">p(\textbf{x}, \textbf{z})</script> 是可以算得出來的，因此目標函數為：</p>

<script type="math/tex; mode=display">

\max_{\theta} L[q(\textbf{z} | \theta )]


</script>

<h2 id="mean-field-variational-inference">Mean-Field Variational Inference</h2>

<p><em>Variational Inference</em> 有很多種作法，其中一種常見的作法為 <em>Mean-Field Variational Inference</em> ，這種方法是，假設 <script type="math/tex">q(\textbf{z}\mid  \mathbf{\theta} )</script> 中的每個維度都是獨立的。這樣會比較容易求出它的值，即：<a name="eq3">（公式三）</a></p>

<script type="math/tex; mode=display">

q(\textbf{z}|\mathbf{\theta}) = \prod_{i} q_{i}(z_{i}| \theta_{i} ) 

</script>

<p>每個獨立的 <script type="math/tex">q(z_{i}\mid \theta)</script> 都是機率分佈，即積分結果為1：</p>

<script type="math/tex; mode=display">

\forall i  ,  \int q_{i}(z_{i}| \theta_{i} )  \text{d} z_{i} = 1

</script>

<h2 id="maximizing-elob">Maximizing ELOB</h2>

<p>再來，用 <em>Mean-Field Variational Inference</em> 的方法，把  <script type="math/tex">L[q(\textbf{z})]</script>  整理一下。</p>

<p><script type="math/tex">L[q(\textbf{z})]</script> 可分成兩部分：<script type="math/tex"> E_{q(\textbf{z}) }[\text{log}p(\textbf{x},\textbf{z})]</script> 和 <script type="math/tex">  E_{ q(\textbf{z} ) }[\text{log}q(\textbf{z})] </script></p>

<p>現在，要分別把<a href="#eq3">（公式三）</a>代入這兩項，並稍做整理，讓它們的值可以被求出來。</p>

<p>首先，來看 <script type="math/tex"> E_{q(\textbf{z}) }[\text{log}p(\textbf{x},\textbf{z})]</script> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& E_{q(\textbf{z}) }[\text{log}p(\textbf{x},\textbf{z})] = \int \prod_{i} q_{i}(z_{i}) \text{log} p (\textbf{z},\textbf{x}) \text{d}\textbf{z}

\\

&

= \int_{z_{1}}\int_{z_{2}} ... \int_{z_{n}} \int \prod_{i} q_{i}(z_{i}) \text{log} p (\textbf{z},\textbf{x}) \text{d}z_{1} \text{d}z_{2}...\text{d}z_{n}

\end{align}

 %]]&gt;</script>

<p>挑出 <script type="math/tex">\textbf{z}</script> 中的某個元素： <script type="math/tex">z_{j}</script> ，整理一下，繼續以上推導過程：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& = \int_{z_{j}} q_{j}(z_{j}) \bigg( \int... \int_{z_{i\neq j }} \prod_{i} q(z_{i}) \text{log} p (\textbf{z},\textbf{x}) \prod_{i \neq j} \text{d}z_{i} \bigg) \text{d}z_{j}

\\

& =  \int_{z_{j}} q_{j}(z_{j}) \bigg( \int... \int_{z_{i\neq j }} \text{log} p (\textbf{z},\textbf{x}) \prod_{i \neq j } q(z_{i})  \text{d}z_{i} \bigg) \text{d}z_{j}

\end{align}

 %]]&gt;</script>

<p>由於 <script type="math/tex"> \prod_{i \neq j } q(z_{i})</script> 是 joint distribution，可以把 <script type="math/tex">p (\textbf{z},\textbf{x})</script> 放到此機率分布的期望值裡，繼續以上推導過程：<a name="eq4">（公式四）</a></p>

<script type="math/tex; mode=display">

=  \int q_{j}(z_{j}) E_{q_{i \neq j}(z)}\bigg[  \text{log} p (\textbf{z},\textbf{x}) \  \bigg] \text{d}z_{j}

</script>

<p>由於 <script type="math/tex">E_{q_{i \neq j}(z)}\bigg[  \text{log} p (\textbf{z},\textbf{x}) \  \bigg]</script> 中，把 <script type="math/tex">p (\textbf{z},\textbf{x})</script> 中不是 <script type="math/tex">z_{j}</script> 的 <script type="math/tex">z_{i}</script> 全都消掉了，所以算完後的 <script type="math/tex">p (\textbf{z},\textbf{x})</script> 只會剩 <script type="math/tex">z_{j}</script> ，令：</p>

<script type="math/tex; mode=display">

\text{log} \tilde{p}(\textbf{x},z_{j}) =  E_{q_{i \neq j}(z)}\bigg[  \text{log} p (\textbf{z},\textbf{x}) \  \bigg] 

</script>

<p>將其代入<a href="#eq4">（公式四）</a>，繼續推導過程，得出<a name="eq5">公式五</a>：</p>

<script type="math/tex; mode=display">

 E_{q(\textbf{z}) }[\text{log}p(\textbf{x},\textbf{z})]  =  \int q_{j}(z_{j})   \text{log} \tilde{p}(\textbf{x},z_{j}) \text{d}z_{j}

</script>

<p>再來看 <script type="math/tex"> E_{ q(\textbf{z} ) }[\text{log}q(\textbf{z})]</script>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& E_{ q(\textbf{z} ) }[\text{log}q(\textbf{z})] = \int \prod_{i} q_{i}(z_{i}) \text{log} \prod_{i} q (z_{i}) \text{d}\textbf{z}

\\

&

= \int \prod_{i} q_{i}(z_{i}) \sum_{i} \text{log} q (z_{i}) \text{d}\textbf{z}

\\

&

= \sum_{i} \bigg(\prod_{k \neq i} \int_{k}q_{k}(z_{k})\text{d}z_{k}  \int  q_{i}(z_{i}) \text{log} q (z_{i}) \text{d}z_{i}  \bigg)

\\

&

= \sum_{i} \bigg(  \int  q_{i}(z_{i}) \text{log} q (z_{i}) \text{d}z_{i}  \bigg)

\end{align}

 %]]&gt;</script>

<p>挑出 <script type="math/tex">\textbf{z}</script> 中的某個元素： <script type="math/tex">z_{j}</script> ，整理一下，繼續以上推導過程：</p>

<script type="math/tex; mode=display">

=  \int  q_{j}(z_{j}) \text{log} q (z_{j}) \text{d}z_{j} + \sum_{i \neq j } \bigg(  \int  q_{i}(z_{i}) \text{log} q (z_{i}) \text{d}z_{i}  \bigg)

</script>

<p>不涉及 <script type="math/tex">j</script>的部分，就當 <script type="math/tex">const</script> 來處理，整理以上公式，得出：<a name="eq6">（公式六）</a></p>

<script type="math/tex; mode=display">

E_{ q(\textbf{z} ) }[\text{log}q(\textbf{z})] =  \int  q_{j}(z_{j}) \text{log} q (z_{j}) \text{d}z_{j} + const

</script>

<p>將 <a href="#eq5">（公式五）</a>和<a href="#eq6">（公式六）</a> 代入 <script type="math/tex">L[q(\textbf{z})]</script> ，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& L[q(\textbf{z})] =\int q_{j}(z_{j})   \text{log} \tilde{p}(\textbf{x},z_{j}) \text{d}z_{j}- \int  q_{j}(z_{j}) \text{log} q (z_{j}) \text{d}z_{j} - const

\\

&

= \int q_{j}(z_{j})   \text{log} \frac{ \tilde{p}(\textbf{x},z_{j}) } { q_{j}(z_{j}) } \text{d} z_{j} - const

\\

& 

= -D_{KL}[\tilde{p}(\textbf{x},z_{j})|| q_{j}(z_{j} ) ] - const

\end{align}

 %]]&gt;</script>

<p>根據以上結果，要將 <script type="math/tex"> L[q(\textbf{z})] =\int q_{j}(z_{j}) </script> 最大化，則：</p>

<script type="math/tex; mode=display">

D_{KL}[\tilde{p}(\textbf{x},z_{j})|| q_{j}(z_{j} ) ]  = 0

</script>

<p>因此：</p>

<script type="math/tex; mode=display">

q_{j}(z_{j}) = \tilde{p}(\textbf{x},z_{j}) 

</script>

<p>由於 <script type="math/tex">z_{j}</script> 有 <script type="math/tex">n</script> 個， 整個運算過程，有點類似 <em>EM</em> 演算法，就是挑選某個 <script type="math/tex">q_{j}(z_{j})</script> 根據以上公式，來更新其值，而其他的 <script type="math/tex">q_{i}(z_{i})</script> 則固定。這樣依序更新每個 <script type="math/tex">q_{j}(z_{j})</script> ，一直循環下去，則最後收斂的結果，即為 <script type="math/tex">L[q(\textbf{z})]</script> 的 <em>Local Maximum</em> 。</p>

<h2 id="reference">Reference</h2>

<p>A Tutorial on Variational Bayesian Inference</p>

<p>http://www.orchid.ac.uk/eprints/40/1/fox_vbtut.pdf</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AdaDelta]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/02/08/optimization-method-adadelta/"/>
    <updated>2016-02-08T16:13:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/02/08/optimization-method-adadelta</id>
    <content type="html"><![CDATA[<h2 id="adagrad">AdaGrad</h2>

<p>本文接續 <a href="http://ckmarkoh.github.io/blog/2015/12/23/optimization-method-adagrad">Optimization Method – Gradient Descent &amp; AdaGrad </a>。所提到的 <em>AdaGrad</em> ，及改良它的方法 – <em>AdaDelta</em> 。</p>

<p>在機器學習最佳化過程中，用 <em>AdaGrad</em> 可以隨著時間來縮小 <em>Learning Rage</em> ，以達到較好的收斂效果。<em>AdaGrad</em> 的公式如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{G}_{t} = \sum_{n=0}^{t} \textbf{g}_{n}^{2} \\

& \textbf{x}_{t+1} = \textbf{x}_{t} - \frac{\eta}{\sqrt{\textbf{G}_{t}}} \textbf{g}_{t} \\

\end{align}

 %]]&gt;</script>

<p>不過， <em>AdaGrad</em> 有個缺點，由於 <script type="math/tex">\textbf{g}_{n}^{2}</script> 恆為正，故 <script type="math/tex">\textbf{G}_{t} </script> 只會隨著時間增加而遞增，所以 <script type="math/tex">\frac{\eta}{\sqrt{\textbf{G}_{t}}} </script> 只會隨著時間增加而一直遞減，如果 <em>Learning Rate</em> <script type="math/tex">\eta</script>的值太小，則 <em>AdaGrad</em> 會較慢才收斂。</p>

<p>舉個例子，如果目標函數為 <script type="math/tex"> f(x,y) = y^2 - x^2  </script> ，起始點為 <script type="math/tex">(x,y) = (0.001,4)</script> ， <em>Learning Rate</em> <script type="math/tex">\eta=0.5</script> ，則整個最佳化的過程如下圖，曲面為目標函數，紅色的點為 <script type="math/tex">(x,y)</script> ：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00157.png" alt="" /></p>

<!--more-->

<p>動畫版：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00158.gif" alt="" /></p>

<p>從上圖來看，一開始紅色點的下降速度很快，但越後面則越慢。</p>

<p>為了解決此問題，在調整 <em>Learning Rate</em> 時，不要往前一直加到最初的時間點，而只要往前加到某段時間即可。</p>

<p>但如果要從某段時間點的 <script type="math/tex">\textbf{g}_{t}</script> 開始累加，則需要儲存某個時間點之後開始的每個 <script type="math/tex">\textbf{g}_{t}</script> ，這樣會造成記憶體的浪費。有種較簡便的做法，即是用衰減係數 <script type="math/tex">\rho</script> ，將上一時間點的  <script type="math/tex">\textbf{G}_{t-1}</script> 乘上 <script type="math/tex"> \rho</script> ，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{G}_{t} = \rho \textbf{G}_{t-1} + (1 - \rho) \textbf{g}_{t}^{2} \\

& 0 < \rho < 1 \\

\end{align}

 %]]&gt;</script>

<p>藉由衰減係數 <script type="math/tex">\rho</script> ，可讓較早期時間點累加的 <script type="math/tex">\textbf{g}_{t}^{2}</script> 衰減至 0 ，因此，不會使得 <em>Learning Rate</em> 只隨著時間而一直遞減。</p>

<h2 id="correct-units-of-x">Correct Units of ΔX</h2>

<p><em>Adagrad</em> 還有另一個問題，就是 <script type="math/tex">\textbf{x}</script> 的修正量– <script type="math/tex">\Delta{\textbf{x}}</script> 為 <script type="math/tex">\frac{\eta}{\sqrt{\textbf{G}_{t}}} \textbf{g}_{t}</script> ，假設它如果有「單位」的話，它的單位會與 <script type="math/tex">\textbf{x}</script> 不同。 因 <script type="math/tex">\Delta{\textbf{x}}</script>  的單位與 <script type="math/tex">\textbf{g}</script> 的單位相同，而會和 <script type="math/tex">\textbf{x}</script> 不同，因為：</p>

<script type="math/tex; mode=display">

   \text{ units of }\Delta{\textbf{x}}  \propto  \text{ units of } \textbf{g} \propto  \dfrac{\partial f}{\partial x } \propto \frac{1}{  \text{ units of } \textbf{x} }

</script>

<p>註：在此假設 <script type="math/tex">f</script> 無單位。</p>

<p>相較之下， <a href="http://ckmarkoh.github.io/blog/2016/01/25/optimization-method-newton"><em>Newton’s Method</em></a> 中， <script type="math/tex">\Delta{\textbf{x}} =  \eta   \textbf{H}^{-1} \textbf{g}</script>， <script type="math/tex">\Delta{\textbf{x}}</script> 的單位與 <script type="math/tex">\textbf{x}</script> 的單位相同，因為：</p>

<script type="math/tex; mode=display">

   \text{ units of }\Delta{\textbf{x}}  \propto  \text{ units of } \textbf{H}^{-1} \textbf{g} \propto 

   \frac{

   \dfrac{\partial f}{\partial x }

   }

   {   \dfrac{\partial^{2} f}{\partial x^{2} }

   }

   \propto   \text{ units of } \textbf{x} 


</script>

<p>但 <em>Newton’s Method</em> 的缺點是，二次微分 <em>Hessian</em> 矩陣的反矩陣 <script type="math/tex">\textbf{H}^{-1}</script> ，計算時間複雜度太高。如果只是為了要單位相同，是沒必要這樣算。</p>

<p>想要簡易求出  <script type="math/tex">\textbf{H}^{-1}</script> 的單位，稍微整理一下以上公式，得出：</p>

<script type="math/tex; mode=display">

   \Delta{\textbf{x}}  \propto  \frac{\dfrac{\partial f}{\partial x } } {\dfrac{\partial^{2} f}{\partial x^{2}}} \Rightarrow  \textbf{H}^{-1} \propto \frac{1 } {\dfrac{\partial^{2} f}{\partial x^{2}}} 

     \propto

   

   \dfrac{\Delta{x}}{\dfrac{\partial f}{\partial x }} \propto  \dfrac{\Delta{x}}{\textbf{g}}  


</script>

<p>因此，若要簡易求出  <script type="math/tex">\textbf{H}^{-1} </script> 的單位，只要算 <script type="math/tex">\dfrac{\Delta{x}}{\textbf{g}}  </script> 即可。</p>

<p>註：如果看不懂這段在寫什麼，請參考<a href="http://arxiv.org/abs/1212.5701">Matthew D. Zeiler. ADADELTA: AN ADAPTIVE LEARNING RATE METHOD.</a></p>

<h2 id="adadelta">AdaDelta</h2>

<p><em>AdaDelta</em> 解決了 <em>AdaGrad</em> 會發生的兩個問題：</p>

<p>(1) <em>Learning Rate</em> 只會隨著時間而一直遞減下去</p>

<p>(2) <script type="math/tex">\Delta{\textbf{x}}</script> 與 <script type="math/tex">\textbf{x}</script> 的單位不同</p>

<p><em>AdaDelta</em> 的公式如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{G}_{t} = \rho \textbf{G}_{t-1} + (1 - \rho) \textbf{g}_{t}^{2} \\


& \Delta \textbf{x}_{t} = - \frac{\sqrt{\textbf{D}_{t - 1}+\epsilon}}{\sqrt{\textbf{G}_{t}+\epsilon}} \textbf{g}_{t} \\


& \textbf{D}_{t} = \rho \textbf{D}_{t-1} + (1 - \rho) \Delta \textbf{x}_{t}^{2} \\


& \textbf{x}_{t+1} = \textbf{x}_{t} + \Delta{x}_{t} \\


\end{align}

 %]]&gt;</script>

<p>其中， <script type="math/tex">\rho</script> 和  <script type="math/tex">\epsilon</script> 為常數。 <script type="math/tex">\rho</script> 的作用為「衰減係數」，而 <script type="math/tex">\epsilon</script> 是為了避免 <script type="math/tex">\frac{\sqrt{\textbf{D}_{t - 1}+\epsilon}}{\sqrt{\textbf{G}_{t}+\epsilon}}</script> 的分母為 0 。</p>

<p>此處的 <script type="math/tex"> \textbf{G}_{t} </script> 有點類似 <em>AdaGrad</em> 裡面的  <script type="math/tex"> \textbf{G}_{t} </script> ，但如前面所述，  <em>AdaDelta</em> 的不是直接把 <script type="math/tex">\textbf{g}_{t}^2</script> 直接累加上去，而是藉由衰減係數 <script type="math/tex">\rho</script> ，可讓較早期時間點累加的 <script type="math/tex">\textbf{g}_{t}^{2}</script> 衰減至 0 ，因此，不會使得 <em>Learning Rate</em> 只隨著時間一直遞減下去。</p>

<p>而 <script type="math/tex">\textbf{D}_{t}</script> 的作用，則是使 <script type="math/tex">\Delta{\textbf{x}}</script> 與 <script type="math/tex">\textbf{x}</script> 有相同的單位，因為 <script type="math/tex"> \frac{\sqrt{\textbf{D}_{t - 1}+\epsilon}}{\sqrt{\textbf{G}_{t}+\epsilon}}</script> 與 <script type="math/tex">\textbf{H}^{-1}</script> 具有相同單位，如下：</p>

<script type="math/tex; mode=display">

 \frac{\sqrt{\textbf{D}_{t - 1}+\epsilon}}{\sqrt{\textbf{G}_{t}+\epsilon}} \propto  \dfrac{\Delta{x}}{\textbf{g}}  \propto  \textbf{H}^{-1}

</script>

<p>根據前一段的結果，若 <script type="math/tex">\Delta{\textbf{x}}  \propto   \textbf{H}^{-1} \textbf{g}</script>，則 <script type="math/tex">\Delta{\textbf{x}}</script> 與 <script type="math/tex">\textbf{x}</script> 的單位相同。</p>

<p>另外，<script type="math/tex">\textbf{D}_{t}</script> 可累加過去時間點的 <script type="math/tex">\Delta{\textbf{x}}</script> ，這樣所造成的效果，有點類似  <a href="http://ckmarkoh.github.io/blog/2016/01/16/optimization-method-momentum"><em>Gradient Descent with Momentum</em></a> ，使得現在時間點的 <script type="math/tex">\Delta{\textbf{x}}</script> ，具有過去時間點的動量。</p>

<p>實際帶數字進去算一次 <em>AdaDelta</em> 。舉前述例子，假設 <script type="math/tex"> f(x,y) = y^2 - x^2  </script> ，起始參數為 <script type="math/tex">(x,y) = (0.001,4)</script> ，則畫出來的圖形如下圖，藍色點為起始點位置：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00161.png" alt="" /></p>

<p>用 <em>AdaDelta</em> 最佳化方法，初始值設 <script type="math/tex">\textbf{G}_{0} = [0,0 ]^{T}  </script> ， <script type="math/tex"> \textbf{D}_{0} = [0,0 ]^{T} </script> ，設參數 <script type="math/tex">\rho = 0.5</script> ， <script type="math/tex">\epsilon = 0.1 </script> ，更新 <script type="math/tex">x,y </script> 的值，如下，（註：以下的向量 <script type="math/tex">\textbf{G}</script> 、 <script type="math/tex">\textbf{D}</script> 、 <script type="math/tex">\Delta \textbf{x}</script> 等等的加減乘除運算，皆為 <em>Element-wise Operation</em> ）：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{g}_{1} = 

\begin{bmatrix}

-2x_{0} \\[0.3em]

2y_{0} \\[0.3em]

\end{bmatrix}

=

\begin{bmatrix}

-2 \times 0.001 \\[0.3em]

2 \times 4 \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

-0.002 \\[0.3em]

8 \\[0.3em]

\end{bmatrix} \\



& \textbf{G}_{1} = 0.5 

\begin{bmatrix}

0  \\[0.3em]

0  \\[0.3em]

\end{bmatrix}

+ (1-0.5 ) 

\begin{bmatrix}

(-0.002)^{2}  \\[0.3em]

8^{2}  \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

2 \times 10^{-6}  \\[0.3em]

32  \\[0.3em]

\end{bmatrix} \\


& \Delta \textbf{x}_{1} = - 

\frac{\sqrt{

\begin{bmatrix}

0   \\[0.3em]

0 \\[0.3em]

\end{bmatrix} 

+ 0.1}}

{\sqrt{

\begin{bmatrix}

2 \times 10^{-6}  \\[0.3em]

32 \\[0.3em]

\end{bmatrix} 

 + 0.1}}

\begin{bmatrix}

-0.002  \\[0.3em]

8  \\[0.3em]

\end{bmatrix} 

= 

\begin{bmatrix}

0.00199998 \\[0.3em]

-0.44651646 \\[0.3em]

\end{bmatrix} \\


& \textbf{D}_{1} = 0.5 

\begin{bmatrix}

0  \\[0.3em]

0  \\[0.3em]

\end{bmatrix}

+ (1-0.5 ) 

\begin{bmatrix}

0.00199998^{2}  \\[0.3em]

(-0.44651646)^{2}  \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

1.99996 \times 10^{-6}  \\[0.3em]

0.09968847  \\[0 .3em]

\end{bmatrix} \\


&

\begin{bmatrix}

x_{1} \\[0 .3em]

y_{1} \\[0 .3em]

\end{bmatrix}

=

\textbf{x}_{1} = 

\begin{bmatrix}

0.001 \\[0.3em]

4 \\[0 .3em]

\end{bmatrix}

+ 

\begin{bmatrix}

0.00199998 \\[0.3em]

-0.44651646 \\[0.3em]

\end{bmatrix}

= 

\begin{bmatrix} 

0.00299998 \\[0.3em]

3.55348354 \\[0.3em]

\end{bmatrix}

\end{align}


 %]]&gt;</script>

<p>更新 <script type="math/tex">x,y</script> 的值， <script type="math/tex">x,y = 0.00299998, 3.55348354 \approx 0.00300,3.55348  </script> ，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00160.png" alt="" /></p>

<p>再往下走一步， 計算 <script type="math/tex">x,y</script> 的值，如下：  </p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{g}_{2} = 

\begin{bmatrix}

-2x_{1} \\[0.3em]

2y_{1} \\[0.3em]

\end{bmatrix}

=

\begin{bmatrix}

-2 \times 0.00299998 \\[0.3em]

2 \times 3.55348354 \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

-0.00599996 \\[0.3em]

7.10696708 \\[0.3em]

\end{bmatrix} \\



& \textbf{G}_{2} = 0.5 

\begin{bmatrix}

2 \times 10^{-6}  \\[0.3em]

32  \\[0.3em]

\end{bmatrix}

+ (1-0.5 ) 

\begin{bmatrix}

(-0.00599996)^{2}  \\[0.3em]

7.10696708^{2}  \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

1.89997600 \times 10^{-5}  \\[0.3em]

41.25449057  \\[0.3em]

\end{bmatrix} \\



& \Delta \textbf{x}_{2} = - 

\frac{\sqrt{

\begin{bmatrix}

1.99996 \times 10^{−6}   \\[0.3em]

0.09968847 \\[0.3em]

\end{bmatrix} 

+ 0.1}}

{\sqrt{

\begin{bmatrix}

1.89997600 \times 10^{-6}  \\[0.3em]

41.25449057 \\[0.3em]

\end{bmatrix} 

 + 0.1}}

\begin{bmatrix}

-0.00599996 \\[0.3em]

7.10696708 \\[0.3em]

\end{bmatrix} 

= 

\begin{bmatrix}

0.00599945 \\[0.3em]

-0.49385501\\[0.3em]

\end{bmatrix} \\


& \textbf{D}_{2} = 0.5 

\begin{bmatrix}

1.99996 \times 10^{−6}  \\[0.3em]

0.09968847  \\[0.3em]

\end{bmatrix}

+ (1-0.5 ) 

\begin{bmatrix}

0.00599945^{2}  \\[0.3em]

(-0.49385501)^{2}  \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

1.89966806 \times 10^{-5}  \\[0.3em]

0.17179062  \\[0 .3em]

\end{bmatrix} \\


&

\begin{bmatrix}

x_{2} \\[0 .3em]

y_{2} \\[0 .3em]

\end{bmatrix}

=

\textbf{x}_{2} = 

\begin{bmatrix}

0.00299998 \\[0.3em]

3.55348354 \\[0 .3em]

\end{bmatrix}

+ 

\begin{bmatrix}

0.00599945 \\[0.3em]

−0.49385501 \\[0.3em]

\end{bmatrix}

= 

\begin{bmatrix} 

0.00899943 \\[0.3em]

3.05962853 \\[0.3em]

\end{bmatrix}

\end{align}


 %]]&gt;</script>

<p>更新 <script type="math/tex">x,y</script> 的值， <script type="math/tex">x,y = 0.00899943, 3.05962853 \approx 0.00900,3.05963  </script> ，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00161.png" alt="" /></p>

<p>重複以上循環，整個過程如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00162.png" alt="" /></p>

<p>動畫版：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00163.gif" alt="" /></p>

<p>將 <em>Gradient Descent</em> （綠） ， <em>AdaGrad</em> （紅） 和 <em>AdaDelta</em> （藍） 畫在同一張圖上比較看看： </p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00164.gif" alt="" /></p>

<p>從上圖可看出， <em>AdaDelta</em> 的 <em>Learning Rate</em> 會隨著坡度而適度調整，不會一直遞減下去，也不會像 <em>Gradient Descent</em> 一樣，容易卡在 <em>saddle point</em> （請見<a href="http://ckmarkoh.github.io/blog/2015/12/23/optimization-method-adagrad"> Optimization Method – Gradient Descent &amp; AdaGrad </a>）。</p>

<h2 id="implementation">Implementation</h2>

<p>再來進入實作的部分</p>

<p>首先，開啟新的檔案 adadelta.py 並貼上以下程式碼：</p>

<figure class="code"><figcaption><span>adadelta.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
<span class="line-number">61</span>
<span class="line-number">62</span>
<span class="line-number">63</span>
<span class="line-number">64</span>
<span class="line-number">65</span>
<span class="line-number">66</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">sqrt</span>
</span><span class="line">
</span><span class="line"><span class="n">XT</span> <span class="o">=</span> <span class="mf">0.001</span>
</span><span class="line"><span class="n">YT</span> <span class="o">=</span> <span class="mi">4</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
</span><span class="line">  <span class="k">return</span> <span class="p">(</span><span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">func_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
</span><span class="line">  <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">y</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s">&#39;r&#39;</span><span class="p">):</span>
</span><span class="line">  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
</span><span class="line">  <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">&#39;3d&#39;</span><span class="p">,</span>
</span><span class="line">        <span class="n">elev</span><span class="o">=</span><span class="mf">35.</span><span class="p">,</span> <span class="n">azim</span><span class="o">=-</span><span class="mi">30</span><span class="p">)</span>
</span><span class="line">  <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">))</span>
</span><span class="line">  <span class="n">Z</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
</span><span class="line">  <span class="n">surf</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">rstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span class="line">    <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</span><span class="line">  <span class="n">ax</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="o">-</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
</span><span class="line">  <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">,</span> <span class="n">func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">),</span><span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">&#39;o&#39;</span> <span class="p">)</span>
</span><span class="line">  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">&quot;x=</span><span class="si">%.5f</span><span class="s">, y=</span><span class="si">%.5f</span><span class="s">, f(x,y)=</span><span class="si">%.5f</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="n">func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">)))</span>
</span><span class="line">  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span><span class="line">  <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">run_adagrad</span><span class="p">():</span>
</span><span class="line">  <span class="n">xt</span><span class="p">,</span> <span class="n">yt</span> <span class="o">=</span> <span class="n">XT</span><span class="p">,</span> <span class="n">YT</span>
</span><span class="line">  <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.5</span>
</span><span class="line">  <span class="n">Gxt</span><span class="p">,</span> <span class="n">Gyt</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
</span><span class="line">  <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
</span><span class="line">    <span class="n">gxt</span><span class="p">,</span> <span class="n">gyt</span> <span class="o">=</span> <span class="n">func_grad</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">)</span>
</span><span class="line">    <span class="n">Gxt</span> <span class="o">+=</span> <span class="n">gxt</span><span class="o">**</span><span class="mi">2</span>
</span><span class="line">    <span class="n">Gyt</span> <span class="o">+=</span> <span class="n">gyt</span><span class="o">**</span><span class="mi">2</span>
</span><span class="line">    <span class="n">xt</span> <span class="o">-=</span> <span class="n">eta</span><span class="o">*</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="p">(</span><span class="n">Gxt</span><span class="o">**</span><span class="mf">0.5</span><span class="p">))</span><span class="o">*</span><span class="n">gxt</span>
</span><span class="line">    <span class="n">yt</span> <span class="o">-=</span> <span class="n">eta</span><span class="o">*</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="p">(</span><span class="n">Gyt</span><span class="o">**</span><span class="mf">0.5</span><span class="p">))</span><span class="o">*</span><span class="n">gyt</span>
</span><span class="line">    <span class="k">if</span> <span class="n">xt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">xt</span> <span class="o">&gt;</span> <span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">:</span>
</span><span class="line">      <span class="k">break</span>
</span><span class="line">    <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">run_adadelta</span><span class="p">():</span>
</span><span class="line">  <span class="n">xt</span><span class="p">,</span> <span class="n">yt</span> <span class="o">=</span> <span class="n">XT</span><span class="p">,</span> <span class="n">YT</span>
</span><span class="line">  <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>
</span><span class="line">  <span class="n">rho</span> <span class="o">=</span> <span class="mf">0.5</span>
</span><span class="line">  <span class="n">Gxt</span><span class="p">,</span> <span class="n">Gyt</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span>
</span><span class="line">  <span class="n">Dxt</span><span class="p">,</span> <span class="n">Dyt</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span>
</span><span class="line">  <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;b&#39;</span><span class="p">)</span>
</span><span class="line">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
</span><span class="line">    <span class="n">gxt</span><span class="p">,</span> <span class="n">gyt</span> <span class="o">=</span> <span class="n">func_grad</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">)</span>
</span><span class="line">    <span class="n">Gxt</span><span class="p">,</span> <span class="n">Gyt</span> <span class="o">=</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">Gxt</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">rho</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">gxt</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="p">,</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">Gyt</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">rho</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">gyt</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</span><span class="line">    <span class="n">dxt</span><span class="p">,</span> <span class="n">dyt</span>  <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">sqrt</span><span class="p">(</span><span class="n">Dxt</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">Gxt</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">))</span> <span class="o">*</span> <span class="n">gxt</span> <span class="p">,</span> \
</span><span class="line">                <span class="o">-</span><span class="p">(</span><span class="n">sqrt</span><span class="p">(</span><span class="n">Dyt</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">Gyt</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">))</span> <span class="o">*</span> <span class="n">gyt</span>
</span><span class="line">    <span class="n">Dxt</span><span class="p">,</span> <span class="n">Dyt</span> <span class="o">=</span>  <span class="n">rho</span> <span class="o">*</span> <span class="n">Dxt</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">rho</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dxt</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="p">,</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">Dyt</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">rho</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dyt</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</span><span class="line">    <span class="n">xt</span> <span class="o">+=</span> <span class="n">dxt</span>
</span><span class="line">    <span class="n">yt</span> <span class="o">+=</span> <span class="n">dyt</span>
</span><span class="line">    <span class="k">if</span> <span class="n">xt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">xt</span> <span class="o">&gt;</span> <span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">:</span>
</span><span class="line">      <span class="k">break</span>
</span><span class="line">    <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;b&#39;</span><span class="p">)</span>
</span><span class="line">
</span></code></pre></td></tr></table></div></figure>

<p>其中， <code>func(x,y)</code> 為目標函數， <code>func_grad(x,y)</code> 為目標函數的 gradient ，而 <code>plot_func(xt,yt,c='r')</code> 可畫出目標函數的曲面， <code>run_adagrad()</code> 用來執行 <em>AdaGrad</em> ， <code>run_adadelta()</code> 用來執行 <em>AdaDelta</em> 。</p>

<p>到 python console 執行：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; import adadelta
</span></code></pre></td></tr></table></div></figure>

<p>執行 <em>AdaGrad</em> ，指令如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; adadelta.run_adagrad<span class="o">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00165.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00166.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00167.png" alt="" /></p>

<p>以此類推</p>

<p>執行 <em>AdaDelta</em> ，指令如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; adadelta.run_adadelta<span class="o">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00168.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00169.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00170.png" alt="" /></p>

<p>以此類推</p>

<h2 id="reference">Reference</h2>

<p><a href="http://arxiv.org/abs/1212.5701">Matthew D. Zeiler. ADADELTA: AN ADAPTIVE LEARNING RATE METHOD.</a></p>

<p><a href="http://imgur.com/a/Hqolp">Visualizing Optimization Algos</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Newton's Method for Optimization]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/01/25/optimization-method-newton/"/>
    <updated>2016-01-25T16:56:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/01/25/optimization-method-newton</id>
    <content type="html"><![CDATA[<h2 id="gradient-descent">Gradient Descent</h2>

<p>機器學習中，用 <em>Gradient Descent</em> 是解最佳化問題，最基本的方法。關於Gradient Descent的公式，請參考：<a href="http://ckmarkoh.github.io/blog/2015/12/23/optimization-method-adagrad">Optimization Method – Gradient Descent &amp; AdaGrad</a></p>

<p>對於 <em>Cost function</em> <script type="math/tex">f(\textbf{x})</script> ，在 <script type="math/tex">\textbf{x} = \textbf{x}_{t}</script> 時， <em>Gradient Descent</em>  走的方向為  <script type="math/tex">  -\nabla f(\textbf{x})</script> 。也就是，用泰勒展開式展開後，用一次微分 <script type="math/tex">f(\textbf{x})</script> 來趨近的方向，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00144.png" alt="" /></p>

<p>註：考慮到 <script type="math/tex">\textbf{x}</script> 為向量的情形，故一次微分寫成  <script type="math/tex">\nabla f(\textbf{x})</script> 。 </p>

<p>其中， <script type="math/tex">f(\textbf{x})</script> 為原本的 <em>Cost function</em> ，而 <script type="math/tex">\tilde{f}(\textbf{x})</script> 為泰勒展開式取一次微分逼近的。 而 <em>Gradient Descent</em> 走的方向為 <script type="math/tex"> - \nabla f(\textbf{x}) </script> ，為沿著 <script type="math/tex">\tilde{f}(\textbf{x})</script> 的方向。</p>

<!--more-->

<p>這會有個問題，如果原本的 <em>Cost function</em> 為較高次函數，只用一次項來逼近是不夠的，有時候，失真情形很嚴重，例如， <em>Cost function</em> 為橢圓 <script type="math/tex">f(x,y) = x^{2}+9y^{2} </script>， 此函數的等高線圖，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00145.png" alt="" /></p>

<p>如果起始點為 <script type="math/tex">(x,y) = (-4,2.5)</script> ，沿著 <script type="math/tex"> - \nabla f(x) </script> 的方向走，也就是說，走梯度最陡的方向（即與等高線垂直的方向），可能會需要多次折返，才能走到最小值，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00146.gif" alt="" /></p>

<h2 id="second-order-taylor-approximation">Second-Order Taylor Approximation</h2>

<p>根據前面的例子得知，只考慮一次微分項，是不夠的，現在要來考慮二次微分項。</p>

<p>對於 <em>Cost function</em> <script type="math/tex">f(\textbf{x})</script> ， 在 <script type="math/tex">\textbf{x} = \textbf{x}_{t}</script> 時，用泰勒展開式展開後，分別用一次微分與二次微分來逼近，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00147.png" alt="" /></p>

<p>其中，橘色的 <script type="math/tex">\tilde{f}(\textbf{x})</script> 為只用了一次微分的逼近，而紫色的 <script type="math/tex">\hat{f}(\textbf{x})</script> 為用了一次與二次微分向的逼近，由此可見， <script type="math/tex">\hat{f}(\textbf{x})</script>  較 <script type="math/tex">\tilde{f}(\textbf{x})</script> 接近原本的 <script type="math/tex">f(\textbf{x})</script></p>

<p>如果要求 <script type="math/tex">f(\textbf{x})</script> 的最小值，可以往 <script type="math/tex">\hat{f}(\textbf{x})</script> 為最小值的方向，一步一步走下去。要找出 <script type="math/tex">\hat{f}(\textbf{x})</script> 的最小值，即：</p>

<script type="math/tex; mode=display">

\min_{x} \hat{f} (\textbf{x})  =  \min_{\textbf{x}}  f(\textbf{x}_{t}) + \nabla f(\textbf{x}_{t})^{T}\textbf{x} + \frac{1}{2} \textbf{x}^{T} \nabla^{2}f(\textbf{x}_{t}) \textbf{x} 

</script>

<p>將 <script type="math/tex">\hat{f}(\textbf{x})</script> 對 <script type="math/tex">\textbf{x}</script> 微分，令微分結果為 <script type="math/tex">0</script> ，得：</p>

<script type="math/tex; mode=display">

0 =  \nabla f(\textbf{x}_{t}) +  \nabla^{2}f(\textbf{x}_{t}) \textbf{x} 

</script>

<p>得 </p>

<script type="math/tex; mode=display">

\textbf{x} = -  \nabla^{2}f(\textbf{x}_{t})^{-1} \nabla f(\textbf{x}_{t}) 

</script>

<p>可以用此 <script type="math/tex">\textbf{x}</script> ，來當作位於 <script type="math/tex">\textbf{x}=\textbf{x}_{t}</script> 時，想走往 <script type="math/tex">f(\textbf{x})</script> 的最小值，要走的方向（與距離）。用一次與二次微分所得出的方向，一步步走下去，最後走到最小值，這種方法即為 <em>Newton’s Method</em> 。</p>

<h2 id="newtons-method-for-optimization">Newton’s Method for Optimization</h2>

<p><em>Newton’s Method</em> 即是考慮二次微分的 <em>Gradient Descent</em> 方法，公式如下：</p>

<script type="math/tex; mode=display">

\textbf{x}_{t+1} \leftarrow \textbf{x}_{t} - \eta   \textbf{H}_{t}^{-1} \textbf{g}_{t}

</script>

<p>其中， <script type="math/tex"> \eta</script> 為 <em>Learning Rate</em> ， <script type="math/tex"> \textbf{H}_{t} = \nabla^{2}f(\textbf{x}_{t}) </script> （ 稱為 <em>Hessian</em> ）， <script type="math/tex">\textbf{g}_{t}=\nabla f(\textbf{x}_{t}) </script> （ 稱為 <em>Gradient</em> ）。</p>

<p>再來看看用 <em>Newton’s Method</em> 來解決 <em>Cost function</em> 為橢圓 <script type="math/tex">f(x,y) = x^{2}+9y^{2} </script> 的情形。首先，畫出起始點 <script type="math/tex">(-4, 2.5) </script> ，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00148.png" alt="" /></p>

<p>先來算 <script type="math/tex">\textbf{g}</script> 和 <script type="math/tex">\textbf{H}^{-1}</script> ，分別為：</p>

<script type="math/tex; mode=display">

\textbf{g} = 

\begin{bmatrix}

  \dfrac{ \partial f(x,y) }{\partial x}  \\[0.3em]

  \dfrac{\partial f(x,y) } {\partial y}  \\[0.3em]

\end{bmatrix} 

= 

\begin{bmatrix}

  2x  \\[0.3em]

  18y \\[0.3em]

\end{bmatrix} 

</script>

<script type="math/tex; mode=display">% &lt;![CDATA[


\textbf{H}^{-1} = 

\begin{bmatrix}

  \dfrac{ \partial^{2} f(x,y) }{\partial x^{2}}  &  

  \dfrac{ \partial^{2} f(x,y) }{\partial xy}

  \\[0.3em]

  \dfrac{ \partial^{2} f(x,y) } {\partial xy} &

  \dfrac{ \partial^{2} f(x,y) }{\partial y^{2}}   

  \\[0.3em]

\end{bmatrix} ^{-1}

= 

\begin{bmatrix}

  2  &  

  0

  \\[0.3em]

  0 &

  18 

  \\[0.3em]

\end{bmatrix} ^{-1}

=

\begin{bmatrix}

  \frac{1}{2}  &  

  0

  \\[0.3em]

  0 &

  \frac{1}{18} 

  \\[0.3em]

\end{bmatrix} 

 %]]&gt;</script>

<p>設 <script type="math/tex"> \eta = 0.5 </script> ，代入起始點  <script type="math/tex">(x_{0},y_{0}) = (-4, 2.5) </script> 、 <script type="math/tex">\textbf{g}</script> 和 <script type="math/tex">\textbf{H}^{-1}</script> 到 <em>Newton’s Method</em> 的公式： <script type="math/tex">\textbf{x}_{t+1} \leftarrow \textbf{x}_{t} - \eta   \textbf{H}_{t}^{-1} \textbf{g}_{t}</script> ，得：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{bmatrix}

  x_{1}

  \\[0.3em]

  y_{1} 

  \\[0.3em]

\end{bmatrix} 

= 

\begin{bmatrix}

  -4 

  \\[0.3em]

  2.5  

  \\[0.3em]

\end{bmatrix} 

- 0.5 

\begin{bmatrix}

  \frac{1}{2}  &  

  0

  \\[0.3em]

  0 &

  \frac{1}{18} 

  \\[0.3em]

\end{bmatrix} 

\begin{bmatrix}

  2 \times (-4)  \\[0.3em]

  18 \times 2.5 \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

  -2  

  \\[0.3em]

  1.25

  \\[0.3em]

\end{bmatrix} 


 %]]&gt;</script>

<p>更新圖上的點， <script type="math/tex">(x_{1},y_{1}) = (-2, 1.25) </script> ，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00155.png" alt="" /></p>

<p>再往下走一步，求 <script type="math/tex">(x_{2},y_{2})</script> 的值，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{bmatrix}

  x_{2}

  \\[0.3em]

  y_{2} 

  \\[0.3em]

\end{bmatrix} 

= 

\begin{bmatrix}

  -2 

  \\[0.3em]

  1.25  

  \\[0.3em]

\end{bmatrix} 

- 0.5 

\begin{bmatrix}

  \frac{1}{2}  &  

  0

  \\[0.3em]

  0 &

  \frac{1}{18} 

  \\[0.3em]

\end{bmatrix} 

\begin{bmatrix}

  2 \times (-2)  \\[0.3em]

  18 \times 1.25 \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

  -1  

  \\[0.3em]

  0.625

  \\[0.3em]

\end{bmatrix} 


 %]]&gt;</script>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00150.png" alt="" /></p>

<p>從以上過程發現，  <em>Newton’s Method</em>  方向不需要一直折返，可以直接往最小值處走下去 ，整個過程如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00151.gif" alt="" /></p>

<p>註：事實上，由於本例的 <em>Cost function</em> <script type="math/tex">f(x,y)</script> 為二次函數，如果是用二次的泰勒展開式逼近，則可以完全貼合 <script type="math/tex">f(x,y)</script> 。所以用  <em>Newton’s Method</em> 的話， 位於 <script type="math/tex">\textbf{x}_{t}</script> 時， <script type="math/tex"> -  \nabla^{2}f(\textbf{x}_{t})^{-1} \nabla f(\textbf{x}_{t}) </script> 即是泰勒展開式最小值的 <script type="math/tex">\textbf{x}</script> 解，也是 <script type="math/tex">f(x,y)</script> 的最小值解，如果設 <script type="math/tex"> \eta = 1 </script> ，只要走一步就可以走到最小值，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{bmatrix}

  x_{1}

  \\[0.3em]

  y_{1} 

  \\[0.3em]

\end{bmatrix} 

= 

\begin{bmatrix}

  -4 

  \\[0.3em]

  2.5  

  \\[0.3em]

\end{bmatrix} 

- 

\begin{bmatrix}

  \frac{1}{2}  &  

  0

  \\[0.3em]

  0 &

  \frac{1}{18} 

  \\[0.3em]

\end{bmatrix} 

\begin{bmatrix}

  2 \times (-4)  \\[0.3em]

  18 \times 2.5 \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

  0

  \\[0.3em]

  0

  \\[0.3em]

\end{bmatrix} 


 %]]&gt;</script>

<p>過程如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00152.png" alt="" /></p>

<h2 id="implementation">Implementation</h2>

<p>再來進入實作的部分</p>

<p>首先，開啟新的檔案 newtons.py 並貼上以下程式碼：</p>

<figure class="code"><figcaption><span>newtons.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
<span class="line-number">61</span>
<span class="line-number">62</span>
<span class="line-number">63</span>
<span class="line-number">64</span>
<span class="line-number">65</span>
<span class="line-number">66</span>
<span class="line-number">67</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</span><span class="line">
</span><span class="line"><span class="n">A</span> <span class="o">=</span> <span class="mi">1</span>
</span><span class="line"><span class="n">B</span> <span class="o">=</span> <span class="mi">9</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">obj_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span><span class="line">    <span class="n">z</span> <span class="o">=</span> <span class="n">A</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">B</span><span class="o">*</span><span class="p">(</span><span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">z</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">obj_func_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span><span class="line">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="o">*</span><span class="n">A</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">B</span><span class="o">*</span><span class="n">y</span><span class="p">])</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">obj_func_hessian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span><span class="line">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="o">*</span><span class="n">A</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
</span><span class="line">                     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">B</span><span class="p">]</span>
</span><span class="line">                     <span class="p">])</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">plot_func</span><span class="p">(</span><span class="n">xts</span><span class="p">,</span> <span class="n">yts</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
</span><span class="line">    <span class="n">delta</span> <span class="o">=</span> <span class="mf">0.1</span>
</span><span class="line">    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
</span><span class="line">    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
</span><span class="line">    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span class="line">    <span class="n">Z</span> <span class="o">=</span> <span class="n">obj_func</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
</span><span class="line">    <span class="n">CS</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s">&#39;gray&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xts</span><span class="p">,</span> <span class="n">yts</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">xt</span><span class="p">,</span> <span class="n">yt</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">xts</span><span class="p">,</span> <span class="n">yts</span><span class="p">):</span>
</span><span class="line">            <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&quot;x=</span><span class="si">%.5f</span><span class="s">, y=</span><span class="si">%.5f</span><span class="s">, f(x, y)=</span><span class="si">%.5f</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">xts</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yts</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">obj_func</span><span class="p">(</span><span class="n">xts</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yts</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])))</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">CS</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">run_gd</span><span class="p">():</span>
</span><span class="line">    <span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">])</span>
</span><span class="line">    <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
</span><span class="line">    <span class="n">xts</span> <span class="o">=</span> <span class="p">[</span><span class="n">xy</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span><span class="line">    <span class="n">yts</span> <span class="o">=</span> <span class="p">[</span><span class="n">xy</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
</span><span class="line">    <span class="n">plot_func</span><span class="p">(</span><span class="n">xts</span><span class="p">,</span> <span class="n">yts</span><span class="p">,</span> <span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">):</span>
</span><span class="line">        <span class="n">gxy</span> <span class="o">=</span> <span class="n">obj_func_grad</span><span class="p">(</span><span class="n">xy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xy</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line">        <span class="n">xy</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">gxy</span>
</span><span class="line">        <span class="n">xts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xy</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span><span class="line">        <span class="n">yts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xy</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line">        <span class="n">plot_func</span><span class="p">(</span><span class="n">xts</span><span class="p">,</span> <span class="n">yts</span><span class="p">,</span> <span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">run_newtons</span><span class="p">():</span>
</span><span class="line">    <span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">])</span>
</span><span class="line">    <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.5</span>
</span><span class="line">    <span class="n">xts</span> <span class="o">=</span> <span class="p">[</span><span class="n">xy</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span><span class="line">    <span class="n">yts</span> <span class="o">=</span> <span class="p">[</span><span class="n">xy</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
</span><span class="line">    <span class="n">plot_func</span><span class="p">(</span><span class="n">xts</span><span class="p">,</span> <span class="n">yts</span><span class="p">,</span> <span class="s">&#39;b&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">):</span>
</span><span class="line">        <span class="n">gxy</span> <span class="o">=</span> <span class="n">obj_func_grad</span><span class="p">(</span><span class="n">xy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xy</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line">        <span class="n">hxy</span> <span class="o">=</span> <span class="n">obj_func_hessian</span><span class="p">(</span><span class="n">xy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xy</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line">        <span class="n">deltax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">hxy</span><span class="p">),</span> <span class="n">gxy</span><span class="p">)</span>
</span><span class="line">        <span class="n">xy</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">deltax</span>
</span><span class="line">        <span class="n">xts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xy</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span><span class="line">        <span class="n">yts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xy</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line">        <span class="n">plot_func</span><span class="p">(</span><span class="n">xts</span><span class="p">,</span> <span class="n">yts</span><span class="p">,</span> <span class="s">&#39;b&#39;</span><span class="p">)</span>
</span><span class="line">
</span></code></pre></td></tr></table></div></figure>

<p>其中， <code>obj_func(x,y)</code> 為目標函數， <code>obj_func_grad(x,y)</code> 為 <script type="math/tex">\textbf{g}</script> ， <code>obj_func_hessian(x,y)</code>  <script type="math/tex">\textbf{H}</script> ，而 <code>plot_function(xt,yt,c='r')</code> 可畫出目標函數的等高線圖， <code>run_gd()</code> 用來執行 <em>Gradient Descent</em> ， <code>run_newtons()</code> 用來執行 <em>Newton’s Method</em> 。 <code>xy</code> 對應到前例的 <script type="math/tex">(x,y)</script> ，而 <code>eta</code> 為 <em>Learning Rate</em> 。 <code>for i in range(20)</code> 表示最多會跑20個迴圈。</p>

<p>到 python console 執行：</p>

<figure class="code"><figcaption><span>newtons.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">newtons</span>
</span></code></pre></td></tr></table></div></figure>

<p>執行 <em>Gradient Descent</em> ，指令如下：</p>

<figure class="code"><figcaption><span>newtons.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="o">&gt;&gt;&gt;</span> <span class="n">newtons</span><span class="o">.</span><span class="n">run_gd</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00153.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00154.png" alt="" /></p>

<p>以此類推</p>

<p>執行 <em>Newton’s Method</em> ，指令如下：</p>

<figure class="code"><figcaption><span>newtons.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="o">&gt;&gt;&gt;</span> <span class="n">netons</span><span class="o">.</span><span class="n">run_newtons</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00155.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00156.png" alt="" /></p>

<p>以此類推</p>

<h2 id="comment">Comment</h2>

<p><em>Newton’s Method</em> 需要計算二次微分 <em>Hessian</em> 矩陣的反矩陣，如果 <em>variable</em> 為高維度向量，則計算這個矩陣的時間複雜度會很高，而且很占記憶體空間，因此有人提出一些 <em>Hessian</em> 矩陣的近似求法，例如 <a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS"><em>L-BFGS</em></a> 。但如果用在像是 <em>Deep Learning</em> 這種有超多 <em>variable</em> 的模型，近似求法仍然太慢，因此解 <em>Deep Learning</em> 問題，通常只會用一次微分的方法，例如 <a href="http://ckmarkoh.github.io/blog/2015/12/23/optimization-method-adagrad"><em>Adagrad</em></a>之類的。</p>

<h2 id="reference">Reference</h2>

<p>本文參考至以下教科書：</p>

<p>Stephen Boyd &amp; Lieven Vandenberghe. Convex Optimization. Chapter 5 Duality.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Gradient Descent With Momentum]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/01/16/optimization-method-momentum/"/>
    <updated>2016-01-16T08:01:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/01/16/optimization-method-momentum</id>
    <content type="html"><![CDATA[<h2 id="gradient-descent">Gradient Descent</h2>

<p>在機器學習的過程中，常需要將 Cost Function 的值減小，通常用 Gradient Descent 來做最佳化的方法來達成。但是用 Gradient Descent 有其缺點，例如，很容易卡在 Local Minimum。</p>

<p><em>Gradient Descent</em> 的公式如下：</p>

<script type="math/tex; mode=display">

\textbf{x}_{t+1} \leftarrow \textbf{x}_{t} - \eta \textbf{g}_{t}

</script>

<p>關於Gradient Descent的公式解說，請參考：<a href="http://ckmarkoh.github.io/blog/2015/12/23/optimization-method-adagrad">Optimization Method – Gradient Descent &amp; AdaGrad</a></p>

<h2 id="getting-stuck-in-local-minimum">Getting Stuck in Local Minimum</h2>

<p>舉個例子，如果 Cost Function 為 <script type="math/tex">0.3y^{3}+y^{2}+0.3x^{3}+x^{2}</script> ，有 Local Minimum <script type="math/tex">(x=0,y=0)</script> ，畫出來的圖形如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00131.png" alt="" /></p>

<!--more-->

<p>當執行 Gradient Descent 的時候，則會卡在 Local Minimum，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00132.gif" alt="" /></p>

<p>解決卡在 Local Minimum 的方法，可加入 Momentum ，使它在 Gradient 等於零的時候，還可繼續前進。</p>

<h2 id="gradient-descent-with-momentum">Gradient Descent with Momentum</h2>

<p>Momentum 的概念如下： 當一顆球從斜坡上滾到平地時，球在平地仍會持續滾動，因為球具有動量，也就是說，它的速度跟上一個時間點的速度有關。</p>

<p>模擬 Momentum的方式很簡單，即是把上一個時間點用 Gradient 得出的變化量也考慮進去。</p>

<p><em>Gradient Descent with Momentum</em> 的公式如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \Delta_{\textbf{x},t+1 } \leftarrow  \beta \Delta_{\textbf{x},t } +  (1-\beta) \eta \textbf{g}_{t} 

& \text{, where }  0 <  \beta < 1 \\

\\

& \textbf{x}_{t+1} \leftarrow \textbf{x}_{t} - \Delta_{\textbf{x},t+1 } 

\end{align}

 %]]&gt;</script>

<p>其中 <script type="math/tex"> \Delta_{\textbf{x},t +1} </script> 為 <script type="math/tex">t+1</script> 時間點，修正 <script type="math/tex">\textbf{x}</script> 值所用的變化量，而 <script type="math/tex">\Delta_{\textbf{x},t }</script> 則是 <script type="math/tex">t</script> 時間點的修正量，而 <script type="math/tex"> \beta </script> 則是用來控制在 <script type="math/tex">t+1</script> 時間點中的 <script type="math/tex">\Delta_{\textbf{x},t+1}</script> 具有上個時間點的 <script type="math/tex">\Delta_{\textbf{x},t}</script> 值的比例。 好比說，在 <script type="math/tex">t+1</script> 時間點時，球的速度會跟 <script type="math/tex">t</script> 時間點有關。 而 <script type="math/tex">(1-\beta)</script> ，則是 <script type="math/tex">t+1</script> 時間點算出之 Gradient <script type="math/tex">\textbf{g}_{t}</script> 乘上 Learning Rate <script type="math/tex">\eta</script> 後，在 <script type="math/tex">\Delta_{\textbf{x},t+1}</script> 中所占的比例。</p>

<p>舉前述例子，若起始參數為 <script type="math/tex">(x=3,y=3)</script>  ，畫出目標函數，藍點為起始點 <script type="math/tex">(x,y)</script> 的位置：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00141.png" alt="" /></p>

<p>用 Gradient Descent with Momentum 來更新 <script type="math/tex">x,y</script> 的值，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \Delta_{x,t+1 } \leftarrow  \beta \Delta_{x,t } +  (1-\beta) \eta \dfrac{\partial f(x_{t},y_{t})}{\partial x_{t}} \\

& \Delta_{y,t+1 } \leftarrow  \beta \Delta_{y,t } +  (1-\beta) \eta \dfrac{\partial f(x_{t},y_{t})}{\partial y_{t}} \\

& x_{t+1} \leftarrow x_{t} - \Delta_{x,t+1 }  \\

& y_{t+1} \leftarrow y_{t} - \Delta_{y,t+1 } 


\end{align}

 %]]&gt;</script>

<p>化減後得：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \Delta_{x,t+1 } \leftarrow  \beta \Delta_{x,t } +  (1-\beta) \eta (0.9 x_{t}^{2} + 2x_{t} ) \\

& \Delta_{y,t+1 } \leftarrow  \beta \Delta_{y,t } +  (1-\beta) \eta (0.9 y_{t}^{2} + 2y_{t} ) \\

& x_{t+1} \leftarrow x_{t} - \Delta_{x,t+1 }  \\

& y_{t+1} \leftarrow y_{t} - \Delta_{y,t+1 } 


\end{align}

 %]]&gt;</script>

<p>設初始化值 <script type="math/tex">  \Delta_{x} = 0,  \Delta_{y} = 0 </script> ，參數 <script type="math/tex">\beta = 0.9, \eta = 0.2 </script> ，代入 <script type="math/tex"> x=3,y=3 </script> ，則：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \Delta_{x,1 } =  0.9 \times  0 +  (1-0.9)\times 0.2 \times (0.9\times 3^{2}+2\times 3) = 0.282 \\

& \Delta_{y,1 } =  0.9 \times  0 +  (1-0.9)\times 0.2 \times (0.9\times 3^{2}+2\times 3) = 0.282 \\

& x_{1} = 3 - \Delta_{x,1 } = 3 - 0.282 = 2.718 \\

& y_{1} = 3 - \Delta_{y,1 } = 3 - 0.282 = 2.718

\end{align}

 %]]&gt;</script>

<p>更新圖上的藍點，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00142.png" alt="" /></p>

<p>再往下走一步， <script type="math/tex"> x,y </script>  的值如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \Delta_{x,2 } =  0.9 \times  0.282 +  (1-0.9)\times 0.2 \times (0.9\times 2.718^{2}+2\times 2.718) = 0.4955 \\

& \Delta_{y,2 } =  0.9 \times  0.282 +  (1-0.9)\times 0.2 \times (0.9\times 2.718^{2}+2\times 2.718) = 0.4955\\

& x_{2} = 3 - \Delta_{x,2 } = 2.718  - 0.4955 = 2.2225 \\

& y_{2} = 3 - \Delta_{y,2 } = 2.718  - 0.4955 = 2.2225

\end{align}

 %]]&gt;</script>

<p>更新圖上的藍點，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00143.png" alt="" /></p>

<p>在以上兩步中，可發現 <script type="math/tex"> \Delta_{x }, \Delta_{y }</script> 的值逐漸變大。由於一開始 <script type="math/tex"> \Delta_{x }, \Delta_{y }</script> 都是零，它會跟前一個時間點的值有關，所以看起來就好像是球從斜坡上滾下來時，慢慢加速，而在球經過 Local Minimum時，也會慢慢減速，不會直接卡在 Local Minimum 。整個過程如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00136.png" alt="" /></p>

<p>動畫版：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00137.gif" alt="" /></p>

<h2 id="implementation">Implementation</h2>

<p>再來進入實作的部分</p>

<p>首先，開啟新的檔案 momentum.py 並貼上以下程式碼：</p>

<figure class="code"><figcaption><span>momentum.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
</span><span class="line">  <span class="k">return</span> <span class="p">(</span><span class="mf">0.3</span><span class="o">*</span><span class="n">y</span><span class="o">**</span><span class="mi">3</span><span class="o">+</span><span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="mf">0.3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="o">+</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">func_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
</span><span class="line">  <span class="k">return</span> <span class="p">(</span><span class="mf">0.9</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.9</span><span class="o">*</span><span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">y</span> <span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s">&#39;r&#39;</span><span class="p">):</span>
</span><span class="line">  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
</span><span class="line">  <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">&#39;3d&#39;</span><span class="p">,</span>
</span><span class="line">        <span class="n">elev</span><span class="o">=</span><span class="mf">7.</span><span class="p">,</span> <span class="n">azim</span><span class="o">=-</span><span class="mi">175</span><span class="p">)</span>
</span><span class="line">  <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">))</span>
</span><span class="line">  <span class="n">Z</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
</span><span class="line">  <span class="n">surf</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">rstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span class="line">    <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</span><span class="line">  <span class="n">ax</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
</span><span class="line">  <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">,</span> <span class="n">func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">),</span><span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">&#39;o&#39;</span> <span class="p">)</span>
</span><span class="line">  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">&quot;x=</span><span class="si">%.5f</span><span class="s">, y=</span><span class="si">%.5f</span><span class="s">, f(x,y)=</span><span class="si">%.5f</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="n">func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">)))</span>
</span><span class="line">  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span><span class="line">  <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">run_grad</span><span class="p">():</span>
</span><span class="line">  <span class="n">xt</span> <span class="o">=</span> <span class="mi">3</span>
</span><span class="line">  <span class="n">yt</span> <span class="o">=</span> <span class="mi">3</span>
</span><span class="line">  <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
</span><span class="line">  <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
</span><span class="line">    <span class="n">gxt</span><span class="p">,</span> <span class="n">gyt</span> <span class="o">=</span> <span class="n">func_grad</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">)</span>
</span><span class="line">    <span class="n">xt</span> <span class="o">=</span> <span class="n">xt</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">gxt</span>
</span><span class="line">    <span class="n">yt</span> <span class="o">=</span> <span class="n">yt</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">gyt</span>
</span><span class="line">    <span class="k">if</span> <span class="n">xt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">xt</span> <span class="o">&gt;</span> <span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">:</span>
</span><span class="line">      <span class="k">break</span>
</span><span class="line">    <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">run_momentum</span><span class="p">():</span>
</span><span class="line">  <span class="n">xt</span> <span class="o">=</span> <span class="mi">3</span>
</span><span class="line">  <span class="n">yt</span> <span class="o">=</span> <span class="mi">3</span>
</span><span class="line">  <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.2</span>
</span><span class="line">  <span class="n">beta</span> <span class="o">=</span> <span class="mf">0.9</span>
</span><span class="line">  <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;b&#39;</span><span class="p">)</span>
</span><span class="line">  <span class="n">delta_x</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">  <span class="n">delta_y</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
</span><span class="line">    <span class="n">gxt</span><span class="p">,</span> <span class="n">gyt</span> <span class="o">=</span> <span class="n">func_grad</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">)</span>
</span><span class="line">    <span class="n">delta_x</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">delta_x</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta</span><span class="p">)</span><span class="o">*</span><span class="n">eta</span><span class="o">*</span><span class="n">gxt</span>
</span><span class="line">    <span class="n">delta_y</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">delta_y</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta</span><span class="p">)</span><span class="o">*</span><span class="n">eta</span><span class="o">*</span><span class="n">gyt</span>
</span><span class="line">    <span class="n">xt</span> <span class="o">=</span> <span class="n">xt</span> <span class="o">-</span> <span class="n">delta_x</span>
</span><span class="line">    <span class="n">yt</span> <span class="o">=</span> <span class="n">yt</span> <span class="o">-</span> <span class="n">delta_y</span>
</span><span class="line">    <span class="k">if</span> <span class="n">xt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">xt</span> <span class="o">&gt;</span> <span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">:</span>
</span><span class="line">      <span class="k">break</span>
</span><span class="line">    <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;b&#39;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>其中， <code>func(x,y)</code> 為目標函數，<code>func_grad(x,y)</code> 為目標函數的 <em>gradient</em> ，而 <code>plot_func(xt,yt,c='r')</code> 可畫出目標函數的曲面， <code>run_grad()</code> 用來執行 <em>Gradient Descent</em> ， <code>run_momentum()</code> 用來執行 <em>Gradient Descent with Momentum</em> 。 <code>xt</code> 和 <code>yt</code> 對應到前例的 <script type="math/tex">(x,y)</script> ，而 <code>eta</code> 為 <em>Learning Rate</em> 。 <code>for i in range(20)</code> 表示最多會跑20個迴圈，而 <code>if xt &lt; -5 or yt &lt; -5 or xt &gt; 5 or yt &gt; 5</code> 表示，如果 <code>xt</code> 和 <code>yt</code> 超出邊界，則會先結束迴圈。</p>

<p>到 python console 執行：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; import momentum
</span></code></pre></td></tr></table></div></figure>

<p>執行 <em>Gradient Descent</em> ，指令如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; momentum.run_grad<span class="o">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00138.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00139.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00140.png" alt="" /></p>

<p>以此類推</p>

<p>執行 <em>Gradient Descent with Momentum</em> ，指令如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; momentum.run_momentum<span class="o">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00141.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00142.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00143.png" alt="" /></p>

<p>以此類推</p>

<h2 id="reference">Reference</h2>

<h4 id="visualizing-optimization-algos">Visualizing Optimization Algos</h4>

<p>http://imgur.com/a/Hqolp</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Gradient Descent & AdaGrad]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/12/23/optimization-method-adagrad/"/>
    <updated>2015-12-23T17:14:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/12/23/optimization-method-adagrad</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>在機器學習的過程中，常需要將 <em>Cost Function</em> 的值減小，需由最佳化的方法來達成。本文介紹 <em>Gradient Descent</em> 和 <em>AdaGrad</em> 兩種常用的最佳化方法。</p>

<h2 id="gradient-descent">Gradient Descent</h2>

<p><em>Gradient Descent</em> 的公式如下：</p>

<script type="math/tex; mode=display">

\textbf{x}_{t+1} \leftarrow \textbf{x}_{t} - \eta \textbf{g}_{t}

</script>

<p>其中， <script type="math/tex">\eta</script> 為 <em>Learning Rate</em> ， <script type="math/tex">\textbf{x} </script> 為最佳化時要調整的參數， <script type="math/tex">\textbf{g}</script> 為最佳化目標函數對 <script type="math/tex">\textbf{x}</script> 的梯度。 <script type="math/tex">\textbf{x}_{t}</script> 為調整之前的 <script type="math/tex">\textbf{x} </script> ，<script type="math/tex">\textbf{x}_{t+1}</script> 為調整之後的 <script type="math/tex">\textbf{x} </script> 。</p>

<p>舉個例子，如果目標函數為 <script type="math/tex"> f(x,y) = y^2 - x^2  </script> ，起始參數為 <script type="math/tex">(x,y) = (0.001,4)</script> ，則畫出來的圖形如下圖，曲面為目標函數，紅色的點為起始參數：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00126.png" alt="" /></p>

<!--more-->

<p>可藉由改變 <script type="math/tex">(x,y)</script> 來讓 <script type="math/tex">f(x,y)</script> 的值減小。 <em>Gradient Descent</em> 所走的方向為梯度最陡的方向，若 <script type="math/tex">eta=0.3</script> 則 ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& 	x \leftarrow  x - \eta  \dfrac{\partial f(x,y)}{\partial x}  \\

&  y \leftarrow  y - \eta  \dfrac{\partial f(x,y)}{\partial y} \\

\end{align}

 %]]&gt;</script>

<p>求出微分後得：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& 	x \leftarrow  x - \eta  \times (-2x)  \\

&  y \leftarrow  y - \eta  \times 2y \\

\end{align}

 %]]&gt;</script>

<p>代入數值，得：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x = 0.001 - 0.3 \times (-2) \times 0.001 = 0.0016 \\

& y = 4 - 0.3 \times 2 \times 4 = 1.6 \\

\end{align}

 %]]&gt;</script>

<p>更新完後的結果如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00127.png" alt="" /></p>

<p>從上圖可看出，紅點移動到比較低的地方，即 <script type="math/tex">f(x,y)</script> 變小了。</p>

<p>經過了數次改變 <script type="math/tex">(x,y)</script> 值的循環之後，<script type="math/tex">f(x,y)</script> 的值會越變越小，紅點移動的路徑如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00118.png" alt="" /></p>

<p>動畫版：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00119.gif" alt="" /></p>

<p>從上圖可發現，紅色的點會卡在 <script type="math/tex">(0,0)</script> 附近（也就是Saddle Point），過了一陣子後才會繼續往下滾。</p>

<h2 id="adagrad">AdaGrad</h2>

<p><em>Gradient Descent</em> 的缺點有：</p>

<p>(1) <em>Learning Rate</em> 不會隨著時間而減少</p>

<p>(2) <em>Learning Rate</em> 在每個方向是固定的</p>

<p>以上的(1)會使得在越接近近目標函數最小值時，越容易走過頭，(2)則會容易卡在目標函數的Saddle Point。</p>

<p>因為 <em>Gradient Descent</em> 只考慮目前的 <em>Gradient</em> ，如果可以利用過去時間在各個方向的 <em>Gradient</em> ，來調整現在時間點在各個方向的 <em>Learning Rate</em> ，則可避免以上兩種情型發生。</p>

<p><em>AdaGrad</em> 的公式如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{G}_{t} = \sum_{n=0}^{t} \textbf{g}_{n}^{2} \\

& \textbf{x}_{t+1} \leftarrow \textbf{x}_{t} - \frac{\eta}{\sqrt{\textbf{G}_{t}}} \textbf{g}_{t} \\

\end{align}

 %]]&gt;</script>

<p>其中，<script type="math/tex"> \textbf{G}_{t} </script> 為過去到現在所有時間點所有的 <script type="math/tex">\textbf{g}</script> 的平方和。由於  <script type="math/tex">\textbf{x}</script> ， <script type="math/tex">\textbf{g}</script>和 <script type="math/tex">\textbf{G}</script> 皆為向量，設 <script type="math/tex">x_{i}</script> ， <script type="math/tex">g_{i}</script> 和 <script type="math/tex">G_{i}</script> 各為其元素，則公式可寫成：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& G_{i,t} = \sum_{n=0}^{t} g_{i,n}^{2} \\

& x_{i,t+1} \leftarrow x_{i,t} - \frac{\eta}{\sqrt{G_{i,t}}} g_{i,t} \\

\end{align}

 %]]&gt;</script>

<p>這公式可修正以上兩個 <em>Gradient Descent</em> 的缺點：</p>

<p>1.若時間越久，則 <em>Gradient</em> 平方和越大，使得 <em>Learning Rate</em> 越小，這樣就可以讓 <em>Learning Rate</em> 隨著時間減少，而在接近目標函數的最小值時，比較不會走過頭。</p>

<p>2.若某方向從過去到現在時間點 <em>Gradient</em> 平方和越小，則 <em>Learning Rate</em> 要越大。（直覺上來講，過去時間點 <em>Gradient</em> 越小的方向，在未來可能越重要，這種概念有點類似<a href="http://ckmarkoh.github.io/blog/2014/04/14/natural-language-processing-tf-idf">tf-idf</a>，在越少文檔中出現的詞，可能越重要。）由於各方向的 <em>Learning Rate</em> 不同，比較不會卡在 <em>Saddle Point</em> 。</p>

<p>前述例子，起始參數為 <script type="math/tex">(x,y) = (0.001,4)</script> ，則畫出來的圖形如下圖，曲面為目標函數，藍點為起始參數：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00128.png" alt="" /></p>

<p>用 <em>AdaGrad</em> 來更新 <script type="math/tex">(x,y)</script> 的值，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& 	x_{t+1} \leftarrow  x_{t} - \frac{\eta}{\sqrt{\sum_{n=0}^{t}  

(\dfrac{\partial f(x_{n},y_{n})}{\partial x_{n}} )^{2} }} 

\dfrac{\partial f(x_{t},y_{t})}{\partial x_{t}}  \\


&  y_{t+1} \leftarrow  y_{t} - \frac{\eta}{\sqrt{\sum_{n=0}^{t}  

(\dfrac{\partial f(x_{n},y_{n})}{\partial y_{n}} )^{2}  }} 

\dfrac{\partial f(x_{t},y_{t})}{\partial y_{t}} \\

\end{align}

 %]]&gt;</script>

<p>化簡後得：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& 	x_{t+1} \leftarrow  x_{t} - \frac{\eta}{\sqrt{\sum_{n=0}^{t}  

( -2x_{n} )^{2} }} 

( -2x_{t} ) \\


&  y_{t+1} \leftarrow  y_{t} - \frac{\eta}{\sqrt{\sum_{n=0}^{t}  

( 2y_{n} )^{2} }} 

( 2y_{t} ) \\

\end{align}

 %]]&gt;</script>

<p>由於 <em>AdaGrad</em> 的 <em>Learning Rate</em> 會隨時間減小，所以初始化時可以給它較大的值，此例中，設 <script type="math/tex">\eta = 1.0</script></p>

<p>代入 <script type="math/tex">(x,y)</script> 的數值，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x = 0.001 -  \frac{1.0}{\sqrt{  ( (-2) \times 0.001 )^2  }} \times (-2) \times 0.001 = 1.001 \\

& x = 4 -  \frac{1.0}{\sqrt{  ( 2 \times 4 )^2  }} \times 2 \times 4 = 3 \\

\end{align}

 %]]&gt;</script>

<p>更新圖上的藍點，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00129.png" alt="" /></p>

<p>再往下走一步， <script type="math/tex">(x,y)</script> 的值如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x = 1.001 -  \frac{1.0}{\sqrt{  ( (-2) \times 0.001 )^2  + ( (-2) \times 1.001 )^2 }} \times (-2) \times 1.001 = 2.001 \\

& x = 3 -  \frac{1.0}{\sqrt{  ( 2 \times 4 )^2 +  ( 2 \times 3 )^2  }} \times 2 \times 3 = 2.4 \\

\end{align}

 %]]&gt;</script>

<p>更新圖上的藍點，如下圖：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00130.png" alt="" /></p>

<p>經過了數次改變 <script type="math/tex">(x,y)</script> 值的循環之後，<script type="math/tex">f(x,y)</script> 的值會越變越小，藍點移動的路徑如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00123.png" alt="" /></p>

<p>動畫版：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00124.gif" alt="" /></p>

<p>由此可以發現， <em>AdaGrad</em> 不會卡在 <em>Saddle Point</em> 。</p>

<p>將 <em>Gradient Descent</em> 和 <em>AdaGrad</em> 畫在同一張圖上，比較兩者差異：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00125.gif" alt="" /></p>

<h2 id="implementation">Implementation</h2>

<p>再來進入實作的部分：</p>

<p>首先,開啟新的檔案 adagrad.py 並貼上以下程式碼</p>

<figure class="code"><figcaption><span>adagrad.py</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
</span><span class="line">  <span class="k">return</span> <span class="p">(</span><span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">func_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
</span><span class="line">  <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">y</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s">&#39;r&#39;</span><span class="p">):</span>
</span><span class="line">  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
</span><span class="line">  <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">&#39;3d&#39;</span><span class="p">,</span>
</span><span class="line">        <span class="n">elev</span><span class="o">=</span><span class="mf">35.</span><span class="p">,</span> <span class="n">azim</span><span class="o">=-</span><span class="mi">30</span><span class="p">)</span>
</span><span class="line">  <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">))</span>
</span><span class="line">  <span class="n">Z</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
</span><span class="line">  <span class="n">surf</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">rstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span class="line">    <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</span><span class="line">  <span class="n">ax</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="o">-</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
</span><span class="line">  <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">,</span> <span class="n">func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">),</span><span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">&#39;o&#39;</span> <span class="p">)</span>
</span><span class="line">  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">&quot;x=</span><span class="si">%.5f</span><span class="s">, y=</span><span class="si">%.5f</span><span class="s">, f(x,y)=</span><span class="si">%.5f</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="n">func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">)))</span>
</span><span class="line">  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span><span class="line">  <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">run_grad</span><span class="p">():</span>
</span><span class="line">  <span class="n">xt</span> <span class="o">=</span> <span class="mf">0.001</span>
</span><span class="line">  <span class="n">yt</span> <span class="o">=</span> <span class="mi">4</span>
</span><span class="line">  <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.3</span>
</span><span class="line">  <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
</span><span class="line">    <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span> <span class="o">=</span> <span class="n">func_grad</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">)</span>
</span><span class="line">    <span class="n">xt</span> <span class="o">=</span> <span class="n">xt</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">gx</span>
</span><span class="line">    <span class="n">yt</span> <span class="o">=</span> <span class="n">yt</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">gy</span>
</span><span class="line">    <span class="k">if</span> <span class="n">xt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">xt</span> <span class="o">&gt;</span> <span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">:</span>
</span><span class="line">      <span class="k">break</span>
</span><span class="line">    <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">run_adagrad</span><span class="p">():</span>
</span><span class="line">  <span class="n">xt</span> <span class="o">=</span> <span class="mf">0.001</span>
</span><span class="line">  <span class="n">yt</span> <span class="o">=</span> <span class="mi">4</span>
</span><span class="line">  <span class="n">eta</span> <span class="o">=</span> <span class="mf">1.0</span>
</span><span class="line">  <span class="n">Gxt</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">  <span class="n">Gyt</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">  <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;b&#39;</span><span class="p">)</span>
</span><span class="line">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
</span><span class="line">    <span class="n">gxt</span><span class="p">,</span><span class="n">gyt</span> <span class="o">=</span> <span class="n">func_grad</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">)</span>
</span><span class="line">    <span class="n">Gxt</span> <span class="o">+=</span> <span class="n">gxt</span><span class="o">**</span><span class="mi">2</span>
</span><span class="line">    <span class="n">Gyt</span> <span class="o">+=</span> <span class="n">gyt</span><span class="o">**</span><span class="mi">2</span>
</span><span class="line">    <span class="n">xt</span> <span class="o">=</span> <span class="n">xt</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="p">(</span><span class="n">Gxt</span><span class="o">**</span><span class="mf">0.5</span><span class="p">))</span><span class="o">*</span><span class="n">gxt</span>
</span><span class="line">    <span class="n">yt</span> <span class="o">=</span> <span class="n">yt</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="p">(</span><span class="n">Gyt</span><span class="o">**</span><span class="mf">0.5</span><span class="p">))</span><span class="o">*</span><span class="n">gyt</span>
</span><span class="line">    <span class="k">if</span> <span class="n">xt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">5</span> <span class="ow">or</span> <span class="n">xt</span> <span class="o">&gt;</span> <span class="mi">5</span> <span class="ow">or</span> <span class="n">yt</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">:</span>
</span><span class="line">      <span class="k">break</span>
</span><span class="line">    <span class="n">plot_func</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="s">&#39;b&#39;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>其中， <code>func(x,y)</code> 為目標函數，<code>func_grad(x,y)</code> 為目標函數的 <em>gradient</em> ，而 <code>plot_func(xt,yt,c='r')</code> 可畫出目標函數的曲面， <code>run_grad()</code> 用來執行 <em>Gradient Descent</em> ， <code>run_adagrad()</code> 用來執行 <em>AdaGrad</em> 。 <code>xt</code> 和 <code>yt</code> 對應到前例的 <script type="math/tex">(x,y)</script> ，而 <code>eta</code> 為 <em>Learning Rate</em> 。 <code>for i in range(20)</code> 表示最多會跑20個迴圈，而 <code>if xt &lt; -5 or yt &lt; -5 or xt &gt; 5 or yt &gt; 5</code> 表示，如果 <code>xt</code> 和 <code>yt</code> 超出邊界，則會先結束迴圈。</p>

<p>到 python console 執行：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; import adagrad
</span></code></pre></td></tr></table></div></figure>

<p>執行 <em>Gradient Descent</em> ，指令如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; adagrad.run_grad<span class="o">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00126.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00127.png" alt="" /></p>

<p>以此類推</p>

<p>執行 <em>Adagrad</em> ，指令如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; adagrad.run_adagrad<span class="o">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00128.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00129.png" alt="" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00130.png" alt="" /></p>

<p>以此類推</p>

<h2 id="reference">Reference</h2>

<h4 id="notes-on-adagrad">Notes on AdaGrad</h4>

<p>http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf</p>

<h4 id="visualizing-optimization-algos">Visualizing Optimization Algos</h4>

<p>http://imgur.com/a/Hqolp</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Duality & KKT Conditions in Convex Optimization]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/11/05/convex-optimization-duality-and-kkt-conditions/"/>
    <updated>2015-11-05T13:18:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/11/05/convex-optimization-duality-and-kkt-conditions</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>在解「 <strong>有條件的最佳化問題</strong> 」時，有時需要把原本的問題轉換成對偶問題(Dual Problem)後，會比較好解。</p>

<p>如果對偶問題有最佳解，原本問題也有最佳解，且這兩個最佳解相同，則必須要滿足 <em>Karush-Kuhn-Tucker (KKT) Conditions</em>：</p>

<ol>
  <li>
    <p>Primal Feasibility </p>
  </li>
  <li>
    <p>Dual Feasibility</p>
  </li>
  <li>
    <p>Complementary Slackness</p>
  </li>
  <li>
    <p>Stationarity</p>
  </li>
</ol>

<p>至於這四項到底是什麼？講起來有點複雜。本文會先從對偶問題的概念開始介紹，再來講解這四個條件。</p>

<h2 id="the-lagrange-dual-function">The Lagrange dual function</h2>

<p>首先，講解一下什麼是對偶問題。</p>

<p>通常，有條件的最佳化問題，可寫成 <a name="eq1">＜公式一＞</a> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{aligned}

& \textbf{minimize} & f_{0}(x) & \\

& \textbf{subject to } & f_{i}(x) \leq 0 ,& i=1, ... ,m \\

& & h_{j}(x) = 0 , &j=1, ... ,p \\

\end{aligned}

 %]]&gt;</script>

<!--more-->

<p>其中, <script type="math/tex">f_{0}(x)</script> 是目標函數，而 <script type="math/tex"> f_{i}(x) \leq 0</script> 為不等式的條件限制， <script type="math/tex">h_{j}(x) = 0</script> 為等式的條件限制。也就是說，最佳化的目標是要求出 <script type="math/tex">x</script> 讓 <script type="math/tex">f_{0}(x)</script> 得出最小值，而這個 <script type="math/tex">x</script> ，必須滿足 <script type="math/tex"> f_{i}(x) \leq 0</script> 和 <script type="math/tex">h_{j}(x) = 0</script> 所限定的條件。若滿足這兩項條件，則表示這個最佳化問題有解，即為滿足 <strong>primal feasibility</strong> 。</p>

<p>此最佳化問題可以轉換成對偶問題，如下：</p>

<script type="math/tex; mode=display">

L(x, \lambda, \nu) = f_{0}(x) + \sum_{i=1}^{m} \lambda_{i} f_{i}(x) + \sum_{j=1}^{p} \nu_{j} h_{j}(x)

</script>

<p>其中，我們把 <script type="math/tex">L(x, \lambda, \nu)</script> 稱為 <em>Lagrangian</em> ，<script type="math/tex"> \lambda_{i} </script> 和 <script type="math/tex">\nu_{j}</script> 稱為 <em>Lagrange multiplier</em> 。</p>

<p>所謂的對偶函數( <em>Lagrange dual function</em> )，即是在給定了 <em>Lagrange multiplier</em> 之下，改變 <script type="math/tex">x</script> 來得出 <em>Lagrangian</em> 最小值，如下：</p>

<script type="math/tex; mode=display">

g(\lambda, \nu) = \inf_{x}( L(x, \lambda, \nu) ) = \inf_{x}( f_{0}(x) + \sum_{i=1}^{m} \lambda_{i} f_{i}(x) + \sum_{j=1}^{p} \nu_{j} h_{j}(x) )

</script>

<p>其中， <script type="math/tex">g(\lambda, \nu) </script> 為對偶函數，而 <script type="math/tex">\inf_{x}</script> 即是在改變 <script type="math/tex">x</script> 的情況下，找出最小值。</p>

<h2 id="lower-bounds-on-optimal-value">Lower bounds on optimal value</h2>

<p>接著來證明，對偶函數可當作原本問題的 <em>Lower Bound</em> 。設原本問題<a href="#eq1">公式一</a>的最佳解為 <script type="math/tex">p^{\star}</script> ，則在所有 <script type="math/tex"> \lambda_{i} \geq 0 </script> （記作 <script type="math/tex">\lambda \succeq 0 </script> ）和  <script type="math/tex">\nu_{j}</script> 為任意數的情況下，會滿足以下條件：</p>

<script type="math/tex; mode=display">

g(\lambda, \nu) \leq p^{\star}

</script>

<p>此性質不難證明，對於所有滿足 <script type="math/tex"> f_{i}(\tilde{x}) \leq 0</script> 和 <script type="math/tex">h_{j}(\tilde{x}) = 0</script> 這兩個條件的 <script type="math/tex">\tilde{x}</script> ，必滿足：</p>

<script type="math/tex; mode=display">

\sum_{i=1}^{m} \lambda_{i} f_{i}(\tilde{x}) + \sum_{j=1}^{p} \nu_{j} h_{j}(\tilde{x}) \leq 0

</script>

<p>則：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{aligned}

& g(\lambda, \nu) = \inf_{x}( L(x, \lambda, \nu) ) = \inf_{x}( f_{0}(x) + \sum_{i=1}^{m} \lambda_{i} f_{i}(x) + \sum_{j=1}^{p} \nu_{j} h_{j}(x) ) \\

& \leq L(\tilde{x}, \lambda, \nu) =  f_{0}(\tilde{x}) + \sum_{i=1}^{m} \lambda_{i} f_{i}(\tilde{x}) + \sum_{j=1}^{p} \nu_{j} h_{j}(\tilde{x}) \\

&\leq  f_{0}(\tilde{x})

\end{aligned}

 %]]&gt;</script>

<p>設原本問題<a href="#eq1">公式一</a>最佳解時的 <script type="math/tex">x</script> 為 <script type="math/tex">x^{\star}</script> ，由於 <script type="math/tex">x^{\star}</script> 必須滿足 滿足 <script type="math/tex"> f_{i}(x^{\star}) \leq 0</script> 和 <script type="math/tex">h_{j}(x^{\star}) = 0</script> 這兩個條件，又因 <script type="math/tex">\lambda \succeq 0 </script> ，故 <script type="math/tex">g(\lambda, \nu) \leq p^{\star}</script> 成立。</p>

<p>舉個例子，下圖中，黑色的實線為目標函數 <script type="math/tex">f_{0}(x)</script> ，此最佳化問題有一個不等式條件限制 <script type="math/tex">f_{1}(x)</script> ，用綠色虛線表示，而滿足於  <script type="math/tex">f_{1}(x) \leq 0 </script> 的 <script type="math/tex">x</script> 範圍落在 <script type="math/tex">[-0.46~0.46]</script> 之間，範圍用兩條鉛直的紅色虛線表示。 紫色的圓圈為最佳解的點 <script type="math/tex">x^{\star} = -0.46, p^{\star} = 1.54</script> 。此最佳化問題沒有等式條件，故沒有 <script type="math/tex">h_{j}</script> 及  <script type="math/tex">\nu_{j}</script> 。則此問題的 <em>Lagrangian</em> 為 <script type="math/tex">L(x, \lambda)</script> 。藍色的點為 <script type="math/tex">\lambda = 0.1, 0.2, ... , 1.0 </script> 所畫出來的 <em>Lagrangian</em> 圖形。 </p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00114.png" alt="" /></p>

<p>在上圖中，兩條紅色虛線之間的範圍內，即 <script type="math/tex">x</script> 滿足不等式的條件限制 <script type="math/tex">f_{1}(x) \leq 0</script> ，此時藍色的虛線皆位於黑色線的下方，滿足 <script type="math/tex">L(x, \lambda) \leq f_{0}(x)</script> 。 </p>

<p>下圖畫出改變不同 <script type="math/tex">\lambda</script> 值時，對偶函數 <script type="math/tex"> g(\lambda) = \inf_{x}( L(x, \lambda) ) </script> 的值，其中，黑色的實線為 <script type="math/tex"> g(\lambda) </script> ，紫色的虛線為 <script type="math/tex">p^{\star}</script> 的值（<script type="math/tex">p^{\star} = 1.54</script> ）。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00115.png" alt="" /></p>

<p>根據此圖，黑色的線恆在紫色的虛線下方，滿足 <script type="math/tex">g(\lambda) \leq p^{\star}</script> 。</p>

<h2 id="the-lagrange-dual-problem">The Lagrange dual problem</h2>

<p>所謂的對偶問題（Lagrange dual problem）即是把原本的問題轉成對偶函數之後，來解最佳化問題。</p>

<p>從前面結論可得知，對偶函數若滿足 <script type="math/tex">\lambda \succeq 0 </script> （也就是所有的 <script type="math/tex">\lambda_{i}</script> 都滿足 <script type="math/tex">\lambda_{i} \geq 0</script> ）的條件，則必足 <script type="math/tex">g(\lambda, \nu) \leq p^{\star}</script> 。如果想找出最接近 <script type="math/tex">p^{\star}</script> 的對偶函數解，則可藉由改變 <script type="math/tex">\lambda, \nu</script> ，來找出 <script type="math/tex">g(\lambda, \nu) </script> 的最大值。此對偶問題如下 <a name="eq2">＜公式二＞</a>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{aligned}

& \textbf{maximize} & g(\lambda, \nu) & \\

& \textbf{subject to } & \lambda \succeq 0 \\

\end{aligned}

 %]]&gt;</script>

<p>如果可以找出一組解，滿足 <script type="math/tex">\lambda \succeq 0 </script> ，可得出 <script type="math/tex">g(\lambda, \nu) </script> 的最大值 <script type="math/tex">d^{\star}</script> ，則此問題滿足 <strong>dual feasibility</strong>。</p>

<p>根據對偶問題的解 <script type="math/tex">d^{\star}</script> 和原本問題的解 <script type="math/tex">p^{\star}</script> 的關係，可將對偶性質分為 <em>weak duality</em> 和  <em>strong duality</em> 兩類：</p>

<h4 id="weak-duality">Weak duality</h4>

<p>若對偶問題的解，小於或等於原本問題的解，即滿足 <em>weak duality</em> ：</p>

<script type="math/tex; mode=display">

d^{\star} \leq p^{\star} 

</script>

<p>根據前面段落 <em>Lower bounds on optimal value</em> 所推導的結論， <em>Weak duality</em> 必定成立。</p>

<h4 id="strong-duality">Strong duality</h4>

<p>若對偶問題的解，等於原本問題的解，即滿足 <em>strong duality</em> ：</p>

<script type="math/tex; mode=display">

d^{\star} = p^{\star} 

</script>

<p>這個條件不一定會成立，如果要成立的話，原本問題<a href="#eq1">公式一</a>的中的 <script type="math/tex">f_{0}(x)</script> 和 <script type="math/tex">f_{i}(x)</script> 要是凸函數，但即使滿足此條件，仍須滿足其他條件才能使 <em>strong duality</em> 成立，這講起來比較複雜，在此先不提。</p>

<h2 id="complementary-slackness">Complementary Slackness</h2>

<p>設原本問題最佳解的 <script type="math/tex">x</script> 為 <script type="math/tex">x^{\star}</script> ，對偶問題最佳解的 <script type="math/tex">(\lambda, \nu)</script> 為 <script type="math/tex">(\lambda^{\star}, \nu^{\star}) </script> ，根據對偶函數 <script type="math/tex">g(\lambda^{\star}, \nu^{\star})</script> 的定義，和前面段落 <em>Lower bounds on optimal value</em> 所推導出的結論，可得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{aligned}

& g(\lambda^{\star}, \nu^{\star}) = \inf_{x}( L(x, \lambda^{\star}, \nu^{\star}) ) \\

& \leq f_{0}(x^{\star}) + \sum_{i=1}^{m} \lambda_{i}^{\star} f_{i}(x^{\star}) + \sum_{j=1}^{p} \nu_{j}^{\star} h_{j}(x^{\star})  \\

& \leq f_{0}(x^{\star}) 

\end{aligned}

 %]]&gt;</script>

<p>若對偶問題滿足 <em>strong duality</em> ：</p>

<script type="math/tex; mode=display">

g(\lambda^{\star}, \nu^{\star}) =  f_{0}(x^{\star})  

</script>

<p>則須滿足：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{aligned}

& \sum_{i=1}^{m} \lambda_{i}^{\star} f_{i}(x^{\star})  = 0 \mspace{40mu} \text{(a)} \\

& \sum_{j=1}^{p} \nu_{j}^{\star} h_{j}(x^{\star}) = 0 \mspace{40mu} \text{(b)}

\end{aligned}

 %]]&gt;</script>

<p>由於 <script type="math/tex">h_{j}(x^{\star}) = 0 </script> 為原本問題的等式條件限制，故 <strong>(b)</strong> 必會成立，若 <strong>(a)</strong> 要成立的話，須滿足：</p>

<script type="math/tex; mode=display">

\lambda_{i}^{\star} f_{i}(x^{\star})  = 0  \mspace{20mu} \textbf{for } i=1,2,...m

</script>

<p>也就是說，<script type="math/tex">f_{i}(x^{\star}) </script> 和  <script type="math/tex">\lambda_{i}^{\star}</script> 的其中一項必須為零，不可以兩項都不為零。這種情形稱為 <strong>complementary slackness</strong> ，也就是林軒田教授在<a href="https://www.coursera.org/course/ntumltwo">機器學習技法</a>課程中所提到的：</p>

<blockquote>
  <p>哈利波特和佛地魔，其中一個必須死掉。</p>
</blockquote>

<h2 id="karush-kuhn-tucker-kkt-conditions">Karush-Kuhn-Tucker (KKT) conditions</h2>

<p><em>KKT conditions</em> 即為下四個條件：</p>

<ol>
  <li>
    <p>Primal Feasibility：即滿足原本問題<a href="#eq1">公式一</a>的限制條件 <script type="math/tex">f_{i}(x) \leq 0</script> 和 <script type="math/tex">h_{j}(x) = 0 </script>  ，使原本問題有解。</p>
  </li>
  <li>
    <p>Dual Feasibility：即滿足對偶問題<a href="#eq2">公式二</a>的限制條件： <script type="math/tex"> \lambda \succeq 0 </script> ，使對偶問題有解。</p>
  </li>
  <li>
    <p>Complementary Slackness：即滿足 <em>strong duality</em> ：<script type="math/tex"> g(\lambda^{\star}, \nu^{\star}) =  f_{0}(x^{\star})  </script> ，使原本問題和對偶問題有相同解。</p>
  </li>
  <li>
    <p>Stationarity（gradient of Lagrangian with respect to x vanishes）：</p>
  </li>
</ol>

<script type="math/tex; mode=display">

\nabla f_{0}(x^{\star}) + \sum_{i=1}^{m} \lambda_{i} \nabla f_{i}(x^{\star}) + \sum_{j=1}^{p} \nu_{j} \nabla h_{j}(x^{\star}) = 0

</script>

<p>第四項雖然前面沒提到，但很容易理解，也就是說，在得出最佳解 <script type="math/tex">x^{\star}</script> 的時候， <em>Lagrangian</em> 對於 <script type="math/tex">x^{\star}</script> 的 <em>gradient</em> 要等於 0 。</p>

<h2 id="reference">Reference</h2>

<p>本文參考至以下教科書，本文中的圖片也取自於以下教科書：</p>

<p>Stephen Boyd &amp; Lieven Vandenberghe. Convex Optimization. Chapter 5 Duality.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Neural Turing Machine]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/10/26/neural-network-neural-turing-machine/"/>
    <updated>2015-10-26T16:25:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/10/26/neural-network-neural-turing-machine</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p><em>Recurrent Neural Network</em> 在進行 <em>Gradient Descent</em> 的時候，會遇到所謂的 <em>Vanishing Gradient Problem</em> ，也就是說，在後面時間點的所算出的修正量，要回傳去修正較前面時間的參數值，此修正量會隨著時間傳遞而衰減。</p>

<p>為了改善此問題，可以用類神經網路模擬記憶體的構造，把前面神經元所算出的值，儲存起來。例如： <em>Long Short-term Memory (LSTM)</em> 即是模擬記憶體讀寫的構造，將某個時間點算出的值給儲存起來，等需要用它的時候再讀出來。</p>

<p>除了模擬單一記憶體的儲存與讀寫功能之外，也可以用類神經網路的構造來模擬 <em>Turing Machine</em> ，也就是說，有個 <em>Controller</em> ，可以更精確地控制，要將什麼值寫入哪一個記憶體區塊，或讀取哪一個記憶體區塊的值，這種類神經網路模型，稱為 <em>Neural Turing Machine</em> 。</p>

<p>如果可以模擬 <em>Turing Machine</em> ，即表示可以學會電腦能做的事。也就是說，這種機器學習模型可以學會電腦程式的邏輯控制與運算。</p>

<h2 id="neural-turing-machine">Neural Turing Machine</h2>

<p><em>Neural Turing Machine</em> 的架構如下：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00100.jpeg" alt="Neural Turing Machine" /></p>

<!--more-->

<p>可分為幾個部分：</p>

<p><strong>Input:</strong> 從外部輸入的值。</p>

<p><strong>Output:</strong> 輸出到外部的值。</p>

<p><strong>Controller:</strong> 相當於電腦的IO和CPU，可以從外部輸入值，或從記憶體讀取值，經過運算，再將算出的結果輸出去，或寫入記憶體， <em>Controller</em> 可以用 <em>feed forward neural network</em> 或者 <em>recurrent neural network</em> （相當於有register的CPU）來模擬。</p>

<p><strong>Read/Write Head:</strong> 記憶體的讀寫頭，相當於pointer ，是要被讀取或被寫入的記憶體的address。</p>

<p><strong>Memory:</strong> 記憶體，相當於電腦的RAM，同一個地址可對應到一整排的記憶體單位，就像電腦一樣，用8個bit組成的一個byte，具有同一個memory address。</p>

<p>以下細講每一部份的數學模型。</p>

<h3 id="memory">Memory</h3>

<p><em>memory</em> 是一個二維陣列。如下圖，一個 <em>memory block</em> 是由數個 <em>memory cell</em> 所構成。同一個 <em>block</em> 中的 <em>cell</em> 有相同的 <em>address</em> 。如下圖中，共有 <script type="math/tex">n</script> 個 <em>block</em> ， 每個 <em>block</em> 有 <script type="math/tex">m</script> 個 <em>cell</em> 。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00101.jpeg" alt="" /></p>

<p>操作 <em>Memory</em> 的動作有三種：即 <em>Read</em> ， <em>Erase</em> 和 <em>Add</em> 。</p>

<h4 id="read">Read</h4>

<p><em>Read</em> 是將記憶體裡面的值，讀出來，並傳給 <em>controller</em> 。由於記憶體有很多個 <em>memory block</em> ，至於要讀取哪個，由讀寫頭（ <em>Read/Write Head</em> ）來控制，讀寫頭為一個向量 <script type="math/tex">\textbf{w}</script> ，其數值表示要讀取記憶體位置的權重，滿足以下條件：</p>

<script type="math/tex; mode=display">

\sum_{i}w(i) = 1 \\

 0 \leq w(i) \leq 1, \forall i 

</script>

<p>讀寫頭內部各元素 <script type="math/tex">w_{i}</script> 的值介於 0 到 1 之間，且加起來的和為 1 ，這可解釋為，讀寫頭存在的位置，是用機率來表示。而讀出來的值，為記憶體區塊所儲存的值，乘上讀寫頭在此區塊 <script type="math/tex">i</script> 的機率 <script type="math/tex">w(i)</script> ，所得出之期望值，如下：</p>

<script type="math/tex; mode=display">

\textbf{r} \leftarrow \sum_{i}w(i)\textbf{M}(i) \mspace{40mu} \text{(1)}

</script>

<p>其中，<script type="math/tex">\textbf{r}</script> 為 <em>Read vector</em> ，即從記憶體讀出來的值，  <script type="math/tex">\textbf{M(i)}</script> 為記憶體 <script type="math/tex">i</script> 區塊的值， 而  <script type="math/tex">w(i)</script> 為讀寫頭 <script type="math/tex">w</script> 在區塊 <script type="math/tex">i</script> 的機率。</p>

<p>例如下圖中， <script type="math/tex">w(0) = 0.9</script> ， <script type="math/tex">w(1) = 0.1</script> ，即表示，讀寫頭在位置 0 的機率為 0.9，在位置 1 的機率為 0.1 。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00102.jpeg" alt="Read" /></p>

<p>將上圖中記憶體內部的值 <script type="math/tex">\textbf{M(i)}</script> ，以及讀寫頭位置的值 <script type="math/tex">w(i)</script> ，代入公式(1)，即可得出</p>

<script type="math/tex; mode=display">

\begin{bmatrix}

      r_{0}  \\[0.3em]

      r_{1}  \\[0.3em]

      r_{2}  \\[0.3em]

    \end{bmatrix}

=\begin{bmatrix}

      1*0.9+2*0.1  \\[0.3em]

      1*0.9+1*0.1  \\[0.3em]

      2*0.9+4*0.1  \\[0.3em]

    \end{bmatrix}

＝

\begin{bmatrix}

      1.1  \\[0.3em]

      1.0  \\[0.3em]

      2.2  \\[0.3em]

    \end{bmatrix}

</script>

<h4 id="erase">Erase</h4>

<p>如果要刪除記憶體內部的值，則要進行 <em>Erase</em> ，過程跟 <em>Read</em> 類似，都需要用讀寫頭 <em>w</em> 來控制。但刪除的動作，需要控制去刪除掉哪個 <em>memory cell</em> 的值，而不是一次就把整個 <em>memory block</em> 的值都刪除。所以需要另一個 <em>erase vector</em> <script type="math/tex">\textbf{e}</script> 來選擇要被刪除的 <em>cell</em> 。 <em>erase vector</em> 為一向量，如下：</p>

<script type="math/tex; mode=display">

 0 \leq e(j) \leq 1,  \mspace{10mu} 0 \leq j \leq m-1 , \mspace{10mu} \forall j 

</script>

<p>其中， <script type="math/tex">j</script> 為一個介於 0~m-1 之間的數， m 為 <em>block size</em> 。向量元素的值 <script type="math/tex">e(j)</script> 介於 0~1 之間。如果值為1，則表示要清空這個 <em>cell</em> 的值，若為 0 則表示保留 <em>cell</em> 原本的值， <em>erase</em> 的公式如下：</p>

<script type="math/tex; mode=display">

\textbf{M}(i) \leftarrow (1-w(i) \textbf{e} ) \textbf{M}(i) \mspace{40mu} \text{(2)}


</script>

<p>其中， <script type="math/tex">\textbf{w}</script> 是用來控制要清除哪個 <em>memory block</em> 而 <script type="math/tex">\textbf{e}</script> 是要控制清除這個 <em>block</em> 裡面的哪些 <em>cell</em> ，如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00103.jpeg" alt="Erase" /></p>

<p>上圖中，根據 <script type="math/tex">\textbf{w}</script> 和 <script type="math/tex">\textbf{e}</script> 這兩個向量所選擇的結果， 在 <script type="math/tex">\textbf{M}</script> 中，共有四個 <em>cell</em> 的值被削減了，分別位於左上角和左下角，用較明亮的背景色表示其位置。</p>

<p>將上圖中 <script type="math/tex">\textbf{w}</script> 、 <script type="math/tex">\textbf{e}</script> 和 <script type="math/tex">\textbf{M}</script> 中的值，代入公式(2) ，可得出此結果，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


M= 

\begin{bmatrix}

      1(1-0.9) & 2(1-0.1) & 3 & ...  \\[0.3em]

      1 & 1 & 2 & ...  \\[0.3em]

      2(1-0.9) & 4(1-0.1) & 1 & ...  \\[0.3em]

    \end{bmatrix}

=\begin{bmatrix}

      0.1 & 1.8 & 3 & ...  \\[0.3em]

      1 & 1 & 2 & ...  \\[0.3em]

      0.2 & 3.6 & 1 & ...  \\[0.3em]

    \end{bmatrix}


 %]]&gt;</script>

<h4 id="add">Add</h4>

<p>將新的值寫入記憶體的動作為 <em>add</em> 。之所以稱為 <em>add</em> （而非 <em>write</em> ）因為這個動作是會把記憶體內原本的值，再「加上」要寫入的值。至於要把哪些值加到記憶體，則需要有一個 <em>add vector</em> ，其維度和 <em>memory block</em> 的大小 <script type="math/tex">m</script> 相同。 <em>Add</em> 的公式如下：</p>

<script type="math/tex; mode=display">

\textbf{M}(i) \leftarrow  \textbf{M}(i)  + w(i) \textbf{a} \mspace{40mu} \text{(3)}

</script>

<p>過程如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00104.jpeg" alt="Add" /></p>

<p>上圖中，位於 <script type="math/tex">M</script> 的左上角，共有四個 <em>cell</em> 的值被增加了。</p>

<p>將上圖中 <script type="math/tex">\textbf{w}</script> 、 <script type="math/tex">\textbf{a}</script> 和 <script type="math/tex">\textbf{M}</script> 中的值，代入公式(3) ，可得出此結果，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


M= 

\begin{bmatrix}

      0.1+0.9 & 1.8+0.1 & 3 & ...  \\[0.3em]

      1.0+0.9 & 1.0+0.1 & 2 & ...  \\[0.3em]

      0.2 & 3.6 & 1 & ...  \\[0.3em]

    \end{bmatrix}

=\begin{bmatrix}

      1.0 & 1.9 & 3 & ...  \\[0.3em]

      1.9 & 1.1 & 2 & ...  \\[0.3em]

      0.2 & 3.6 & 1 & ...  \\[0.3em]

    \end{bmatrix}

 %]]&gt;</script>

<h3 id="controller">Controller</h3>

<p><em>Controller</em> 為控制器，它可以用類神經網路之類的機器學習模型來代替，但其實可以把它當成是黑盒子，只要可以符合下圖中所要求的 <em>input</em> 、 <em>output</em> 以及各種參數的值，就可以當 <em>controller</em> 。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00105.jpeg" alt="Controller" /></p>

<p>上圖中， <em>controller</em> 根據外部環境的輸入值 <em>input</em>，以及 <em>read vector</em> <script type="math/tex">\textbf{r}</script> ，經過其內部運算，會輸出 <em>output</em> 值到外在環境，還有 <em>erase vector</em> <script type="math/tex">\textbf{e}</script> 和 <em>add vector</em> <script type="math/tex">\textbf{a}</script> ，來控制記憶體的清除與寫入。但還缺少了讀寫頭向量 <script type="math/tex">\textbf{w}</script> 。</p>

<p>如果要產生讀寫頭向量 <script type="math/tex">\textbf{w}</script> ， 需要透過一連串的 <em>Addressing Mechanisms</em> 的運算，最後即可得出讀寫頭位置。而 <em>controller</em> 則負責產生出 <em>Addressing Mechanisms</em> 所需的參數。</p>

<h3 id="addressing-mechanism">Addressing Mechanism</h3>

<p><em>controller</em> 會產生五個參數來進行 <em>addressing mechanisms</em> ，這些參數分別為： <script type="math/tex">\textbf{k}, \beta, g , \textbf{s}, \gamma </script> 。其中， <script type="math/tex">\textbf{k}</script> 和 <script type="math/tex">\textbf{s}</script> 為向量，其餘參數為純量，這些參數的意義，在以下篇章會解釋，整個 <em>addressing mechanisms</em>  的過程如下圖所示。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00106.jpeg" alt="Addressing Mechanism" /></p>

<p>上圖中，總共有四個步驟，這四個步驟共需要用到這五種參數，經過了這一連串的過程之後，最後所產生出的 <script type="math/tex">\textbf{w}</script> 即為讀寫頭位置，如上圖左下角所示。以下細講每個步驟在做什麼。</p>

<h4 id="content-addressing">Content Addressing</h4>

<p>首先，是找出記憶體中跟參數 <em>memory key</em> <script type="math/tex">\textbf{k}</script> 值最相近的記憶體區塊。</p>

<p>讀寫頭的位置 <script type="math/tex">w</script> ，就先根據記憶體區塊中，跟 <script type="math/tex">\textbf{k}</script> 的相似度來決定，公示如下：</p>

<script type="math/tex; mode=display">

w(i) \leftarrow \frac{e^{\beta K[\textbf{k},\textbf{M}(i)] } }{ \sum_{j} e^{ \beta K[\textbf{k},\textbf{M}(j)] } }

</script>

<p>其中， <script type="math/tex">K[\textbf{k},\textbf{M}(i)]</script> 表示 <em>memory key</em> <script type="math/tex">\textbf{k}</script> 跟記憶體區塊 <script type="math/tex">M(i)</script> 的 <em>cosine similarity</em> ，即兩向量的夾角，如果 <script type="math/tex">\textbf{k}</script> 跟 <script type="math/tex">M(i)</script> 的內容越接近的話，則 <script type="math/tex"> K[\textbf{k},\textbf{M}(i)]</script> 算出來的值會越大。 最後算出來的值 <script type="math/tex">w(i)</script> ，即是 <script type="math/tex">\textbf{k}</script> 跟 <script type="math/tex">M(i)</script> 的相似度，除以記憶體內所有區塊相似度，標準化的結果。</p>

<p><em>cosine similarity</em> 的公式如下：</p>

<script type="math/tex; mode=display">

K[\textbf{u},\textbf{v} ] = \frac{ \textbf{u} \cdot \textbf{v} }{ |\textbf{u}| \cdot |\textbf{v}| } 

</script>

<p>經過了 <em>cosine similarity</em> 後，越相似的向量，值會越大，而參數 <script type="math/tex">\beta</script> 是個大於0的參數，可用來控制 <script type="math/tex">\textbf{w}</script> 內的元素值，集中與分散程度，如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00107.jpeg" alt="Content Addressing" /></p>

<p>上圖中，向量 <script type="math/tex">\textbf{k}</script> 中的值，與記憶體中第三行區塊的值最相似（用較淺色的背景表示）。但如果 <script type="math/tex">\beta</script> 很大（例如： <script type="math/tex">\beta=50</script>），算出來的 <script type="math/tex">\textbf{w}</script> 值會集中在第三個位置，也就是說，只有第三個位置的值是1，其他都是0（用較淺色的背景表示），如上圖的左下方。如果 <script type="math/tex">\beta</script> 很小（例如： <script type="math/tex">\beta=0</script>），則算出來的 <script type="math/tex">\textbf{w}</script> 值會平均分散到每個元素之中，如上圖的右下方。 </p>

<h4 id="interpolation">Interpolation</h4>

<p>讀寫頭其實也是有「記憶」的，也就是說，目前時間點的 <script type="math/tex">\textbf{w}_{t} </script> ，也可能會受到上個時間點 <script type="math/tex">\textbf{w}_{t-1}</script> 的影響，要達到這樣的效果，就是用 <em>content addressing</em> 所算出的值 <script type="math/tex">\textbf{w}_{t} </script> ，和上個時間點的讀寫頭位置 <script type="math/tex">\textbf{w}_{t-1}</script> 做 <em>interpolation</em> ，公式如下：</p>

<script type="math/tex; mode=display">

\textbf{w}_{t} \leftarrow g \textbf{w}_{t} + (1-g) \textbf{w}_{t-1} 

</script>

<p>其中，參數 <script type="math/tex">g</script> 用來表示 <script type="math/tex"> \textbf{w} </script> 有多少比例是這個時間點 <em>content addressing</em> 所算出的值，還是上個時間點的值。如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00108.jpeg" alt="Interpolation" /></p>

<p>如果 <script type="math/tex">g=1</script> ，則 <script type="math/tex">\textbf{w}</script> 的值會完全取決於這個時間點 <em>content addressing</em> 所算出的值，如上圖的左下方，若 <script type="math/tex">g=0</script> ，  <script type="math/tex">\textbf{w}</script> 會完全取決於上個時間點的值，如上圖的右下方。</p>

<h4 id="convolutional-shift">Convolutional Shift</h4>

<p>如果要讓讀寫頭的位置可以稍微往左或往右移動，這就要用 <em>Convolutional Shift</em> 來做調整。 參數 <script type="math/tex">\textbf{s}</script> 是一個向量，用 <em>convolutional shift</em> ，來將 <script type="math/tex">\textbf{w}</script> 的值往左或往右平移，公式如下：</p>

<script type="math/tex; mode=display">

w(i) \leftarrow \sum_{j} w(j) s(i-j)

</script>

<p>舉個例子，如果 <script type="math/tex">\textbf{s}</script> 中有三個元素：<script type="math/tex">s_{-1}, s_{0}, s_{1}</script> ，則 <script type="math/tex">w(i)</script> 經過了以上公式後，結果如下：</p>

<script type="math/tex; mode=display">

w(i) \leftarrow w(i+1) s(-1) + w(i)s(0) + w(i-1)s(1)

</script>

<p>根據此公式， <script type="math/tex">w(i)</script> 的值，如下圖所示：</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00109.jpeg" alt="Convolutional" /></p>

<p>也就是說， <script type="math/tex">s_{-1}=1</script> 時，可讓原本的 <script type="math/tex">w_{i+1}</script> 往左移一格，移到 <script type="math/tex">w_{i}</script> ，若 <script type="math/tex">s_{1}=1</script> 時，可讓原本的 <script type="math/tex">w_{i-1}</script> 往右移一格，移到 <script type="math/tex">w_{i}</script> 。</p>

<p>舉個例子，如果 <script type="math/tex">s_{-1} = 1, s_{0}=0, s_{1} = 0</script> ，則  <script type="math/tex">\textbf{w}</script> 中的值，全部往左移動一格，若碰到邊界則再循環到最右邊，如下圖左方所示。 如果 <script type="math/tex">s_{-1} = 0, s_{0}=0, s_{1} = 1</script> ，則  <script type="math/tex">\textbf{w}</script> 中的值，全部往右移動一格。若 <script type="math/tex">s_{-1} = 0.5, s_{0}=0, s_{1} = 0.5</script> ，則 <script type="math/tex">\textbf{w}</script> 為往左和往右移動後的平均，如下圖右方所示。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00110.jpeg" alt="Convolutional Shift" /></p>

<h4 id="sharpening">Sharpening</h4>

<p>此過程是再一次調整 <script type="math/tex">\textbf{w}</script> 的集中與分散程度，公示如下：</p>

<script type="math/tex; mode=display">

w(i) \leftarrow \frac{w(i)^{\gamma}}{\sum_{j}w(j)^{\gamma}}

</script>

<p>其中， <script type="math/tex">\gamma</script> 的功能和 <em>Content Addressing</em> 中的 <script type="math/tex">\beta</script> 是一樣的，但是經過了接下來的 <em>Interpolation</em> 跟 <em>Convolutional Shift</em> 之後，<script type="math/tex">\textbf{w}</script> 裡面的集中度又會改變，所以要再重新調整一次。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00111.jpeg" alt="Sharpening" /></p>

<h2 id="experiment-repeat-copy">Experiment: Repeat Copy</h2>

<p>關於 <em>Neural Turing Machine</em> 的學習能力，可以參考以下例子。</p>

<p>在訓練資料中，給定一個區塊的 <em>data</em> （如下圖左上角紅色區塊）做為 <em>input data</em> ，將這個區塊複製成七份，做為 <em>output data</em> 。則 <em>Neural Turing Machine</em> 有辦法學會這個「複製」過程所需的運算程序，也就是重複跑七次輸出一樣的東西。</p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00112.png" alt="Experiment" /></p>

<p><img src="http://ckmarkoh.github.io/images/pic/pic_00113.png" alt="Experiment" /></p>

<p>從上圖中，可看到讀寫頭的移動，重複走了相同的路徑，走了七次，依序將記憶體中儲存的 <em>input data</em> 的值，讀出來並輸出到 <em>output</em> 。</p>

<p>有個完整的  <em>Neural Turing Machine</em> 套件，以及此實驗的相關程式碼於：https://github.com/fumin/ntm</p>

<h2 id="reference">Reference</h2>

<p><a href="http://arxiv.org/abs/1410.5401">Alex Graves, Greg Wayne, Ivo Danihelka. Neural Turing Machines. 2014</a></p>
]]></content>
  </entry>
  
</feed>
