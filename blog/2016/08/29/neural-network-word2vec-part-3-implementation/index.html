
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>類神經網路 -- Word2vec (Part 3 : Implementation) - Mark Chang's Blog</title>
  <meta name="author" content="Mark Chang">

  
  <meta name="description" content="Introduction 本文接續 word2vec (part2) ，介紹如何根據推導出來的 backward propagation 公式，從頭到尾實作一個簡易版的 word2vec 。 本例的 input layer 採用 skip-gram ， output layer 採用 &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://ckmarkoh.github.io/blog/2016/08/29/neural-network-word2vec-part-3-implementation/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Mark Chang's Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

<!-- MathJax Configuration -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/SVG"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Mark Chang's Blog</a></h1>
  
    <h2>Machine Learning, Deep Learning and Python</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="ckmarkoh.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">類神經網路 -- Word2vec (Part 3 : Implementation)</h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2016-08-29T11:17:00+08:00'><span class='date'><span class='date-month'>Aug</span> <span class='date-day'>29</span><span class='date-suffix'>th</span>, <span class='date-year'>2016</span></span> <span class='time'>11:17 am</span></time>
        
      </p>
    
  </header>


<div class="entry-content"><h2 id="introduction">Introduction</h2>

<p>本文接續 <a href="/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation">word2vec (part2)</a> ，介紹如何根據推導出來的 <em>backward propagation</em> 公式，從頭到尾實作一個簡易版的 <em>word2vec</em> 。</p>

<p>本例的 input layer 採用 <em>skip-gram</em> ， output layer 採用 <em>negative sampling</em></p>

<p>本例用唐詩語料庫：https://github.com/ckmarkoh/coscup_nndl/blob/master/poem.txt</p>

<p>首先，載入所需的模組</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">json</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span><span class="p">,</span> <span class="n">OrderedDict</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">random</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">math</span>
</span></code></pre></td></tr></table></div></figure>

<!--more-->

<h2 id="build-dictionray">Build Dictionray</h2>

<p>再來是建立字典，即將每個字給一個id來對應。</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">LearnVocabFromTrainFile</span><span class="p">():</span>
</span><span class="line">		
</span><span class="line">    <span class="c"># 開啟唐詩語料庫</span>
</span><span class="line">    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">&quot;poem.txt&quot;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 統計唐詩語料庫中每個字出現的頻率</span>
</span><span class="line">    <span class="n">vcount</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
</span><span class="line">    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">():</span>
</span><span class="line">        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s">&quot;utf-8&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
</span><span class="line">            <span class="n">vcount</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 僅保留出現次數大於五的字，並按照出現次數排序</span>
</span><span class="line">    <span class="n">vcount_list</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">vcount</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
</span><span class="line">                         <span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 建立字典，將每個字給一個id ，字為 key, id 為 value</span>
</span><span class="line">    <span class="n">vocab_dict</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vcount_list</span><span class="p">)))</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 建立詞頻統計用的字典，給定某字，可查到其出現頻率</span>
</span><span class="line">    <span class="n">vocab_freq_dict</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">vcount_list</span><span class="p">))</span>
</span><span class="line">    <span class="k">return</span> <span class="n">vocab_dict</span><span class="p">,</span> <span class="n">vocab_freq_dict</span>
</span><span class="line">
</span><span class="line"><span class="n">vocab_dict</span><span class="p">,</span> <span class="n">vocab_freq_dict</span> <span class="o">=</span>  <span class="n">LearnVocabFromTrainFile</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>印出字典檔，每個字對應到一個id（編號）</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">for</span> <span class="n">w</span><span class="p">,</span><span class="n">wid</span> <span class="ow">in</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span class="line">    <span class="k">print</span> <span class="s">&quot;</span><span class="si">%s</span><span class="s"> : </span><span class="si">%s</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">wid</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">不 : 0
</span><span class="line">人 : 1
</span><span class="line">山 : 2
</span><span class="line">無 : 3
</span><span class="line">風 : 4
</span><span class="line">......
</span><span class="line">謏 : 5496
</span><span class="line">笮 : 5497
</span><span class="line">躠 : 5498
</span><span class="line">噆 : 5499
</span></code></pre></td></tr></table></div></figure>

<p>印出詞頻統計用的字典，給定某字，可查詢到其出現頻率：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">for</span> <span class="n">w</span><span class="p">,</span><span class="n">wfreq</span> <span class="ow">in</span> <span class="n">vocab_freq_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span class="line">    <span class="k">print</span> <span class="s">&quot;</span><span class="si">%s</span><span class="s"> : </span><span class="si">%s</span><span class="s">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">wfreq</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">不 : 26426
</span><span class="line">人 : 20966
</span><span class="line">山 : 16056
</span><span class="line">無 : 15795
</span><span class="line">風 : 15618
</span><span class="line">...
</span><span class="line">謏 : 5
</span><span class="line">笮 : 5
</span><span class="line">躠 : 5
</span><span class="line">噆 : 5
</span></code></pre></td></tr></table></div></figure>

<h2 id="build-unigram-table">Build Unigram Table</h2>

<p>本例採用 <em>negative sampling</em> ，需要先建立 <em>unigram table</em> 以便進行 <em>negative sampling</em> 。</p>

<p>所謂的 <em>Unigram Table</em> 即是一個 <em>array</em> ，其中每個元素為某字的id，而某字的頻率，即為此id在此 <em>table</em> 中出現的次數的 0.75次方。</p>

<p>例如，id 為 5496 的字，詞頻為 5 ，則在此 <em>Unigram Table</em> 中，5496 的次數為：</p>

<script type="math/tex; mode=display">

5^{0.75} = 3.34 \approx 3

</script>

<p>由於 <em>array</em> 中的元素個數必須是整數，所以 5496 在 <em>Unigram Table</em> 中出現三次。</p>

<p>建立 <em>Unigram Table</em> 的程式碼如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">InitUnigramTable</span><span class="p">(</span><span class="n">vocab_freq_dict</span><span class="p">):</span>
</span><span class="line">    <span class="n">table_freq_list</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">0.75</span><span class="p">)),</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab_freq_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()))</span>
</span><span class="line">    <span class="n">table_size</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">table_freq_list</span><span class="p">])</span>
</span><span class="line">    <span class="n">table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">table_size</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
</span><span class="line">    <span class="n">offset</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">table_freq_list</span><span class="p">:</span>
</span><span class="line">        <span class="n">table</span><span class="p">[</span><span class="n">offset</span><span class="p">:</span><span class="n">offset</span> <span class="o">+</span> <span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">        <span class="n">offset</span> <span class="o">+=</span> <span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">    <span class="k">return</span> <span class="n">table</span>
</span><span class="line">
</span><span class="line"><span class="n">table</span> <span class="o">=</span> <span class="n">InitUnigramTable</span><span class="p">(</span><span class="n">vocab_freq_dict</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>得出的 <em>Unigram Table</em> 如下：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="p">[</span>   <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>
</span><span class="line"><span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>  <span class="o">...</span> <span class="p">,</span> <span class="mi">5495</span> <span class="mi">5495</span> <span class="mi">5495</span> <span class="mi">5496</span>  <span class="mi">5496</span> <span class="mi">5496</span> <span class="mi">5497</span> <span class="mi">5497</span> <span class="mi">5497</span> <span class="mi">5498</span>
</span><span class="line"><span class="mi">5498</span> <span class="mi">5498</span> <span class="mi">5499</span> <span class="mi">5499</span> <span class="mi">5499</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>

<h2 id="training-word2vec">Training word2vec</h2>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
<span class="line-number">61</span>
<span class="line-number">62</span>
<span class="line-number">63</span>
<span class="line-number">64</span>
<span class="line-number">65</span>
<span class="line-number">66</span>
<span class="line-number">67</span>
<span class="line-number">68</span>
<span class="line-number">69</span>
<span class="line-number">70</span>
<span class="line-number">71</span>
<span class="line-number">72</span>
<span class="line-number">73</span>
<span class="line-number">74</span>
<span class="line-number">75</span>
<span class="line-number">76</span>
<span class="line-number">77</span>
<span class="line-number">78</span>
<span class="line-number">79</span>
<span class="line-number">80</span>
<span class="line-number">81</span>
<span class="line-number">82</span>
<span class="line-number">83</span>
<span class="line-number">84</span>
<span class="line-number">85</span>
<span class="line-number">86</span>
<span class="line-number">87</span>
<span class="line-number">88</span>
<span class="line-number">89</span>
<span class="line-number">90</span>
<span class="line-number">91</span>
<span class="line-number">92</span>
<span class="line-number">93</span>
<span class="line-number">94</span>
<span class="line-number">95</span>
<span class="line-number">96</span>
<span class="line-number">97</span>
<span class="line-number">98</span>
<span class="line-number">99</span>
<span class="line-number">100</span>
<span class="line-number">101</span>
<span class="line-number">102</span>
<span class="line-number">103</span>
<span class="line-number">104</span>
<span class="line-number">105</span>
<span class="line-number">106</span>
<span class="line-number">107</span>
<span class="line-number">108</span>
<span class="line-number">109</span>
<span class="line-number">110</span>
<span class="line-number">111</span>
<span class="line-number">112</span>
<span class="line-number">113</span>
<span class="line-number">114</span>
<span class="line-number">115</span>
<span class="line-number">116</span>
<span class="line-number">117</span>
<span class="line-number">118</span>
<span class="line-number">119</span>
<span class="line-number">120</span>
<span class="line-number">121</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">vocab_dict</span><span class="p">,</span> <span class="n">vocab_freq_dict</span><span class="p">,</span> <span class="n">table</span><span class="p">):</span>
</span><span class="line">		
</span><span class="line">    <span class="n">total_words</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">vocab_freq_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
</span><span class="line">    <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab_dict</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 參數設定</span>
</span><span class="line">    <span class="n">layer1_size</span> <span class="o">=</span> <span class="mi">30</span> <span class="c"># hidden layer 的大小，即向量大小</span>
</span><span class="line">    <span class="n">window</span> <span class="o">=</span> <span class="mi">2</span> <span class="c"># 上下文寬度的上限</span>
</span><span class="line">    <span class="n">alpha_init</span> <span class="o">=</span> <span class="mf">0.025</span> <span class="c"># learning rate</span>
</span><span class="line">    <span class="n">sample</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="c"># 用來隨機丟棄高頻字用</span>
</span><span class="line">    <span class="n">negative</span> <span class="o">=</span> <span class="mi">10</span> <span class="c"># negative sampling 的數量</span>
</span><span class="line">    <span class="n">ite</span> <span class="o">=</span> <span class="mi">2</span> <span class="c"># iteration 次數</span>
</span><span class="line">
</span><span class="line">    <span class="c"># Weights 初始化</span>
</span><span class="line">    <span class="c"># syn0 : input layer 到 hidden layer 之間的 weights ，用隨機值初始化</span>
</span><span class="line">    <span class="c"># syn1 : hidden layer 到 output layer 之間的 weights ，用0初始化</span>
</span><span class="line">    <span class="n">syn0</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">layer1_size</span><span class="p">))</span> <span class="o">/</span> <span class="n">layer1_size</span>
</span><span class="line">    <span class="n">syn1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layer1_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 印出進度用</span>
</span><span class="line">    <span class="n">train_words</span> <span class="o">=</span> <span class="mi">0</span> <span class="c"># 總共訓練了幾個字</span>
</span><span class="line">    <span class="n">p_count</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">    <span class="n">avg_err</span> <span class="o">=</span> <span class="mf">0.</span>
</span><span class="line">    <span class="n">err_count</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">
</span><span class="line">    <span class="k">for</span> <span class="n">local_iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ite</span><span class="p">):</span>
</span><span class="line">        <span class="k">print</span> <span class="s">&quot;local_iter&quot;</span><span class="p">,</span> <span class="n">local_iter</span>
</span><span class="line">        <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">&quot;poem.txt&quot;</span><span class="p">)</span>
</span><span class="line">        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">():</span>
</span><span class="line">
</span><span class="line">            <span class="c">#用來暫存要訓練的字，一次訓練一個句子</span>
</span><span class="line">            <span class="n">sen</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">
</span><span class="line">            <span class="c"># 取出要被訓練的字</span>
</span><span class="line">            <span class="k">for</span> <span class="n">word_raw</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s">&quot;utf-8&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
</span><span class="line">                <span class="n">last_word</span> <span class="o">=</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word_raw</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">                <span class="c"># 丟棄字典中沒有的字（頻率太低）</span>
</span><span class="line">                <span class="k">if</span> <span class="n">last_word</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
</span><span class="line">                    <span class="k">continue</span>
</span><span class="line">                <span class="n">cn</span> <span class="o">=</span> <span class="n">vocab_freq_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word_raw</span><span class="p">)</span>
</span><span class="line">                <span class="n">ran</span> <span class="o">=</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cn</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">sample</span> <span class="o">*</span> <span class="n">total_words</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">*</span> <span class="p">(</span><span class="n">sample</span> <span class="o">*</span> <span class="n">total_words</span><span class="p">)</span> <span class="o">/</span> <span class="n">cn</span>
</span><span class="line">
</span><span class="line">                <span class="c"># 根據字的頻率，隨機丟棄，頻率越高的字，越有機會被丟棄</span>
</span><span class="line">                <span class="k">if</span> <span class="n">ran</span> <span class="o">&lt;</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">():</span>
</span><span class="line">                    <span class="k">continue</span>
</span><span class="line">                <span class="n">train_words</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">
</span><span class="line">                <span class="c"># 將要被訓練的字加到 sen</span>
</span><span class="line">                <span class="n">sen</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">last_word</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">            <span class="c"># 根據訓練過的字數，調整 learning rate</span>
</span><span class="line">            <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha_init</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">train_words</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">ite</span> <span class="o">*</span> <span class="n">total_words</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
</span><span class="line">            <span class="k">if</span> <span class="n">alpha</span> <span class="o">&lt;</span> <span class="n">alpha_init</span> <span class="o">*</span> <span class="mf">0.0001</span><span class="p">:</span>
</span><span class="line">                <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha_init</span> <span class="o">*</span> <span class="mf">0.0001</span>
</span><span class="line">
</span><span class="line">            <span class="c"># 逐一訓練 sen 中的字</span>
</span><span class="line">            <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sen</span><span class="p">):</span>
</span><span class="line">
</span><span class="line">            		<span class="c"># 隨機調整 window 大小</span>
</span><span class="line">                <span class="n">b</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">window</span><span class="p">)</span>
</span><span class="line">                <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span><span class="line">
</span><span class="line">                    <span class="c"># input 為 window 範圍中，上下文的某一字</span>
</span><span class="line">                    <span class="k">if</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">c</span> <span class="o">==</span> <span class="n">a</span> <span class="ow">or</span> <span class="n">c</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sen</span><span class="p">):</span>
</span><span class="line">                        <span class="k">continue</span>
</span><span class="line">                    <span class="n">last_word</span> <span class="o">=</span> <span class="n">sen</span><span class="p">[</span><span class="n">c</span><span class="p">]</span>
</span><span class="line">										
</span><span class="line">                    <span class="c"># h_err 暫存 hidden layer 的 error 用</span>
</span><span class="line">                    <span class="n">h_err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layer1_size</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">                    <span class="c"># 進行 negative sampling</span>
</span><span class="line">                    <span class="k">for</span> <span class="n">negcount</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">negative</span><span class="p">):</span>
</span><span class="line">
</span><span class="line">                    		<span class="c"># positive example，從 sen 中取得，模型要輸出 1</span>
</span><span class="line">                        <span class="k">if</span> <span class="n">negcount</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">                            <span class="n">target_word</span> <span class="o">=</span> <span class="n">word</span>
</span><span class="line">                            <span class="n">label</span> <span class="o">=</span> <span class="mi">1</span>
</span><span class="line">
</span><span class="line">                        <span class="c"># negative example，從 table 中抽樣，模型要輸出 0 </span>
</span><span class="line">                        <span class="k">else</span><span class="p">:</span>
</span><span class="line">                            <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
</span><span class="line">                                <span class="n">target_word</span> <span class="o">=</span> <span class="n">table</span><span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">table</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
</span><span class="line">                                <span class="k">if</span> <span class="n">target_word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sen</span><span class="p">:</span>
</span><span class="line">                                    <span class="k">break</span>
</span><span class="line">                            <span class="n">label</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">
</span><span class="line">                        <span class="c"># 模型預測結果</span>
</span><span class="line">                        <span class="n">o_pred</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">syn0</span><span class="p">[</span><span class="n">last_word</span><span class="p">,</span> <span class="p">:],</span> <span class="n">syn1</span><span class="p">[:,</span> <span class="n">target_word</span><span class="p">])))</span>
</span><span class="line">
</span><span class="line">                        <span class="c"># 預測結果和標準答案的差距</span>
</span><span class="line">                        <span class="n">o_err</span> <span class="o">=</span> <span class="n">o_pred</span> <span class="o">-</span> <span class="n">label</span>
</span><span class="line">
</span><span class="line">                        <span class="c"># backward propagation</span>
</span><span class="line">                        <span class="c"># 此部分請參照 word2vec part2 的公式推導結果</span>
</span><span class="line">
</span><span class="line">                        <span class="c"># 1.將 error 傳遞到 hidden layer                        </span>
</span><span class="line">                        <span class="n">h_err</span> <span class="o">+=</span> <span class="n">o_err</span> <span class="o">*</span> <span class="n">syn1</span><span class="p">[:,</span> <span class="n">target_word</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">                        <span class="c"># 2.更新 syn1</span>
</span><span class="line">                        <span class="n">syn1</span><span class="p">[:,</span> <span class="n">target_word</span><span class="p">]</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">o_err</span> <span class="o">*</span> <span class="n">syn0</span><span class="p">[</span><span class="n">last_word</span><span class="p">]</span>
</span><span class="line">                        <span class="n">avg_err</span> <span class="o">+=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">o_err</span><span class="p">)</span>
</span><span class="line">                        <span class="n">err_count</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">
</span><span class="line">                    <span class="c"># 3.更新 syn0</span>
</span><span class="line">                    <span class="n">syn0</span><span class="p">[</span><span class="n">last_word</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">h_err</span>
</span><span class="line">
</span><span class="line">                    <span class="c"># 印出目前結果</span>
</span><span class="line">                    <span class="n">p_count</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">                    <span class="k">if</span> <span class="n">p_count</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">                        <span class="k">print</span> <span class="s">&quot;Iter: </span><span class="si">%s</span><span class="s">, Alpha </span><span class="si">%s</span><span class="s">, Train Words </span><span class="si">%s</span><span class="s">, Average Error: </span><span class="si">%s</span><span class="s">&quot;</span> \
</span><span class="line">                              <span class="o">%</span> <span class="p">(</span><span class="n">local_iter</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">train_words</span><span class="p">,</span> <span class="n">avg_err</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">err_count</span><span class="p">))</span>
</span><span class="line">                        <span class="n">avg_err</span> <span class="o">=</span> <span class="mf">0.</span>
</span><span class="line">                        <span class="n">err_count</span> <span class="o">==</span> <span class="mf">0.</span>
</span><span class="line">
</span><span class="line">        <span class="c"># 每一個 iteration 儲存一次訓練完的模型</span>
</span><span class="line">        <span class="n">model_name</span> <span class="o">=</span> <span class="s">&quot;w2v_model_blog_</span><span class="si">%s</span><span class="s">.json&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">local_iter</span><span class="p">)</span>
</span><span class="line">        <span class="k">print</span> <span class="s">&quot;save model: </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</span><span class="line">        <span class="n">fm</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="s">&quot;w&quot;</span><span class="p">)</span>
</span><span class="line">        <span class="n">fm</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">syn0</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>
</span><span class="line">        <span class="n">fm</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>

<p>開始訓練：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">train</span><span class="p">(</span><span class="n">vocab_dict</span><span class="p">,</span> <span class="n">vocab_freq_dict</span><span class="p">,</span> <span class="n">table</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>輸出結果如下，可以看到，當訓練過的字數增加時， Error 也跟著降低</p>

<p>大概要花幾十分鐘左右訓練完</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">Iter: 0, Alpha 0.0249923666923, Train Words 475200, Average Error: 0.499999254842
</span><span class="line">Iter: 0, Alpha 0.0249846739501, Train Words 954100, Average Error: 0.249998343836
</span><span class="line">Iter: 0, Alpha 0.0249771900316, Train Words 1420000, Average Error: 0.166660116256
</span><span class="line">Iter: 0, Alpha 0.0249693430813, Train Words 1908500, Average Error: 0.124949913475
</span><span class="line">Iter: 0, Alpha 0.024961329072, Train Words 2407400, Average Error: 0.0993522008349
</span><span class="line">Iter: 0, Alpha 0.0249531817368, Train Words 2914600, Average Error: 0.0787704454331
</span><span class="line">Iter: 0, Alpha 0.0249453540624, Train Words 3401900, Average Error: 0.06351951221
</span><span class="line">Iter: 0, Alpha 0.0249377801891, Train Words 3873400, Average Error: 0.0495117808015
</span><span class="line">..........
</span></code></pre></td></tr></table></div></figure>

<h2 id="show-result">Show Result</h2>

<p>檢視 word2vec 訓練結果的方法，即是看使用 <em>cosine similarity</em> 計算，是否能得出與某字語意相近的字。</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="c"># 讀取訓練好的模型</span>
</span><span class="line"><span class="n">f2</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">&quot;w2v_model_1.json&quot;</span><span class="p">,</span> <span class="s">&quot;r&quot;</span><span class="p">)</span>
</span><span class="line"><span class="n">w2v_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="s">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">f2</span><span class="o">.</span><span class="n">readlines</span><span class="p">())))</span>
</span><span class="line"><span class="n">f2</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span><span class="line">
</span><span class="line"><span class="n">vocab_dict_reversed</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">([(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
</span><span class="line">
</span><span class="line"><span class="c"># 計算 cosine similarity 最高的前五字</span>
</span><span class="line"><span class="k">def</span> <span class="nf">get_top</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
</span><span class="line">    <span class="n">wid</span> <span class="o">=</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 將某字與模型中所有的字向量做內積</span>
</span><span class="line">    <span class="n">dot_result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2v_model</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">w2v_model</span><span class="p">[</span><span class="n">wid</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</span><span class="line">    <span class="n">norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 計算 cosine similarity</span>
</span><span class="line">    <span class="n">cosine_result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">dot_result</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">norm</span><span class="o">*</span><span class="n">norm</span><span class="p">[</span><span class="n">wid</span><span class="p">])</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 根據 cosine similarity 的值排序</span>
</span><span class="line">    <span class="n">final_result</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">wid</span><span class="p">,</span>
</span><span class="line">                          <span class="p">[(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cosine_result</span><span class="p">)]),</span>
</span><span class="line">                          <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class="line">    <span class="k">print</span> <span class="n">word</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 印出語意最接近的前五字，以及其 cosine similarity</span>
</span><span class="line">    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">final_result</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
</span><span class="line">        <span class="k">print</span> <span class="n">vocab_dict_reversed</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>

<p>分別計算「山、峰、河、日」這四字語意最相近的字</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">get_top</span><span class="p">(</span><span class="s">u&quot;山&quot;</span><span class="p">)</span>
</span><span class="line"><span class="n">get_top</span><span class="p">(</span><span class="s">u&quot;峰&quot;</span><span class="p">)</span>
</span><span class="line"><span class="n">get_top</span><span class="p">(</span><span class="s">u&quot;河&quot;</span><span class="p">)</span>
</span><span class="line"><span class="n">get_top</span><span class="p">(</span><span class="s">u&quot;日&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下，可看出，計算所得出語意最相近的字，實際上，語意也相近，例如，山和峰、嶺的語意都很接近。</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">山
</span><span class="line">嶺 0.854901128361
</span><span class="line">嵩 0.846620438864
</span><span class="line">峰 0.842831270385
</span><span class="line">岡 0.838129842909
</span><span class="line">嶂 0.834701215189
</span><span class="line">峰
</span><span class="line">山 0.842831270385
</span><span class="line">嶽 0.83917452917
</span><span class="line">嶺 0.8219837161
</span><span class="line">頂 0.821088331571
</span><span class="line">嶂 0.809565794884
</span><span class="line">河
</span><span class="line">湟 0.787726187693
</span><span class="line">涇 0.770652269018
</span><span class="line">淮 0.751135710239
</span><span class="line">川 0.742243126005
</span><span class="line">汾 0.740643816278
</span><span class="line">日
</span><span class="line">旦 0.869047480855
</span><span class="line">又 0.842383624714
</span><span class="line">曛 0.830549707539
</span><span class="line">夕 0.826327222048
</span><span class="line">暉 0.82616774597
</span></code></pre></td></tr></table></div></figure>

<p>向量加減運算後的 <em>cosine similarity</em> ，例如： 女 + 父 - 男 = 母</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">get_calculated_top</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">w3</span><span class="p">):</span>
</span><span class="line">    <span class="n">wid1</span><span class="p">,</span> <span class="n">wid2</span><span class="p">,</span> <span class="n">wid3</span> <span class="o">=</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">w1</span><span class="p">),</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">w2</span><span class="p">),</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">w3</span><span class="p">)</span>
</span><span class="line">    <span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">,</span> <span class="n">v3</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="p">[</span><span class="n">wid1</span><span class="p">],</span> <span class="n">w2v_model</span><span class="p">[</span><span class="n">wid2</span><span class="p">],</span> <span class="n">w2v_model</span><span class="p">[</span><span class="n">wid3</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">    <span class="c"># 得出加減運算後的向量</span>
</span><span class="line">    <span class="n">combined_vec</span> <span class="o">=</span> <span class="n">v1</span> <span class="o">+</span> <span class="p">(</span><span class="n">v2</span> <span class="o">-</span> <span class="n">v3</span><span class="p">)</span>
</span><span class="line">    <span class="n">dot_result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2v_model</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">combined_vec</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</span><span class="line">    <span class="n">norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</span><span class="line">    <span class="n">cvec_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">combined_vec</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
</span><span class="line">    <span class="n">cosine_result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">dot_result</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">norm</span> <span class="o">*</span> <span class="n">cvec_norm</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="n">final_result</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">wid1</span><span class="p">,</span> <span class="n">wid2</span><span class="p">,</span> <span class="n">wid3</span><span class="p">],</span>
</span><span class="line">                                 <span class="p">[(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cosine_result</span><span class="p">)]),</span>
</span><span class="line">                          <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class="line">    <span class="k">print</span> <span class="s">&quot;</span><span class="si">%s</span><span class="s"> + </span><span class="si">%s</span><span class="s"> - </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">w3</span><span class="p">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">final_result</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
</span><span class="line">        <span class="k">print</span> <span class="n">vocab_dict_reversed</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">get_calculated_top</span><span class="p">(</span><span class="s">u&quot;女&quot;</span><span class="p">,</span> <span class="s">u&quot;父&quot;</span><span class="p">,</span> <span class="s">u&quot;男&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

<p>結果如下，如預期，運算結果的語意接近「母」：</p>

<figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">女 + 父 - 男
</span><span class="line">母 0.731002049447
</span><span class="line">娥 0.707469857054
</span><span class="line">客 0.69027387716
</span><span class="line">娃 0.687831493041
</span><span class="line">侶 0.681667240226
</span></code></pre></td></tr></table></div></figure>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Mark Chang</span></span>

      




<time class='entry-date' datetime='2016-08-29T11:17:00+08:00'><span class='date'><span class='date-month'>Aug</span> <span class='date-day'>29</span><span class='date-suffix'>th</span>, <span class='date-year'>2016</span></span> <span class='time'>11:17 am</span></time>
      


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://ckmarkoh.github.io/blog/2016/08/29/neural-network-word2vec-part-3-implementation/" data-via="" data-counturl="http://ckmarkoh.github.io/blog/2016/08/29/neural-network-word2vec-part-3-implementation/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation/" title="Previous Post: 類神經網路 -- word2vec (part 2 : Backward Propagation)">&laquo; 類神經網路 -- word2vec (part 2 : Backward Propagation)</a>
      
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2016/08/29/neural-network-word2vec-part-3-implementation/">類神經網路 -- Word2vec (Part 3 : Implementation)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation/">類神經網路 -- Word2vec (Part 2 : Backward Propagation)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/07/12/neural-network-word2vec-part-1-overview/">類神經網路 -- Word2vec (Part 1 : Overview)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/07/10/nlp-vector-space-semantics/">自然語言處理 -- Vector Space of Semantics</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/07/09/pgm-gibbs-sampling/">機率圖模型 -- Gibbs Sampling</a>
      </li>
    
  </ul>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2016 - Mark Chang -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
