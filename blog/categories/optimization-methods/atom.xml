<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Optimization Methods | Mark Chang's Blog]]></title>
  <link href="http://ckmarkoh.github.io/blog/categories/optimization-methods/atom.xml" rel="self"/>
  <link href="http://ckmarkoh.github.io/"/>
  <updated>2017-05-31T01:34:10+08:00</updated>
  <id>http://ckmarkoh.github.io/</id>
  <author>
    <name><![CDATA[Mark Chang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Linear Programming 1: Standard Form of Linear Programming]]></title>
    <link href="http://ckmarkoh.github.io/blog/2017/05/30/optimization-linear-programming-standard-form/"/>
    <updated>2017-05-30T09:05:30+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2017/05/30/optimization-linear-programming-standard-form</id>
    <content type="html"><![CDATA[<h2 id="what-is-linear-programming">What is Linear Programming?</h2>

<p>Linear Programming 是一種最佳化問題，在此最佳化問題中。Objective Function 和Constraints 都是線性的。</p>

<p>舉個例子，以下為一個 Linear Programming 的問題<a name="eq1">（公式一）</a>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}
&\text{Objective Function} \\
&\mspace{20mu}\text{maximize: }  2x_1+3x_2  \\
&\text{Constraints} \\
&\mspace{20mu} x_1 + x_2  \leq 2 \\
&\mspace{20mu} x_1 + 2x_2 \leq 3 \\
&\mspace{20mu} x_1 , x_2  \geq 0  \\
\end{align}

 %]]&gt;</script>

<p>將 Constraints 畫在平面上，得出下圖：</p>

<p><img src="/images/new/linearprog1.png" alt="" /></p>

<!--more-->

<p>其中，淺藍色區域 <script type="math/tex">\mathbf{P}</script> 是 <script type="math/tex">(x_1,x_2)</script> 滿足 Constraints 的範圍，稱為 Feasible Domain。</p>

<script type="math/tex; mode=display">

P = \{(x_1,x_2) \mid x_1+x_2 \leq 2 , x_1+2x_2 \leq 3, x_1,x_2 \geq 0\}

</script>

<p>而從 Feasible Domain 中，可以使 Objective Function 得到最大值的 <script type="math/tex">(x_1,x_2)</script> 為最佳解。 
此問題的最佳解為 <script type="math/tex">(x_1,x_2)=(1,1)</script> ，因為它使 Objective Function 得最大值：</p>

<script type="math/tex; mode=display">
2x_1+3x_2 = 2\times 1 + 3\times 1 = 5
</script>

<p>Linear Programming 的解法主要有兩種：分別是Simplex Method 和 Interior Point Method。本文先不講解這兩種方法，而會在後續文章講解。</p>

<h2 id="formulate-a-linear-programming-problem">Formulate A Linear Programming Problem</h2>

<p>在現實生活的應用中，有許多問題是可以用 Linear Programming 來解，本段舉例，如何將有限的資源下將利潤最大化的問題，寫成 Linear Programming 的形式，例如：</p>

<p>某工廠生產兩種產品X和Y ，這兩種產品都需要用到兩種原料 A和B，生產1公噸X需要1公噸的A和1公噸的B，生產1公噸Y需要1公噸的A和2公噸的B。工廠存有原料A有2公噸，B有3公噸。而1公噸X可賣2百萬元，1公噸Y可賣3百萬元。 以工廠的現有的資源，要生產多少X和Y怎樣才能獲得最大利潤?</p>

<p>設工廠要生產 <script type="math/tex">x_1</script> 公噸的 X ， <script type="math/tex">y_1</script> 公噸的 Y ，才能獲得最大利潤。</p>

<p>目標是要將利潤最大化，由於而1公噸X可賣2百萬元，1公噸Y可賣3百萬元，所以 Objective Function 如下（省略百萬元）：</p>

<script type="math/tex; mode=display">
\begin{align}
\text{maximize: }  2x_1+3x_2  \\
\end{align}
</script>

<p>但是 X 和 Y 的產量受限於目前工廠僅存的原料量， <script type="math/tex">x_1 + x_2</script> 為原料A的用量，不可多餘2公噸，以此類推，<script type="math/tex">x_1 + 2x_2</script> 為原料 B的用量，不可多餘3公噸。又由於產量不可能為負，因此 <script type="math/tex">x_1,x_2</script> 都不能為負， 所以 Constraints 如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\mspace{40mu} x_1 + x_2  \leq 2 \\
&\mspace{40mu} x_1 + 2x_2 \leq 3 \\
&\mspace{40mu} x_1 , x_2  \geq 0  \\
\end{align}
 %]]&gt;</script>

<p>結合以上的 Objective Function 和 Constraints，即為<a href="#eq1">（公式一）</a>的 Linear Programming 問題。</p>

<h2 id="standard-form-of-linear-programming">Standard Form of Linear Programming</h2>

<p>在解 Linear Programming 的問題時，會先把問題轉成 Standard Form，使得不同的 Linear Programming 問題能有一致性，以便之後能用相同的演算法來求解。
Linear Programming 的 Standard Form 如下<a name="eq2">（公式二）</a>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}
&\text{1.Objective Function} \\
&\mspace{20mu}\text{minimize: } z = c_1x_1+c_2x_2+...+c_nx_n \\
&\text{2.Equality Constraints} \\
&\mspace{20mu} a_{11}x_1 + a_{12}x_2 +...+a_{1n}x_n = b_1 \\
&\mspace{20mu} a_{21}x_1 + a_{22}x_2 +...+a_{2n}x_n = b_2 \\
&\mspace{20mu} \vdots \\
&\mspace{20mu} a_{m1}x_1 + a_{m2}x_2 +...+a_{mn}x_n = b_m \\
&\text{3.Non-Negative Variables} \\
&\mspace{20mu} x_1 , x_2,...,x_n  \geq 0  \\
\end{align}

 %]]&gt;</script>

<p>其中，$n$ 為 Variables 的個數， $m$ 為 Constraints 的個數。通常 <script type="math/tex">n >  m</script> ，這樣才需要從 Feasible Domain 中挑選出最佳解，不然 Feasible Domain 可能會只有一個點 ， 或者根本無解。</p>

<p>而 Standard Form 須滿足以下三個條件：</p>

<ol>
  <li>目標是將 Objective Function 最小化，而非最大化。</li>
  <li>Constraints 皆為 Equality Constraints ，沒有 Inequality Constraints。</li>
  <li>Variables 都必須是非負實數。</li>
</ol>

<p>為了以矩陣運算來加速，也可以把<a href="#eq2">（公式二）</a> 的 Standard Form 寫成矩陣的形式<a name="eq3">（公式三）</a>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\mathbf{c} = \begin{bmatrix}
c_1 \\
c_2 \\
\vdots \\
c_n \\
\end{bmatrix}
\mathbf{x} = \begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n \\
\end{bmatrix}
\mathbf{b} = \begin{bmatrix}
b_1 \\
b_2 \\
\vdots \\
b_m \\
\end{bmatrix} \\
&\mathbf{A} = \begin{bmatrix}
a_{11} &a_{12} &\cdots &a_{1n} \\
a_{21} &a_{22} &\cdots &a_{2n} \\
\vdots &\vdots &\vdots &\vdots \\
a_{m1} &a_{m2} &\cdots &a_{mn} \\
\end{bmatrix} \\
\end{align}
 %]]&gt;</script>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\text{Objective Function} \\
&\mspace{40mu} \text{minimize: } z =\mathbf{c}^T\mathbf{x}  \\
&\text{Equality Constraints} \\
&\mspace{40mu} \mathbf{A}\mathbf{x} = \mathbf{b} \\
&\text{Non-Negative Variables} \\
&\mspace{40mu} \mathbf{x}\geq 0 \\
\end{align}
 %]]&gt;</script>

<h2 id="converting-into-standard-form">Converting into Standard Form</h2>

<p>我們可以把前例<a href="#eq1">（公式一）</a>轉換成<a href="#eq2">（公式二）</a> 的 standard form 。</p>

<p>首先，為了滿足條件 1.， 「目標是將 Objective Function 最小化。」
必須將 Objective Function 改為 Minimize。這步驟很簡單，把原本的 Objective Function 乘上負號就可以了：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\text{maximize: }  2x_1+3x_2  \\
&\Rightarrow \text{minimize: }  -2x_1-3x_2  \\
\end{align}
 %]]&gt;</script>

<p>下一步，為了滿足條件 2.，「 Constraints 皆為 Equality Constraints 」，則須將 Inequality Constraints 轉換成 Equality Constraints 。 這可以藉由加上 Slack Variable 來達成。</p>

<p>例如 Inequalitn Constraint：</p>

<script type="math/tex; mode=display">
x_1 + x_2  \leq 2 
</script>

<p>可以將它加上 Slack Variable ，<script type="math/tex">x_3</script> 。它可以補足原本不等式少於2的部分，使之等於2。如下：</p>

<script type="math/tex; mode=display">
\Rightarrow  x_1 + x_2 + x_3  = 2 , \mspace{20mu} x_3 \geq 0 
</script>

<p>同理，另一個 Constraints 也可以加上另一個 Slack Variable ，<script type="math/tex">x_4</script> 。</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&x_1 + 2x_2 \leq 3 \\
&\Rightarrow  x_1 + 2x_2 + x_4  = 3 , \mspace{20mu} x_4 \geq 0 \\
\end{align}
 %]]&gt;</script>

<p>經過這些轉換後，<a href="#eq1">（公式一）</a> 變成 Standard Form ，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\text{Objective Function} \\
&\mspace{20mu}\text{minimize: } -2x_1-3x_2  \\
&\text{Constraints} \\
&\mspace{20mu} x_1 + x_2 + x_3 =  2 \\
&\mspace{20mu} x_1 + 2x_2 + x_4 = 3 \\
&\mspace{20mu} x_1 , x_2 , x_3 , x_4 \geq 0  \\
\end{align}
 %]]&gt;</script>

<p>可以再將它寫成矩陣形式：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\mathbf{c} = \begin{bmatrix}
-2 \\
-3 \\
0 \\
0 \\
\end{bmatrix}
\mathbf{x} = \begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
\end{bmatrix}
\mathbf{b} = \begin{bmatrix}
2 \\
3 \\
\end{bmatrix} \\
&\mathbf{A} = \begin{bmatrix}
1 &1 &1 &0 \\
1 &2 &0 &1 \\
\end{bmatrix} \\
\end{align}
 %]]&gt;</script>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\text{minimize: } z =\mathbf{c}^T\mathbf{x}  \\
&\text{subject to: } \mathbf{A}\mathbf{x} = \mathbf{b} \\
&\mathbf{x}\geq 0 \\
\end{align}
 %]]&gt;</script>

<p>至於如何得出 Linear Programming 的最佳解？會在後續文章講解。</p>

<h2 id="reference">Reference</h2>

<p>本文參考交通大學開放式課程：線性規劃</p>

<p>http://ocw.nctu.edu.tw/course_detail.php?bgid=3&amp;gid=0&amp;nid=245#.WS2AasklFYc</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AdaDelta]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/02/08/optimization-method-adadelta/"/>
    <updated>2016-02-08T16:13:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/02/08/optimization-method-adadelta</id>
    <content type="html"><![CDATA[<h2 id="adagrad">AdaGrad</h2>

<p>本文接續 <a href="/blog/2015/12/23/optimization-method-adagrad">Optimization Method – Gradient Descent &amp; AdaGrad </a>。所提到的 <em>AdaGrad</em> ，及改良它的方法 – <em>AdaDelta</em> 。</p>

<p>在機器學習最佳化過程中，用 <em>AdaGrad</em> 可以隨著時間來縮小 <em>Learning Rage</em> ，以達到較好的收斂效果。<em>AdaGrad</em> 的公式如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{G}_{t} = \sum_{n=0}^{t} \textbf{g}_{n}^{2} \\

& \textbf{x}_{t+1} = \textbf{x}_{t} - \frac{\eta}{\sqrt{\textbf{G}_{t}}} \textbf{g}_{t} \\

\end{align}

 %]]&gt;</script>

<p>不過， <em>AdaGrad</em> 有個缺點，由於 <script type="math/tex">\textbf{g}_{n}^{2}</script> 恆為正，故 <script type="math/tex">\textbf{G}_{t} </script> 只會隨著時間增加而遞增，所以 <script type="math/tex">\frac{\eta}{\sqrt{\textbf{G}_{t}}} </script> 只會隨著時間增加而一直遞減，如果 <em>Learning Rate</em> <script type="math/tex">\eta</script>的值太小，則 <em>AdaGrad</em> 會較慢才收斂。</p>

<p>舉個例子，如果目標函數為 <script type="math/tex"> f(x,y) = y^2 - x^2  </script> ，起始點為 <script type="math/tex">(x,y) = (0.001,4)</script> ， <em>Learning Rate</em> <script type="math/tex">\eta=0.5</script> ，則整個最佳化的過程如下圖，曲面為目標函數，紅色的點為 <script type="math/tex">(x,y)</script> ：</p>

<p><img src="/images/pic/pic_00157.png" alt="" /></p>

<!--more-->

<p>動畫版：</p>

<p><img src="/images/pic/pic_00158.gif" alt="" /></p>

<p>從上圖來看，一開始紅色點的下降速度很快，但越後面則越慢。</p>

<p>為了解決此問題，在調整 <em>Learning Rate</em> 時，不要往前一直加到最初的時間點，而只要往前加到某段時間即可。</p>

<p>但如果要從某段時間點的 <script type="math/tex">\textbf{g}_{t}</script> 開始累加，則需要儲存某個時間點之後開始的每個 <script type="math/tex">\textbf{g}_{t}</script> ，這樣會造成記憶體的浪費。有種較簡便的做法，即是用衰減係數 <script type="math/tex">\rho</script> ，將上一時間點的  <script type="math/tex">\textbf{G}_{t-1}</script> 乘上 <script type="math/tex"> \rho</script> ，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{G}_{t} = \rho \textbf{G}_{t-1} + (1 - \rho) \textbf{g}_{t}^{2} \\

& 0 < \rho < 1 \\

\end{align}

 %]]&gt;</script>

<p>藉由衰減係數 <script type="math/tex">\rho</script> ，可讓較早期時間點累加的 <script type="math/tex">\textbf{g}_{t}^{2}</script> 衰減至 0 ，因此，不會使得 <em>Learning Rate</em> 只隨著時間而一直遞減。</p>

<h2 id="correct-units-of-x">Correct Units of ΔX</h2>

<p><em>Adagrad</em> 還有另一個問題，就是 <script type="math/tex">\textbf{x}</script> 的修正量– <script type="math/tex">\Delta{\textbf{x}}</script> 為 <script type="math/tex">\frac{\eta}{\sqrt{\textbf{G}_{t}}} \textbf{g}_{t}</script> ，假設它如果有「單位」的話，它的單位會與 <script type="math/tex">\textbf{x}</script> 不同。 因 <script type="math/tex">\Delta{\textbf{x}}</script>  的單位與 <script type="math/tex">\textbf{g}</script> 的單位相同，而會和 <script type="math/tex">\textbf{x}</script> 不同，因為：</p>

<script type="math/tex; mode=display">

   \text{ units of }\Delta{\textbf{x}}  \propto  \text{ units of } \textbf{g} \propto  \dfrac{\partial f}{\partial x } \propto \frac{1}{  \text{ units of } \textbf{x} }

</script>

<p>註：在此假設 <script type="math/tex">f</script> 無單位。</p>

<p>相較之下， <a href="/blog/2016/01/25/optimization-method-newton"><em>Newton’s Method</em></a> 中， <script type="math/tex">\Delta{\textbf{x}} =  \eta   \textbf{H}^{-1} \textbf{g}</script>， <script type="math/tex">\Delta{\textbf{x}}</script> 的單位與 <script type="math/tex">\textbf{x}</script> 的單位相同，因為：</p>

<script type="math/tex; mode=display">

   \text{ units of }\Delta{\textbf{x}}  \propto  \text{ units of } \textbf{H}^{-1} \textbf{g} \propto 

   \frac{

   \dfrac{\partial f}{\partial x }

   }

   {   \dfrac{\partial^{2} f}{\partial x^{2} }

   }

   \propto   \text{ units of } \textbf{x} 


</script>

<p>但 <em>Newton’s Method</em> 的缺點是，二次微分 <em>Hessian</em> 矩陣的反矩陣 <script type="math/tex">\textbf{H}^{-1}</script> ，計算時間複雜度太高。如果只是為了要單位相同，是沒必要這樣算。</p>

<p>想要簡易求出  <script type="math/tex">\textbf{H}^{-1}</script> 的單位，稍微整理一下以上公式，得出：</p>

<script type="math/tex; mode=display">

   \Delta{\textbf{x}}  \propto  \frac{\dfrac{\partial f}{\partial x } } {\dfrac{\partial^{2} f}{\partial x^{2}}} \Rightarrow  \textbf{H}^{-1} \propto \frac{1 } {\dfrac{\partial^{2} f}{\partial x^{2}}} 

     \propto

   

   \dfrac{\Delta{x}}{\dfrac{\partial f}{\partial x }} \propto  \dfrac{\Delta{x}}{\textbf{g}}  


</script>

<p>因此，若要簡易求出  <script type="math/tex">\textbf{H}^{-1} </script> 的單位，只要算 <script type="math/tex">\dfrac{\Delta{x}}{\textbf{g}}  </script> 即可。</p>

<p>註：如果看不懂這段在寫什麼，請參考<a href="http://arxiv.org/abs/1212.5701">Matthew D. Zeiler. ADADELTA: AN ADAPTIVE LEARNING RATE METHOD.</a></p>

<h2 id="adadelta">AdaDelta</h2>

<p><em>AdaDelta</em> 解決了 <em>AdaGrad</em> 會發生的兩個問題：</p>

<p>(1) <em>Learning Rate</em> 只會隨著時間而一直遞減下去</p>

<p>(2) <script type="math/tex">\Delta{\textbf{x}}</script> 與 <script type="math/tex">\textbf{x}</script> 的單位不同</p>

<p><em>AdaDelta</em> 的公式如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{G}_{t} = \rho \textbf{G}_{t-1} + (1 - \rho) \textbf{g}_{t}^{2} \\


& \Delta \textbf{x}_{t} = - \frac{\sqrt{\textbf{D}_{t - 1}+\epsilon}}{\sqrt{\textbf{G}_{t}+\epsilon}} \textbf{g}_{t} \\


& \textbf{D}_{t} = \rho \textbf{D}_{t-1} + (1 - \rho) \Delta \textbf{x}_{t}^{2} \\


& \textbf{x}_{t+1} = \textbf{x}_{t} + \Delta{x}_{t} \\


\end{align}

 %]]&gt;</script>

<p>其中， <script type="math/tex">\rho</script> 和  <script type="math/tex">\epsilon</script> 為常數。 <script type="math/tex">\rho</script> 的作用為「衰減係數」，而 <script type="math/tex">\epsilon</script> 是為了避免 <script type="math/tex">\frac{\sqrt{\textbf{D}_{t - 1}+\epsilon}}{\sqrt{\textbf{G}_{t}+\epsilon}}</script> 的分母為 0 。</p>

<p>此處的 <script type="math/tex"> \textbf{G}_{t} </script> 有點類似 <em>AdaGrad</em> 裡面的  <script type="math/tex"> \textbf{G}_{t} </script> ，但如前面所述，  <em>AdaDelta</em> 的不是直接把 <script type="math/tex">\textbf{g}_{t}^2</script> 直接累加上去，而是藉由衰減係數 <script type="math/tex">\rho</script> ，可讓較早期時間點累加的 <script type="math/tex">\textbf{g}_{t}^{2}</script> 衰減至 0 ，因此，不會使得 <em>Learning Rate</em> 只隨著時間一直遞減下去。</p>

<p>而 <script type="math/tex">\textbf{D}_{t}</script> 的作用，則是使 <script type="math/tex">\Delta{\textbf{x}}</script> 與 <script type="math/tex">\textbf{x}</script> 有相同的單位，因為 <script type="math/tex"> \frac{\sqrt{\textbf{D}_{t - 1}+\epsilon}}{\sqrt{\textbf{G}_{t}+\epsilon}}</script> 與 <script type="math/tex">\textbf{H}^{-1}</script> 具有相同單位，如下：</p>

<script type="math/tex; mode=display">

 \frac{\sqrt{\textbf{D}_{t - 1}+\epsilon}}{\sqrt{\textbf{G}_{t}+\epsilon}} \propto  \dfrac{\Delta{x}}{\textbf{g}}  \propto  \textbf{H}^{-1}

</script>

<p>根據前一段的結果，若 <script type="math/tex">\Delta{\textbf{x}}  \propto   \textbf{H}^{-1} \textbf{g}</script>，則 <script type="math/tex">\Delta{\textbf{x}}</script> 與 <script type="math/tex">\textbf{x}</script> 的單位相同。</p>

<p>另外，<script type="math/tex">\textbf{D}_{t}</script> 可累加過去時間點的 <script type="math/tex">\Delta{\textbf{x}}</script> ，這樣所造成的效果，有點類似  <a href="/blog/2016/01/16/optimization-method-momentum"><em>Gradient Descent with Momentum</em></a> ，使得現在時間點的 <script type="math/tex">\Delta{\textbf{x}}</script> ，具有過去時間點的動量。</p>

<p>實際帶數字進去算一次 <em>AdaDelta</em> 。舉前述例子，假設 <script type="math/tex"> f(x,y) = y^2 - x^2  </script> ，起始參數為 <script type="math/tex">(x,y) = (0.001,4)</script> ，則畫出來的圖形如下圖，藍色點為起始點位置：</p>

<p><img src="/images/pic/pic_00161.png" alt="" /></p>

<p>用 <em>AdaDelta</em> 最佳化方法，初始值設 <script type="math/tex">\textbf{G}_{0} = [0,0 ]^{T}  </script> ， <script type="math/tex"> \textbf{D}_{0} = [0,0 ]^{T} </script> ，設參數 <script type="math/tex">\rho = 0.5</script> ， <script type="math/tex">\epsilon = 0.1 </script> ，更新 <script type="math/tex">x,y </script> 的值，如下，（註：以下的向量 <script type="math/tex">\textbf{G}</script> 、 <script type="math/tex">\textbf{D}</script> 、 <script type="math/tex">\Delta \textbf{x}</script> 等等的加減乘除運算，皆為 <em>Element-wise Operation</em> ）：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{g}_{1} = 

\begin{bmatrix}

-2x_{0} \\[0.3em]

2y_{0} \\[0.3em]

\end{bmatrix}

=

\begin{bmatrix}

-2 \times 0.001 \\[0.3em]

2 \times 4 \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

-0.002 \\[0.3em]

8 \\[0.3em]

\end{bmatrix} \\



& \textbf{G}_{1} = 0.5 

\begin{bmatrix}

0  \\[0.3em]

0  \\[0.3em]

\end{bmatrix}

+ (1-0.5 ) 

\begin{bmatrix}

(-0.002)^{2}  \\[0.3em]

8^{2}  \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

2 \times 10^{-6}  \\[0.3em]

32  \\[0.3em]

\end{bmatrix} \\


& \Delta \textbf{x}_{1} = - 

\frac{\sqrt{

\begin{bmatrix}

0   \\[0.3em]

0 \\[0.3em]

\end{bmatrix} 

+ 0.1}}

{\sqrt{

\begin{bmatrix}

2 \times 10^{-6}  \\[0.3em]

32 \\[0.3em]

\end{bmatrix} 

 + 0.1}}

\begin{bmatrix}

-0.002  \\[0.3em]

8  \\[0.3em]

\end{bmatrix} 

= 

\begin{bmatrix}

0.00199998 \\[0.3em]

-0.44651646 \\[0.3em]

\end{bmatrix} \\


& \textbf{D}_{1} = 0.5 

\begin{bmatrix}

0  \\[0.3em]

0  \\[0.3em]

\end{bmatrix}

+ (1-0.5 ) 

\begin{bmatrix}

0.00199998^{2}  \\[0.3em]

(-0.44651646)^{2}  \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

1.99996 \times 10^{-6}  \\[0.3em]

0.09968847  \\[0 .3em]

\end{bmatrix} \\


&

\begin{bmatrix}

x_{1} \\[0 .3em]

y_{1} \\[0 .3em]

\end{bmatrix}

=

\textbf{x}_{1} = 

\begin{bmatrix}

0.001 \\[0.3em]

4 \\[0 .3em]

\end{bmatrix}

+ 

\begin{bmatrix}

0.00199998 \\[0.3em]

-0.44651646 \\[0.3em]

\end{bmatrix}

= 

\begin{bmatrix} 

0.00299998 \\[0.3em]

3.55348354 \\[0.3em]

\end{bmatrix}

\end{align}


 %]]&gt;</script>

<p>更新 <script type="math/tex">x,y</script> 的值， <script type="math/tex">x,y = 0.00299998, 3.55348354 \approx 0.00300,3.55348  </script> ，如下圖：</p>

<p><img src="/images/pic/pic_00160.png" alt="" /></p>

<p>再往下走一步， 計算 <script type="math/tex">x,y</script> 的值，如下：  </p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{g}_{2} = 

\begin{bmatrix}

-2x_{1} \\[0.3em]

2y_{1} \\[0.3em]

\end{bmatrix}

=

\begin{bmatrix}

-2 \times 0.00299998 \\[0.3em]

2 \times 3.55348354 \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

-0.00599996 \\[0.3em]

7.10696708 \\[0.3em]

\end{bmatrix} \\



& \textbf{G}_{2} = 0.5 

\begin{bmatrix}

2 \times 10^{-6}  \\[0.3em]

32  \\[0.3em]

\end{bmatrix}

+ (1-0.5 ) 

\begin{bmatrix}

(-0.00599996)^{2}  \\[0.3em]

7.10696708^{2}  \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

1.89997600 \times 10^{-5}  \\[0.3em]

41.25449057  \\[0.3em]

\end{bmatrix} \\



& \Delta \textbf{x}_{2} = - 

\frac{\sqrt{

\begin{bmatrix}

1.99996 \times 10^{−6}   \\[0.3em]

0.09968847 \\[0.3em]

\end{bmatrix} 

+ 0.1}}

{\sqrt{

\begin{bmatrix}

1.89997600 \times 10^{-6}  \\[0.3em]

41.25449057 \\[0.3em]

\end{bmatrix} 

 + 0.1}}

\begin{bmatrix}

-0.00599996 \\[0.3em]

7.10696708 \\[0.3em]

\end{bmatrix} 

= 

\begin{bmatrix}

0.00599945 \\[0.3em]

-0.49385501\\[0.3em]

\end{bmatrix} \\


& \textbf{D}_{2} = 0.5 

\begin{bmatrix}

1.99996 \times 10^{−6}  \\[0.3em]

0.09968847  \\[0.3em]

\end{bmatrix}

+ (1-0.5 ) 

\begin{bmatrix}

0.00599945^{2}  \\[0.3em]

(-0.49385501)^{2}  \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

1.89966806 \times 10^{-5}  \\[0.3em]

0.17179062  \\[0 .3em]

\end{bmatrix} \\


&

\begin{bmatrix}

x_{2} \\[0 .3em]

y_{2} \\[0 .3em]

\end{bmatrix}

=

\textbf{x}_{2} = 

\begin{bmatrix}

0.00299998 \\[0.3em]

3.55348354 \\[0 .3em]

\end{bmatrix}

+ 

\begin{bmatrix}

0.00599945 \\[0.3em]

−0.49385501 \\[0.3em]

\end{bmatrix}

= 

\begin{bmatrix} 

0.00899943 \\[0.3em]

3.05962853 \\[0.3em]

\end{bmatrix}

\end{align}


 %]]&gt;</script>

<p>更新 <script type="math/tex">x,y</script> 的值， <script type="math/tex">x,y = 0.00899943, 3.05962853 \approx 0.00900,3.05963  </script> ，如下圖：</p>

<p><img src="/images/pic/pic_00161.png" alt="" /></p>

<p>重複以上循環，整個過程如下圖：</p>

<p><img src="/images/pic/pic_00162.png" alt="" /></p>

<p>動畫版：</p>

<p><img src="/images/pic/pic_00163.gif" alt="" /></p>

<p>將 <em>Gradient Descent</em> （綠） ， <em>AdaGrad</em> （紅） 和 <em>AdaDelta</em> （藍） 畫在同一張圖上比較看看： </p>

<p><img src="/images/pic/pic_00164.gif" alt="" /></p>

<p>從上圖可看出， <em>AdaDelta</em> 的 <em>Learning Rate</em> 會隨著坡度而適度調整，不會一直遞減下去，也不會像 <em>Gradient Descent</em> 一樣，容易卡在 <em>saddle point</em> （請見<a href="/blog/2015/12/23/optimization-method-adagrad"> Optimization Method – Gradient Descent &amp; AdaGrad </a>）。</p>

<h2 id="implementation">Implementation</h2>

<p>再來進入實作的部分</p>

<p>首先，開啟新的檔案 adadelta.py 並貼上以下程式碼：</p>

<p>```python adadelta.py
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
import matplotlib.pyplot as plt
import numpy as np
from math import sqrt</p>

<p>XT = 0.001
YT = 4</p>

<p>def func(x,y):
  return (y<strong>2-x</strong>2)</p>

<p>def func_grad(x,y):
  return (-2<em>x, 2</em>y)</p>

<p>def plot_func(xt,yt,c=’r’):
  fig = plt.figure()
  ax = fig.gca(projection=’3d’,
        elev=35., azim=-30)
  X, Y = np.meshgrid(np.arange(-5, 5, 0.25), np.arange(-5, 5, 0.25))
  Z = func(X,Y) 
  surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, 
    cmap=cm.coolwarm, linewidth=0.1, alpha=0.3)
  ax.set_zlim(-50, 50)
  ax.scatter(xt, yt, func(xt,yt),c=c, marker=’o’ )
  ax.set_title(“x=%.5f, y=%.5f, f(x,y)=%.5f”%(xt,yt,func(xt,yt))) 
  plt.show()
  plt.close()</p>

<p>def run_adagrad():
  xt, yt = XT, YT
  eta = 0.5 
  Gxt, Gyt = 0, 0
  plot_func(xt,yt,’r’)
  for i in range(20):
    gxt, gyt = func_grad(xt, yt)
    Gxt += gxt<strong>2
    Gyt += gyt</strong>2
    xt -= eta<em>(1./(Gxt<strong>0.5))*gxt
    yt -= eta*(1./(Gyt</strong>0.5))</em>gyt
    if xt &lt; -5 or yt &lt; -5 or xt &gt; 5 or yt &gt; 5:
      break
    plot_func(xt,yt,’r’)</p>

<p>def run_adadelta():
  xt, yt = XT, YT
  epsilon = 0.1
  rho = 0.5
  Gxt, Gyt = 0., 0.
  Dxt, Dyt = 0., 0.
  plot_func(xt,yt,’b’)
  for i in range(20):
    gxt, gyt = func_grad(xt, yt)
    Gxt, Gyt = rho * Gxt + (1-rho) * (gxt<strong>2) , rho * Gyt + (1-rho) * (gyt</strong>2)
    dxt, dyt  = -(sqrt(Dxt + epsilon) / sqrt(Gxt + epsilon)) * gxt , \
                -(sqrt(Dyt + epsilon) / sqrt(Gyt + epsilon)) * gyt
    Dxt, Dyt =  rho * Dxt + (1-rho) * (dxt<strong>2) , rho * Dyt + (1-rho) * (dyt</strong>2)
    xt += dxt
    yt += dyt
    if xt &lt; -5 or yt &lt; -5 or xt &gt; 5 or yt &gt; 5:
      break
    plot_func(xt,yt,’b’)</p>

<p>```</p>

<p>其中， <code>func(x,y)</code> 為目標函數， <code>func_grad(x,y)</code> 為目標函數的 gradient ，而 <code>plot_func(xt,yt,c='r')</code> 可畫出目標函數的曲面， <code>run_adagrad()</code> 用來執行 <em>AdaGrad</em> ， <code>run_adadelta()</code> 用來執行 <em>AdaDelta</em> 。</p>

<p>到 python console 執行：</p>

<p>```sh</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>import adadelta</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>執行 <em>AdaGrad</em> ，指令如下：</p>

<p>```sh</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>adadelta.run_adagrad()</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="/images/pic/pic_00165.png" alt="" /></p>

<p><img src="/images/pic/pic_00166.png" alt="" /></p>

<p><img src="/images/pic/pic_00167.png" alt="" /></p>

<p>以此類推</p>

<p>執行 <em>AdaDelta</em> ，指令如下：</p>

<p>```sh</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>adadelta.run_adadelta()</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="/images/pic/pic_00168.png" alt="" /></p>

<p><img src="/images/pic/pic_00169.png" alt="" /></p>

<p><img src="/images/pic/pic_00170.png" alt="" /></p>

<p>以此類推</p>

<h2 id="reference">Reference</h2>

<p><a href="http://arxiv.org/abs/1212.5701">Matthew D. Zeiler. ADADELTA: AN ADAPTIVE LEARNING RATE METHOD.</a></p>

<p><a href="http://imgur.com/a/Hqolp">Visualizing Optimization Algos</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Newton's Method for Optimization]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/01/25/optimization-method-newton/"/>
    <updated>2016-01-25T16:56:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/01/25/optimization-method-newton</id>
    <content type="html"><![CDATA[<h2 id="gradient-descent">Gradient Descent</h2>

<p>機器學習中，用 <em>Gradient Descent</em> 是解最佳化問題，最基本的方法。關於Gradient Descent的公式，請參考：<a href="/blog/2015/12/23/optimization-method-adagrad">Optimization Method – Gradient Descent &amp; AdaGrad</a></p>

<p>對於 <em>Cost function</em> <script type="math/tex">f(\textbf{x})</script> ，在 <script type="math/tex">\textbf{x} = \textbf{x}_{t}</script> 時， <em>Gradient Descent</em>  走的方向為  <script type="math/tex">  -\nabla f(\textbf{x})</script> 。也就是，用泰勒展開式展開後，用一次微分 <script type="math/tex">f(\textbf{x})</script> 來趨近的方向，如下圖：</p>

<p><img src="/images/pic/pic_00144.png" alt="" /></p>

<p>註：考慮到 <script type="math/tex">\textbf{x}</script> 為向量的情形，故一次微分寫成  <script type="math/tex">\nabla f(\textbf{x})</script> 。 </p>

<p>其中， <script type="math/tex">f(\textbf{x})</script> 為原本的 <em>Cost function</em> ，而 <script type="math/tex">\tilde{f}(\textbf{x})</script> 為泰勒展開式取一次微分逼近的。 而 <em>Gradient Descent</em> 走的方向為 <script type="math/tex"> - \nabla f(\textbf{x}) </script> ，為沿著 <script type="math/tex">\tilde{f}(\textbf{x})</script> 的方向。</p>

<!--more-->

<p>這會有個問題，如果原本的 <em>Cost function</em> 為較高次函數，只用一次項來逼近是不夠的，有時候，失真情形很嚴重，例如， <em>Cost function</em> 為橢圓 <script type="math/tex">f(x,y) = x^{2}+9y^{2} </script>， 此函數的等高線圖，如下圖：</p>

<p><img src="/images/pic/pic_00145.png" alt="" /></p>

<p>如果起始點為 <script type="math/tex">(x,y) = (-4,2.5)</script> ，沿著 <script type="math/tex"> - \nabla f(x) </script> 的方向走，也就是說，走梯度最陡的方向（即與等高線垂直的方向），可能會需要多次折返，才能走到最小值，如下圖：</p>

<p><img src="/images/pic/pic_00146.gif" alt="" /></p>

<h2 id="second-order-taylor-approximation">Second-Order Taylor Approximation</h2>

<p>根據前面的例子得知，只考慮一次微分項，是不夠的，現在要來考慮二次微分項。</p>

<p>對於 <em>Cost function</em> <script type="math/tex">f(\textbf{x})</script> ， 在 <script type="math/tex">\textbf{x} = \textbf{x}_{t}</script> 時，用泰勒展開式展開後，分別用一次微分與二次微分來逼近，如下圖：</p>

<p><img src="/images/pic/pic_00147.png" alt="" /></p>

<p>其中，橘色的 <script type="math/tex">\tilde{f}(\textbf{x})</script> 為只用了一次微分的逼近，而紫色的 <script type="math/tex">\hat{f}(\textbf{x})</script> 為用了一次與二次微分向的逼近，由此可見， <script type="math/tex">\hat{f}(\textbf{x})</script>  較 <script type="math/tex">\tilde{f}(\textbf{x})</script> 接近原本的 <script type="math/tex">f(\textbf{x})</script></p>

<p>如果要求 <script type="math/tex">f(\textbf{x})</script> 的最小值，可以往 <script type="math/tex">\hat{f}(\textbf{x})</script> 為最小值的方向，一步一步走下去。要找出 <script type="math/tex">\hat{f}(\textbf{x})</script> 的最小值，即：</p>

<script type="math/tex; mode=display">

\min_{x} \hat{f} (\textbf{x})  =  \min_{\textbf{x}}  f(\textbf{x}_{t}) + \nabla f(\textbf{x}_{t})^{T}\textbf{x} + \frac{1}{2} \textbf{x}^{T} \nabla^{2}f(\textbf{x}_{t}) \textbf{x} 

</script>

<p>將 <script type="math/tex">\hat{f}(\textbf{x})</script> 對 <script type="math/tex">\textbf{x}</script> 微分，令微分結果為 <script type="math/tex">0</script> ，得：</p>

<script type="math/tex; mode=display">

0 =  \nabla f(\textbf{x}_{t}) +  \nabla^{2}f(\textbf{x}_{t}) \textbf{x} 

</script>

<p>得 </p>

<script type="math/tex; mode=display">

\textbf{x} = -  \nabla^{2}f(\textbf{x}_{t})^{-1} \nabla f(\textbf{x}_{t}) 

</script>

<p>可以用此 <script type="math/tex">\textbf{x}</script> ，來當作位於 <script type="math/tex">\textbf{x}=\textbf{x}_{t}</script> 時，想走往 <script type="math/tex">f(\textbf{x})</script> 的最小值，要走的方向（與距離）。用一次與二次微分所得出的方向，一步步走下去，最後走到最小值，這種方法即為 <em>Newton’s Method</em> 。</p>

<h2 id="newtons-method-for-optimization">Newton’s Method for Optimization</h2>

<p><em>Newton’s Method</em> 即是考慮二次微分的 <em>Gradient Descent</em> 方法，公式如下：</p>

<script type="math/tex; mode=display">

\textbf{x}_{t+1} \leftarrow \textbf{x}_{t} - \eta   \textbf{H}_{t}^{-1} \textbf{g}_{t}

</script>

<p>其中， <script type="math/tex"> \eta</script> 為 <em>Learning Rate</em> ， <script type="math/tex"> \textbf{H}_{t} = \nabla^{2}f(\textbf{x}_{t}) </script> （ 稱為 <em>Hessian</em> ）， <script type="math/tex">\textbf{g}_{t}=\nabla f(\textbf{x}_{t}) </script> （ 稱為 <em>Gradient</em> ）。</p>

<p>再來看看用 <em>Newton’s Method</em> 來解決 <em>Cost function</em> 為橢圓 <script type="math/tex">f(x,y) = x^{2}+9y^{2} </script> 的情形。首先，畫出起始點 <script type="math/tex">(-4, 2.5) </script> ，如下圖：</p>

<p><img src="/images/pic/pic_00148.png" alt="" /></p>

<p>先來算 <script type="math/tex">\textbf{g}</script> 和 <script type="math/tex">\textbf{H}^{-1}</script> ，分別為：</p>

<script type="math/tex; mode=display">

\textbf{g} = 

\begin{bmatrix}

  \dfrac{ \partial f(x,y) }{\partial x}  \\[0.3em]

  \dfrac{\partial f(x,y) } {\partial y}  \\[0.3em]

\end{bmatrix} 

= 

\begin{bmatrix}

  2x  \\[0.3em]

  18y \\[0.3em]

\end{bmatrix} 

</script>

<script type="math/tex; mode=display">% &lt;![CDATA[


\textbf{H}^{-1} = 

\begin{bmatrix}

  \dfrac{ \partial^{2} f(x,y) }{\partial x^{2}}  &  

  \dfrac{ \partial^{2} f(x,y) }{\partial xy}

  \\[0.3em]

  \dfrac{ \partial^{2} f(x,y) } {\partial xy} &

  \dfrac{ \partial^{2} f(x,y) }{\partial y^{2}}   

  \\[0.3em]

\end{bmatrix} ^{-1}

= 

\begin{bmatrix}

  2  &  

  0

  \\[0.3em]

  0 &

  18 

  \\[0.3em]

\end{bmatrix} ^{-1}

=

\begin{bmatrix}

  \frac{1}{2}  &  

  0

  \\[0.3em]

  0 &

  \frac{1}{18} 

  \\[0.3em]

\end{bmatrix} 

 %]]&gt;</script>

<p>設 <script type="math/tex"> \eta = 0.5 </script> ，代入起始點  <script type="math/tex">(x_{0},y_{0}) = (-4, 2.5) </script> 、 <script type="math/tex">\textbf{g}</script> 和 <script type="math/tex">\textbf{H}^{-1}</script> 到 <em>Newton’s Method</em> 的公式： <script type="math/tex">\textbf{x}_{t+1} \leftarrow \textbf{x}_{t} - \eta   \textbf{H}_{t}^{-1} \textbf{g}_{t}</script> ，得：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{bmatrix}

  x_{1}

  \\[0.3em]

  y_{1} 

  \\[0.3em]

\end{bmatrix} 

= 

\begin{bmatrix}

  -4 

  \\[0.3em]

  2.5  

  \\[0.3em]

\end{bmatrix} 

- 0.5 

\begin{bmatrix}

  \frac{1}{2}  &  

  0

  \\[0.3em]

  0 &

  \frac{1}{18} 

  \\[0.3em]

\end{bmatrix} 

\begin{bmatrix}

  2 \times (-4)  \\[0.3em]

  18 \times 2.5 \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

  -2  

  \\[0.3em]

  1.25

  \\[0.3em]

\end{bmatrix} 


 %]]&gt;</script>

<p>更新圖上的點， <script type="math/tex">(x_{1},y_{1}) = (-2, 1.25) </script> ，如下圖：</p>

<p><img src="/images/pic/pic_00155.png" alt="" /></p>

<p>再往下走一步，求 <script type="math/tex">(x_{2},y_{2})</script> 的值，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{bmatrix}

  x_{2}

  \\[0.3em]

  y_{2} 

  \\[0.3em]

\end{bmatrix} 

= 

\begin{bmatrix}

  -2 

  \\[0.3em]

  1.25  

  \\[0.3em]

\end{bmatrix} 

- 0.5 

\begin{bmatrix}

  \frac{1}{2}  &  

  0

  \\[0.3em]

  0 &

  \frac{1}{18} 

  \\[0.3em]

\end{bmatrix} 

\begin{bmatrix}

  2 \times (-2)  \\[0.3em]

  18 \times 1.25 \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

  -1  

  \\[0.3em]

  0.625

  \\[0.3em]

\end{bmatrix} 


 %]]&gt;</script>

<p><img src="/images/pic/pic_00150.png" alt="" /></p>

<p>從以上過程發現，  <em>Newton’s Method</em>  方向不需要一直折返，可以直接往最小值處走下去 ，整個過程如下圖：</p>

<p><img src="/images/pic/pic_00151.gif" alt="" /></p>

<p>註：事實上，由於本例的 <em>Cost function</em> <script type="math/tex">f(x,y)</script> 為二次函數，如果是用二次的泰勒展開式逼近，則可以完全貼合 <script type="math/tex">f(x,y)</script> 。所以用  <em>Newton’s Method</em> 的話， 位於 <script type="math/tex">\textbf{x}_{t}</script> 時， <script type="math/tex"> -  \nabla^{2}f(\textbf{x}_{t})^{-1} \nabla f(\textbf{x}_{t}) </script> 即是泰勒展開式最小值的 <script type="math/tex">\textbf{x}</script> 解，也是 <script type="math/tex">f(x,y)</script> 的最小值解，如果設 <script type="math/tex"> \eta = 1 </script> ，只要走一步就可以走到最小值，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{bmatrix}

  x_{1}

  \\[0.3em]

  y_{1} 

  \\[0.3em]

\end{bmatrix} 

= 

\begin{bmatrix}

  -4 

  \\[0.3em]

  2.5  

  \\[0.3em]

\end{bmatrix} 

- 

\begin{bmatrix}

  \frac{1}{2}  &  

  0

  \\[0.3em]

  0 &

  \frac{1}{18} 

  \\[0.3em]

\end{bmatrix} 

\begin{bmatrix}

  2 \times (-4)  \\[0.3em]

  18 \times 2.5 \\[0.3em]

\end{bmatrix} 

=

\begin{bmatrix}

  0

  \\[0.3em]

  0

  \\[0.3em]

\end{bmatrix} 


 %]]&gt;</script>

<p>過程如下圖所示：</p>

<p><img src="/images/pic/pic_00152.png" alt="" /></p>

<h2 id="implementation">Implementation</h2>

<p>再來進入實作的部分</p>

<p>首先，開啟新的檔案 newtons.py 並貼上以下程式碼：</p>

<p>```python newtons.py
import numpy as np
import matplotlib.pyplot as plt</p>

<p>A = 1
B = 9</p>

<p>def obj_func(x, y):
    z = A*(x<strong>2) + B*(y</strong>2)
    return z</p>

<p>def obj_func_grad(x, y):
    return np.array([2<em>A</em>x, 2<em>B</em>y])</p>

<p>def obj_func_hessian(x, y):
    return np.array([[2<em>A, 0],
                     [0, 2</em>B]
                     ])</p>

<p>def plot_func(xts, yts, c):
    delta = 0.1
    x = np.arange(-5.0, 5.0, delta)
    y = np.arange(-3.0, 3.0, delta)
    X, Y = np.meshgrid(x, y)
    Z = obj_func(X, Y)
    plt.figure(figsize=(10, 5))
    CS = plt.contour(X, Y, Z, colors=’gray’)
    plt.plot(xts, yts, c=’r’)
    for xt, yt in zip(xts, yts):
            plt.scatter(xt, yt, c=’r’)
    plt.title(“x=%.5f, y=%.5f, f(x, y)=%.5f”%(xts[-1], yts[-1], obj_func(xts[-1], yts[-1])))
    plt.clabel(CS, inline=1, fontsize=10)
    plt.show()</p>

<p>def run_gd():
    xy = np.array([-4, 2.5])
    eta = 0.1
    xts = [xy[0]]
    yts = [xy[1]]
    plot_func(xts, yts, ‘r’)
    for i in range(1, 20):
        gxy = obj_func_grad(xy[0], xy[1])
        xy -= eta * gxy
        xts.append(xy[0])
        yts.append(xy[1])
        plot_func(xts, yts, ‘r’)</p>

<p>def run_newtons():
    xy = np.array([-4, 2.5])
    eta = 0.5
    xts = [xy[0]]
    yts = [xy[1]]
    plot_func(xts, yts, ‘b’)
    for i in range(1, 20):
        gxy = obj_func_grad(xy[0], xy[1])
        hxy = obj_func_hessian(xy[0], xy[1])
        deltax = np.dot(np.linalg.inv(hxy), gxy)
        xy -= eta * deltax
        xts.append(xy[0])
        yts.append(xy[1])
        plot_func(xts, yts, ‘b’)</p>

<p>```</p>

<p>其中， <code>obj_func(x,y)</code> 為目標函數， <code>obj_func_grad(x,y)</code> 為 <script type="math/tex">\textbf{g}</script> ， <code>obj_func_hessian(x,y)</code>  <script type="math/tex">\textbf{H}</script> ，而 <code>plot_function(xt,yt,c='r')</code> 可畫出目標函數的等高線圖， <code>run_gd()</code> 用來執行 <em>Gradient Descent</em> ， <code>run_newtons()</code> 用來執行 <em>Newton’s Method</em> 。 <code>xy</code> 對應到前例的 <script type="math/tex">(x,y)</script> ，而 <code>eta</code> 為 <em>Learning Rate</em> 。 <code>for i in range(20)</code> 表示最多會跑20個迴圈。</p>

<p>到 python console 執行：</p>

<p>```</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>import newtons</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>執行 <em>Gradient Descent</em> ，指令如下：</p>

<p>```</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>newtons.run_gd()</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="/images/pic/pic_00153.png" alt="" /></p>

<p><img src="/images/pic/pic_00154.png" alt="" /></p>

<p>以此類推</p>

<p>執行 <em>Newton’s Method</em> ，指令如下：</p>

<p>```</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>netons.run_newtons()</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="/images/pic/pic_00155.png" alt="" /></p>

<p><img src="/images/pic/pic_00156.png" alt="" /></p>

<p>以此類推</p>

<h2 id="comment">Comment</h2>

<p><em>Newton’s Method</em> 需要計算二次微分 <em>Hessian</em> 矩陣的反矩陣，如果 <em>variable</em> 為高維度向量，則計算這個矩陣的時間複雜度會很高，而且很占記憶體空間，因此有人提出一些 <em>Hessian</em> 矩陣的近似求法，例如 <a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS"><em>L-BFGS</em></a> 。但如果用在像是 <em>Deep Learning</em> 這種有超多 <em>variable</em> 的模型，近似求法仍然太慢，因此解 <em>Deep Learning</em> 問題，通常只會用一次微分的方法，例如 <a href="/blog/2015/12/23/optimization-method-adagrad"><em>Adagrad</em></a>之類的。</p>

<h2 id="reference">Reference</h2>

<p>本文參考至以下教科書：</p>

<p>Stephen Boyd &amp; Lieven Vandenberghe. Convex Optimization. Chapter 5 Duality.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Gradient Descent With Momentum]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/01/16/optimization-method-momentum/"/>
    <updated>2016-01-16T08:01:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/01/16/optimization-method-momentum</id>
    <content type="html"><![CDATA[<h2 id="gradient-descent">Gradient Descent</h2>

<p>在機器學習的過程中，常需要將 Cost Function 的值減小，通常用 Gradient Descent 來做最佳化的方法來達成。但是用 Gradient Descent 有其缺點，例如，很容易卡在 Local Minimum。</p>

<p><em>Gradient Descent</em> 的公式如下：</p>

<script type="math/tex; mode=display">

\textbf{x}_{t+1} \leftarrow \textbf{x}_{t} - \eta \textbf{g}_{t}

</script>

<p>關於Gradient Descent的公式解說，請參考：<a href="/blog/2015/12/23/optimization-method-adagrad">Optimization Method – Gradient Descent &amp; AdaGrad</a></p>

<h2 id="getting-stuck-in-local-minimum">Getting Stuck in Local Minimum</h2>

<p>舉個例子，如果 Cost Function 為 <script type="math/tex">0.3y^{3}+y^{2}+0.3x^{3}+x^{2}</script> ，有 Local Minimum <script type="math/tex">(x=0,y=0)</script> ，畫出來的圖形如下：</p>

<p><img src="/images/pic/pic_00131.png" alt="" /></p>

<!--more-->

<p>當執行 Gradient Descent 的時候，則會卡在 Local Minimum，如下圖：</p>

<p><img src="/images/pic/pic_00132.gif" alt="" /></p>

<p>解決卡在 Local Minimum 的方法，可加入 Momentum ，使它在 Gradient 等於零的時候，還可繼續前進。</p>

<h2 id="gradient-descent-with-momentum">Gradient Descent with Momentum</h2>

<p>Momentum 的概念如下： 當一顆球從斜坡上滾到平地時，球在平地仍會持續滾動，因為球具有動量，也就是說，它的速度跟上一個時間點的速度有關。</p>

<p>模擬 Momentum的方式很簡單，即是把上一個時間點用 Gradient 得出的變化量也考慮進去。</p>

<p><em>Gradient Descent with Momentum</em> 的公式如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \Delta_{\textbf{x},t+1 } \leftarrow  \beta \Delta_{\textbf{x},t } +  (1-\beta) \eta \textbf{g}_{t} 

& \text{, where }  0 <  \beta < 1 \\

\\

& \textbf{x}_{t+1} \leftarrow \textbf{x}_{t} - \Delta_{\textbf{x},t+1 } 

\end{align}

 %]]&gt;</script>

<p>其中 <script type="math/tex"> \Delta_{\textbf{x},t +1} </script> 為 <script type="math/tex">t+1</script> 時間點，修正 <script type="math/tex">\textbf{x}</script> 值所用的變化量，而 <script type="math/tex">\Delta_{\textbf{x},t }</script> 則是 <script type="math/tex">t</script> 時間點的修正量，而 <script type="math/tex"> \beta </script> 則是用來控制在 <script type="math/tex">t+1</script> 時間點中的 <script type="math/tex">\Delta_{\textbf{x},t+1}</script> 具有上個時間點的 <script type="math/tex">\Delta_{\textbf{x},t}</script> 值的比例。 好比說，在 <script type="math/tex">t+1</script> 時間點時，球的速度會跟 <script type="math/tex">t</script> 時間點有關。 而 <script type="math/tex">(1-\beta)</script> ，則是 <script type="math/tex">t+1</script> 時間點算出之 Gradient <script type="math/tex">\textbf{g}_{t}</script> 乘上 Learning Rate <script type="math/tex">\eta</script> 後，在 <script type="math/tex">\Delta_{\textbf{x},t+1}</script> 中所占的比例。</p>

<p>舉前述例子，若起始參數為 <script type="math/tex">(x=3,y=3)</script>  ，畫出目標函數，藍點為起始點 <script type="math/tex">(x,y)</script> 的位置：</p>

<p><img src="/images/pic/pic_00141.png" alt="" /></p>

<p>用 Gradient Descent with Momentum 來更新 <script type="math/tex">x,y</script> 的值，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \Delta_{x,t+1 } \leftarrow  \beta \Delta_{x,t } +  (1-\beta) \eta \dfrac{\partial f(x_{t},y_{t})}{\partial x_{t}} \\

& \Delta_{y,t+1 } \leftarrow  \beta \Delta_{y,t } +  (1-\beta) \eta \dfrac{\partial f(x_{t},y_{t})}{\partial y_{t}} \\

& x_{t+1} \leftarrow x_{t} - \Delta_{x,t+1 }  \\

& y_{t+1} \leftarrow y_{t} - \Delta_{y,t+1 } 


\end{align}

 %]]&gt;</script>

<p>化減後得：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \Delta_{x,t+1 } \leftarrow  \beta \Delta_{x,t } +  (1-\beta) \eta (0.9 x_{t}^{2} + 2x_{t} ) \\

& \Delta_{y,t+1 } \leftarrow  \beta \Delta_{y,t } +  (1-\beta) \eta (0.9 y_{t}^{2} + 2y_{t} ) \\

& x_{t+1} \leftarrow x_{t} - \Delta_{x,t+1 }  \\

& y_{t+1} \leftarrow y_{t} - \Delta_{y,t+1 } 


\end{align}

 %]]&gt;</script>

<p>設初始化值 <script type="math/tex">  \Delta_{x} = 0,  \Delta_{y} = 0 </script> ，參數 <script type="math/tex">\beta = 0.9, \eta = 0.2 </script> ，代入 <script type="math/tex"> x=3,y=3 </script> ，則：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \Delta_{x,1 } =  0.9 \times  0 +  (1-0.9)\times 0.2 \times (0.9\times 3^{2}+2\times 3) = 0.282 \\

& \Delta_{y,1 } =  0.9 \times  0 +  (1-0.9)\times 0.2 \times (0.9\times 3^{2}+2\times 3) = 0.282 \\

& x_{1} = 3 - \Delta_{x,1 } = 3 - 0.282 = 2.718 \\

& y_{1} = 3 - \Delta_{y,1 } = 3 - 0.282 = 2.718

\end{align}

 %]]&gt;</script>

<p>更新圖上的藍點，如下圖：</p>

<p><img src="/images/pic/pic_00142.png" alt="" /></p>

<p>再往下走一步， <script type="math/tex"> x,y </script>  的值如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \Delta_{x,2 } =  0.9 \times  0.282 +  (1-0.9)\times 0.2 \times (0.9\times 2.718^{2}+2\times 2.718) = 0.4955 \\

& \Delta_{y,2 } =  0.9 \times  0.282 +  (1-0.9)\times 0.2 \times (0.9\times 2.718^{2}+2\times 2.718) = 0.4955\\

& x_{2} = 3 - \Delta_{x,2 } = 2.718  - 0.4955 = 2.2225 \\

& y_{2} = 3 - \Delta_{y,2 } = 2.718  - 0.4955 = 2.2225

\end{align}

 %]]&gt;</script>

<p>更新圖上的藍點，如下圖：</p>

<p><img src="/images/pic/pic_00143.png" alt="" /></p>

<p>在以上兩步中，可發現 <script type="math/tex"> \Delta_{x }, \Delta_{y }</script> 的值逐漸變大。由於一開始 <script type="math/tex"> \Delta_{x }, \Delta_{y }</script> 都是零，它會跟前一個時間點的值有關，所以看起來就好像是球從斜坡上滾下來時，慢慢加速，而在球經過 Local Minimum時，也會慢慢減速，不會直接卡在 Local Minimum 。整個過程如下圖：</p>

<p><img src="/images/pic/pic_00136.png" alt="" /></p>

<p>動畫版：</p>

<p><img src="/images/pic/pic_00137.gif" alt="" /></p>

<h2 id="implementation">Implementation</h2>

<p>再來進入實作的部分</p>

<p>首先，開啟新的檔案 momentum.py 並貼上以下程式碼：</p>

<p>```python momentum.py
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
import matplotlib.pyplot as plt
import numpy as np</p>

<p>def func(x,y):
  return (0.3<em>y<strong>3+y</strong>2+0.3</em>x<strong>3+x</strong>2)</p>

<p>def func_grad(x,y):
  return (0.9<em>x<strong>2+2*x, 0.9*y</strong>2+2</em>y )</p>

<p>def plot_func(xt,yt,c=’r’):
  fig = plt.figure()
  ax = fig.gca(projection=’3d’,
        elev=7., azim=-175)
  X, Y = np.meshgrid(np.arange(-5, 5, 0.25), np.arange(-5, 5, 0.25))
  Z = func(X,Y) 
  surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, 
    cmap=cm.coolwarm, linewidth=0.1, alpha=0.3)
  ax.set_zlim(-20, 100)
  ax.scatter(xt, yt, func(xt,yt),c=c, marker=’o’ )
  ax.set_title(“x=%.5f, y=%.5f, f(x,y)=%.5f”%(xt,yt,func(xt,yt))) 
  plt.show()
  plt.close()</p>

<p>def run_grad():
  xt = 3 
  yt = 3 
  eta = 0.1
  plot_func(xt,yt,’r’)
  for i in range(20):
    gxt, gyt = func_grad(xt,yt)
    xt = xt - eta * gxt
    yt = yt - eta * gyt
    if xt &lt; -5 or yt &lt; -5 or xt &gt; 5 or yt &gt; 5:
      break
    plot_func(xt,yt,’r’)</p>

<p>def run_momentum():
  xt = 3 
  yt = 3 
  eta = 0.2
  beta = 0.9
  plot_func(xt,yt,’b’)
  delta_x = 0
  delta_y = 0
  for i in range(20):
    gxt, gyt = func_grad(xt,yt)
    delta_x = beta * delta_x + (1-beta)<em>eta</em>gxt
    delta_y = beta * delta_y + (1-beta)<em>eta</em>gyt
    xt = xt - delta_x
    yt = yt - delta_y 
    if xt &lt; -5 or yt &lt; -5 or xt &gt; 5 or yt &gt; 5:
      break
    plot_func(xt,yt,’b’)</p>

<p>```</p>

<p>其中， <code>func(x,y)</code> 為目標函數，<code>func_grad(x,y)</code> 為目標函數的 <em>gradient</em> ，而 <code>plot_func(xt,yt,c='r')</code> 可畫出目標函數的曲面， <code>run_grad()</code> 用來執行 <em>Gradient Descent</em> ， <code>run_momentum()</code> 用來執行 <em>Gradient Descent with Momentum</em> 。 <code>xt</code> 和 <code>yt</code> 對應到前例的 <script type="math/tex">(x,y)</script> ，而 <code>eta</code> 為 <em>Learning Rate</em> 。 <code>for i in range(20)</code> 表示最多會跑20個迴圈，而 <code>if xt &lt; -5 or yt &lt; -5 or xt &gt; 5 or yt &gt; 5</code> 表示，如果 <code>xt</code> 和 <code>yt</code> 超出邊界，則會先結束迴圈。</p>

<p>到 python console 執行：</p>

<p>```sh</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>import momentum</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>執行 <em>Gradient Descent</em> ，指令如下：</p>

<p>```sh</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>momentum.run_grad()</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="/images/pic/pic_00138.png" alt="" /></p>

<p><img src="/images/pic/pic_00139.png" alt="" /></p>

<p><img src="/images/pic/pic_00140.png" alt="" /></p>

<p>以此類推</p>

<p>執行 <em>Gradient Descent with Momentum</em> ，指令如下：</p>

<p>```sh</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>momentum.run_momentum()</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="/images/pic/pic_00141.png" alt="" /></p>

<p><img src="/images/pic/pic_00142.png" alt="" /></p>

<p><img src="/images/pic/pic_00143.png" alt="" /></p>

<p>以此類推</p>

<h2 id="reference">Reference</h2>

<h4 id="visualizing-optimization-algos">Visualizing Optimization Algos</h4>

<p>http://imgur.com/a/Hqolp</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Gradient Descent & AdaGrad]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/12/23/optimization-method-adagrad/"/>
    <updated>2015-12-23T17:14:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/12/23/optimization-method-adagrad</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>在機器學習的過程中，常需要將 <em>Cost Function</em> 的值減小，需由最佳化的方法來達成。本文介紹 <em>Gradient Descent</em> 和 <em>AdaGrad</em> 兩種常用的最佳化方法。</p>

<h2 id="gradient-descent">Gradient Descent</h2>

<p><em>Gradient Descent</em> 的公式如下：</p>

<script type="math/tex; mode=display">

\textbf{x}_{t+1} \leftarrow \textbf{x}_{t} - \eta \textbf{g}_{t}

</script>

<p>其中， <script type="math/tex">\eta</script> 為 <em>Learning Rate</em> ， <script type="math/tex">\textbf{x} </script> 為最佳化時要調整的參數， <script type="math/tex">\textbf{g}</script> 為最佳化目標函數對 <script type="math/tex">\textbf{x}</script> 的梯度。 <script type="math/tex">\textbf{x}_{t}</script> 為調整之前的 <script type="math/tex">\textbf{x} </script> ，<script type="math/tex">\textbf{x}_{t+1}</script> 為調整之後的 <script type="math/tex">\textbf{x} </script> 。</p>

<p>舉個例子，如果目標函數為 <script type="math/tex"> f(x,y) = y^2 - x^2  </script> ，起始參數為 <script type="math/tex">(x,y) = (0.001,4)</script> ，則畫出來的圖形如下圖，曲面為目標函數，紅色的點為起始參數：</p>

<p><img src="/images/pic/pic_00126.png" alt="" /></p>

<!--more-->

<p>可藉由改變 <script type="math/tex">(x,y)</script> 來讓 <script type="math/tex">f(x,y)</script> 的值減小。 <em>Gradient Descent</em> 所走的方向為梯度最陡的方向，若 <script type="math/tex">eta=0.3</script> 則 ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& 	x \leftarrow  x - \eta  \dfrac{\partial f(x,y)}{\partial x}  \\

&  y \leftarrow  y - \eta  \dfrac{\partial f(x,y)}{\partial y} \\

\end{align}

 %]]&gt;</script>

<p>求出微分後得：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& 	x \leftarrow  x - \eta  \times (-2x)  \\

&  y \leftarrow  y - \eta  \times 2y \\

\end{align}

 %]]&gt;</script>

<p>代入數值，得：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x = 0.001 - 0.3 \times (-2) \times 0.001 = 0.0016 \\

& y = 4 - 0.3 \times 2 \times 4 = 1.6 \\

\end{align}

 %]]&gt;</script>

<p>更新完後的結果如下：</p>

<p><img src="/images/pic/pic_00127.png" alt="" /></p>

<p>從上圖可看出，紅點移動到比較低的地方，即 <script type="math/tex">f(x,y)</script> 變小了。</p>

<p>經過了數次改變 <script type="math/tex">(x,y)</script> 值的循環之後，<script type="math/tex">f(x,y)</script> 的值會越變越小，紅點移動的路徑如下圖所示：</p>

<p><img src="/images/pic/pic_00118.png" alt="" /></p>

<p>動畫版：</p>

<p><img src="/images/pic/pic_00119.gif" alt="" /></p>

<p>從上圖可發現，紅色的點會卡在 <script type="math/tex">(0,0)</script> 附近（也就是Saddle Point），過了一陣子後才會繼續往下滾。</p>

<h2 id="adagrad">AdaGrad</h2>

<p><em>Gradient Descent</em> 的缺點有：</p>

<p>(1) <em>Learning Rate</em> 不會隨著時間而減少</p>

<p>(2) <em>Learning Rate</em> 在每個方向是固定的</p>

<p>以上的(1)會使得在越接近近目標函數最小值時，越容易走過頭，(2)則會容易卡在目標函數的Saddle Point。</p>

<p>因為 <em>Gradient Descent</em> 只考慮目前的 <em>Gradient</em> ，如果可以利用過去時間在各個方向的 <em>Gradient</em> ，來調整現在時間點在各個方向的 <em>Learning Rate</em> ，則可避免以上兩種情型發生。</p>

<p><em>AdaGrad</em> 的公式如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{G}_{t} = \sum_{n=0}^{t} \textbf{g}_{n}^{2} \\

& \textbf{x}_{t+1} \leftarrow \textbf{x}_{t} - \frac{\eta}{\sqrt{\textbf{G}_{t}}} \textbf{g}_{t} \\

\end{align}

 %]]&gt;</script>

<p>其中，<script type="math/tex"> \textbf{G}_{t} </script> 為過去到現在所有時間點所有的 <script type="math/tex">\textbf{g}</script> 的平方和。由於  <script type="math/tex">\textbf{x}</script> ， <script type="math/tex">\textbf{g}</script>和 <script type="math/tex">\textbf{G}</script> 皆為向量，設 <script type="math/tex">x_{i}</script> ， <script type="math/tex">g_{i}</script> 和 <script type="math/tex">G_{i}</script> 各為其元素，則公式可寫成：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& G_{i,t} = \sum_{n=0}^{t} g_{i,n}^{2} \\

& x_{i,t+1} \leftarrow x_{i,t} - \frac{\eta}{\sqrt{G_{i,t}}} g_{i,t} \\

\end{align}

 %]]&gt;</script>

<p>這公式可修正以上兩個 <em>Gradient Descent</em> 的缺點：</p>

<p>1.若時間越久，則 <em>Gradient</em> 平方和越大，使得 <em>Learning Rate</em> 越小，這樣就可以讓 <em>Learning Rate</em> 隨著時間減少，而在接近目標函數的最小值時，比較不會走過頭。</p>

<p>2.若某方向從過去到現在時間點 <em>Gradient</em> 平方和越小，則 <em>Learning Rate</em> 要越大。（直覺上來講，過去時間點 <em>Gradient</em> 越小的方向，在未來可能越重要，這種概念有點類似<a href="/blog/2014/04/14/natural-language-processing-tf-idf">tf-idf</a>，在越少文檔中出現的詞，可能越重要。）由於各方向的 <em>Learning Rate</em> 不同，比較不會卡在 <em>Saddle Point</em> 。</p>

<p>前述例子，起始參數為 <script type="math/tex">(x,y) = (0.001,4)</script> ，則畫出來的圖形如下圖，曲面為目標函數，藍點為起始參數：</p>

<p><img src="/images/pic/pic_00128.png" alt="" /></p>

<p>用 <em>AdaGrad</em> 來更新 <script type="math/tex">(x,y)</script> 的值，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& 	x_{t+1} \leftarrow  x_{t} - \frac{\eta}{\sqrt{\sum_{n=0}^{t}  

(\dfrac{\partial f(x_{n},y_{n})}{\partial x_{n}} )^{2} }} 

\dfrac{\partial f(x_{t},y_{t})}{\partial x_{t}}  \\


&  y_{t+1} \leftarrow  y_{t} - \frac{\eta}{\sqrt{\sum_{n=0}^{t}  

(\dfrac{\partial f(x_{n},y_{n})}{\partial y_{n}} )^{2}  }} 

\dfrac{\partial f(x_{t},y_{t})}{\partial y_{t}} \\

\end{align}

 %]]&gt;</script>

<p>化簡後得：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& 	x_{t+1} \leftarrow  x_{t} - \frac{\eta}{\sqrt{\sum_{n=0}^{t}  

( -2x_{n} )^{2} }} 

( -2x_{t} ) \\


&  y_{t+1} \leftarrow  y_{t} - \frac{\eta}{\sqrt{\sum_{n=0}^{t}  

( 2y_{n} )^{2} }} 

( 2y_{t} ) \\

\end{align}

 %]]&gt;</script>

<p>由於 <em>AdaGrad</em> 的 <em>Learning Rate</em> 會隨時間減小，所以初始化時可以給它較大的值，此例中，設 <script type="math/tex">\eta = 1.0</script></p>

<p>代入 <script type="math/tex">(x,y)</script> 的數值，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x = 0.001 -  \frac{1.0}{\sqrt{  ( (-2) \times 0.001 )^2  }} \times (-2) \times 0.001 = 1.001 \\

& x = 4 -  \frac{1.0}{\sqrt{  ( 2 \times 4 )^2  }} \times 2 \times 4 = 3 \\

\end{align}

 %]]&gt;</script>

<p>更新圖上的藍點，如下圖：</p>

<p><img src="/images/pic/pic_00129.png" alt="" /></p>

<p>再往下走一步， <script type="math/tex">(x,y)</script> 的值如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x = 1.001 -  \frac{1.0}{\sqrt{  ( (-2) \times 0.001 )^2  + ( (-2) \times 1.001 )^2 }} \times (-2) \times 1.001 = 2.001 \\

& x = 3 -  \frac{1.0}{\sqrt{  ( 2 \times 4 )^2 +  ( 2 \times 3 )^2  }} \times 2 \times 3 = 2.4 \\

\end{align}

 %]]&gt;</script>

<p>更新圖上的藍點，如下圖：</p>

<p><img src="/images/pic/pic_00130.png" alt="" /></p>

<p>經過了數次改變 <script type="math/tex">(x,y)</script> 值的循環之後，<script type="math/tex">f(x,y)</script> 的值會越變越小，藍點移動的路徑如下圖所示：</p>

<p><img src="/images/pic/pic_00123.png" alt="" /></p>

<p>動畫版：</p>

<p><img src="/images/pic/pic_00124.gif" alt="" /></p>

<p>由此可以發現， <em>AdaGrad</em> 不會卡在 <em>Saddle Point</em> 。</p>

<p>將 <em>Gradient Descent</em> 和 <em>AdaGrad</em> 畫在同一張圖上，比較兩者差異：</p>

<p><img src="/images/pic/pic_00125.gif" alt="" /></p>

<h2 id="implementation">Implementation</h2>

<p>再來進入實作的部分：</p>

<p>首先,開啟新的檔案 adagrad.py 並貼上以下程式碼</p>

<p>```python adagrad.py
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
import matplotlib.pyplot as plt
import numpy as np</p>

<p>def func(x,y):
  return (y<strong>2-x</strong>2)</p>

<p>def func_grad(x,y):
  return (-2<em>x, 2</em>y)</p>

<p>def plot_func(xt,yt,c=’r’):
  fig = plt.figure()
  ax = fig.gca(projection=’3d’,
        elev=35., azim=-30)
  X, Y = np.meshgrid(np.arange(-5, 5, 0.25), np.arange(-5, 5, 0.25))
  Z = func(X,Y) 
  surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, 
    cmap=cm.coolwarm, linewidth=0.1, alpha=0.3)
  ax.set_zlim(-50, 50)
  ax.scatter(xt, yt, func(xt,yt),c=c, marker=’o’ )
  ax.set_title(“x=%.5f, y=%.5f, f(x,y)=%.5f”%(xt,yt,func(xt,yt))) 
  plt.show()
  plt.close()</p>

<p>def run_grad():
  xt = 0.001 
  yt = 4 
  eta = 0.3 
  plot_func(xt,yt,’r’)
  for i in range(20):
    gx, gy = func_grad(xt, yt)
    xt = xt - eta<em>gx
    yt = yt - eta</em>gy
    if xt &lt; -5 or yt &lt; -5 or xt &gt; 5 or yt &gt; 5:
      break
    plot_func(xt,yt,’r’)</p>

<p>def run_adagrad():
  xt = 0.001
  yt = 4 
  eta = 1.0 
  Gxt = 0
  Gyt = 0
  plot_func(xt,yt,’b’)
  for i in range(20):
    gxt,gyt = func_grad(xt, yt)
    Gxt += gxt<strong>2
    Gyt += gyt</strong>2
    xt = xt - eta<em>(1./(Gxt<strong>0.5))*gxt
    yt = yt - eta*(1./(Gyt</strong>0.5))</em>gyt
    if xt &lt; -5 or yt &lt; -5 or xt &gt; 5 or yt &gt; 5:
      break
    plot_func(xt,yt,’b’)</p>

<p>```</p>

<p>其中， <code>func(x,y)</code> 為目標函數，<code>func_grad(x,y)</code> 為目標函數的 <em>gradient</em> ，而 <code>plot_func(xt,yt,c='r')</code> 可畫出目標函數的曲面， <code>run_grad()</code> 用來執行 <em>Gradient Descent</em> ， <code>run_adagrad()</code> 用來執行 <em>AdaGrad</em> 。 <code>xt</code> 和 <code>yt</code> 對應到前例的 <script type="math/tex">(x,y)</script> ，而 <code>eta</code> 為 <em>Learning Rate</em> 。 <code>for i in range(20)</code> 表示最多會跑20個迴圈，而 <code>if xt &lt; -5 or yt &lt; -5 or xt &gt; 5 or yt &gt; 5</code> 表示，如果 <code>xt</code> 和 <code>yt</code> 超出邊界，則會先結束迴圈。</p>

<p>到 python console 執行：</p>

<p>```sh</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>import adagrad</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>執行 <em>Gradient Descent</em> ，指令如下：</p>

<p>```sh</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>adagrad.run_grad()</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="/images/pic/pic_00126.png" alt="" /></p>

<p><img src="/images/pic/pic_00127.png" alt="" /></p>

<p>以此類推</p>

<p>執行 <em>Adagrad</em> ，指令如下：</p>

<p>```sh</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>adagrad.run_adagrad()</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="/images/pic/pic_00128.png" alt="" /></p>

<p><img src="/images/pic/pic_00129.png" alt="" /></p>

<p><img src="/images/pic/pic_00130.png" alt="" /></p>

<p>以此類推</p>

<h2 id="reference">Reference</h2>

<h4 id="notes-on-adagrad">Notes on AdaGrad</h4>

<p>http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf</p>

<h4 id="visualizing-optimization-algos">Visualizing Optimization Algos</h4>

<p>http://imgur.com/a/Hqolp</p>
]]></content>
  </entry>
  
</feed>
