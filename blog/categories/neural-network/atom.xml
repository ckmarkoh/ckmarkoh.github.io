<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Neural_network | Mark Chang's Blog]]></title>
  <link href="http://ckmarkoh.github.io/blog/categories/neural-network/atom.xml" rel="self"/>
  <link href="http://ckmarkoh.github.io/"/>
  <updated>2016-12-10T21:43:24+08:00</updated>
  <id>http://ckmarkoh.github.io/</id>
  <author>
    <name><![CDATA[Mark Chang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[類神經網路 -- Recurrent Neural Network]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/06/06/neural-network-recurrent-neural-network/"/>
    <updated>2015-06-06T09:32:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/06/06/neural-network-recurrent-neural-network</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>在數位電路裡面，如果一個電路沒有 <em>latch</em> 或 <em>flip flop</em> 這類的元件，它的輸出值只會取決於目前的輸入值，和上個時間點的輸入值是無關的，這種的電路叫作 <em>combinational circuit</em> 。</p>

<p>對於類神經網路而言，如果它的值只是從輸入端一層層地依序傳到輸出端，不會再把值從輸出端傳回輸入端，這種神經元就相當於 <em>combinational circuit</em> ，也就是說它的輸出值只取決於目前時刻的輸入值，這樣的類神經網路稱為 <em>feedforward neural network</em> 。</p>

<p>如果一個電路有 <em>latch</em> 或 <em>flip flop</em> 這類的元件，它的輸出值就跟上個時間點的輸入值有關，這種的電路它稱為 <em>sequential circuit</em> 。</p>

<p>所謂的 <em>Recurrent Neural Network</em> ，是一種把輸出端再接回輸入端的類神經網路，這樣可以把上個時間點的輸出值再傳回來，記錄在神經元中，達成和 <em>latch</em> 類似的效果，使得下個時間點的輸出值，跟上個時間點有關，也就是說，這樣的神經網路是有 <em>記憶</em> 的。</p>

<h2 id="recurrent-neural-network">Recurrent Neural Network</h2>

<p>由一個簡單神經元所構成的 <em>Recurrent Neural Network</em> ，構造如下：</p>

<p><img src="/images/pic/pic_00095.png" alt="" /></p>

<p>這個神經元在 <script type="math/tex">t</script> 時間，訓練資料的輸入值為 <script type="math/tex">x_{t}</script> ，訓練資料的答案為 <script type="math/tex">y_{t}</script> ，神經元 <script type="math/tex">n</script> 的輸出值 <script type="math/tex">n_{out,t}</script> ，可用以下公式表示：</p>

<!--more-->

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{aligned}

& n_{in,t} = w_{c}x_{t}+ w_{p}n_{out,t-1} + w_{b} \\

& n_{out,t} = \frac{1}{1+e^{-n_{in,t}}} \\

\end{aligned}

 %]]&gt;</script>

<p>其中， <script type="math/tex">n_{in,t}</script> 為輸入神經元 <script type="math/tex">n</script> 的值， <script type="math/tex">w_{c}</script> 是給目前的時間(current)時，輸入值 <script type="math/tex">x_{t}</script> 的權重， <script type="math/tex">w_{p}</script> 是給上個時間點(previous)時，輸出值 <script type="math/tex">n_{out,t-1}</script> 的權重，而 <script type="math/tex">w_{b}</script> 為 <em>bias</em> 。從上圖可看出，紫色的線將神經網路的輸出端 <script type="math/tex">n_{out}</script> 連回輸入端 <script type="math/tex">n_{in}</script> ，使得於時間 <script type="math/tex">t</script> 的輸出值跟上個時間點 <script type="math/tex">t-1</script> 的輸出值有關。</p>

<p>可以把這個神經元從時間點 <script type="math/tex">0</script> 到時間點 <script type="math/tex">t</script> 的運算，展開成下圖：</p>

<p><img src="/images/pic/pic_00096.png" alt="" /></p>

<p>從上圖，最左邊開始，依序將 <script type="math/tex">x_{0},x_{1},...,x_{t}</script> 輸入神經元 <script type="math/tex">n</script> ，而依序得出的值為 <script type="math/tex">n_{out,0},n_{out,1},...,n_{out,t}</script> 。神經元 <script type="math/tex">n</script> 在時間點 <script type="math/tex">t-1</script> 的輸出值 <script type="math/tex">n_{out,t-1}</script> ，會接到時間點 <script type="math/tex">t</script> 時的輸入值 <script type="math/tex">n_{in,t}</script> 。 </p>

<h2 id="training-recurrent-neural-network">Training Recurrent Neural Network</h2>

<p>訓練 <em>recurrent neural network</em> 的方法，和訓練 <em>feedforward neural network</em> 的方法一樣，都可以用 <em>back propagation</em> 。但是在 <em>recurrent neural network</em> 中，要依據時間順序，將值從最後一個時間點，回傳到第一個時間點。</p>

<p>在時間點 <script type="math/tex">t</script> 時的 <em>cost function</em> 為：</p>

<script type="math/tex; mode=display">

J=−y_{t}log(n_{out,t})−(1−y_{t})log(1−n_{out,t})

</script>

<p>計算 <em>recurrent neural network</em> 的 <em>back propagation</em> 要分為兩部分來算，先算好時間點位於 <script type="math/tex">t</script> 的偏微分 <script type="math/tex">\dfrac{\partial J}{ \partial n_{in} } </script> 值，再依序往前算出時間點 <script type="math/tex">t</script> 之前的偏微分值，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\dfrac{\partial J}{ \partial n_{in,s} } = 

\begin{cases} 

(\dfrac{\partial J}{ \partial n_{out,s} } )

(\dfrac{ \partial n_{out,s}}{\partial n_{in,s} } ) & \text{if } s = t \\

(\dfrac{\partial J}{ \partial n_{in,s+1} } )

(\dfrac{ \partial n_{in,s+1}}{\partial n_{out,s} } )

(\dfrac{ \partial n_{out,s}}{\partial n_{in,s} } )

  & \text{otherwise}

\end{cases}

 %]]&gt;</script>

<p>其中， <script type="math/tex">s</script> 為 <script type="math/tex">0</script> 到 <script type="math/tex">t</script> 中的其中一個時間點。用 <a href="/blog/2015/05/28/neural-network-backward-propagation">Backward Propagation 詳細推導過程</a> 所提到的推導方法，可推導出 <script type="math/tex"> (\dfrac{\partial J}{ \partial n_{out,s} } )(\dfrac{ \partial n_{out,s}}{\partial n_{in,s} } )</script> 、 <script type="math/tex">(\dfrac{ \partial n_{in,s+1}}{\partial n_{out,s} } )</script> 與 <script type="math/tex">(\dfrac{ \partial n_{out,s}}{\partial n_{in,s} } )</script> 的值，並令 <script type="math/tex"> \delta_{in,s} = \dfrac{\partial J}{ \partial n_{in,s} } </script> 代入以上公式，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

\delta_{in,s}  = 

\begin{cases} 

n_{out,s} - y_{s}  & \text{if } s = t \\

\delta_{in,s+1} w_{p} n_{out,s} (1-n_{out,s})  & \text{otherwise}

\end{cases}

\end{align}

 %]]&gt;</script>

<p>此公是可分為兩部分，當 <script type="math/tex">s = t</script> 時，與  <script type="math/tex">s\neq t</script> 時。計算 <script type="math/tex">\delta_{s}</script> 的方式不同。</p>

<p>在 <script type="math/tex">s = t</script> 時， <script type="math/tex">\delta_{s}</script> 的傳遞過程就如同 <em>feedforward neural network</em> ，如下圖：</p>

<p><img src="/images/pic/pic_00097.png" alt="" /></p>

<p>若 <script type="math/tex">s\neq t</script> 時， 要算 <script type="math/tex">\delta_{in,s}</script> 之前，要先從 <script type="math/tex">s+1</script> 時間點將 <script type="math/tex">\delta_{in,s+1}</script> 傳遞過來，傳遞過程如下圖：</p>

<p><img src="/images/pic/pic_00098.png" alt="" /></p>

<p>因為需要把 <script type="math/tex">\delta</script> 從後面的時間點往前面傳，故這個過程又稱為 <em>back propagation through time</em> 。</p>

<p>於時間點 <script type="math/tex">s</script> 計算完 <script type="math/tex">\delta</script> 後，用以下公式將 <script type="math/tex">s</script> 時間點算出的偏微分值，更新到神經元的權重：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{c} \leftarrow w_{c} - 

\eta \dfrac{\partial J}{ \partial n_{in,s} } \dfrac{\partial n_{in,s}}{\partial w_{c}} \\ 

& w_{b} \leftarrow w_{b} - 

\eta \dfrac{\partial J}{ \partial n_{in,s} } \dfrac{\partial n_{in,s}}{\partial w_{b}} \\

& w_{p} \leftarrow w_{p} - 

\eta \dfrac{\partial J}{ \partial n_{in,s} } \dfrac{\partial n_{in,s}}{\partial w_{p}}  \\

\end{align}

 %]]&gt;</script>

<p>用 <a href="/blog/2015/05/28/neural-network-backward-propagation">Backward Propagation 詳細推導過程</a> ，求出 <script type="math/tex">\dfrac{\partial n_{in,s}}{\partial w_{c}}</script> 、 <script type="math/tex">\dfrac{\partial n_{in,s}}{\partial w_{b}} </script> 和 <script type="math/tex">\dfrac{\partial n_{in,s}}{\partial w_{p}}</script> 的值，並換成用 <script type="math/tex">\delta_{in,s}</script> 代替 <script type="math/tex">\dfrac{\partial J}{ \partial n_{in,s} } </script> 代入以上公式，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{c} \leftarrow w_{c} - \eta \delta_{in,s} x_{s} \\ 

& w_{b} \leftarrow w_{b} - \eta \delta_{in,s} \\

& w_{p} \leftarrow w_{p} - \eta \delta_{in,s} n_{out,s-1} \\

\end{align}

 %]]&gt;</script>

<p>此過程如下圖所示：</p>

<p><img src="/images/pic/pic_00099.png" alt="" /></p>

<h2 id="implementation">Implementation</h2>

<p>再來是實作的部分，以下是個簡單的應用，用 <em>Recurrent Neural Network</em> 來預測一個字串序列中，下一個可能出現的字是什麼。例如，給定以下字串：</p>

<p>```
001001001</p>

<p>```</p>

<p>根據這個字串的特徵，如果連續出現了兩個 <em>0</em> ，可以預測下個出現的為 <em>1</em> ，若前面兩個字為 <em>10</em> 則可預測下個出現的自為 <em>0</em> ，以此類推。</p>

<p>以下為實作部分：</p>

<p>```python simple_rnn.py
from math import e                                                                                  <br />
from random import random</p>

<p>def sigmoid(x):
    return 1/float(1+e<em>*(-1</em>x))</p>

<p>def my_rand():
    return random()-0.5</p>

<p>def rnn(x):
    r = 0.05
    w_p, w_c, w_b = my_rand(),my_rand(),my_rand()
    l = len(x)
    n_in = [0]<em>l
    n_out = [0]</em>l
    for h in range(10000):
        for i in range(l-1): 
            n_in[i] = w_c * x[i] + w_p * n_out[i] + w_b 
            n_out[i+1] = sigmoid(n_in[i])
        for i in range(l-1): 
            for j in range(i+1):
                k =  (i-j)
                if j == 0:
                    d_c = n_out[k+1] - x[k+1]
                else:
                    d_c = w_p * n_out[k+1] * (1-n_out[k+1]) * d_c
                w_c = w_c - r * d_c * x [k]
                w_b = w_b - r * d_c
                w_p = w_p - r * d_c * n_out[k]</p>

<pre><code>for i in range(l-1):
    n_in[i] = w_c * x[i] + w_p * n_out[i] + w_b
    n_out[i+1] = sigmoid(n_in[i])
for w in zip(x,n_out):
    print w[0],w[1]
</code></pre>

<p>```</p>

<p>其中， <code>x</code> 為輸入的序列， <code>n_out</code> 為神經元預測的結果。進行這個演算法之前，首先，先給權重 <code>w_p, w_c, w_b</code> 的初始值用介於 <em>-0.5~0.5</em> 之間的隨機值。再來是進行訓練過程，用 <em>for loop</em> 進行了 <em>10000</em> 次的訓練，在每次的訓練過程中，先進行 <em>forward propagation</em> 依時間順序，算出每個時間點的 <code>n_out</code> 。再來是用 <em>back propagation through time</em> 來更新 <code>w_p, w_c, w_b</code> 的值。訓練完後，進行一次 <em>forward propogation</em> 用訓練過程得出的權重來預測序列的下一個字，並將預測結果印出。</p>

<p>到 <em>interactive mode</em> 執行以下程式，輸入序列 <em>001001001</em> 。</p>

<p>```</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>import smallrnn
smallrnn.rnn([0,0,1,0,0,1,0,0,1])
0 0
0 0.203697155215
1 0.93315712478
0 0.00354811782422
0 0.215230877663
1 0.945971073339
0 0.00455854449755
0 0.218600850027
1 0.949254861129</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>左側為輸入序列，右側為預測的結果，可以發現 <em>recurrent neural network</em> 可以預測出下個字可能會是 <em>0</em> 還是 <em>1</em> 。當左側為 <em>1</em> 時，右側的數字會接近於 <em>1</em> 。</p>

<h2 id="further-reading">Further Reading</h2>

<p>關於 <em>recurrent neural network</em> 可參考 coursera 課程 Geoffrey Hinton. Neural Networks for Machine Learning</p>

<p>https://www.coursera.org/course/neuralnets</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[類神經網路 -- Backward Propagation 詳細推導過程]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/05/28/neural-network-backward-propagation/"/>
    <updated>2015-05-28T07:47:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/05/28/neural-network-backward-propagation</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>在做 <a href="/blog/2014/03/15/logisti-regression-model">Logistic Regression</a>的時候，可以用 <em>gradient descent</em> 來做訓練，而類神經網路本身即是很多層的 <em>Logistic Regression</em> 所構成，也可以用同樣方法來做訓練。</p>

<p>但類神經網路在訓練過程時，需要分為兩個步驟，為： <em>Forward Phase</em> 與 <em>Backward Phase</em> 。 也就是要先從 <em>input</em> 把值傳到 <em>output</em>，再從 <em>output</em> 往回傳遞 <em>error</em> 到每一層的神經元，去更新層與層之間權重的參數。</p>

<h2 id="forward-phase">Forward Phase</h2>

<p>在 <em>Forward Phase</em> 時，先從 <em>input</em> 將值一層層傳遞到 <em>output</em>。</p>

<p>對於一個簡單的神經元 <script type="math/tex">n</script> ，如下圖 <a name="pic1">＜圖一＞</a>：</p>

<p><img src="/images/pic/pic_00086.png" alt="" /></p>

<p>將一筆訓練資料 <script type="math/tex">x_{1},x_{2}</script> 和 <em>bias</em> <script type="math/tex">b</script> 輸入到神經元 <script type="math/tex">n</script> 到輸出的過程，分成兩步，分別為 <script type="math/tex">n_{in}</script>， <script type="math/tex">n_{out}</script> ，過程如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& n_{in} = w_{1} x_{1}+w_{2}  x_{2}+w_{b} \\

& n_{out} = \frac{1} {1+e^{-n_{in} } }

\end{align}

 %]]&gt;</script>

<!--more-->

<p>在輸入神經元時， <script type="math/tex">n_{in}</script> 先將 <em>input</em> 值和其權重作乘積。</p>

<p>在輸出神經元時， <script type="math/tex">n_{out}</script> 將 <script type="math/tex">n_{in}</script> 的值用 <em>sigmoid function</em> 轉成值範圍從 <em>0</em> 到 <em>1</em> 的函數。</p>

<p>傳遞到 <script type="math/tex">n_{out}</script> 後，可與訓練資料的答案 <script type="math/tex">y</script> 用 <em>cost function</em> 來計算其差值，並用 <em>backward propagation</em> 修正權重 <script type="math/tex">w_{1}</script> 、 <script type="math/tex">w_{2}</script> 和 <script type="math/tex">w_{b}</script> 。</p>

<p>對於一個簡單的類神經網路，共有兩層，四個神經元，如下圖<a name="pic2">＜圖二＞</a>：</p>

<p><img src="/images/pic/pic_00087.png" alt="" /></p>

<p>其值傳遞的過程如下：</p>

<p>1.把 <script type="math/tex">x</script> 和 <script type="math/tex">y</script> 和 <em>bias</em> <script type="math/tex">b</script> 傳入到第一層神經元 <script type="math/tex">n_{11}</script> 及 <script type="math/tex">n_{12}</script> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& n_{11(in)} = w_{11,x} x+w_{11,y} y+w_{11,b} \\

& n_{12(in)} = w_{12,x} x+w_{12,y} y+w_{12,b} \\

& n_{11(out)} = \frac{1} {1+e^{-n_{11(in)} } } \\

& n_{12(out)} = \frac{1} {1+e^{-n_{12(in)} } } \\

\end{align}

 %]]&gt;</script>

<p>其中，<script type="math/tex">n_{11(in)}</script> 表示傳入神經元 <script type="math/tex">n_{11}</script> 的值，而 <script type="math/tex">n_{11(out)}</script> 表示傳出神經元 <script type="math/tex">n_{11}</script> 的值，而 <script type="math/tex">w_{11,x}</script> 表示值從 <script type="math/tex">x</script> 傳入 <script type="math/tex">n_{11}</script> 時，所乘上的權重</p>

<p>2.第一層神經元將其輸出值 <script type="math/tex"> n_{11(out)}</script> 和 <script type="math/tex">n_{12(out)}</script> 傳到第二層神經元 <script type="math/tex"> n_{21}</script> 和 <script type="math/tex"> n_{22}</script>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& n_{21(in)} = w_{21,11} n_{11(out)} + w_{21,12} n_{12(out)}+w_{21,b} \\

& n_{22(in)} = w_{22,11} n_{11(out)} + w_{22,12} n_{12(out)}+w_{22,b} \\

& n_{21(out)} = \frac{1} {1+e^{-n_{21(in)} } } \\

& n_{22(out)} = \frac{1} {1+e^{-n_{22(in)} } } \\

\end{align}

 %]]&gt;</script>

<p>傳遞完後，可與訓練資料的答案 <script type="math/tex">z_{1}</script> 和 <script type="math/tex">z_{2}</script> 用 <em>cost function</em> 來計算其差值，並用 <em>backward propagation</em> 修正權重。</p>

<h2 id="derivation-of-gradient-descent">Derivation of Gradient Descent</h2>

<p>在講解 <em>backward Phase</em> 之前，先推導類神經網路的 <em>gradient descent</em> 公式和 <em>backward propagation</em> 的原理：</p>

<p>對於<a href="#pic1">＜圖一＞</a>中的一個簡單的神經元 <script type="math/tex">n</script>，將一筆訓練資料 <script type="math/tex">x_{1},x_{2}</script> 傳遞到 <script type="math/tex">n_{out}</script> 所得出的值和 <script type="math/tex">y</script> 的值做比較，我們可用以下的 <em>cost function</em> 來計算：</p>

<script type="math/tex; mode=display">

J =- y\times log(n_{out}) - (1-y)\times log (1 -n_{out} ) 

</script>

<p>從以上 <em>cost function</em> 可得知，如果 <script type="math/tex">n_{out}</script> 和 <script type="math/tex">y</script> 都等於 <em>0</em> ，或者都等於 <em>1</em> ，則 <em>cost</em> 會是 <em>0</em> ，若 <script type="math/tex">n_{out}</script> 和 <script type="math/tex">y</script> 其中有一個是 <em>1</em> ，而另一個是 <em>0</em> ，則 <em>cost</em> 會趨近於無限大。</p>

<p>用 <em>gradient Descent</em> 調整 <script type="math/tex">w_{1}</script> 、 <script type="math/tex">w_{2} </script> 和 <script type="math/tex">w_{b}</script> 來做訓練時，可用以下公式<a name="eq1">＜公式一＞</a>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{1} \leftarrow w_{1} - \eta \dfrac{\partial J}{\partial w_{1}} \\

& w_{2} \leftarrow w_{2} - \eta \dfrac{\partial J}{\partial w_{2}} \\

& w_{b} \leftarrow w_{b} - \eta \dfrac{\partial J}{\partial w_{b}} \\

\end{align}

 %]]&gt;</script>

<p>其中， <script type="math/tex">\eta</script> 為 <em>learning rate</em> ，用來控制訓練的速度。</p>

<p>接著要推導這個公式怎麼算，首先，將 <script type="math/tex">\dfrac{\partial J}{\partial w_{1}}</script> 的微分用 <em>chain rule</em> 展開，如下 <a name="eq2">＜公式二＞</a>：</p>

<script type="math/tex; mode=display">

\dfrac{\partial J}{\partial w_{1}}

= \dfrac{\partial J}{\partial n_{out}} 

 \dfrac{\partial n_{out}}{\partial n_{in}}

 \dfrac{\partial n_{in}}{\partial w_{1}}

</script>

<p>以上公式，總共有 <script type="math/tex">\dfrac{\partial J}{\partial n_{out}} </script> 、 <script type="math/tex">\dfrac{\partial n_{out}}{\partial n_{in}}</script> 與 <script type="math/tex">\dfrac{\partial n_{in}}{\partial w_{1}}</script> 三個部份的微分要算。</p>

<p>1.<script type="math/tex">\dfrac{\partial J}{\partial n_{out}} </script>：</p>

<script type="math/tex; mode=display">

 \dfrac{\partial J}{\partial n_{out}} 

 = -y \dfrac{\partial log(n_{out})}{\partial n_{out}}- (1-y) \dfrac{\partial log (1 -n_{out} )}{\partial n_{out}} \\

 = -\frac{y}{n_{out}}+\frac{1-y}{1-n_{out}}

</script>

<p>2.<script type="math/tex">\dfrac{\partial n_{out}}{\partial n_{in}}</script>：</p>

<script type="math/tex; mode=display">

\dfrac{\partial n_{out}}{\partial n_{in}} = \dfrac{\partial}{\partial n_{in}}(\frac{1}{1+e^{-n_{in}}})

=\frac{e^{-n_{in}}}{(1+e^{-n_{in}})^{2}} 

= \frac{1}{1+e^{-n_{in}}}  \frac{e^{-n_{in}}}{1+e^{-n_{in}}} \\

= n_{out}(1- n_{out})


</script>

<p>3.<script type="math/tex">\dfrac{\partial n_{in}}{\partial w_{1}}</script>：</p>

<script type="math/tex; mode=display">

\dfrac{\partial n_{in}}{\partial w_{1}} = \dfrac{\partial}{\partial w_{1}} (w_{1} x_{1}+w_{2} x_{2}+w_{b}) \\

= x_{1}

</script>

<p>代入以上三個結果到<a href="#eq2">＜公式二＞</a>，可得出 <script type="math/tex">\dfrac{\partial J}{\partial w_{1}}</script> 的值，如下：</p>

<script type="math/tex; mode=display">

\dfrac{\partial J}{\partial w_{1}} 

= \dfrac{\partial J}{\partial n_{out}} 

 \dfrac{\partial n_{out}}{\partial n_{in}}

 \dfrac{\partial n_{in}}{\partial w_{1}} \\

 = (-\frac{y}{n_{out}}+\frac{1-y}{1-n_{out}})  n_{out}(1- n_{out})  x_{1} \\

 = (n_{out}-y) x_{1}

</script>

<p>同理可得出 <script type="math/tex"> \dfrac{\partial J}{\partial w_{2}} </script> 與 <script type="math/tex">\dfrac{\partial J}{\partial w_{b}} </script> 的值，分別為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \dfrac{\partial J}{\partial w_{2}} 

= \dfrac{\partial J}{\partial n_{out}} 

 \dfrac{\partial n_{out}}{\partial n_{in}}

 \dfrac{\partial n_{in}}{\partial w_{2}}

=(n_{out}-y) x_{2} \\

& \dfrac{\partial J}{\partial w_{b}} 

= \dfrac{\partial J}{\partial n_{out}} 

 \dfrac{\partial n_{out}}{\partial n_{in}}

 \dfrac{\partial n_{in}}{\partial w_{b}}

= (n_{out}-y)

\end{align}

 %]]&gt;</script>

<p>其中， <script type="math/tex">\dfrac{\partial n_{in}}{\partial w_{b}}</script> 的結果為：</p>

<script type="math/tex; mode=display">

\dfrac{\partial n_{in}}{\partial w_{b}} = \dfrac{\partial}{\partial w_{b}} (w_{1} x_{1}+w_{2} x_{2}+w_{b}) = 1


</script>

<p>將 <script type="math/tex"> \dfrac{\partial J}{\partial w_{1}}</script> 、 <script type="math/tex"> \dfrac{\partial J}{\partial w_{2}} </script> 和 <script type="math/tex"> \dfrac{\partial J}{\partial w_{b}} </script> 的結果代入<a href="#eq1">＜公式一＞</a>，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{1} \leftarrow w_{1} - \eta (n_{out}-y) x_{1} \\

& w_{2} \leftarrow w_{2} - \eta (n_{out}-y) x_{2}  \\

& w_{b} \leftarrow w_{b} - \eta (n_{out}-y) \\

\end{align}

 %]]&gt;</script>

<h2 id="derivation-of-backward-propagation">Derivation of Backward Propagation</h2>

<p>若要推導超過一層的類神經網路的 <em>gradient descent</em> 公式，就要用到 <em>backward propagation</em> 。</p>

<p>對於<a href="#pic2">＜圖二＞</a>中的一個簡單的類神經網路，它的 <em>cost function</em> 如下：</p>

<script type="math/tex; mode=display">

J =-( z_{1}\times log(n_{21(out)}) + (1-z_{1})\times log (1 -n_{21(out)} ) + z_{2}\times log(n_{22(out)}) + (1-z_{2})\times log (1 -n_{22(out)} ))

</script>

<p>對於最後一層與倒數第二層之間的權重改變，可用 <em>gradient descent</em> ，如下<a name="eq3">＜公式三＞</a>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{21,11} \leftarrow w_{21,11} - \eta \dfrac{\partial J}{\partial w_{21,11}} \\

& w_{21,12} \leftarrow w_{21,12} - \eta \dfrac{\partial J}{\partial w_{21,12}} \\

& w_{21,b} \leftarrow w_{21,b} - \eta \dfrac{\partial J}{\partial w_{21,b}} \\

\end{align}

 %]]&gt;</script>

<p>可用先前推導出單一神經元時的微分結果，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}


& \dfrac{\partial J}{\partial w_{21,11}} 

= \dfrac{\partial J}{\partial n_{21(out)}} 

 \dfrac{\partial n_{21(out)}}{\partial n_{21(in)}}

 \dfrac{\partial n_{21(in)}}{\partial w_{21,11}}

= (n_{21(out)}-z_{1}) n_{11(out)} \\


& \dfrac{\partial J}{\partial w_{21,12}} 

= \dfrac{\partial J}{\partial n_{21(out)}} 

 \dfrac{\partial n_{21(out)}}{\partial n_{21(in)}}

 \dfrac{\partial n_{21(in)}}{\partial w_{21,12}}

= (n_{21(out)}-z_{1})n_{12(out)} \\


& \dfrac{\partial J}{\partial w_{21,b}} 

= \dfrac{\partial J}{\partial n_{21(out)}} 

 \dfrac{\partial n_{21(out)}}{\partial n_{21(in)}}

 \dfrac{\partial n_{21(in)}}{\partial w_{21,b}}

= (n_{21(out)}-z_{1})


\end{align}

 %]]&gt;</script>

<p>同理可求出 <script type="math/tex">w_{22,11}</script> 、 <script type="math/tex">w_{22,12}</script> 和 <script type="math/tex">w_{22,b}</script> 相對應的公式。</p>

<p>在要推導更往前一層的權重變化公式之前，先觀察以上公式，發現它們有共同的部分： <script type="math/tex">n_{21(out)}-z_{1}</script> ，可以用 <script type="math/tex">\delta_{21(in)}</script> 來表示這個值，即：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \delta_{21(in)} = \dfrac{\partial J}{\partial n_{21(out)}} 

 \dfrac{\partial n_{21(out)}}{\partial n_{21(in)}}

= n_{21(out)}-z_{1} \\

& \dfrac{\partial J}{\partial w_{21,11}} = \delta_{21(in)}  n_{11(out)}\\

& \dfrac{\partial J}{\partial w_{21,12}} = \delta_{21(in)}  n_{12(out)}\\

& \dfrac{\partial J}{\partial w_{21,b}} = \delta_{21(in)}


\end{align}

 %]]&gt;</script>

<p><script type="math/tex">\delta_{21(in)}</script> 的物理意義如下圖所示：</p>

<p><img src="/images/pic/pic_00088.png" alt="" /></p>

<p>圖中， <script type="math/tex">\delta_{21(out)}</script> 是 <script type="math/tex">J</script> 在神經元 <script type="math/tex">n_{21}</script> 輸出點的微分值，可以把 <script type="math/tex">\delta_{21(in)}</script> 看成是 <script type="math/tex">\delta_{21(out)}</script> <strong>從神經元</strong> <script type="math/tex">n_{21}</script> <strong>的輸出點往回傳到輸入點</strong>，即乘上 <script type="math/tex">\dfrac{\partial n_{21(out)}}{\partial n_{21(in)}}</script> 。因此，這過程又稱為 <em>backward propagation</em> 。</p>

<p>將 <script type="math/tex">\delta_{21(in)}</script> 置換到<a href="#eq3">＜公式三＞</a>，得出這一層推導的最後結果：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{21,11} \leftarrow w_{21,11} - \eta  \delta_{21(in)}  n_{11(out)} \\

& w_{21,12} \leftarrow w_{21,12} - \eta  \delta_{21(in)}  n_{12(out)} \\

& w_{21,b} \leftarrow w_{21,b} - \eta  \delta_{21(in)} \\

\end{align}

 %]]&gt;</script>

<p>同理， <script type="math/tex">w_{22,11},w_{22,12}, w_{22,b} </script> 的 <em>gradient descent</em> 公式，也可用相同方法推導出來：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{22,11} \leftarrow w_{22,11} - \eta  \delta_{22(in)}  n_{11(out)} \\

& w_{22,12} \leftarrow w_{22,12} - \eta  \delta_{22(in)}  n_{12(out)} \\

& w_{22,b} \leftarrow w_{22,b} - \eta  \delta_{22(in)} \\

\end{align}

 %]]&gt;</script>

<p>再來，要推導更往前一層的權重變化公式，要用 <em>gradient descent</em> <a name="eq4">＜公式四＞</a>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{11,x} \leftarrow w_{11,x} - \eta \dfrac{\partial J}{\partial w_{11,x}} \\

& w_{11,y} \leftarrow w_{11,y} - \eta \dfrac{\partial J}{\partial w_{11,y}} \\

& w_{11,b} \leftarrow w_{11,b} - \eta \dfrac{\partial J}{\partial w_{11,b}} \\

\end{align}

 %]]&gt;</script>

<p>舉 <script type="math/tex">w_{11,x}</script> 為例，用 <em>chain rule</em> 求出 <script type="math/tex"> \dfrac{\partial J}{\partial w_{11,x}}</script> 的值，如下<a name="eq5">＜公式五＞</a>：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&\dfrac{\partial J}{\partial w_{11,x}}

=\dfrac{\partial J}{\partial n_{21(out)}}  \dfrac{\partial n_{21(out)}}{\partial w_{11,x}}

+ \dfrac{\partial J}{\partial n_{22(out)}}  \dfrac{\partial n_{22(out)}}{\partial w_{11,x}}

\\

&= \dfrac{\partial J}{\partial n_{21(out)}} 

\dfrac{\partial n_{21(out)}}{\partial n_{21(in)}} 

\dfrac{\partial n_{21(in)}}{\partial n_{11(out)}} 

\dfrac{\partial n_{11(out)}}{\partial n_{11(in)}} 

\dfrac{\partial n_{11(in)}}{\partial w_{11,x}} 

+ \dfrac{\partial J_{2}}{\partial n_{22(out)}} 

\dfrac{\partial n_{22(out)}}{\partial n_{22(in)}} 

\dfrac{\partial n_{22(in)}}{\partial n_{11(out)}} 

\dfrac{\partial n_{11(out)}}{\partial n_{11(in)}} 

\dfrac{\partial n_{11(in)}}{\partial w_{11,x}} \\


& = (

\dfrac{\partial J}{\partial n_{21(out)}} 

\dfrac{\partial n_{21(out)}}{\partial n_{21(in)}} 

\dfrac{\partial n_{21(in)}}{\partial n_{11(out)}} 

+ \dfrac{\partial J}{\partial n_{22(out)}} 

\dfrac{\partial n_{22(out)}}{\partial n_{22(in)}} 

\dfrac{\partial n_{22(in)}}{\partial n_{11(out)}} 

)  

\dfrac{\partial n_{11(out)}}{\partial n_{11(in)}} 

\dfrac{\partial n_{11(in)}}{\partial w_{11,x}} 

\end{align}

 %]]&gt;</script>

<p>其中， <script type="math/tex">\dfrac{\partial n_{21(in)}}{\partial n_{11(out)}} </script> 、 <script type="math/tex">\dfrac{\partial n_{22(in)}}{\partial n_{11(out)}} </script> 、 <script type="math/tex">\dfrac{\partial n_{11(out)}}{\partial n_{11(in)}} </script> 和 <script type="math/tex">\dfrac{\partial n_{11(in)}}{\partial w_{11,x}}</script> 這四項的值分別為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&\dfrac{\partial n_{21(in)}}{\partial n_{11(out)}} 

= \dfrac{\partial }{\partial n_{11(out)}}( w_{21,11}n_{11(out)} + w_{21,12}n_{12(out)}+w_{21,b} )

= w_{21,11} \\

&\dfrac{\partial n_{22(in)}}{\partial n_{11(out)}} 

= \dfrac{\partial }{\partial n_{11(out)}}( w_{22,11} n_{11(out)} + w_{22,12} n_{12(out)}+w_{22,b} )

= w_{22,11} \\

& \dfrac{\partial n_{11(out)}}{\partial n_{11(in)}} 

= \dfrac{\partial} {\partial n_{11(in)}}( \frac{1} {1+e^{-n_{11(in)} } } )

= n_{11(out)}(1-n_{11(out)}) \\

& \dfrac{\partial n_{11(in)}}{\partial w_{11,x}}

= \dfrac{\partial}{\partial w_{11,x}} (w_{11,x} x+w_{11,y} y+w_{11,b} ) 

= x \\

\end{align}

 %]]&gt;</script>

<p>再代入這些值與之前推導出的 <script type="math/tex">\dfrac{\partial J}{\partial n_{21(out)}} \dfrac{\partial n_{21(out)}}{\partial n_{21(in)}} </script> 和 <script type="math/tex">\dfrac{\partial J}{\partial n_{22(out)}} \dfrac{\partial n_{22(out)}}{\partial n_{22(in)}} </script> 的值到<a href="#eq5">＜公式五＞</a>，可求出 <script type="math/tex"> \dfrac{\partial J}{\partial w_{11,x}}</script> 為：</p>

<script type="math/tex; mode=display">

 \dfrac{\partial J}{\partial w_{11,x}}

 = ( (n_{21(out)}-z_{1})  w_{21,11} + (n_{22(out)}-z_{2})  w_{22,11} )  n_{11(out)}(1-n_{11(out)})  x

</script>

<p>同理，可求出 <script type="math/tex"> \dfrac{\partial J}{\partial w_{11,y}}</script> 和  <script type="math/tex"> \dfrac{\partial J}{\partial w_{11,b}}</script> 的值分別為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&\dfrac{\partial J}{\partial w_{11,y}} = ( (n_{21(out)}-z_{1})  w_{21,11} + (n_{22(out)}-z_{2})  w_{22,11} ) n_{11(out)}(1-n_{11(out)})  y \\

&\dfrac{\partial J}{\partial w_{11,b}} = ( (n_{21(out)}-z_{1})  w_{21,11} + (n_{22(out)}-z_{2})  w_{22,11} )  n_{11(out)}(1-n_{11(out)}) \\

\end{align}

 %]]&gt;</script>

<p>如同前一層所推導的，以上公式也有相同部分，也可以用 <script type="math/tex">\delta_{11(in)}</script> 來簡化它們，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&\delta_{11(in)} = ( (n_{21(out)}-z_{1})  w_{21,11} + (n_{22(out)}-z_{2})  w_{22,11} )  n_{11(out)}(1-n_{11(out)}) \\

& \dfrac{\partial J}{\partial w_{11,x}} =\delta_{11(in)}  x \\

&\dfrac{\partial J}{\partial w_{11,y}} = \delta_{11(in)}  y \\

&\dfrac{\partial J}{\partial w_{11,b}} =\delta_{11(in)} \\

\end{align}

 %]]&gt;</script>

<p>可把 <script type="math/tex">\delta_{11(in)}</script> 用後面層傳回來的的 <script type="math/tex">\delta</script> 來表示，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&\delta_{11(out)} = w_{21,11}  \delta_{21(in)} + w_{22,11}  \delta_{22(in)} \\

&\delta_{11(in)} = \delta_{11(out)}  n_{11(out)}(1-n_{11(out)}) = \delta_{11(out)}  \dfrac{\partial n_{11(out)}}{\partial n_{11(in)}}

\end{align}

 %]]&gt;</script>

<p>這些 <script type="math/tex">\delta</script> 的物理意義如下圖所示：</p>

<p><img src="/images/pic/pic_00089.png" alt="" /></p>

<p>從圖中可以看到， <script type="math/tex">\delta_{11(out)}</script> 是 <strong>由</strong> <script type="math/tex">\delta_{21(in)}</script> <strong>和</strong> <script type="math/tex">\delta_{22(in)}</script> <strong>往反方向傳遞，再乘上其權重</strong> <script type="math/tex">w_{21,11}</script> <strong>與</strong> <script type="math/tex">w_{22,11}</script> <strong>所得出的</strong> 。 </p>

<p>將 <script type="math/tex">\delta_{11(in)}</script> 置換到<a href="#eq4">＜公式四＞</a>，得出這一層推導的最後結果：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{11,x} \leftarrow w_{11,x} - \eta  \delta_{11(in)}  x \\

& w_{11,y} \leftarrow w_{11,y} - \eta  \delta_{11(in)}  y \\

& w_{11,b} \leftarrow w_{11,b} - \eta  \delta_{11(in)} \\

\end{align}

 %]]&gt;</script>

<p>同理， <script type="math/tex">w_{12,x},w_{12,y}, w_{12,b} </script> 的 <em>gradient descent</em> 的公式，也可用相同方法推導出來：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{12,x} \leftarrow w_{12,x} - \eta  \delta_{12(in)}  x \\

& w_{12,y} \leftarrow w_{12,y} - \eta  \delta_{12(in)}  y \\

& w_{12,b} \leftarrow w_{12,b} - \eta  \delta_{12(in)} \\

\end{align}

 %]]&gt;</script>

<h2 id="backward-phase">Backward Phase</h2>

<p><em>backward phase</em> 要做的即是 <em>backward propagation</em> ，也就是從 <em>output</em> 把 <script type="math/tex">\delta</script> 算出來，並更新權重 <script type="math/tex">w</script> ，再把 <script type="math/tex">\delta</script> 往回傳一層，再更新那層的權重 <script type="math/tex">w</script>，這樣一直傳下去直到 <em>input</em> 。</p>

<p>首先，把 <script type="math/tex">\delta_{21(in)}</script> 和 <script type="math/tex">\delta_{22(in)}</script> 算出來：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \delta_{21(in)} = n_{21(out)} - z_{1} \\

& \delta_{22(in)} = n_{22(out)} - z_{2} \\

\end{align}

 %]]&gt;</script>

<p><img src="/images/pic/pic_00090.png" alt="" /></p>

<p>再來，用 <script type="math/tex">\delta_{21(in)}</script> 和 <script type="math/tex">\delta_{22(in)}</script> 更新以下權重的值：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{21,11} \leftarrow w_{21,11} - \eta  \delta_{21(in)}  n_{11(out)} \\

& w_{21,12} \leftarrow w_{21,12} - \eta  \delta_{21(in)}  n_{12(out)} \\

& w_{21,b} \leftarrow w_{21,b} - \eta  \delta_{21(in)} \\

& w_{22,11} \leftarrow w_{22,11} - \eta  \delta_{22(in)}  n_{11(out)} \\

& w_{22,12} \leftarrow w_{22,12} - \eta  \delta_{22(in)}  n_{12(out)} \\

& w_{22,b} \leftarrow w_{22,b} - \eta  \delta_{22(in)} \\

\end{align}

 %]]&gt;</script>

<p><img src="/images/pic/pic_00091.png" alt="" /></p>

<p>再來，把 <script type="math/tex">\delta_{21(in)}</script> 和 <script type="math/tex">\delta_{22(in)}</script> 乘上權重，算出 <script type="math/tex">\delta_{11(in)}</script> 和 <script type="math/tex">\delta_{12(in)}</script>的值：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&\delta_{11(in)} = ( w_{21,11}  \delta_{21(in)} + w_{22,11}  \delta_{22(in)})  n_{11(out)}(1-n_{11(out)}) \\

&\delta_{12(in)} = ( w_{21,12}  \delta_{21(in)} + w_{22,12}  \delta_{22(in)})  n_{12(out)}(1-n_{12(out)}) \\

\end{align}

 %]]&gt;</script>

<p><img src="/images/pic/pic_00092.png" alt="" /></p>

<p>最後，用 <script type="math/tex">\delta_{11(in)}</script> 和 <script type="math/tex">\delta_{12(in)}</script> 更新以下權重的值：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{11,x} \leftarrow w_{11,x} - \eta  \delta_{11(in)}  x \\

& w_{11,y} \leftarrow w_{11,y} - \eta  \delta_{11(in)}  y \\

& w_{11,b} \leftarrow w_{11,b} - \eta  \delta_{11(in)} \\

& w_{12,x} \leftarrow w_{12,x} - \eta  \delta_{12(in)}  x \\

& w_{12,y} \leftarrow w_{12,y} - \eta  \delta_{12(in)}  y \\

& w_{12,b} \leftarrow w_{12,b} - \eta  \delta_{12(in)} \\

\end{align}

 %]]&gt;</script>

<p><img src="/images/pic/pic_00093.png" alt="" /></p>

<p>更新完後，即結束了在資料 <script type="math/tex">x,y</script> 上的這一輪訓練。</p>

<p>以下為整個過程的動畫版：</p>

<p><img src="/images/pic/pic_00094.gif" alt="" /></p>

<h2 id="reference">Reference</h2>

<p>本文參考 coursera 課程 Andrew Ng. Machine Learning</p>

<p>https://www.coursera.org/course/ml</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[類神經網路 -- Hierarchical Probabilistic Neural Network Language Model (Hierarchical Softmax)]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/05/23/-hierarchical-probabilistic-neural-networks-neural-network-language-model/"/>
    <updated>2015-05-23T15:33:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/05/23/-hierarchical-probabilistic-neural-networks-neural-network-language-model</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>將類神經網路應用在自然語言處理領域的模型有<a href="/blog/2015/05/15/neural-network-neural-probabilistic-language-model">Neural Probabilistic Language Model(NPLM)</a>，但在實際應用時，運算瓶頸在於 <em>output layer</em> 的神經元個數，等同於總字彙量 <script type="math/tex">\mid V\mid </script> 。</p>

<p>每訓練一個字時，要讓 <em>output layer</em> 在那個字所對應的神經元輸出值為 <script type="math/tex">1</script> ，而其他 <script type="math/tex">\mid V\mid -1</script> 個神經元的輸出為 <script type="math/tex">0</script> ， 這樣總共要計算 <script type="math/tex">\mid V\mid </script> 次，會使得訓練變得沒效率。</p>

<p>若要減少於 <em>output layer</em> 的訓練時間，可以把 <em>output layer</em> 的字作分類階層，先判別輸出的字是屬於哪類，再判斷其子類別，最後再判斷是哪個字。 </p>

<h2 id="hierarchical-softmax">Hierarchical Softmax</h2>

<p>給定訓練資料為<script type="math/tex">X</script> ，輸出字的集合為 <script type="math/tex">Y</script> 。當輸入的字串為 <script type="math/tex">x</script> ，輸出的字為 <script type="math/tex">y</script> 時，訓練的演算法要將機率 <script type="math/tex">P (Y=y \mid  X=x) </script> 最佳化。</p>

<p>如果 <script type="math/tex">Y</script> 有 <em>10000</em> 種字，若沒有分類階層，訓練時就要直接對 <script type="math/tex">P (Y = y \mid  X = x) </script> 做計算，即是對這 <em>10000</em>種字做計算，使 <script type="math/tex">y</script> 所對應的神經元輸出為 <em>1</em> ，其它 <em>9999</em> 個神經元輸出 <em>0</em> ，這樣要計算 <em>10000</em> 次，如下圖：</p>

<p><img src="/images/pic/pic_00079.png" alt="" /></p>

<p>若在訓練前，就事先把 <script type="math/tex">Y</script> 中的字彙分類好，以 <script type="math/tex">C(y)</script> 代表字 <script type="math/tex">y</script> 的類別，則可以改成用以下機率做最佳化：</p>

<!--more-->

<script type="math/tex; mode=display">

P (Y = y | X = x) =

P (Y = y | C = c(y), X) \times P (C = c(y) | X = x)


</script>

<p>根據以上公式，先判斷 <script type="math/tex">y</script> 是在哪類，要算 <script type="math/tex">P (C = c(y) \mid  X = x)</script> 的值，再判斷 <script type="math/tex">y</script> 是 <script type="math/tex">c(y)</script> 中的哪個字，要算 <script type="math/tex">P (Y = y \mid  C = c(y), X) </script> 的值。</p>

<p>若把它們分成 <em>100</em> 類，則每個類別有 <em>100</em> 種字。訓練時需要分成兩步來計算，第一步先計算 <script type="math/tex">P (C = c(y) \mid  X = x)</script> ，此時只要對 <em>100</em> 種分類做計算，使 <script type="math/tex">C(y)</script> 所對應的神經元輸出 <em>1</em> ，其它 <em>99</em> 個神經元輸出 <em>0</em> 。第二步是算 <script type="math/tex">P (Y = y \mid  C = c(y), X)</script>，即是讓 <script type="math/tex">C(y)</script> 底下對應到 <script type="math/tex">y</script> 的神經元輸出 <em>1</em> ，其它<em>99</em> 個神經元輸出 <em>0</em> ，這樣只需要計算 <em>200</em> 次即可，所做的計算只有原本的 <script type="math/tex">\frac{1}{50}</script> ，如下圖：</p>

<p><img src="/images/pic/pic_00080.png" alt="" /></p>

<p>更進一步地，可以將 <em>output layer</em> 分成更多層，也就是說，將它用成樹狀結構，每一個節點代表一次分類，這樣一層層分下去，直到葉子節點，可分出是哪個字彙。分越多層，訓練時要計算的次數就會越小，分到最細的情況下，就變成二元分類樹，這樣所需的計算總量只有原本的 <script type="math/tex">\frac{log_{2}\mid V\mid }{\mid V\mid }</script> 倍。</p>

<p>根據這個二元分類樹，可以把 <script type="math/tex">Y</script> 中的每一個字，對應到一個由 <em>0</em> 和 <em>1</em> 所組成的二元字串 <script type="math/tex">(b_{1}(y), . . . b_{m}(y))</script> ，若字串中第一個數字為 <em>0</em> ，則表示這個字在第一層神經元所計算出的值為 <em>0</em>，以此類推。若要用這個二元分類樹計算 <script type="math/tex">P (Y = y \mid  X = x) </script> 可用以下公式來計算：</p>

<script type="math/tex; mode=display">

P( Y = y | X = x) =

\prod_{j=1}^{m}P(b_{j}(y) | b_{j-1}(y),b_{j-2}(y),...,b_{1}(y), X=x)

</script>

<p>其中， <script type="math/tex">（b_{j−1}(y),b_{j−2}(y),...,b_{1}(y))</script> 為長度小於 <script type="math/tex">m</script> 的二元字串，即是在二元分類樹中不是葉子也不是根的節點。</p>

<p>例如字串 <script type="math/tex">( b_{1}=0, b_{2}=1 )</script>即表示先從根節點往 <em>0</em> 的分支走到子節點，再從這個子節點走到 <em>1</em> 的分支的子節點，此子節點可用字串 <em>01</em> 來表示。若要計算以上公式的條件機率，方法為，先給這些子節點有它們自己的語意向量，再用 <em>NPLM</em> 把子節點的語意向量和 <script type="math/tex">x</script> 當成輸入值一起輸入 <em>NPLM</em> 來計算。</p>

<p>例如，假設詞庫裡總共有 <em>8</em> 個字， 則 <script type="math/tex">m=3</script> ，若字彙 <script type="math/tex">y</script> 的二元字串值 <script type="math/tex">b_{1} = 1, b_{2} =0, b_{3}=0 </script> ，則它在二元分類樹的位置如下圖所示：</p>

<p><img src="/images/pic/pic_00081.png" alt="" /></p>

<p>則 <script type="math/tex">P( Y = y \mid  X = x) </script> 為：</p>

<script type="math/tex; mode=display">

P(b_{1} = 1 | X=x) \times

P(b_{2} = 0 | b_{1} = 1, X=x) \times 

P(b_{3} = 0| b_{2} = 0, b_{1} = 1, X=x)

</script>

<p>訓練時，將訓練資料 <script type="math/tex">x</script> 輸入到 <em>NPLM</em> 的類神經網路中，並以二元分類樹的節點來代替它的 <em>output layer</em> 。</p>

<p>首先，先從根節點的神經元開始訓練，這一步是要算 <script type="math/tex">P(b_{1} = 1 \mid  , X=x) </script> 。由於 <script type="math/tex">b_{1} = 1 </script>，所以要以根節點的神經元輸出為 <em>1</em> 來調整 <em>NPLM</em> 模型的參數，如下圖：</p>

<p><img src="/images/pic/pic_00082.png" alt="" /></p>

<p>再來，從根節點往左走，選 <em>1</em> 分支的子神經元來訓練，這一步是要算 <script type="math/tex">P(b_{2} = 0 \mid  b_{1} = 1, X=x) </script> 。因為這是基於 <script type="math/tex">b_{1} = 1</script> 的條件機率，所以要用字串 <em>1</em> 所對應它的語意向量和 <script type="math/tex">x</script> 一起輸入到 <em>NPLM</em> 。另外，由於 <script type="math/tex">b_{2} = 0 </script>，要以輸出為 <em>0</em> 來調整 <em>NPLM</em> 模型的參數，如下圖：</p>

<p><img src="/images/pic/pic_00083.png" alt="" /></p>

<p>然後，從剛剛那個神經元的節點開始，選 <em>0</em> 分支的子神經元來訓練，這一步是要算 <script type="math/tex">P(b_{3} = 0\mid  b_{2} = 0, b_{1} = 1, X=x) </script> 。因為這是基於 <script type="math/tex">b_{1} = 1 , b_{2} = 0 </script> 的條件機率，所以要先將字串 <em>10</em> 所對應的語意向量和 <script type="math/tex">x</script> 一起輸入到 <em>NPLM</em> 。另外，由於 <script type="math/tex">b_{3} = 0 </script>，要以輸出為 <em>0</em> 來調整 <em>NPLM</em> 模型的參數，如下圖：</p>

<p><img src="/images/pic/pic_00084.png" alt="" /></p>

<p>最後，選擇 <em>0</em> 的分支往下走，即可走到 <script type="math/tex">y</script> ，結束了針對 <script type="math/tex">x</script> 這筆資料的訓練過程。</p>

<p><img src="/images/pic/pic_00085.png" alt="" /></p>

<h2 id="hierarchical-clustering">Hierarchical Clustering</h2>

<p>在訓練之前，要先把字彙的階層分類給建立好。要建立這個分類階層，有很多種方式，例如，使用 <em>WordNet</em> 所建立的上下位關係，來建立階層，或者可以用 <em>Hierarchical Clustering</em> ，像是<a href="/blog/2014/10/25/natural-language-processing-brown-clustering">Brown Clustering</a>之類的，自動從訓練資料中建立分類階層。</p>

<h2 id="conclusion">Conclusion</h2>

<p>用 <em>Hierarchical Softmax</em> 來置換掉原本 <em>NPLM</em> 的 <em>output layer</em> ，可以使得原本要計算 <script type="math/tex">\mid V\mid </script> 次的訓練，縮減為 <script type="math/tex">log_{2}\mid V\mid </script> 次，大幅提升了訓練速度。因此， <em>Hierarchical Softmax</em> 被日後眾多種類神經網路相關的模型所採用，包括近年來很熱門的 <em>word2vec</em> 也是。</p>

<h2 id="reference">Reference</h2>

<p>Frederic Morin , Yoshua Bengio. Hierarchical probabilistic neural network language model. 2005</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[類神經網路 -- Neural Probabilistic Language Model]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/05/15/neural-network-neural-probabilistic-language-model/"/>
    <updated>2015-05-15T02:38:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/05/15/neural-network-neural-probabilistic-language-model</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>傳統的語言模型 <em>Ngram</em> ，只考慮一個句子中的前面幾個字，來預測下個字是什麼。</p>

<p>當使用較長的 <em>Ngram</em> 的時候，則在測試資料中，找到和訓練資料相同的機率，就會大幅降低，甚至使機率為0，此種現象稱為 <em>curse of dimensionality</em> 。這類問題，傳統上可用 <a href="/blog/2014/03/28/equations-for-nlp-ngram-smoothing"><em>Smoothing, Interpolation</em> 或 <em>Backoff</em></a>來處理。</p>

<p>另一種對付 <em>curse of dimensionality</em> 的方法，可以用 <em>Distributional Semantics</em> 的概念，即把詞彙的語意用向量來表示。在訓練資料中，根據這個詞和鄰近周圍的詞的關係，計算出向量中各個維度的值，得出來的值即可表示這個詞的語意。例如，假設在訓練資料中出現 <em>The cat is walking in the bedroom</em> 和 <em>A dog was running in a room</em> 這兩個句子，計算出 <em>dog</em> 和 <em>cat</em> 語意向量中的詞，可得出這兩個詞在語意上是相近的。</p>

<p>所謂的 <em>Neural Probabilistic Language Model</em> ，即是用類神經網路，從語料庫中計算出各個詞彙的語意向量值。</p>

<h2 id="neural-probabilistic-language-model">Neural Probabilistic Language Model</h2>

<p>給定訓練資料為 <script type="math/tex"> w_{1} \cdots w_{T} </script> 一連串的字，每個字都可包含於詞庫 <script type="math/tex">V</script> 中，即 <script type="math/tex">w_{t} \in V</script> ，然後，用此資料訓練出語言模型：</p>

<script type="math/tex; mode=display">


f(w_{t},...,w_{t-n+1}) = P(w_{t} \mid w_{1}^{t-1} )


</script>

<!--more-->

<p>其中， <script type="math/tex"> P(w_{t} \mid w_{1}^{t-1} ) </script> 為，給定第 <script type="math/tex">w_{1}</script> 到 <script type="math/tex">w_{t-1}</script> 的條件下，出現 <script type="math/tex">w_{t}</script> 的機率，為了簡化計算，通常只考慮 <script type="math/tex">w_{t}</script> 前面 <script type="math/tex">n-1</script> 個字，故語言模型的函數為 <script type="math/tex">f(w_{t},...,w_{t-n+1})</script> 。</p>

<p>可用以下兩部分來組成這個式子：</p>

<p>1.建立一個矩陣 <script type="math/tex">C</script> 其維度為 <script type="math/tex">\mid V\mid \times m</script> 。此矩陣中的每一列，即代表詞庫 <script type="math/tex">V</script> 中的每個字所對應之維度為 <script type="math/tex">m</script> 的語意向量 ，例如， <script type="math/tex">w_{t}</script> 的語意向量為 <script type="math/tex">C(w_{t}) </script> 。</p>

<p>2.語料庫中句子的某個字，跟其前面 <script type="math/tex">n-1</script> 個字的語意向量 <script type="math/tex">( C(w_{t-1}), ..., C(w_{t-n+1}) )</script> 有關， 用函數 <script type="math/tex">g</script> 來表示字 <script type="math/tex">w_{t}</script> 與前面 <script type="math/tex">n-1</script> 個字的關係，公式如下：</p>

<script type="math/tex; mode=display">

P(w_{t} \mid w_{1}^{t-1} ) = g(w_{t},w_{t-1}, ..., w_{t-n+1})


</script>

<p>用類神經網路，將 <script type="math/tex">( C(w_{t-1}), ..., C(w_{t-n+1}) )</script> 當成 <em>input</em> ， <script type="math/tex">w_{t}</script> 當成 <em>output</em> ，若 <script type="math/tex">w_{t}=i</script> 時，求 <script type="math/tex">P(w_{t}=i \mid w_{1}^{t-1} )</script> 的機率，如下圖：</p>

<p><img src="/images/pic/pic_00076.png" alt="Neural Network of Neural Probabilistic Language Model" /></p>

<p>其中，此類神經網路有個 <em>tanh</em> 函數的 <em>hidden layer</em> ， <em>input layer</em> 和 <em>hidden layer</em> 都連結到 <em>output layer</em> ，而 <em>output layer</em> 為一個 <em>softmax</em> 函數，將輸出做正規化，即是將所有 <em>output</em> 的值轉化後，其和為 <em>1</em> ， <em>softmax</em> 函數如下：</p>

<script type="math/tex; mode=display">

P(w_{t} \mid w_{t-1} , ... , w_{t-n+1} ) = \frac{e^{y_{w_{t}}}}{\sum_{i} e^{y_{i}}}  

</script>

<p><script type="math/tex">y</script> 為此類神經網路在 <em>softmax</em> 做正規化之前，所算出的值：</p>

<script type="math/tex; mode=display">


y = b+ W\times x + U \times tanh(d+H\times x)

</script>

<p>其中， <script type="math/tex">b</script> 和 <script type="math/tex">d</script> 為 <em>bias</em> ， <script type="math/tex">x</script> 為 <script type="math/tex">( C(w_{t-1}), ..., C(w_{t-n+1}) )</script>，而 <script type="math/tex">H</script> 、 <script type="math/tex">U</script> 與 <script type="math/tex">W</script> 為類神經網路從上一層傳到下一層時，各個值所佔的權重比例。此公式中各矩陣（或向量）維度如下圖所示：</p>

<p><img src="/images/pic/pic_00077.png" alt="" /></p>

<p>在類神經網路中所對應的位置如下圖所示：</p>

<p><img src="/images/pic/pic_00078.png" alt="" /></p>

<p>於此類神經網路中，可經由訓練而調整的參數為 <script type="math/tex">\theta </script> ，如下：</p>

<script type="math/tex; mode=display">

\theta = (b,d,W,U,H,C)

</script>

<p>從 <script type="math/tex">b,d,W,U,H,C</script> 的維度，可得出共有 <script type="math/tex">\mid V\mid (1 + nm + h) + h(1 + (n − 1)m)</script> 個參數可調整。</p>

<h2 id="training-the-neural-probabilistic-language-model">Training the Neural Probabilistic Language Model</h2>

<p>要訓練此類神經網路，即是對 <script type="math/tex">P(w_{t} \mid w_{1}^{t-1} )</script> 做最佳化，並作 <em>regularization</em> 以防 <em>overfitting</em> ，即讓以下公式 <script type="math/tex">L</script> 為最佳解：</p>

<script type="math/tex; mode=display">

L = \frac{1}{T}\sum_{t} log(w_{t},w_{t-1},...,w_{t-n+1},\theta) +R(\theta)

</script>

<p>其中，<script type="math/tex">T</script> 為訓練資料的總字數，而 <script type="math/tex">R(\theta)</script> 作為 <em>regularization</em> 的用途。</p>

<p>可用 <em>Stochastic Gradient Descent</em> 的方式來求得最佳解，其中 <script type="math/tex">\epsilon</script> 為 <em>learning rate</em> ：</p>

<script type="math/tex; mode=display">

\theta \leftarrow \theta + \epsilon \dfrac{\partial log P(w_{t} \mid w_{t-1} , ... , w_{t-n+1} )}{\partial \theta}

</script>

<p>一般而言，類神經網路都是以 <em>Backward Propogation</em> 的方法來訓練，此方法分為兩個階段： <em>Forward Phase</em> 與 <em>Backward Phase</em> 。即是先計算從 <em>input</em> 透過中間的每一層把值傳遞到 <em>output</em> ，再從 <em>output</em> 往回傳遞 <em>error</em> 到中間的每一層，去更新層與層之間權重的參數 。</p>

<p>在此類神經網路的訓練過程中，運算的瓶頸在於從 <em>hidden layer</em> 到 <em>output layer</em> 這段，需要計算 <script type="math/tex">\mid V\mid (1+(n−1)m+h)</script> 個參數，因此通常採取平行化的運算，將此步驟的運算分配到不同的 <em>CPU</em> 。</p>

<p>由於 <em>output layer</em>  的大小為 <script type="math/tex">\mid V\mid </script> ，若總共有 <script type="math/tex">M</script> 個 <em>CPU</em> ，則可將 <em>output layer</em> 分成 <script type="math/tex">M</script> 個區塊，則每個 <em>CPU</em> 負責計算一個區塊，也就是 <script type="math/tex"> ⌈\mid V\mid /M⌉ </script> 個 <em>output unit</em> ，而第 <script type="math/tex">i</script> 個 <em>CPU</em> 則從第 <script type="math/tex">i × ⌈\mid V \mid /M⌉ </script> 個 <em>output unit</em> 開始計算，詳細過程如下：</p>

<p><strong>1.Forward Phase</strong></p>

<p>(a) 根據 <em>input</em> 的詞彙，從矩陣 <script type="math/tex">C</script> 中取出相對應的語意向量：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x(k) \leftarrow C(w_{t-k}) \\

& x = (x(1),x(2),...,x(n-1))

\end{align}

 %]]&gt;</script>

<p>(b) 將 <em>input</em> 的值傳遞到 <em>hidden layer</em> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& o \leftarrow d + H \times x \\

& a \leftarrow tanh(o)

\end{align}

 %]]&gt;</script>

<p>(c) 將 <em>hidden layer</em> 的值傳到 <em>output layer</em> 。</p>

<p>這步驟要平行運算，即第 <script type="math/tex">i</script> 個 <em>CPU</em> 負責運算第 <script type="math/tex">i</script> 個區塊的 <em>output layer</em> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& s_{i} \leftarrow 0 \\ 

& \text{Loop over } j \text{ in the i-th block }  \\

& 1. \mspace{20mu}	y_{j} \leftarrow b_{j} + U_{j} \times a + W_{j} \times x  \\

& 2. \mspace{20mu}	p_{j} \leftarrow e^{y_{j}} \\

& 3. \mspace{20mu} s_{i} \leftarrow s_{i} + p_{j} \\


\end{align}

 %]]&gt;</script>

<p>(d) 合併各個 <em>CPU</em> 所算出來的 <script type="math/tex">s_{i}</script>：</p>

<script type="math/tex; mode=display">

\begin{align}

S = \Sigma_{i}s_{i}

\end{align}

</script>

<p>(e) 將每個 <script type="math/tex">P_{w_{t}}</script> 正規化後取 <em>log</em> ，得出它們的 <script type="math/tex">L</script> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& p_{w_{t}}  \leftarrow \frac{ p_{w_{t}} }{ S }\\

& L = log(p_{w_{t}}) 

\end{align}

 %]]&gt;</script>

<p><em>註：本文所寫的演算法省略掉 Regularization 的過程。</em></p>

<p><strong>2.Backward Phase</strong></p>

<p>(a) 從 <em>output layer</em> 執行 <em>back propogation</em> 。</p>

<p>在此過程中，第 <script type="math/tex">i</script> 個 <em>CPU</em> 負責計算 <em>output layer</em> 中第 <script type="math/tex">i</script> 個區塊的 <em>gradient</em> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&\dfrac{ \partial L} { \partial a} \leftarrow 0 \\

&\dfrac{ \partial L} { \partial x} \leftarrow 0 \\

& \text{Loop over } j \text{ in the i-th block }  \\

& 1. \mspace{20mu}	\dfrac{ \partial L} { \partial y_{j}}  

\leftarrow 1_{j == w_{t}} - p_{j} \\

& 2. \mspace{20mu}	b_{j} \leftarrow b_{j} + \epsilon \dfrac{\partial L }{\partial y_{j}} \\

& 3. \mspace{20mu} \dfrac{ \partial L} { \partial x} \leftarrow 

\dfrac{ \partial L} { \partial x} + \dfrac{ \partial L} { \partial y_{j}}W_{j} \\

& 4. \mspace{20mu} \dfrac{ \partial L} { \partial a} \leftarrow 

\dfrac{ \partial L} { \partial a} + \dfrac{ \partial L} { \partial y_{j}}U_{j} \\

& 5. \mspace{20mu} W_{j} \leftarrow W_{j} + \epsilon\dfrac{ \partial L} { \partial y_{j}}x

\\

& 6. \mspace{20mu} U_{j} \leftarrow U_{j} + \epsilon\dfrac{ \partial L} { \partial y_{j}}a

\end{align}

 %]]&gt;</script>

<p>(b) 把每個 <em>CPU</em> 算出來的 <script type="math/tex">\dfrac{ \partial L} { \partial a}</script> 與 <script type="math/tex">\dfrac{ \partial L} { \partial x}</script> 都加起來。</p>

<p>(c) 用 <em>back propogation</em> 更新 <em>hidden layer</em> 到 <em>input layer</em> 之間的參數：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \text{Loop over } k \text{ between } 1 \text{ and } h  \\

& \mspace{20mu} \dfrac{\partial L}{\partial o_{k}} \leftarrow (1-a_{k}^{2}) \dfrac{\partial L}{\partial a_{k}} \\

& \dfrac{\partial L}{\partial x} \leftarrow \dfrac{\partial L}{\partial x} + H'\dfrac{\partial  L}{\partial o} \\

& d \leftarrow d + \epsilon \dfrac{\partial L}{\partial o} \\

& H \leftarrow H + \epsilon \dfrac{\partial L}{\partial o} x'

\end{align}

 %]]&gt;</script>

<p>(d) 更新 <em>input layer</em> 的語意向量值（從第一個字到第 <script type="math/tex">n-1</script> 個字）：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \text{Loop over } k \text{ between } 1 \text{ and } n-1  \\

& \mspace{20mu} C(w_{t-k}) \leftarrow C(w_{t-k}) + \epsilon \dfrac{\partial L}{\partial x(k)}

\end{align}

 %]]&gt;</script>

<h2 id="conclusion">Conclusion</h2>

<p><em>Neural Probabilistic Language Model</em> 是2003年期間所提出的語言模型，但受限於當時的電腦運算能力，這個模型的複雜度實在太高，難以實際應用。</p>

<p>近幾年來由於平行運算和 <em>GPU</em> 的發展，大幅提升了矩陣運算的效率，促成類神經網路與深度學習的發展。因此，建構在 <em>Neural Probabilistic Language Model</em> 的基礎之上，發展出了各種類神經網路相關的語言模型，一再突破了以往自然語言處理的瓶頸，包括近兩年來很熱門的 <em>word2vec</em> ，也是從它發展而來。</p>

<h2 id="reference">Reference</h2>

<p>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. J. Mach. Learn. Res. 3 (March 2003), 1137-1155.</p>

]]></content>
  </entry>
  
</feed>
