<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Natural Language Processing | Mark Chang's Blog]]></title>
  <link href="http://ckmarkoh.github.io/blog/categories/natural-language-processing/atom.xml" rel="self"/>
  <link href="http://ckmarkoh.github.io/"/>
  <updated>2016-12-11T15:32:34+08:00</updated>
  <id>http://ckmarkoh.github.io/</id>
  <author>
    <name><![CDATA[Mark Chang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Word2vec (Part 3 : Implementation)]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/08/29/neural-network-word2vec-part-3-implementation/"/>
    <updated>2016-08-29T11:17:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/08/29/neural-network-word2vec-part-3-implementation</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>本文接續 <a href="/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation">word2vec (part2)</a> ，介紹如何根據推導出來的 <em>backward propagation</em> 公式，從頭到尾實作一個簡易版的 <em>word2vec</em> 。</p>

<p>本例的 input layer 採用 <em>skip-gram</em> ， output layer 採用 <em>negative sampling</em></p>

<p>本例用唐詩語料庫：https://github.com/ckmarkoh/coscup_nndl/blob/master/poem.txt</p>

<p>首先，載入所需的模組</p>

<p>```python
import json
from collections import Counter, OrderedDict
import numpy as np
import random
import math</p>

<p>```</p>

<!--more-->

<h2 id="build-dictionray">Build Dictionray</h2>

<p>再來是建立字典，即將每個字給一個id來對應。</p>

<p>```python
def LearnVocabFromTrainFile():</p>

<pre><code># 開啟唐詩語料庫
f = open("poem.txt")
 
# 統計唐詩語料庫中每個字出現的頻率
vcount = Counter()
for line in f.readlines():
    for w in line.decode("utf-8").strip().split():
        vcount.update(w)
        
# 僅保留出現次數大於五的字，並按照出現次數排序
vcount_list = sorted(filter(lambda x: x[1] &gt;= 5, vcount.items())
                     , reverse=True, key=lambda x: x[1])
                     
# 建立字典，將每個字給一個id ，字為 key, id 為 value
vocab_dict = OrderedDict(map(lambda x: (x[1][0], x[0]), enumerate(vcount_list)))

# 建立詞頻統計用的字典，給定某字，可查到其出現頻率
vocab_freq_dict = OrderedDict(map(lambda x: (x[0], x[1]), vcount_list))
return vocab_dict, vocab_freq_dict
</code></pre>

<p>vocab_dict, vocab_freq_dict =  LearnVocabFromTrainFile()</p>

<p>```</p>

<p>印出字典檔，每個字對應到一個id（編號）</p>

<p>```python
for w,wid in vocab_dict.items():
    print “%s : %s”%(w,wid)</p>

<p>```</p>

<p>結果如下：</p>

<p>```sh
不 : 0
人 : 1
山 : 2
無 : 3
風 : 4
……
謏 : 5496
笮 : 5497
躠 : 5498
噆 : 5499</p>

<p>```</p>

<p>印出詞頻統計用的字典，給定某字，可查詢到其出現頻率：</p>

<p>```python
for w,wfreq in vocab_freq_dict.items():
    print “%s : %s”%(w,wfreq)</p>

<p>```</p>

<p>結果如下：</p>

<p>```sh
不 : 26426
人 : 20966
山 : 16056
無 : 15795
風 : 15618
…
謏 : 5
笮 : 5
躠 : 5
噆 : 5</p>

<p>```</p>

<h2 id="build-unigram-table">Build Unigram Table</h2>

<p>本例採用 <em>negative sampling</em> ，需要先建立 <em>unigram table</em> 以便進行 <em>negative sampling</em> 。</p>

<p>所謂的 <em>Unigram Table</em> 即是一個 <em>array</em> ，其中每個元素為某字的id，而某字的頻率，即為此id在此 <em>table</em> 中出現的次數的 0.75次方。</p>

<p>例如，id 為 5496 的字，詞頻為 5 ，則在此 <em>Unigram Table</em> 中，5496 的次數為：</p>

<script type="math/tex; mode=display">

5^{0.75} = 3.34 \approx 3

</script>

<p>由於 <em>array</em> 中的元素個數必須是整數，所以 5496 在 <em>Unigram Table</em> 中出現三次。</p>

<p>建立 <em>Unigram Table</em> 的程式碼如下：</p>

<p>```python
def InitUnigramTable(vocab_freq_dict):
    table_freq_list = map(lambda x: (x[0], int(x[1][1] ** 0.75)), enumerate(vocab_freq_dict.items()))
    table_size = sum([x[1] for x in table_freq_list])
    table = np.zeros(table_size).astype(int)
    offset = 0
    for item in table_freq_list:
        table[offset:offset + item[1]] = item[0]
        offset += item[1]</p>

<pre><code>return table
</code></pre>

<p>table = InitUnigramTable(vocab_freq_dict)</p>

<p>```</p>

<p>得出的 <em>Unigram Table</em> 如下：</p>

<p>```
[   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  <br />
0    0    0    0  … , 5495 5495 5495 5496  5496 5496 5497 5497 5497 5498 
5498 5498 5499 5499 5499]</p>

<p>```</p>

<h2 id="training-word2vec">Training word2vec</h2>

<p>```python
def train(vocab_dict, vocab_freq_dict, table):</p>

<pre><code>total_words = sum([x[1] for x in vocab_freq_dict.items()])
vocab_size = len(vocab_dict)

# 參數設定
layer1_size = 30 # hidden layer 的大小，即向量大小
window = 2 # 上下文寬度的上限
alpha_init = 0.025 # learning rate
sample = 0.001 # 用來隨機丟棄高頻字用
negative = 10 # negative sampling 的數量
ite = 2 # iteration 次數

# Weights 初始化
# syn0 : input layer 到 hidden layer 之間的 weights ，用隨機值初始化
# syn1 : hidden layer 到 output layer 之間的 weights ，用0初始化
syn0 = (0.5 - np.random.rand(vocab_size, layer1_size)) / layer1_size 
syn1 = np.zeros((layer1_size, vocab_size))

# 印出進度用
train_words = 0 # 總共訓練了幾個字
p_count = 0
avg_err = 0.
err_count = 0

for local_iter in range(ite):
    print "local_iter", local_iter
    f = open("poem.txt")
    for line in f.readlines():
        
        #用來暫存要訓練的字，一次訓練一個句子
        sen = []
        
        # 取出要被訓練的字
        for word_raw in line.decode("utf-8").strip().split():
            last_word = vocab_dict.get(word_raw, -1)
            
            # 丟棄字典中沒有的字（頻率太低）
            if last_word == -1:
                continue
            cn = vocab_freq_dict.get(word_raw)
            ran = (math.sqrt(cn / float(sample * total_words + 1))) * (sample * total_words) / cn
            
            # 根據字的頻率，隨機丟棄，頻率越高的字，越有機會被丟棄
            if ran &lt; random.random():
                continue
            train_words += 1
            
            # 將要被訓練的字加到 sen
            sen.append(last_word)
            
        # 根據訓練過的字數，調整 learning rate
        alpha = alpha_init * (1 - train_words / float(ite * total_words + 1))
        if alpha &lt; alpha_init * 0.0001:
            alpha = alpha_init * 0.0001
            
        # 逐一訓練 sen 中的字
        for a, word in enumerate(sen):
        
        		# 隨機調整 window 大小
            b = random.randint(1, window)
            for c in range(a - b, a + b + 1):
                
                # input 為 window 範圍中，上下文的某一字
                if c &lt; 0 or c == a or c &gt;= len(sen):
                    continue
                last_word = sen[c]
									
                # h_err 暫存 hidden layer 的 error 用
                h_err = np.zeros((layer1_size))
                
                # 進行 negative sampling
                for negcount in range(negative):
                
                		# positive example，從 sen 中取得，模型要輸出 1
                    if negcount == 0:
                        target_word = word
                        label = 1
                    
                    # negative example，從 table 中抽樣，模型要輸出 0 
                    else:
                        while True:
                            target_word = table[random.randint(0, len(table) - 1)]
                            if target_word not in sen:
                                break
                        label = 0
                    
                    # 模型預測結果
                    o_pred = 1 / (1 + np.exp(- np.dot(syn0[last_word, :], syn1[:, target_word])))
                    
                    # 預測結果和標準答案的差距
                    o_err = o_pred - label
                    
                    # backward propagation
                    # 此部分請參照 word2vec part2 的公式推導結果
                    
                    # 1.將 error 傳遞到 hidden layer                        
                    h_err += o_err * syn1[:, target_word]
                    
                    # 2.更新 syn1
                    syn1[:, target_word] -= alpha * o_err * syn0[last_word]
                    avg_err += abs(o_err)
                    err_count += 1
                
                # 3.更新 syn0
                syn0[last_word, :] -= alpha * h_err
                
                # 印出目前結果
                p_count += 1
                if p_count % 10000 == 0:
                    print "Iter: %s, Alpha %s, Train Words %s, Average Error: %s" \
                          % (local_iter, alpha, 100 * train_words, avg_err / float(err_count))
                    avg_err = 0.
                    err_count == 0.
                    
    # 每一個 iteration 儲存一次訓練完的模型
    model_name = "w2v_model_blog_%s.json" % (local_iter)
    print "save model: %s" % (model_name)
    fm = open(model_name, "w")
    fm.write(json.dumps(syn0.tolist(), indent=4))
    fm.close()
</code></pre>

<p>```</p>

<p>開始訓練：</p>

<p>```python
train(vocab_dict, vocab_freq_dict, table)</p>

<p>```</p>

<p>輸出結果如下，可以看到，當訓練過的字數增加時， Error 也跟著降低</p>

<p>大概要花幾十分鐘左右訓練完</p>

<p>```sh
Iter: 0, Alpha 0.0249923666923, Train Words 475200, Average Error: 0.499999254842
Iter: 0, Alpha 0.0249846739501, Train Words 954100, Average Error: 0.249998343836
Iter: 0, Alpha 0.0249771900316, Train Words 1420000, Average Error: 0.166660116256
Iter: 0, Alpha 0.0249693430813, Train Words 1908500, Average Error: 0.124949913475
Iter: 0, Alpha 0.024961329072, Train Words 2407400, Average Error: 0.0993522008349
Iter: 0, Alpha 0.0249531817368, Train Words 2914600, Average Error: 0.0787704454331
Iter: 0, Alpha 0.0249453540624, Train Words 3401900, Average Error: 0.06351951221
Iter: 0, Alpha 0.0249377801891, Train Words 3873400, Average Error: 0.0495117808015
……….</p>

<p>```</p>

<h2 id="show-result">Show Result</h2>

<p>檢視 word2vec 訓練結果的方法，即是看使用 <em>cosine similarity</em> 計算，是否能得出與某字語意相近的字。</p>

<p>```python</p>

<h1 id="section">讀取訓練好的模型</h1>
<p>f2 = open(“w2v_model_1.json”, “r”) 
w2v_model = np.array(json.loads(““.join(f2.readlines())))
f2.close()</p>

<p>vocab_dict_reversed = OrderedDict([(x[1], x[0]) for x in vocab_dict.items()])</p>

<h1 id="cosine-similarity-">計算 cosine similarity 最高的前五字</h1>
<p>def get_top(word):
    wid = vocab_dict.get(word)</p>

<pre><code># 將某字與模型中所有的字向量做內積
dot_result = np.dot(w2v_model, np.expand_dims(w2v_model[wid], axis=1))
norm = np.sqrt(np.sum(np.power(w2v_model.T, 2), axis=0))

# 計算 cosine similarity
cosine_result = np.divide(dot_result[:, 0], norm*norm[wid])

# 根據 cosine similarity 的值排序
final_result = sorted(filter(lambda x:x[0] != wid, 
                      [(x[0], x[1]) for x in enumerate(cosine_result)]),
                      key=lambda x: x[1], reverse=True)
print word

# 印出語意最接近的前五字，以及其 cosine similarity
for x in final_result[:5]:
    print vocab_dict_reversed.get(x[0]), x[1]
</code></pre>

<p>```</p>

<p>分別計算「山、峰、河、日」這四字語意最相近的字</p>

<p>```python
get_top(u”山”)
get_top(u”峰”)
get_top(u”河”)
get_top(u”日”)</p>

<p>```</p>

<p>結果如下，可看出，計算所得出語意最相近的字，實際上，語意也相近，例如，山和峰、嶺的語意都很接近。</p>

<p>```sh
山
嶺 0.854901128361
嵩 0.846620438864
峰 0.842831270385
岡 0.838129842909
嶂 0.834701215189
峰
山 0.842831270385
嶽 0.83917452917
嶺 0.8219837161
頂 0.821088331571
嶂 0.809565794884
河
湟 0.787726187693
涇 0.770652269018
淮 0.751135710239
川 0.742243126005
汾 0.740643816278
日
旦 0.869047480855
又 0.842383624714
曛 0.830549707539
夕 0.826327222048
暉 0.82616774597</p>

<p>```</p>

<p>向量加減運算後的 <em>cosine similarity</em> ，例如： 女 + 父 - 男 = 母</p>

<p>```python
def get_calculated_top(w1, w2, w3):
    wid1, wid2, wid3 = vocab_dict.get(w1), vocab_dict.get(w2), vocab_dict.get(w3)
    v1, v2, v3 = w2v_model[wid1], w2v_model[wid2], w2v_model[wid3]</p>

<pre><code># 得出加減運算後的向量
combined_vec = v1 + (v2 - v3)
dot_result = np.dot(w2v_model, np.expand_dims(combined_vec, axis=1))
norm = np.sqrt(np.sum(np.power(w2v_model.T, 2), axis=0))
cvec_norm = np.sqrt(np.sum(np.power(combined_vec, 2)))
cosine_result = np.divide(dot_result[:, 0], norm * cvec_norm)

final_result = sorted(filter(lambda x: x[0] not in [wid1, wid2, wid3],
                             [(x[0], x[1]) for x in enumerate(cosine_result)]),
                      key=lambda x: x[1], reverse=True)
print "%s + %s - %s" % (w1, w2, w3)
for x in final_result[:5]:
    print vocab_dict_reversed.get(x[0]), x[1]
</code></pre>

<p>```</p>

<p>```python
get_calculated_top(u”女”, u”父”, u”男”)</p>

<p>```</p>

<p>結果如下，如預期，運算結果的語意接近「母」：</p>

<p>```sh
女 + 父 - 男
母 0.731002049447
娥 0.707469857054
客 0.69027387716
娃 0.687831493041
侶 0.681667240226</p>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Word2vec (Part 2 : Backward Propagation)]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation/"/>
    <updated>2016-07-12T09:21:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>本文接續 <a href="/blog/2016/07/12/neural-network-word2vec-part-1-overview">word2vec (part1)</a> ，介紹 <em>word2vec</em> 訓練過程的 <em>backward propagation</em> 公式推導。</p>

<p><em>word2vec</em> 的訓練過程中，輸出的結果，跟上下文有關的字，在 <em>output layer</em> 輸出為 1 ，跟上下文無關的字，在 <em>output layer</em> 輸出為 0。 在此，把跟上下文有關的，稱為 <em>positive example</em> ，而跟上下文無關的，稱為 <em>negative example</em> 。</p>

<p>根據 <a href="/blog/2016/07/12/neural-network-word2vec-part-1-overview">word2vec (part1)</a> 中提到的例子， <em>cat</em> 的向量為 <script type="math/tex">\textbf{v}_2</script> ， <em>run</em> 的向量為 <script type="math/tex">\textbf{w}_3</script> ， <em>fly</em> 的向量為 <script type="math/tex">\textbf{w}_4</script> ，由於 <em>cat</em> 的上下文有 <em>run</em> ，所以 <em>run</em> 為 <em>positive example</em> ，而 <em>cat</em> 的上下文沒有 <em>fly</em> ，所以 <em>fly</em> 為 <em>negative example</em> ，如下圖所示：</p>

<p><img src="/images/pic/pic_00187.png" alt="" /></p>

<!--more-->

<h2 id="objective-function">Objective Function</h2>

<p>訓練類神經網路需要有個目標函數，如果希望 <em>positive example</em> 輸出為 1 ， <em>negative example</em> 輸出為 0，則可以將以下函數 <script type="math/tex">J</script> 做最小化。</p>

<script type="math/tex; mode=display">

J = - \text{log}(\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{pos} }}) - \sum_{neg} \text{log} ( 1- \frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{neg} }} )

</script>

<p>其中 <script type="math/tex">\textbf{v}_I</script> 為輸入端的字向量，而 <script type="math/tex">\textbf{w}_{pos}</script> 和 <script type="math/tex">\textbf{w}_{neg}</script> 為輸出端的字向量。 <script type="math/tex">\textbf{w}_{pos}</script> 為 <em>positive example</em> ，而  <script type="math/tex">\textbf{w}_{neg}</script> 為 <em>negative example</em> 。通常，對於每筆 <script type="math/tex">\textbf{v}_I</script> 而言，會找一個 <em>positive example</em> 和多個 <em>negative example</em> ，因此用 <script type="math/tex">\sum</script> 將這些 <em>negative example</em> 算出的結果給加起來。</p>

<p>先看這公式前半部的部分：</p>

<script type="math/tex; mode=display">

-\text{log}(\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{pos} }})

</script>

<p>從以上公式得知，當 <script type="math/tex">\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{pos} }} = 0</script> 時，  <script type="math/tex">J</script> 會趨近無限大，而當 <script type="math/tex">\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{pos} }} = 1</script> 時 ，  <script type="math/tex">J</script> 會趨近 0 ，所以，降低 <script type="math/tex">J</script> 的值，會迫使  <script type="math/tex">\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{pos} }}</script> 接近 1 。</p>

<p>再來看另一部分：</p>

<script type="math/tex; mode=display">

-\text{log}(1 - \frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{neg} }})

</script>

<p>當 <script type="math/tex">\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{neg} }} = 1</script> 時 <script type="math/tex">J</script> 會趨近無限大，反之亦然，同理，降低 <script type="math/tex">J</script> 的值，會迫使  <script type="math/tex">1- \frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{neg} }}</script> 接近 0 。</p>

<h2 id="backward-propagation">Backward Propagation</h2>

<p>至於要怎麼調整 <script type="math/tex">\textbf{v}</script> 和  <script type="math/tex">\textbf{w}</script> 的值，才能讓 <script type="math/tex">J</script> 變小？ 就是要用到 <em>backward propagation</em> 。</p>

<h3 id="positive-example">Positive Example</h3>

<p>這邊先看 <em>positive example</em> 的部分， 輸入端為 <em>dog</em> ，輸出端為 <em>run</em> ，在進行偏微分公式推導之前，先定義一些符號，以便之後推導：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x_{13} = \textbf{v}_1 \cdot \textbf{w}_3 \\

& y_{13} = \dfrac{1}{1+e^{-x_{13}}}  \\

& J = - \text{log} (y_{13} )

\end{align}

 %]]&gt;</script>

<p>這些符號，如下圖所示， <script type="math/tex">x_{13}</script> 是 <em>output layer</em> 在 <em>sigmoid</em> 之前的值， <script type="math/tex">y_{13} </script> 是通過 <em>sigmoid</em> 後的值， <script type="math/tex">J</script> 是針對這筆 <em>positive example</em> 所算出來的 <em>cost function</em> 。</p>

<p><img src="/images/pic/pic_00188.png" alt="" /></p>

<p>如果要更新 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_3</script> 的值，讓 <script type="math/tex">J</script> 變小，就要用 <em>gradient descent</em> 的方式，過程如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{v}_1 \leftarrow \textbf{v}_1 - \eta \nabla_{\textbf{v}_1}  J  \\

& \textbf{w}_3 \leftarrow \textbf{w}_3 - \eta \nabla_{\textbf{w}_1}  J   

\end{align}

 %]]&gt;</script>

<p>想瞭解更多關於 <em>gradient descent</em> ，請參考：<a href="/blog/2015/12/23/optimization-method-adagrad">Gradient Descent &amp; AdaGrad </a></p>

<p>其中， <script type="math/tex">\eta</script> 為 <em>learning rate</em> ，為一常數，就是決定每一步要走多大，至於 <script type="math/tex">\nabla_{\textbf{v}_1} J </script> 這項要怎麼算？</p>

<p>先看看它每個維度上的值：</p>

<script type="math/tex; mode=display">

\nabla_{\textbf{v}_1} J = 

\begin{bmatrix}

\frac{\partial J}{\partial v_{11}} \\

\frac{\partial J}{\partial v_{12}} \\

\frac{\partial J}{\partial v_{13}} \\

\end{bmatrix}

</script>

<p>先看 <script type="math/tex">\frac{\partial J}{\partial v_{11}}</script> 這項，可以用 <em>chain rule</em> 把它拆開：</p>

<script type="math/tex; mode=display">

\frac{\partial J}{\partial v_{11} } = \frac{\partial J}{\partial y_{13}} \times \frac{\partial y_{13}}{\partial x_{13}}  \times \frac{\partial x_{13}}{\partial v_{11}}

</script>

<p>將 <script type="math/tex">\frac{\partial J}{\partial v_{11} }</script> 拆成 <script type="math/tex">\frac{\partial J}{\partial y_{13}}</script> 、 <script type="math/tex">\frac{\partial y_{13}}{\partial x_{13}}</script> 和 <script type="math/tex">\frac{\partial x_{13}}{\partial v_{11}}</script> 這三項。而這三項的值可分別求出來：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \frac{\partial J}{\partial y_{13}} = \frac{\partial   - \text{log} (y_{13} )}{\partial y_{13}} = \frac{-1}{y_{13}} \\ 

& \frac{\partial y_{13}}{\partial x_{13}} = \frac{\partial (\frac{1}{1+e^{-x_{13}}})}{\partial x_{13}} = \frac{1}{1+e^{-x_{13}}}( 1- \frac{1}{1+e^{-x_{13}}}) = y_{13} ( 1- y_{13}) \\

& \frac{\partial x_{13}}{\partial v_{11}} = \frac{\partial \textbf{v}_{1} \cdot \textbf{w}_3} {\partial v_{11}} = w_{31}


\end{align}

 %]]&gt;</script>

<p>代回這三項的結果到 <script type="math/tex">\frac{\partial J}{\partial v_{11} }</script> ，得出：</p>

<script type="math/tex; mode=display">

\frac{\partial J}{\partial v_{11} } = \frac{-1}{y_{13}} \times y_{13} ( 1- y_{13}) \times  w_{31} = ( y_{13} - 1)  \times w_{31}

</script>

<p>而 <script type="math/tex">\frac{\partial J}{\partial v_{12}}</script> 和 <script type="math/tex">\frac{\partial J}{\partial v_{13}}</script> 也可用同樣方式得出其值， 如下：</p>

<script type="math/tex; mode=display">

\nabla_{\textbf{v}_1} J = 

\begin{bmatrix}

\frac{\partial J}{\partial v_{11}} \\

\frac{\partial J}{\partial v_{12}} \\

\frac{\partial J}{\partial v_{13}} \\

\end{bmatrix}

=

\begin{bmatrix}

 ( y_{13} - 1)  \times w_{31} \\

 ( y_{13} - 1)  \times w_{32} \\

 ( y_{13} - 1)  \times w_{33} \\

 \end{bmatrix}

 = 

  ( y_{13} - 1)  \textbf{w}_3

</script>

<p>因此，可得出 <script type="math/tex">\textbf{v}_1</script> 要調整的量：</p>

<script type="math/tex; mode=display">

\textbf{v}_1 \leftarrow \textbf{v}_1 - \eta  ( y_{13} - 1)  \textbf{w}_3

</script>

<p>同理， <script type="math/tex">\textbf{w}_3</script> 要調整的量：</p>

<script type="math/tex; mode=display">

\textbf{w}_3 \leftarrow \textbf{w}_3 - \eta  ( y_{13} - 1)   \textbf{v}_1

</script>

<p>其中，可以把 <script type="math/tex">( y_{13} - 1)</script> 看成是，模型預測出來的，和我們所想要的值，差距多少。因為在 <em>positive example</em> 的情況下，我們希望模型輸出結果 <script type="math/tex">y_{13}</script> 為 1 。如果  <script type="math/tex">y_{13} = 1</script> 表示模型預測對了，則不需要修正 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_3</script> ，如果  <script type="math/tex">y_{13} \neq 1</script> 時，才要修正 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_3</script> 。</p>

<p>還有，之所以把這過程，稱為 <em>backward propagation</em> ，是因為可以把 <em>chain rule</em> 拆解 <script type="math/tex">\nabla_{\textbf{v}_1} J </script> 的過程，看成是將 <script type="math/tex">\frac{\partial J}{\partial y_{13} }</script> 的值， 由 <em>output layer</em> 往前傳遞，如下圖：</p>

<p><img src="/images/pic/pic_00189.png" alt="" /></p>

<p>想瞭解更多關於 <em>backward propagation</em> 的推導，請參考： <a href="/blog/2015/05/28/neural-network-backward-propagation">Backward Propagation 詳細推導過程 </a></p>

<h3 id="negative-example">Negative Example</h3>

<p>再來看看 <em>negative example</em> 的部分， 輸入端為 <em>dog</em> ，輸出端為 <em>fly</em> ，在進行偏微分公式推導之前，先定義一些符號，以便之後推導：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x_{14} = \textbf{v}_1 \cdot \textbf{w}_4 \\

& y_{14} = \dfrac{1}{1+e^{-x_{14}}}  \\

& J = - \text{log} (1 - y_{14} )

\end{align}

 %]]&gt;</script>

<p>這些符號，如下圖所示， <script type="math/tex">x_{14}</script> 是 <em>output layer</em> 在 <em>sigmoid</em> 之前的值， <script type="math/tex">y_{14} </script> 是通過 <em>sigmoid</em> 後的值， <script type="math/tex">J</script> 是針對這筆 <em>positive example</em> 所算出來的 <em>cost function</em> 。</p>

<p><img src="/images/pic/pic_00190.png" alt="" /></p>

<p>同之前 <em>positive example</em> ，如果要更新 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_4</script> 的值，讓 <script type="math/tex">J</script> 變小，就要用 <em>gradient descent</em> 的方式：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{v}_1 \leftarrow \textbf{v}_1 - \eta \nabla_{\textbf{v}_1}  J  \\

& \textbf{w}_4 \leftarrow \textbf{w}_4 - \eta \nabla_{\textbf{w}_1}  J   

\end{align}

 %]]&gt;</script>

<p>剩下的推導和 <em>positive example</em> 時，幾乎一樣，只有 <script type="math/tex">J</script> 不一樣。此處只需推導 <script type="math/tex">\frac{\partial J}{\partial y_{14}}</script> 的結果。</p>

<script type="math/tex; mode=display">

\frac{\partial J}{\partial y_{14}} = \frac{\partial   - \text{log} (1 - y_{14} )}{\partial y_{14}} = \frac{1}{1 - y_{14}} 

</script>

<p>代回此結果到 <script type="math/tex">\frac{\partial J}{\partial v_{11} }</script> ，得出：</p>

<script type="math/tex; mode=display">

\frac{\partial J}{\partial v_{11} } = \frac{1}{ 1- y_{14}} \times y_{14} ( 1- y_{14}) \times  w_{41} = ( y_{14} - 0)  \times w_{41}

</script>

<p>於是可以得出要修正的量：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{v}_1 \leftarrow \textbf{v}_1 - \eta  ( y_{14} - 0)  \textbf{w}_4 \\ 

& \textbf{w}_4 \leftarrow \textbf{w}_4 - \eta  ( y_{14} - 0)   \textbf{v}_1

\end{align}

 %]]&gt;</script>

<p>其中，可以把 <script type="math/tex">( y_{14} - 0)</script> 看成是，模型預測出來的，和我們所想要的值，差距多少。因為在 <em>negative example</em> 的情況下，我們希望模型輸出結果 <script type="math/tex">y_{14}</script> 為 0 。如果  <script type="math/tex">y_{13} = 0</script> 表示模型預測對了，則不需要修正 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_4</script> ，如果  <script type="math/tex">y_{14} \neq 0</script> 時，才要修正 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_4</script> 。</p>

<h2 id="further-reading">Further Reading</h2>

<p>關於如何從無到有，不使用任何自動微分的套件，實作一個 <em>word2vec</em> ，請看：<a href="/blog/2016/08/29/neural-network-word2vec-part-3-implementation">word2vec (part3)</a></p>

<p>註： 實際上， <em>word2vec</em> 的 <em>input layer</em> 和 <em>output layer</em> 各有兩種架構。<em>input layer</em> 有 <em>cbow</em> 和 <em>skip-gram</em> 兩種，<em>output layer</em> 有 <em>hierarchical sofrmax</em> 和 <em>negative sampling</em> 兩種，本文所寫的為 <em>skip-gram</em> 搭配 <em>negative sampling</em> 的架構。關於 <em>hierarchical softmax</em> 請參考<a href="/blog/2015/05/23/-hierarchical-probabilistic-neural-networks-neural-network-language-model">hierarchical sofrmax</a> 。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Word2vec (Part 1 : Overview)]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/07/12/neural-network-word2vec-part-1-overview/"/>
    <updated>2016-07-12T09:19:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/07/12/neural-network-word2vec-part-1-overview</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>文字的語意可以用向量來表示，在上一篇 <a href="/blog/2016/07/10/nlp-vector-space-semantics"> Vector Space of Semantics </a> 中提到，如果把每種字當成一個維度，假設總共有一萬總字，那向量就會有一萬個維度。有兩種方法可降低維度，分別是 <em>singular value decomposition</em> 和 <em>word2vec</em> 。</p>

<p>本文講解 <em>word2vec</em> 的原理。 <em>word2vec</em> 流程，總結如下：</p>

<p><img src="/images/pic/pic_00191.png" alt="" /></p>

<p>首先，將文字做 <em>one-hot encoding</em> ，然後再用 <em>word2vec</em> 類神經網路計算，求出壓縮後（維度降低後）的語意向量。</p>

<!--more-->

<h2 id="one-hot-encoding">One-Hot Encoding</h2>

<p>一開始，不知道哪個字和哪個字語意相近，所以就假設每個字的語意是不相干的。也就是說，每個字的向量都是互相垂直。</p>

<p>這邊舉個比較簡單的例子，假設總字彙量只有 4 個， 分別為 <em>dog, cat, run, fly</em> ，那麼，經過 <em>one-hot encoding</em> 的結果如下：</p>

<p><img src="/images/pic/pic_00192.png" alt="" /></p>

<p>如上圖， <em>dog</em> 的向量為 (1,0,0,0) ，只有在第一個維度是 1 ，其他維度是 0 ，而 <em>cat</em> 的向量為 (0,1,0,0) 只有在第一個維度是 1 ，其他維度是 0 。</p>

<p>也就是說，每個字都有一個代表它的維度，而它 <em>one hot encoding</em> 的結果，只有在那個維度上是 1 ，其他維度都是 0 。這樣的話，任意兩個 <em>one hot encoding</em> 的向量內積結果，都會是 0 ，內積結果為 0 ，表示兩向量是垂直的。</p>

<p>註：實際應用中，字彙量即是語料庫中的單字種類，通常會有幾千個甚至一萬個以上。</p>

<h2 id="word2vec">word2vec</h2>

<p><em>word2vec</em> 的神經網路架構如下，總共有三層， <em>input layer</em> 和 <em>output layer</em> 一樣大，中間的 <em>hidden layer</em> 比較小。</p>

<p><img src="/images/pic/pic_00193.png" alt="" /></p>

<p>如上圖，總字彙量有 4 個，那麼 <em>input layer</em> 和 <em>output layer</em> 的維度為 4， 每個維度分別代表一個字。 如果想要把向量維度降至三維， <em>hidden layer</em> 的維度為 3。</p>

<p>另外要注意的是， <em>hidden layer</em> 沒有非線性的 <em>activation funciton</em> ，而 <em>output layer</em> 的 <em>activation function</em> 是 <em>sigmoid</em> ，這兩點會有什麼影響，之後會提到。</p>

<p>其中，在 <em>input layer</em> 和 <em>hidden layer</em> 之間， 有 <script type="math/tex">4 \times 3</script> 個 <em>weight</em> ，在此命名為 <script type="math/tex">\textbf{V}</script> ，而在 <em>hidden layer</em> 到 <em>output layer</em> 之間， 有 <script type="math/tex">3 \times 4</script> 個 <em>weight</em> ，在此命名為 <script type="math/tex">\textbf{W}</script> 。</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\textbf{V}=

\begin{bmatrix}

    v_{11} & v_{12} & v_{13}  \\

    v_{21} & v_{22} & v_{23}  \\

    v_{31} & v_{32} & v_{33}  \\

    v_{41} & v_{42} & v_{43}  \\

\end{bmatrix}

\mspace{30mu}

\textbf{W}^T=

\begin{bmatrix}

    w_{11} & w_{12} & w_{13}  \\

    w_{21} & w_{22} & w_{23}  \\

    w_{31} & w_{32} & w_{33}  \\

    w_{41} & w_{42} & w_{43}  \\

\end{bmatrix}

 %]]&gt;</script>

<p>由於 <em>input</em> 是 <em>one hot encoding</em> 的向量，又因為 <em>hidden layer</em> 沒有 <em>sigmoid</em> 之類的非線性 <em>activation function</em>。 輸入到類神經網路後，在 <em>hidden layer</em> 所取得的值，即是 <script type="math/tex">\textbf{V}</script> 中某個橫排的值，如下：</p>

<p><img src="/images/pic/pic_00194.png" alt="" /></p>

<p>例如，輸入的是 <em>dog</em> 的 <em>one hot encoding</em> ，只有在第 1 個維度是 1 ，與 <script type="math/tex">\textbf{V}</script> 作矩陣相乘後，在 <em>hidden layer</em> 取得的值是 <script type="math/tex">\textbf{V}</script> 中的第一個橫排： <script type="math/tex">(v_{11}, v_{12}, v_{13})</script> ，這個向量就是 <em>dog</em> 壓縮後的語意向量。運算過程如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{bmatrix}

1 & 0 & 0 & 0 

\end{bmatrix}

\begin{bmatrix}

    v_{11} & v_{12} & v_{13}  \\

    v_{21} & v_{22} & v_{23}  \\

    v_{31} & v_{32} & v_{33}  \\

    v_{41} & v_{42} & v_{43}  \\

\end{bmatrix}

= 

\begin{bmatrix}

v_{11} & v_{12} & v_{13}

\end{bmatrix}

 %]]&gt;</script>

<p>因此， <script type="math/tex">\textbf{V}</script> 中的某個橫排，就是某個字的語意向量。從反方向來看，由於 <em>output layer</em> 也是對應到字彙的 <em>one hot encoding</em> 因此， <script type="math/tex">\textbf{W}^T</script> 中的某個橫排，就是某個字的語意向量。</p>

<p>所以，一個字分別在 <script type="math/tex">\textbf{V}</script> 和 <script type="math/tex">\textbf{W}</script> 中各有一個語意向量。但通常會選擇 <script type="math/tex">\textbf{V}</script> 中的語意向量，作為 <em>word2vec</em> 的輸出結果。</p>

<h2 id="initializing-word2vec">Initializing word2vec</h2>

<p>至於如何訓練這個類神經網路？ 訓練一個類神經網路的過程，第一步就是要先將 <em>weight</em> 作初始化。初始化即是隨機給每個 <em>weight</em> 不同的數值，這些數值介於 <script type="math/tex"> -N \sim N</script> 之間。</p>

<p>因此，在還沒開始訓練之前，這些向量的方向都是隨機的，跟語意無關。</p>

<p>舉 <em>dog</em> 和 <em>cat</em> 在 <script type="math/tex">\textbf{V}</script> 中的向量，為 <script type="math/tex">\textbf{V}_1,\textbf{V}_2</script> ，以及 <em>run</em> 和 <em>fly</em> 在 <script type="math/tex">\textbf{W}</script> 中的向量 為 <script type="math/tex">\textbf{W}_3,\textbf{W}_4</script> ，為例：</p>

<p><img src="/images/pic/pic_00195.png" alt="" /></p>

<p>由於 <script type="math/tex">\textbf{V}</script> 和 <script type="math/tex">\textbf{W}</script> 都是隨機初始化的，因此 <script type="math/tex">\textbf{V}_1, \textbf{V}_2, \textbf{W}_3, \textbf{W}_4 </script> 這些向量的方向都是隨機的，跟語意無關，如下圖所示：</p>

<p><img src="/images/pic/pic_00196.png" alt="" /></p>

<h2 id="training-word2vec">Training word2vec</h2>

<p>訓練 <em>word2vec</em> 的目的，是希望讓語意向量真的跟語意有關。，在上一篇 <a href="/blog/2016/07/10/nlp-vector-space-semantics"> Vector Space of Semantics </a> 中提到，某字的語意，可從其上下文有哪些字來判斷。因此，可以用某字上下文的字，來做訓練，讓語意向量能抓到文字的語意。</p>

<p>若 <em>dog</em> 的上下文中有 <em>run</em> ， 令 <em>dog</em> 為 <em>word2vec</em> 的 <em>input</em> ， <em>run</em> 為 <em>output</em> 則輸入類神經網路後，在 <em>run</em> 的位置，在經過 <em>sigmoid</em> 之前，得到的結果是 <script type="math/tex">\textbf{V}_1</script> 和 <script type="math/tex">\textbf{W}_3</script> 的內積。經過了，<em>sigmoid</em> ，得到的值為：</p>

<script type="math/tex; mode=display">

\frac{1}{1+e^{- \textbf{V}_1 \cdot \textbf{W}_3}}

</script>

<p>由於 <em>run</em> 出現在 <em>dog</em> 的上下文中，所以要訓練類神經網路，在 <em>run</em> 位置可以輸出 1，如下：</p>

<script type="math/tex; mode=display">

\frac{1}{1+e^{- \textbf{V}_1 \cdot \textbf{W}_3}} \approx 1

</script>

<p>過程如下圖所示：</p>

<p><img src="/images/pic/pic_00197.png" alt="" /></p>

<p>根據上圖，如果要讓 <em>run</em> 的位置輸出為 1 ，則 <script type="math/tex">\textbf{V}_1</script> 和 <script type="math/tex">\textbf{W}_3</script> 的內積要越大越好。</p>

<p>內積要大，就是向量角度要越小，訓練過程中，會修正這兩個向量的角度，如下圖：</p>

<p><img src="/images/pic/pic_00198.png" alt="" /></p>

<p>上圖左方為先正之前，各向量的方向，上圖右方為修正之後的方向，其中，深藍色為修正後的，淺藍色為修正前的，畫在一起以便作比較。修正完後， <script type="math/tex">\textbf{V}_1</script> 和 <script type="math/tex">\textbf{W}_3</script> 的角度又更接近了。</p>

<p>同理，若 <em>cat</em> 的上下文中有 <em>run</em> ，則用 <em>word2vec</em> 做同樣訓練，如下圖：</p>

<p><img src="/images/pic/pic_00199.png" alt="" /></p>

<p>修正向量的角度後，<script type="math/tex">\textbf{V}_2</script> 和 <script type="math/tex">\textbf{W}_3</script> 的角度會更接近，結果如下圖：</p>

<p><img src="/images/pic/pic_00200.png" alt="" /></p>

<p>不過，以上訓練方法有個問題，就是訓練完後， <em>所有的向量都會位於同一條直線上，而無法分辨出每個字語意的差異</em> 。如果要讓 <em>word2vec</em> 學會分辨語意的差異，就需要加入反例，也就是 <em>不是出現在上下文的字</em> 。</p>

<p>如果 <em>dog</em> 的上下文中，不會出現 <em>fly</em> ， <em>fly</em> 的向量為 <script type="math/tex">\textbf{W}_4</script> ，將 <em>dog</em> 輸入類神經網路後，在 <em>fly</em> 的位置，訓練其輸出結果為 0 ，如下：</p>

<script type="math/tex; mode=display">

\frac{1}{1+e^{- \textbf{V}_1 \cdot \textbf{W}_3}} \approx 0

</script>

<p>如下圖所示：</p>

<p><img src="/images/pic/pic_00201.png" alt="" /></p>

<p>如果要讓輸出結果接近 0，則 <script type="math/tex">\textbf{V}_1</script> 和 <script type="math/tex">\textbf{W}_4</script> 的內積要越小越好，也就是說，它們之間的角度要越大越好。修正這兩個向量的角度，如下圖：</p>

<p><img src="/images/pic/pic_00202.png" alt="" /></p>

<p>同理，若 <em>cat</em> 的上下文中沒有 <em>fly</em> ，則訓練其輸出 0 ：</p>

<p><img src="/images/pic/pic_00203.png" alt="" /></p>

<p>修正  <script type="math/tex">\textbf{V}_2</script> 和 <script type="math/tex">\textbf{W}_4</script> 的夾角，如下圖：</p>

<p><img src="/images/pic/pic_00204.png" alt="" /></p>

<p>訓練後，得出的這些語意向量，語意相近的，夾角越小，語意相差越遠的，夾角越大，如下圖：</p>

<p><img src="/images/pic/pic_00205.png" alt="" /></p>

<h2 id="further-reading">Further Reading</h2>

<ol>
  <li>
    <p><em>word2vec</em> 的 <em>backward propagation</em> 公式要怎麼推導，請看：<a href="/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation">word2vec (part2 : Backward Propagation)</a></p>
  </li>
  <li>
    <p>如何從無到有，不使用任何自動微分的套件，實作一個 <em>word2vec</em> ，請看：<a href="/blog/2016/08/29/neural-network-word2vec-part-3-implementation">word2vec (part3 : Implementation)</a></p>
  </li>
</ol>

<p>註： 實際上， <em>word2vec</em> 的 <em>input layer</em> 和 <em>output layer</em> 各有兩種架構。<em>input layer</em> 有 <em>cbow</em> 和 <em>skip-gram</em> 兩種，<em>output layer</em> 有 <em>hierarchical sofrmax</em> 和 <em>negative sampling</em> 兩種，本文所寫的為 <em>skip-gram</em> 搭配 <em>negative sampling</em> 的架構。關於 <em>hierarchical softmax</em> 請參考<a href="/blog/2015/05/23/-hierarchical-probabilistic-neural-networks-neural-network-language-model">hierarchical sofrmax</a> 。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vector Space of Semantics]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/07/10/nlp-vector-space-semantics/"/>
    <updated>2016-07-10T14:06:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/07/10/nlp-vector-space-semantics</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>如果要判斷某個字的語意，可以用它鄰近的字來判斷，例如以下句子：</p>

<blockquote>
  <p>The dog run.
A cat run.
A dog sleep.
The cat sleep.
A dog bark.
The cat meows.
The bird fly.
A bird sleep.</p>
</blockquote>

<p>由於 <em>dog</em> 和 <em>cat</em> 這兩個字出現在類似的上下文情境中，因此，可以判斷出 <em>dog</em> 和 <em>cat</em> 語意相近。</p>

<p>如果要能夠用數學運算，來計算語意相近程度，可以把文字的語意用向量表示，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{array}{c|c c c c c c c c c }

     &a &bark &bird &cat &dog &fly &meow & run & sleep & the \\ \hline

 dog &2 &1 &0 &0 &0 &0 &0 &1 &1 &1 \\

 cat &1 &0 &0 &0 &0 &0 &1 &1 &1 &2 \\

 bird &1 &0 &0 &0 &0 &1 &0 &0 &1 &1 

\end{array}

 %]]&gt;</script>

<p>其中， <em>dog</em> 的向量為  ( 2, 1, 0, 0, 0, 0, 0, 1, 1, 1 ) ，第一個維度表示 <em>dog</em> 在 <em>a</em> 旁邊的次數有 2 次，第二個維度表示在 <em>bark</em> 旁邊的次數有 1 次，以此類推。</p>

<!--more-->

<h2 id="vector-space-of-semantics">Vector Space of Semantics</h2>

<p>有了向量就可以用 <em>cosine similarity</em> 來計算語意相近程度。</p>

<p>給定兩向量 <script type="math/tex">A = (a_1,a_2,...,a_n)</script> 和<script type="math/tex">B = (b_1,b_2,...,b_n)</script> ，則這兩向量的 <em>cosine similarity</em> 為：</p>

<script type="math/tex; mode=display">

\frac{A \cdot B}{|A||B|}=  \frac{\sum a_i b_i}{\sqrt{\sum_i a_i^2}\sqrt{\sum_i b_i^2}}

</script>

<p><em>cosine similarity</em> 算出來的值，即是計算 <script type="math/tex">A</script> 和 <script type="math/tex">B</script> 兩向量的夾角的 <em>cosine</em> 值。 <em>cosine</em> 值越接近 1 ，表示兩向量夾角越小，表示兩向量的語意越接近。</p>

<p>根據以上例子， <em>dog</em> ( 2, 1, 0, 0, 0, 0, 0, 1, 1, 1 ) 和 <em>cat</em> ( 1, 0, 0, 0, 0, 0, 1, 1, 1, 2 )  的 <em>cosine similarity</em> 為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \frac{ 2 \times 1 + 1 \times 0 + 0 \times 0 + 0 \times 0 + 0 \times 0 + 0 \times 0 + 0 \times 1 + 1 \times 1 + 1 \times 1 + 1 \times 2 }{ \sqrt{ 2^2 + 1^2 + 0^2 + 0^2 + 0^2 + 0^2 + 0^2 + 1^2 + 1^2 + 1^2 } \sqrt{ 1^2 + 0^2 + 0^2 + 0^2 + 0^2 + 0^2 + 1^2 + 1^2 + 1^2 + 2^2} } \\

& = \frac{ 6 }{ \sqrt{ 8 } \sqrt{ 8} } 

= 0.75 

\end{align}

 %]]&gt;</script>

<p>而 <em>bird</em> 的向量為：( 1, 0, 0, 0, 0, 1, 0, 0, 1, 1 )  ，計算 <em>dog</em>  和  <em>bird</em> 的 <em>cosine similarity</em> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \frac{ 2 \times 1 + 1 \times 0 + 0 \times 0 + 0 \times 0 + 0 \times 0 + 0 \times 1 + 0 \times 0 + 1 \times 0 + 1 \times 1 + 1 \times 1 }{ \sqrt{ 2^2 + 1^2 + 0^2 + 0^2 + 0^2 + 0^2 + 0^2 + 1^2 + 1^2 + 1^2 } \sqrt{ 1^2 + 0^2 + 0^2 + 0^2 + 0^2 + 1^2 + 0^2 + 0^2 + 1^2 + 1^2} }  \\

& = \frac{ 4 }{ \sqrt{ 8 } \sqrt{ 4} } 

\approx 0.707106781187 

\end{align}

 %]]&gt;</script>

<p>由於 0.75 &gt; 0.707 ，因此 <em>dog</em> 和 <em>cat</em> 語意比較接近， <em>dog</em> 和 <em>bird</em> 語意比較不同。</p>

<p>此種語意向量有個缺點，就是向量的維度等於總字彙量。例如英文單字種共有好幾萬種，那麼，向量就有好幾萬個維度，向量維度過大的缺點，會造成資料過度稀疏，以及占記憶體的空間。</p>

<p>降低向量維度的方式，有兩種方法，分別是：</p>

<ol>
  <li>
    <p><a href="https://ccjou.wordpress.com/2009/09/01/%E5%A5%87%E7%95%B0%E5%80%BC%E5%88%86%E8%A7%A3-svd/">singular value decompisition (SVD)</a></p>
  </li>
  <li>
    <p><a href="http://blog.csdn.net/itplus/article/details/37969519">word2vec</a></p>
  </li>
</ol>

<p>本文先不詳細介紹這部分。</p>

<h2 id="implementation">Implementation</h2>

<p>在此實作將文字轉成向量，並用 SVD 降為至二維，作視覺化</p>

<p>```python
import numpy as np
import matplotlib.pyplot as plt</p>

<p>text = [
    [“the”, “dog”, “run”, ],
    [“a”, “cat”, “run”, ],
    [“a”, “dog”, “sleep”, ],
    [“the”, “cat”, “sleep”, ],
    [“a”, “dog”, “bark”, ],
    [“the”, “cat”, “meows”, ],
    [“the”, “bird”, “fly”, ],
    [“a”, “bird”, “sleep”, ],
]</p>

<p>def build_word_vector(text):
    word2id = {w: i for i, w in enumerate(sorted(list(set(reduce(lambda a, b: a + b, text)))))}
    id2word = {x[1]: x[0] for x in word2id.items()}
    wvectors = np.zeros((len(word2id), len(word2id)))
    for sentence in text:
        for word1, word2 in zip(sentence[:-1], sentence[1:]):
            id1, id2 = word2id[word1], word2id[word2]
            wvectors[id1, id2] += 1
            wvectors[id2, id1] += 1
    return wvectors, word2id, id2word</p>

<p>def cosine_sim(v1, v2):
    return np.dot(v1, v2) / (np.sqrt(np.sum(np.power(v1, 2))) * np.sqrt(np.sum(np.power(v1, 2))))</p>

<p>def visualize(wvectors, id2word):
    np.random.seed(10)
    fig = plt.figure()
    U, sigma, Vh = np.linalg.svd(wvectors)
    ax = fig.add_subplot(111)
    ax.axis([-1, 1, -1, 1])
    for i in id2word:
        ax.text(U[i, 0], U[i, 1], id2word[i], alpha=0.3, fontsize=20)
    plt.show()</p>

<p>```</p>

<p>本程式中， <code>text</code> 是輸入的文字， <code>build_word_vector</code> 將文字轉成向量：</p>

<p>```sh</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>wvectors, word2id, id2word = build_word_vector(text)</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>其中， <code>wvectors</code> 是根據前後文統計後，得出各文字的向量：</p>

<p>```sh</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>print wvectors
[[ 0.  0.  1.  1.  2.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]
 [ 1.  0.  0.  0.  0.  1.  0.  0.  1.  1.]
 [ 1.  0.  0.  0.  0.  0.  1.  1.  1.  2.]
 [ 2.  1.  0.  0.  0.  0.  0.  1.  1.  1.]
 [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  1.  1.  0.  0.  0.  0.  0.]
 [ 0.  0.  1.  1.  1.  0.  0.  0.  0.  0.]
 [ 0.  0.  1.  2.  1.  0.  0.  0.  0.  0.]]</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>每一橫排（或直排）代表著某個字的向量，但從它看不出是哪個字，所以 <code>word2id</code> 則是給定文字，找出是第幾個向量，而 <code>id2word</code> 則是給定第幾個向量，找出所對應的文字。</p>

<p>```sh</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>print word2id
{‘a’: 0, ‘fly’: 5, ‘run’: 7, ‘the’: 9, ‘dog’: 4, ‘cat’: 3, 
‘meows’: 6, ‘sleep’: 8, ‘bark’: 1, ‘bird’: 2}</p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <blockquote>
      <p>print id2word
{0: ‘a’, 1: ‘bark’, 2: ‘bird’, 3: ‘cat’, 4: ‘dog’, 5: ‘fly’, 
6: ‘meows’, 7: ‘run’, 8: ‘sleep’, 9: ‘the’}</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>例如 <em>dog</em> 是在 <code>wvectors</code> 中， 第 5 排的向量（註：index 從0開始算），用 <code>word2id</code> 可從 <code>wvector</code> 中，取出其對應向量：</p>

<p>```sh</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>print wvectors[word2id[“dog”]]
[ 2.  1.  0.  0.  0.  0.  0.  1.  1.  1.]</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>程式碼中的 <code>cosine_sim</code> ，則可計算兩向量間的 <em>cosine similarity</em> ，例如，</p>

<p>計算 <em>dog</em> 和 <em>cat</em> 與 <em>dog</em> 和 <em>bird</em> 之間的  <em>cosine similarity</em>  ：</p>

<p>```sh</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>print cosine_sim(wvectors[word2id[“dog”]], wvectors[word2id[“cat”]])
0.75</p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <blockquote>
      <p>print cosine_sim(wvectors[word2id[“dog”]], wvectors[word2id[“bird”]])
0.707106781187</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>再來是用 <code>visualize</code> 將這些語意向量在平面上作視覺化：</p>

<p>```sh</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>visualize(wvectors, id2word)</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>在平面上作視覺化的方法，是用 SVD 將語意向量降為至二維，就可以把這些向量所對應的字，畫在平面上，結果如下：</p>

<p><img src="/images/pic/pic_00186.png" alt="" /></p>

<p>此結果顯示，   <em>dog</em> 、 <em>cat</em> 和 <em>bird</em> 是語意相近的一群， <em>a</em> 和 <em>the</em> 語意相近，以此類推。</p>

<h2 id="further-reading">Further Reading</h2>

<p>Simple Word Vector representations</p>

<p>http://cs224d.stanford.edu/lectures/CS224d-Lecture2.pdf</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hierarchical Probabilistic Neural Network Language Model (Hierarchical Softmax)]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/05/23/-hierarchical-probabilistic-neural-networks-neural-network-language-model/"/>
    <updated>2015-05-23T15:33:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/05/23/-hierarchical-probabilistic-neural-networks-neural-network-language-model</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>將類神經網路應用在自然語言處理領域的模型有<a href="/blog/2015/05/15/neural-network-neural-probabilistic-language-model">Neural Probabilistic Language Model(NPLM)</a>，但在實際應用時，運算瓶頸在於 <em>output layer</em> 的神經元個數，等同於總字彙量 <script type="math/tex">\mid V\mid </script> 。</p>

<p>每訓練一個字時，要讓 <em>output layer</em> 在那個字所對應的神經元輸出值為 <script type="math/tex">1</script> ，而其他 <script type="math/tex">\mid V\mid -1</script> 個神經元的輸出為 <script type="math/tex">0</script> ， 這樣總共要計算 <script type="math/tex">\mid V\mid </script> 次，會使得訓練變得沒效率。</p>

<p>若要減少於 <em>output layer</em> 的訓練時間，可以把 <em>output layer</em> 的字作分類階層，先判別輸出的字是屬於哪類，再判斷其子類別，最後再判斷是哪個字。 </p>

<h2 id="hierarchical-softmax">Hierarchical Softmax</h2>

<p>給定訓練資料為<script type="math/tex">X</script> ，輸出字的集合為 <script type="math/tex">Y</script> 。當輸入的字串為 <script type="math/tex">x</script> ，輸出的字為 <script type="math/tex">y</script> 時，訓練的演算法要將機率 <script type="math/tex">P (Y=y \mid  X=x) </script> 最佳化。</p>

<p>如果 <script type="math/tex">Y</script> 有 <em>10000</em> 種字，若沒有分類階層，訓練時就要直接對 <script type="math/tex">P (Y = y \mid  X = x) </script> 做計算，即是對這 <em>10000</em>種字做計算，使 <script type="math/tex">y</script> 所對應的神經元輸出為 <em>1</em> ，其它 <em>9999</em> 個神經元輸出 <em>0</em> ，這樣要計算 <em>10000</em> 次，如下圖：</p>

<p><img src="/images/pic/pic_00079.png" alt="" /></p>

<p>若在訓練前，就事先把 <script type="math/tex">Y</script> 中的字彙分類好，以 <script type="math/tex">C(y)</script> 代表字 <script type="math/tex">y</script> 的類別，則可以改成用以下機率做最佳化：</p>

<!--more-->

<script type="math/tex; mode=display">

P (Y = y | X = x) =

P (Y = y | C = c(y), X) \times P (C = c(y) | X = x)


</script>

<p>根據以上公式，先判斷 <script type="math/tex">y</script> 是在哪類，要算 <script type="math/tex">P (C = c(y) \mid  X = x)</script> 的值，再判斷 <script type="math/tex">y</script> 是 <script type="math/tex">c(y)</script> 中的哪個字，要算 <script type="math/tex">P (Y = y \mid  C = c(y), X) </script> 的值。</p>

<p>若把它們分成 <em>100</em> 類，則每個類別有 <em>100</em> 種字。訓練時需要分成兩步來計算，第一步先計算 <script type="math/tex">P (C = c(y) \mid  X = x)</script> ，此時只要對 <em>100</em> 種分類做計算，使 <script type="math/tex">C(y)</script> 所對應的神經元輸出 <em>1</em> ，其它 <em>99</em> 個神經元輸出 <em>0</em> 。第二步是算 <script type="math/tex">P (Y = y \mid  C = c(y), X)</script>，即是讓 <script type="math/tex">C(y)</script> 底下對應到 <script type="math/tex">y</script> 的神經元輸出 <em>1</em> ，其它<em>99</em> 個神經元輸出 <em>0</em> ，這樣只需要計算 <em>200</em> 次即可，所做的計算只有原本的 <script type="math/tex">\frac{1}{50}</script> ，如下圖：</p>

<p><img src="/images/pic/pic_00080.png" alt="" /></p>

<p>更進一步地，可以將 <em>output layer</em> 分成更多層，也就是說，將它用成樹狀結構，每一個節點代表一次分類，這樣一層層分下去，直到葉子節點，可分出是哪個字彙。分越多層，訓練時要計算的次數就會越小，分到最細的情況下，就變成二元分類樹，這樣所需的計算總量只有原本的 <script type="math/tex">\frac{log_{2}\mid V\mid }{\mid V\mid }</script> 倍。</p>

<p>根據這個二元分類樹，可以把 <script type="math/tex">Y</script> 中的每一個字，對應到一個由 <em>0</em> 和 <em>1</em> 所組成的二元字串 <script type="math/tex">(b_{1}(y), . . . b_{m}(y))</script> ，若字串中第一個數字為 <em>0</em> ，則表示這個字在第一層神經元所計算出的值為 <em>0</em>，以此類推。若要用這個二元分類樹計算 <script type="math/tex">P (Y = y \mid  X = x) </script> 可用以下公式來計算：</p>

<script type="math/tex; mode=display">

P( Y = y | X = x) =

\prod_{j=1}^{m}P(b_{j}(y) | b_{j-1}(y),b_{j-2}(y),...,b_{1}(y), X=x)

</script>

<p>其中， <script type="math/tex">（b_{j−1}(y),b_{j−2}(y),...,b_{1}(y))</script> 為長度小於 <script type="math/tex">m</script> 的二元字串，即是在二元分類樹中不是葉子也不是根的節點。</p>

<p>例如字串 <script type="math/tex">( b_{1}=0, b_{2}=1 )</script>即表示先從根節點往 <em>0</em> 的分支走到子節點，再從這個子節點走到 <em>1</em> 的分支的子節點，此子節點可用字串 <em>01</em> 來表示。若要計算以上公式的條件機率，方法為，先給這些子節點有它們自己的語意向量，再用 <em>NPLM</em> 把子節點的語意向量和 <script type="math/tex">x</script> 當成輸入值一起輸入 <em>NPLM</em> 來計算。</p>

<p>例如，假設詞庫裡總共有 <em>8</em> 個字， 則 <script type="math/tex">m=3</script> ，若字彙 <script type="math/tex">y</script> 的二元字串值 <script type="math/tex">b_{1} = 1, b_{2} =0, b_{3}=0 </script> ，則它在二元分類樹的位置如下圖所示：</p>

<p><img src="/images/pic/pic_00081.png" alt="" /></p>

<p>則 <script type="math/tex">P( Y = y \mid  X = x) </script> 為：</p>

<script type="math/tex; mode=display">

P(b_{1} = 1 | X=x) \times

P(b_{2} = 0 | b_{1} = 1, X=x) \times 

P(b_{3} = 0| b_{2} = 0, b_{1} = 1, X=x)

</script>

<p>訓練時，將訓練資料 <script type="math/tex">x</script> 輸入到 <em>NPLM</em> 的類神經網路中，並以二元分類樹的節點來代替它的 <em>output layer</em> 。</p>

<p>首先，先從根節點的神經元開始訓練，這一步是要算 <script type="math/tex">P(b_{1} = 1 \mid  , X=x) </script> 。由於 <script type="math/tex">b_{1} = 1 </script>，所以要以根節點的神經元輸出為 <em>1</em> 來調整 <em>NPLM</em> 模型的參數，如下圖：</p>

<p><img src="/images/pic/pic_00082.png" alt="" /></p>

<p>再來，從根節點往左走，選 <em>1</em> 分支的子神經元來訓練，這一步是要算 <script type="math/tex">P(b_{2} = 0 \mid  b_{1} = 1, X=x) </script> 。因為這是基於 <script type="math/tex">b_{1} = 1</script> 的條件機率，所以要用字串 <em>1</em> 所對應它的語意向量和 <script type="math/tex">x</script> 一起輸入到 <em>NPLM</em> 。另外，由於 <script type="math/tex">b_{2} = 0 </script>，要以輸出為 <em>0</em> 來調整 <em>NPLM</em> 模型的參數，如下圖：</p>

<p><img src="/images/pic/pic_00083.png" alt="" /></p>

<p>然後，從剛剛那個神經元的節點開始，選 <em>0</em> 分支的子神經元來訓練，這一步是要算 <script type="math/tex">P(b_{3} = 0\mid  b_{2} = 0, b_{1} = 1, X=x) </script> 。因為這是基於 <script type="math/tex">b_{1} = 1 , b_{2} = 0 </script> 的條件機率，所以要先將字串 <em>10</em> 所對應的語意向量和 <script type="math/tex">x</script> 一起輸入到 <em>NPLM</em> 。另外，由於 <script type="math/tex">b_{3} = 0 </script>，要以輸出為 <em>0</em> 來調整 <em>NPLM</em> 模型的參數，如下圖：</p>

<p><img src="/images/pic/pic_00084.png" alt="" /></p>

<p>最後，選擇 <em>0</em> 的分支往下走，即可走到 <script type="math/tex">y</script> ，結束了針對 <script type="math/tex">x</script> 這筆資料的訓練過程。</p>

<p><img src="/images/pic/pic_00085.png" alt="" /></p>

<h2 id="hierarchical-clustering">Hierarchical Clustering</h2>

<p>在訓練之前，要先把字彙的階層分類給建立好。要建立這個分類階層，有很多種方式，例如，使用 <em>WordNet</em> 所建立的上下位關係，來建立階層，或者可以用 <em>Hierarchical Clustering</em> ，像是<a href="/blog/2014/10/25/natural-language-processing-brown-clustering">Brown Clustering</a>之類的，自動從訓練資料中建立分類階層。</p>

<h2 id="conclusion">Conclusion</h2>

<p>用 <em>Hierarchical Softmax</em> 來置換掉原本 <em>NPLM</em> 的 <em>output layer</em> ，可以使得原本要計算 <script type="math/tex">\mid V\mid </script> 次的訓練，縮減為 <script type="math/tex">log_{2}\mid V\mid </script> 次，大幅提升了訓練速度。因此， <em>Hierarchical Softmax</em> 被日後眾多種類神經網路相關的模型所採用，包括近年來很熱門的 <em>word2vec</em> 也是。</p>

<h2 id="reference">Reference</h2>

<p>Frederic Morin , Yoshua Bengio. Hierarchical probabilistic neural network language model. 2005</p>
]]></content>
  </entry>
  
</feed>
