<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Information Theory | Mark Chang's Blog]]></title>
  <link href="http://ckmarkoh.github.io/blog/categories/information-theory/atom.xml" rel="self"/>
  <link href="http://ckmarkoh.github.io/"/>
  <updated>2016-12-26T00:02:20+08:00</updated>
  <id>http://ckmarkoh.github.io/</id>
  <author>
    <name><![CDATA[Mark Chang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Pointwise Mutual Information]]></title>
    <link href="http://ckmarkoh.github.io/blog/2014/04/25/natural-language-processing-pointwise-mutual-information/"/>
    <updated>2014-04-25T17:58:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2014/04/25/natural-language-processing-pointwise-mutual-information</id>
    <content type="html"><![CDATA[<h2 id="introduction">1.Introduction</h2>

<p>在自然語言處理中, 想要探討兩個字之間, 是否存在某種關係, 例如某些字比較容易一起出現, 這些字一起出現時, 可能帶有某種訊息</p>

<p>例如, 在新聞報導中, 有 <em>New</em> , <em>York</em>  , 這兩個字一起出現, 可以代表一個地名 <em>New York</em>  , 所以當出現了 <em>New</em> 這個字, 則有可能出現 <em>York</em> </p>

<p>這可以用 <em>Pointwise Mutual Information(PMI)</em> 計算</p>

<p><em>Pointwise Mutual Information</em> 的公式如下：</p>

<script type="math/tex; mode=display">

pmi(x,y)=log \frac{P(x,y)}{P(x) \times P(y)}

</script>

<!--more-->

<p>其中, <script type="math/tex">P(x,y)</script> 代表文字 <em>x</em> 和文字 <em>y</em> 一起出現的機率, 而 <script type="math/tex">P(x)</script> 為文字 <em>x</em> 出現的機率 , <script type="math/tex">P(y)</script> 為文字 <em>y</em> 出現的機率</p>

<p>如果某兩個字的出現是獨立事件, 則 <em>PMI</em> 為 <em>0</em></p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& P(x,y) = P(x) \times P(y)\\

& pmi(x,y)=log \frac{P(x,y)}{P(x) \times P(y)} = log \frac{P(x) \times P(y)}{P(x) \times P(y)} =log1=0

\end{align}

 %]]&gt;</script>

<p>若有兩個字出現的機率不是獨立事件, 某個字出現時提昇另一個字的出現的機率, 則 <em>PMI</em> 大於 <em>0</em></p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& P(x,y) > P(x) \times P(y)\\

& pmi(x,y)=log \frac{P(x,y)}{P(x) \times P(y)} > 0

\end{align}

 %]]&gt;</script>

<p>也就是說, 如果這兩個字的出現越不是偶然, 則 <em>Pointwise Mutual Information</em> 算出來的值越高, 越有可能帶有某種訊息</p>

<p>舉個例子, 以下為一個表格, 統計一篇文章中, 某些字是否一起出現</p>

<p>表格中的數字, 代表左方的字和上方的字, 一起出現的次數</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


    \begin{array}{|c|c|}

    \hline            &  \text{computer}  &  \text{data}  & \text{pinch}  & \text{result}  & \text{sugar} \\

\hline \text{aprocot}    &  0        &    0    &   1    &     0   &     1 \\

\hline \text{pineapple}  &  0        &    0    &   1    &     0   &     1 \\

\hline \text{digital}    &  2        &    1    &   0    &     1   &     0 \\

\hline \text{information} &  1       &    6    &   0    &     4   &     0 \\

\hline

	\end{array}

 %]]&gt;</script>

<p>如果這篇文章總共只有 <em>19</em> 個字, 來計算 <em>information</em> 和 <em>data</em> 這兩字的 <em>PMI</em> , 得出</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& P(x = information,y=data) 

\mspace{5mu}=\mspace{5mu} \frac{6}{19} 

\mspace{5mu}=\mspace{5mu} 0.32 \\

& P(x = information ) 

\mspace{5mu}=\mspace{5mu} \frac{6+4+1}{19} 

\mspace{5mu}=\mspace{5mu} \frac{11}{19} 

\mspace{5mu}=\mspace{5mu} 0.58 \\

& P(y = data ) 

\mspace{5mu}=\mspace{5mu} \frac{6+1}{19} 

\mspace{5mu}=\mspace{5mu} \frac{7}{19}  

\mspace{5mu}=\mspace{5mu} 0.37 \\[10pt]

& pmi(x = information,y = data)  \\

& \mspace{5mu}=\mspace{5mu} log\frac{P(x =information,y=data)}{P(x=information)\times P(y=data)} \\

&\mspace{5mu}=\mspace{5mu} log1.49 \\

&\mspace{5mu}=\mspace{5mu} 0.57  \\

\end{align}

 %]]&gt;</script>

<p>算出來的數字大於 0 , 表示 <em>information</em> 和 <em>data</em> 這兩個字的出現, 不是獨立事件</p>

<h2 id="implementation">2.Implementation</h2>

<p>我們用 <code>python nltk</code> 的 <em>brown corpus</em> 新聞類別文章, 來計算 <em>New</em> , <em>York</em> 的 <em>PMI</em> 和 <em>New</em> , <em>The</em> 的 <em>PMI</em> , 並比較兩者差異</p>

<p>將以下程式碼複製到 <em>pmi.py</em> 這個檔案</p>

<p>```python pmi.py</p>

<p>import nltk
from nltk.corpus import brown
from nltk import WordNetLemmatizer
from math import log 
wnl=WordNetLemmatizer()</p>

<p>_Fdist = nltk.FreqDist([wnl.lemmatize(w.lower()) for w in brown.words(categories=’news’)])</p>

<p>_Sents = [[wnl.lemmatize(j.lower()) for j in i] for i in brown.sents(categories=’news’)]</p>

<p>def p(x):
       return _Fdist[x]/float(len(_Fdist))</p>

<p>def pxy(x,y):
       return (len(filter(lambda s :  x in s and y in s ,_Sents))+1)/ float(len(_Sents) )</p>

<p>def pmi(x,y):
       return  log(pxy(x,y)/(p(x)*p(y)),2) </p>

<p>```</p>

<p>其中 <code>_Fdist </code> 是單字出現的頻率 , <code>_Sents</code> 是文章中所有的句子 , <code>p(x)</code> 計算單字 <em>x</em> 出現的機率,  ` pxy(x,y)<code> 計算單字 *x* 和單字 *y* 出現在同一個句子的機率, </code>pmi(x,y)` 計算單字 <em>x</em> 和單字 <em>y</em> 的 <em>Pointwise Mutual Information</em> </p>

<p>到 <em>python</em> 的 <em>interactive mode</em> 載入這個檔案</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>from pmi import *</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>先來計算 <em>new</em> 和 <em>york</em> 各別出現的機率</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>p(‘new’)
0.020265724857046755</p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <blockquote>
      <p>p(‘york’)
0.004372687521022536</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>計算 <em>new</em> , <em>york</em> 出現在同一句子的機率</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>pxy(‘new’,’york’)
0.011031797534068787</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>計算 <em>new</em> , <em>york</em> 的 <em>PMI</em></p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>pmi(‘new’,’york’)
6.959890136179789</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>再來看看, <em>new</em> , <em>the</em> 出現在同一句子的機率</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>pxy(‘new’,’the’)
0.04023361453601557</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>比 <em>new</em> , <em>york</em> 出現在同一句子的機率還高, 但是因為 <em>the</em> 出現次數較多</p>

<p>算一下 <em>the</em> 出現的機率</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>p(‘the’)
0.5369996636394214</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>計算<em>new</em> , <em>the</em> 的 <em>PMI</em> , 還是沒有比 <em>new</em> , <em>york</em> 高</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>pmi(‘new’,’the’)
1.8863664858873235</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>以上結果顯示, <em>PMI</em> 可以得出, <em>new</em> 和 <em>york</em> 一起出現的情形, 比較不是偶然的事件, 表示這兩個詞一起出現可能帶有較多訊息</p>

<p>雖然 <em>new</em> 和 <em>the</em> 出現的頻率比較多, 但同時 <em>the</em> 出現的頻率也比較多, 計算結果 <em>PMI</em> 可以把 <em>the</em> 的出現頻率除掉, 得出 <em>new</em> 和 <em>the</em> 的出現, 比較像是獨立事件</p>

<h2 id="reference">3.Reference</h2>

<p>本文參考至coursera線上課程</p>

<h4 id="d-jurafsky-and-c-manning--natural-language-processing">D. Jurafsky, and C. Manning.  Natural Language Processing</h4>

<p>https://www.coursera.org/course/nlp</p>
]]></content>
  </entry>
  
</feed>
