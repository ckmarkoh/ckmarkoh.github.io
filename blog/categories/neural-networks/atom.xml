<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Neural Networks | Mark Chang's Blog]]></title>
  <link href="http://ckmarkoh.github.io/blog/categories/neural-networks/atom.xml" rel="self"/>
  <link href="http://ckmarkoh.github.io/"/>
  <updated>2017-01-01T14:22:44+08:00</updated>
  <id>http://ckmarkoh.github.io/</id>
  <author>
    <name><![CDATA[Mark Chang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Torch NN Tutorial 4: Backward Propagation ( Part 1 : Overview & Linear Regression)]]></title>
    <link href="http://ckmarkoh.github.io/blog/2017/01/01/torch-nn-tutorial-4-backward-propagation/"/>
    <updated>2017-01-01T14:00:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2017/01/01/torch-nn-tutorial-4-backward-propagation</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>本文以 Linear Regression 為例，介紹 Torch nn 如何進行 Backward Propagation。</p>

<p>Linear Regression 是以機器學習的方式，學出以下函數：</p>

<script type="math/tex; mode=display">

y = wx+b

</script>

<p>以 Gradient Descent 的方式來進行 Linear regression 的流程如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\text{1.} \mspace{20mu} \text{initialize } w \text{ and } b \\
&\text{2.} \mspace{20mu} \text{for i=0 ; i < max_epoch; i++} \\
&\text{3.} \mspace{40mu} \bar{y} = wx+b \\
&\text{4.} \mspace{40mu} J = (\bar{y} - y )^2 \\
&\text{5.} \mspace{40mu} \Delta w = 0 \\
&   \mspace{60mu} \Delta b = 0 \\
&\text{6.} \mspace{40mu} \frac{\partial J}{\partial y} = 2(\bar{y} - y) \\
&\text{7.} \mspace{40mu} \Delta w =  \frac{\partial J}{\partial w} =  \frac{\partial J}{\partial y}  \frac{\partial y}{\partial w} =  \frac{\partial J}{\partial y} \times x  \\ 
&   \mspace{60mu} \Delta b =  \frac{\partial J}{\partial b} =  \frac{\partial J}{\partial y}  \frac{\partial y}{\partial b}  = \frac{\partial J}{\partial y} \times 1 \\
&\text{8.} \mspace{40mu} w \leftarrow w  - \eta \Delta w \\ 
&   \mspace{60mu} b \leftarrow b - \eta \Delta b \\
\end{align}
 %]]&gt;</script>

<p>其中 <script type="math/tex">x</script> 為 training data， <script type="math/tex">y</script> 為 golden value ， <script type="math/tex">\bar{y}</script> 為 predicted value， <script type="math/tex">w</script> 和 <script type="math/tex">b</script> 分別為 weight 和 bias 。 max_epoch 為 for 迴圈執行次數。</p>

<p>以下詳細講解整個流程，並實作之。</p>

<!--more-->

<h2 id="linear-regression-by-gradient-descent">Linear Regression by Gradient Descent</h2>

<p>首先，載入 <code>nn</code> 套件，並產生 Training Data <script type="math/tex">x</script> 及 <script type="math/tex">y</script> ：</p>

<p>``` lua</p>

<p>require ‘nn’
x = torch.linspace(1,3,3):resize(3,1)
y = x*2+1
print(x)
print(y)</p>

<p>```</p>

<p>以上程式，假設 <script type="math/tex">y</script> 是由 <script type="math/tex">y = 2 x + 1 </script> 產生出來的。而訓練的目標，是要讓 <script type="math/tex">w=2, b=1</script> 。</p>

<p>產生出來的 <code>x</code> 為 <code>[1,2,3]</code> ， <code>y</code> 為 <code>[3,5,7]</code> 如下：</p>

<p>``` sh</p>

<p>1
 2
 3
[torch.DoubleTensor of size 3x1]</p>

<p>3
 5
 7
[torch.DoubleTensor of size 3x1]</p>

<p>```</p>

<p>為了方便以 batch 計算，將 <code>x</code> 及 <code>y</code> 調整成 3x1 的大小。</p>

<p>建立完 training data 之後，可以開始進行 Linear Regression。</p>

<p>第一步，將 <script type="math/tex">w</script> 和 <script type="math/tex">b</script> 以隨機值初始化。</p>

<script type="math/tex; mode=display"> \text{1.}  \mspace{20mu} \text{initialize } w \text{ and } b </script>

<p>建立 <code>nn.Linear</code> ，命名為 <code>l1</code> ，如下： </p>

<p>``` lua</p>

<p>l1 = nn.Linear(1,1)
print(l1.weight)
print(l1.bias)</p>

<p>```</p>

<p>一開始， <code>weight</code> 和 <code>bias</code> 會以隨機值初始化，結果如下：</p>

<p>``` sh</p>

<p>0.2055
[torch.DoubleTensor of size 1x1]</p>

<p>0.7159
[torch.DoubleTensor of size 1]</p>

<p>```</p>

<p>再來是建立 loss function，命名為 <code>c1</code> ：</p>

<p>``` sh</p>

<p>c1 = nn.MSECriterion()</p>

<p>```</p>

<p>由於 loss function 為 Mimimum Square Error，所以 criterion 採用 <code>nn.MSECriterion</code></p>

<p>再來，這裡先講解 for 迴圈中進行的運算。</p>

<p>第三步算 linear 的 forward propagation ：</p>

<script type="math/tex; mode=display">\text{3.} \mspace{20mu} \bar{y} = wx+b </script>

<p>將 <script type="math/tex">x,w,b</script> 的數值帶入以上公式，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \bar{y} = wx+b\\
& =
0.2055
\begin{bmatrix}
1 \\
2 \\
3 \\
\end{bmatrix}
+ 0.7159 \\
& =
\begin{bmatrix}
 0.2055 \times 1 + 0.7159 \\
 0.2055 \times 2 + 0.7159 \\
 0.2055 \times 3 + 0.7159 \\
\end{bmatrix} \\
& =
\begin{bmatrix}
 0.9214 \\
 1.1269 \\
 1.3325 \\
\end{bmatrix}
\end{align}
 %]]&gt;</script>

<p>程式碼如下：</p>

<p>``` lua</p>

<p>y_ = l1:forward(x)
print(y_)</p>

<p>```</p>

<p>結果如下：</p>

<p>``` sh</p>

<p>0.9214
 1.1269
 1.3325
[torch.DoubleTensor of size 3x1]</p>

<p>```</p>

<p>第四步，計算 coss function 的 forward propagation ：</p>

<script type="math/tex; mode=display">\text{4.} \mspace{20mu} J = (\bar{y} - y )^2 </script>

<p>將 <script type="math/tex">y,\bar{y}</script> 的數值帶入以上公式，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& J = (\bar{y} - y)^2\\
& =
\begin{bmatrix}
(0.9214 - 3)^2 \\
(1.1269 - 5)^2 \\
(1.3325 - 7)^2 \\
\end{bmatrix} \\
& =
\begin{bmatrix}
 4.3206 \\
 15.0009 \\
 32.1206 \\
\end{bmatrix}
\end{align}
 %]]&gt;</script>

<p>如果輸入的值有多個維度，則 loss function 會將個維度的值，加起來平均，結果如下：</p>

<script type="math/tex; mode=display">

J = \frac{4.3206 + 15.0009 + 32.1206}{3} = 17.147313377985 

</script>

<p>計算 <script type="math/tex">J</script> 值的程式碼如下：</p>

<p>``` lua</p>

<p>j = c1:forward(y_,y)
print(j)</p>

<p>```</p>

<p>loss 值 <code>j</code> 如下：</p>

<p>``` sh</p>

<p>17.147313377985 </p>

<p>```</p>

<p>再來，要進行 backward propagation。</p>

<p>第五步，先將 <script type="math/tex"> \Delta w </script> 和 <script type="math/tex"> \Delta b </script> 歸零：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\text{5.} \mspace{40mu} \Delta w = 0 \\
&   \mspace{60mu} \Delta b = 0 \\
\end{align}
 %]]&gt;</script>

<p><script type="math/tex"> \Delta w </script> 和 <script type="math/tex"> \Delta b </script> 在程式中所對應的值，是 <code>l1</code> 的 <code>gradWeight</code> 和 <code>gradBias</code> 。在還沒歸零之前，這兩變數可能是任意值，印出這兩數的值，程式如下：</p>

<p>``` lua</p>

<p>print(l1.gradWeight)
print(l1.gradBias)</p>

<p>```</p>

<p>結果如下：</p>

<p>``` sh</p>

<p>1e-154 *
 -1.4917
[torch.DoubleTensor of size 1x1]</p>

<p>1e-154 *
-1.4917
[torch.DoubleTensor of size 1]</p>

<p>```</p>

<p>可用函式 <code>zeroGradParameters</code> 將這兩個值歸零，程式如下：</p>

<p>``` lua</p>

<p>l1:zeroGradParameters()
print(l1.gradWeight)
print(l1.gradBias)</p>

<p>```</p>

<p>結果為0，表示已歸零：</p>

<p>``` sh</p>

<p>0
[torch.DoubleTensor of size 1x1]</p>

<p>0
[torch.DoubleTensor of size 1]</p>

<p>```</p>

<p>第六步，計算 <script type="math/tex">\frac{\partial J}{\partial y}</script> 的值。</p>

<script type="math/tex; mode=display"> \text{6.} \mspace{40mu} \frac{\partial J}{\partial y} = 2(\bar{y} - y) </script>

<p>將 <script type="math/tex">y,\bar{y}</script> 的數值帶入以上公式，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \frac{\partial J}{\partial y} = 2(\bar{y} - y)\\
& =
\begin{bmatrix}
2(0.9214 - 3) \\
2(1.1269 - 5) \\
2(1.3325 - 7) \\
\end{bmatrix} \\
& =
\begin{bmatrix}
-1.3857 \\
-2.5820 \\
-3.7784 \\
\end{bmatrix}
\end{align}
 %]]&gt;</script>

<p>由於 <script type="math/tex">\bar{y}</script> 有三筆資料，每筆資料都會各算出一個微分結果。
程式中，計算此值的方式即是呼叫 <code>c1</code> 的 <code>backward</code> ，如下：</p>

<p>``` lua</p>

<p>dj_dy_ =c1:backward(y<em>,y)
print(dj_dy</em>)</p>

<p>```</p>

<p>得出的結果即是 <script type="math/tex">\frac{\partial J}{\partial \bar{y} } </script> ，結果如下：</p>

<p>``` sh </p>

<p>-1.3857
-2.5820
-3.7784
[torch.DoubleTensor of size 3x1]</p>

<p>```</p>

<p>第七步，計算 <script type="math/tex"> \Delta w </script> 和 <script type="math/tex"> \Delta b </script> 的值：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\text{7.} \mspace{40mu} \Delta w =  \frac{\partial J}{\partial w} =  \frac{\partial J}{\partial y}  \frac{\partial y}{\partial w} =  \frac{\partial J}{\partial y} \times x  \\ 
&   \mspace{60mu} \Delta b =  \frac{\partial J}{\partial b} =  \frac{\partial J}{\partial y}  \frac{\partial y}{\partial b}  = \frac{\partial J}{\partial y} \times 1 \\
\end{align}
 %]]&gt;</script>

<p>先看 <script type="math/tex">\Delta w </script> 的數值，將 <script type="math/tex">x</script> 的值，以及先前算出的 <script type="math/tex"> \frac{\partial J}{\partial y}</script> 值代入，即可算出它：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \Delta w =  \frac{\partial J}{\partial y} \times x \\
&=
\begin{bmatrix}
-1.3857 \times 1\\
-2.5820 \times 2\\
-3.7784 \times 3\\
\end{bmatrix}  \\
&=
\begin{bmatrix}
-1.3857 \\
-5.1640 \\
-11.3352 \\
\end{bmatrix}
\end{align}
 %]]&gt;</script>

<p>如果輸值為有多筆資料，則 <script type="math/tex">\Delta w</script> 的最終結果會將每筆的結果累加起來，如下：</p>

<script type="math/tex; mode=display">
\Delta w = -1.3857 + -5.1640 + -11.3352 = -17.8849
</script>

<p>而 <script type="math/tex">\Delta b</script> 的值是把 <script type="math/tex"> \frac{\partial J}{\partial y}</script> 中的每筆資料結果累積起來。</p>

<script type="math/tex; mode=display">
\Delta b = -1.3857 + -2.5820 + -3.7784 = -7.7461
</script>

<p>以程式來計算此兩值。呼叫 <code>l1</code> 的 <code>backward</code> ，輸入 <script type="math/tex">\frac{\partial J}{\partial \bar{y} }</script> 到 <code>backward</code> 後，它與  <script type="math/tex">\frac{\partial \bar{y}}{\partial w}</script> 和 <script type="math/tex">\frac{\partial \bar{y}}{\partial b}</script> 相乘後結果分別為 <script type="math/tex">\Delta w</script> 和 <script type="math/tex">\Delta b</script> ，此兩數分別儲存於 <code>gradWeight</code> 和 <code>gradBias</code> 。</p>

<p>``` lua</p>

<p>l1:backward(x, dj_dy_ )
print(l1.gradWeight)
print(l1.gradBias)</p>

<p>```</p>

<p>結果如下：</p>

<p>``` sh</p>

<p>-17.8849
[torch.DoubleTensor of size 1x1]</p>

<p>-7.7461
[torch.DoubleTensor of size 1]</p>

<p>```</p>

<p>第八步，更新 weight 和 bias ，公式如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}
&\text{8.} \mspace{40mu} w \leftarrow w  - \eta \Delta w \\ 
&   \mspace{60mu} b \leftarrow b - \eta \Delta b \\
\end{align}

 %]]&gt;</script>

<p>其中， <script type="math/tex">eta</script>  是指 learning rate 。令 <script type="math/tex">eta=0.2</script> ，更新 <script type="math/tex">w</script> 和 <script type="math/tex">b</script> ，數值如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& w  - \eta \Delta w 
= 0.2055 - 0.02 \times (- 17.8849) = 0.5632 \\
& b - \eta \Delta b 
= 0.7159 - 0.02 \times (-7.7461) = 0.8708 \\
\end{align}
 %]]&gt;</script>

<p>程式中，更新 <code>l1</code> 的 <code>weight</code> 和 <code>bias</code> 可以用 <code>updateParameters</code> ，它的輸入即是 <script type="math/tex">eta</script> 。令 <script type="math/tex">eta=0.2</script> ，程式如下：</p>

<p>``` lua</p>

<p>l1:updateParameters(0.02)
print(l1.weight)
print(l1.bias)</p>

<p>```</p>

<p>更新完後印出 <code>weight</code> 和 <code>bias</code> 的值，結果如下：</p>

<p>``` sh</p>

<p>0.5632
[torch.DoubleTensor of size 1x1]</p>

<p>0.8708
[torch.DoubleTensor of size 1]</p>

<p>```</p>

<p>把第三到第八步的 for 迴圈，串連起來。此 for 迴圈連續跑 500 次，每 50 次印出一次結果，程式碼如下：</p>

<p>``` lua</p>

<p>for i = 1,500 do
    y_ = l1:forward(x)
    j = c1:forward(y<em>,y)
    l1:zeroGradParameters()
    dj_dy</em> =c1:backward(y<em>,y) 
    l1:backward(x,dj_dy</em>)
    l1:updateParameters(0.02)
    if i%50 == 0 then
        print(“i:” .. tostring(i)
           .. “ loss:” .. tostring(j) 
           .. “ weight:” .. tostring(l1.weight[1][1])
           .. “ bias:” .. tostring(l1.bias[1]) .. “\n”)
    end
end</p>

<p>```</p>

<p>執行此程式，在訓練完 500 次以後，<code>loss</code> 會接近 0 ，而 <code>weight</code> 和 <code>bias</code> 會接近 2 和 1 了。結果如下：</p>

<p>``` sh</p>

<p>i:50 loss:0.015879173659737 weight:1.8543434015475 bias:1.3310995564975</p>

<p>i:100 loss:0.0098066687579948 weight:1.885537390716 bias:1.2602004152583</p>

<p>…  </p>

<p>i:450 loss:0.00033602903432248 weight:1.9788119352928 bias:1.0481654513272</p>

<p>i:500 loss:0.00020752499774989 weight:1.9833490852405 bias:1.0378514430405</p>

<p>```  </p>

<h2 id="materials">Materials</h2>

<p>本次教學的完整程式碼於此：</p>

<p><a href="https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/4_backward_propagation_part_1.ipynb">https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/4_backward_propagation_part_1.ipynb
</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Torch NN Tutorial 3 : NN.Criterion & NN.MSECriterion]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/12/25/torch-nn-tutorial-3-nn-dot-criterion/"/>
    <updated>2016-12-25T18:48:14+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/12/25/torch-nn-tutorial-3-nn-dot-criterion</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>torch 的 <code>nn.Module</code> 是組成 neural network 的部分，但是要訓練一個 neural network ，就需要有 loss function 。而 <code>nn.Criterion</code> 就是用來計算 loss function 的值。<code>nn.Criterion</code> 是個抽象類別，所有種類的 loss function 都繼承於它。</p>

<p>例如， loss funciton 用 Minimum Square Error 時， <script type="math/tex">\bar{y}</script> 為模型預測出來的數值， <script type="math/tex">y</script> 為正確的數值 ，則 loss function <script type="math/tex"> L (y, \bar{y}) </script> 的公式如下：</p>

<script type="math/tex; mode=display">

l( y , \bar{y}) = (y  - \bar{y} )^2 

</script>

<p>在 torch 中，負責計算 Minimum Square Error 的 criterion 為 <code>nn.MSECriterion</code> 。</p>

<!-- more -->

<p>實作部份如下，首先，載入 <code>nn</code> 模組：</p>

<p>``` lua</p>

<p>require ‘nn’</p>

<p>```</p>

<p>建立 <code>nn.MSECriterion</code> ，命名為 <code>c1</code> ，如下：</p>

<p>``` lua</p>

<p>c1 = nn.MSECriterion()</p>

<p>```</p>

<p>假設模型預測出的數值 <script type="math/tex"> \bar{y} = 5</script> ，正確數值 <script type="math/tex">y=3</script> ，則：</p>

<script type="math/tex; mode=display">

l( y , \bar{y}) = (y  - \bar{y} )^2  = (3-5)^2 = 4

</script>

<p>實作如下，將 <code>y_ = 5</code> 與 <code>y = 3</code> 兩數值傳入 <code>c1</code> ，進行 forward propagation ，得出 loss function <code>l</code> 的數值。</p>

<p>``` lua</p>

<p>y_ = torch.Tensor{5}
y = torch.Tensor{3}
l = c1:forward(y_,y)
print(l)</p>

<p>```</p>

<p>執行結果， <code>l = 4</code> ，如下：</p>

<p>``` sh</p>

<p>4	</p>

<p>```</p>

<h2 id="nncriterion--nnmsecriterion">nn.Criterion &amp; nn.MSECriterion</h2>

<p>這邊講解 <code>nn.Criterion</code> 以及 <code>nn.MSECriterion</code> 的程式碼：</p>

<p><code>nn.Criterion</code> 程式碼：<a href="https://github.com/torch/nn/blob/master/Criterion.lua">https://github.com/torch/nn/blob/master/Criterion.lua</a></p>

<p><code>nn.MSECriterion</code> 程式碼： <a href="https://github.com/torch/nn/blob/master/MSECriterion.lua">https://github.com/torch/nn/blob/master/MSECriterion.lua</a></p>

<p>首先看到 <code>nn.Criterion</code> 的部分，先從 <code>init</code> 開始。</p>

<p>``` lua nn/Criterion.lua</p>

<p>local Criterion = torch.class(‘nn.Criterion’)</p>

<p>function Criterion:__init()
   self.gradInput = torch.Tensor()
   self.output = 0
end</p>

<p>```</p>

<p>從以上程式碼得知， <code>nn.Criterion</code> 雖然有 <code>output</code> 和 <code>gradInput</code> 這兩個變量，但它並沒有繼承 <code>nn.Module</code> 。</p>

<p>再來看到 forward propagation 的部分， <code>nn.Criterion</code> 的 <code>forward</code> 和 <code>nn.Module</code> 的不一樣，因為它的輸入有 <code>input</code> 和 <code>target</code> 這兩個變量，如下：</p>

<p>因為 loss function 的 forward propagation 需要有 input 和 target 這兩個數值才算得出來，而 module 中的 forward propagation 部分，通常只需要 input 的數值即可。</p>

<p>``` lua nn/Criterion.lua</p>

<p>function Criterion:forward(input, target)
   return self:updateOutput(input, target)
end</p>

<p>```</p>

<p>再來看到 <code>nn.MSECriterion</code> 的程式碼。</p>

<p>``` lua nn/MSECriterion.lua</p>

<p>function MSECriterion:__init(sizeAverage)
   parent.__init(self)
   if sizeAverage ~= nil then
     self.sizeAverage = sizeAverage
   else
     self.sizeAverage = true
   end
end</p>

<p>```</p>

<p>其中， <code>sizeAverage</code> 是設定要不要將 output 根據 input 的維度來做平均，預設值為 <code>true</code> 。</p>

<p>舉個例子，<code>y2_</code> 和 <code>y2</code> 分別為 input 與 target ，分別建立 <code>c2</code> 與 <code>c3</code> 兩個 <code>nn.MSECriterion</code> ， <code>c2</code> 的 <code>sizeAverage</code> 為 <code>true</code> ，而 <code>c3</code> 的 <code>sizeAverage</code> 為 <code>false</code> 。</p>

<p>``` lua</p>

<p>y2_ = torch.Tensor{5,5}
y2 = torch.Tensor{3,3}
c2 = nn.MSECriterion(true)
c3 = nn.MSECriterion(false)</p>

<p>```</p>

<p>將 <code>y2_</code> 和 <code>y2</code> 都輸入 <code>c2</code> 和 <code>c3</code> ，進行 forward propagation ，比較其結果差異，程式碼如下：</p>

<p>``` lua</p>

<p>print(c2:forward(y2<em>,y2))
print(c3:forward(y2</em>,y2))</p>

<p>```</p>

<p>則輸出結果如下：</p>

<p>``` sh</p>

<p>4	
8</p>

<p>```</p>

<p>以上， 4 是 <code>sizeAverage</code> 為 <code>true</code> 的結果，也就是將 <code>y2_</code> 和 <code>y2</code> 的每個元素相減後再平均，如下：</p>

<script type="math/tex; mode=display">

\frac{(5-3)^2 + (5-3)^2 }{2} = 4

</script>

<p>而 8 是 <code>sizeAverage</code> 為 <code>false</code> 的結果，也就是將 <code>y2_</code> 和 <code>y2</code> 的每個元素相減，但沒平均，如下：</p>

<script type="math/tex; mode=display">

(5-3)^2 + (5-3)^2  = 8

</script>

<p>這裡要注意的是，不管 <code>sizeAverage</code> 是 <code>ture</code> 或 <code>false</code> ，也不論 input 的維度有多少，output 都是一維的純量。</p>

<p>再來看到 <code>updateOutput</code> 的程式碼：</p>

<p>``` lua nn/Criterion.lua</p>

<p>function MSECriterion:updateOutput(input, target)
   self.output_tensor = self.output_tensor or input.new(1)
   input.THNN.MSECriterion_updateOutput(
      input:cdata(),
      target:cdata(),
      self.output_tensor:cdata(),
      self.sizeAverage
   )
   self.output = self.output_tensor[1]
   return self.output
end</p>

<p>``` </p>

<p>此部分的核心運算是用 C 來加速，在第三行會呼叫 C 的函數 <code>MSECriterion_updateOutput</code> ，再將結果存在 <code>output_tensor</code> ，而 lua 則從 <code>output_tensor</code> 中取出結果，傳到 <code>output</code> 。</p>

<p>C 的程式碼在 <a href="https://github.com/torch/nn/blob/master/lib/THNN/generic/MSECriterion.c">MSECriterion.c</a> 檔案中：</p>

<p>``` c  nn/lib/THNN/generic/MSECriterion.c</p>

<p>void THNN_(MSECriterion_updateOutput)(
          THNNState *state,
          THTensor *input,
          THTensor *target,
          THTensor *output,
          bool sizeAverage)
{
  THNN_CHECK_NELEMENT(input, target);
  THNN_CHECK_DIM_SIZE(output, 1, 0, 1);</p>

<p>real sum = 0;</p>

<p>TH_TENSOR_APPLY2(real, input, real, target,
    real z = (<em>input_data - *target_data);
    sum += z</em>z;
  );</p>

<p>if (sizeAverage)
    sum /= THTensor_(nElement)(input);</p>

<p>THTensor_(set1d)(output, 0, sum);
}</p>

<p>```</p>

<p>在第 14 ~ 15 行中可看到將 <code>input_data</code> 與 <code>target_data</code> 相減後得出 <code>z</code> ，並將 <code>z</code> 平方後累加，得到 <code>sum</code> ，而在 18 ~ 19 行中， <code>sum</code> 會根依序 <code>sizeAverage</code> 的值，來決定要不要除以所有元素的個數。</p>

<p>最後一行則將 <code>sum</code> 的值存到 <code>output</code> 中，這個 <code>output</code> 所對應到 lua 中的 <code>output_tensor</code> 。</p>

<p>關於 <code>updateGradInput</code> 以及 backward propagation 的部分，將在下回介紹。</p>

<h2 id="materials">Materials</h2>

<p>本次教學的完整程式碼於此：</p>

<p><a href="https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/3_nn_criterion_and_msecriterion.ipynb">https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/3_nn_criterion_and_msecriterion.ipynb</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Torch NN Tutorial 2 : NN.Container & NN.Sequential]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/12/24/torch-nn-tutorial-2-nn-container/"/>
    <updated>2016-12-24T00:24:25+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/12/24/torch-nn-tutorial-2-nn-container</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>在<a href="/blog/2016/12/19/torch-nn-tutorial-1-nn-module/">Torch NN tutorial 1 : NN.Module &amp; NN.Linear</a>中，介紹了構成 neural network 的最基本的單位，也就是 <code>nn.Module</code> ，並介紹了 <code>nn.Linear</code> 可以進行一次線性運算，但通常一個 neural network 是經由多個線性和非線性的運算構成的。要把多個 Module 串接起來，就需要用到 <code>nn.Contaner</code> 。</p>

<p>例如，有個 neural network 的輸入為 x ，輸出為 z ，中間經過了一次線性運算與一個sigmoid function ，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\textbf{y} = \textbf{W}\textbf{x} + \textbf{b}  \\
&\textbf{z} = \frac{1}{1+e^{-\textbf{y}}}
\end{align}
 %]]&gt;</script>

<p>假設 <script type="math/tex">\textbf{x}</script> 為 2 維的向量，而 <script type="math/tex">\textbf{y}</script> 為 3 維向量， 
<script type="math/tex">\textbf{W},\textbf{b}</script> 分別為 weight 和 bias ，此兩參數皆以隨機值進行初始化，
<script type="math/tex">\textbf{z}</script>  是 <script type="math/tex">\textbf{y}</script> 經過 sigmoid 非線性運算的輸出結果。</p>

<!--more-->

<p>使用 torch 實作此運算：</p>

<p>首先，載入 <code>nn</code> 套件：</p>

<p>``` lua</p>

<p>require ‘nn’</p>

<p>```</p>

<p>建立以上兩個運算所需的Module，分別為 <code>nn.Linear</code> 和 <code>nn.Sigmoid</code> ，並將它們分別命名為 <code>l1</code> 和 <code>s1</code> ，如下：</p>

<p>``` lua</p>

<p>l1 = nn.Linear(2,3)
s1 = nn.Sigmoid()</p>

<p>```</p>

<p>其中， <code>nn.Sigmoid</code> 為進行 sigmoid 運算所需的 module 。</p>

<p>如果要進行運算，方法如下：</p>

<p>假設 x 為一個二維向量 <script type="math/tex">[0,1]</script> ，先輸入 <code>l1</code> ，進行 forward propagation ，將運算結果傳給 <code>y</code> ，如下：</p>

<p>``` lua</p>

<p>x = torch.Tensor{0,1}
y = l1:forward(x)
print(y)</p>

<p>```</p>

<p>執行完後會印出 <code>l1</code> 的輸出值，也就是 <code>y</code> 的值，是個三維的向量，
y 的值會與 <code>l1.weight</code> 和 <code>l1.bias</code> 的初始值值關， <code>y</code> 值如下：</p>

<p>``` sh</p>

<p>0.1948
 0.9780
 0.3982
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<p>再來將 <code>y</code> 輸入到 <code>s1</code> 中， 進行 forward propagation，將運算結果傳給 <code>z</code> ，如下：</p>

<p>``` lua</p>

<p>z = s1:forward(y)
print(z)</p>

<p>```</p>

<p>執行完後會印出 <code>z</code> 值，此值是由 <code>y</code> 的值經過 sigmoid function 的運算所得出來的， <code>z</code> 值如下：</p>

<p>``` sh</p>

<p>0.5485
 0.7267
 0.5983
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<p>以上過程中，給定了 <code>x</code> 值，經過每一個 module 都要呼叫一次 <code>forward</code> ，最後才能取得 <code>z</code> 的值，
如果一個 network 是由很多個 module 所組成的，這樣要呼叫很多次 <code>forward</code> ，會很麻煩。</p>

<p>而 <code>nn.Container</code> 則是一個容器，它可以把多個 module 放進去，並自動處理這些 module 輸入與輸出值的傳遞。 <code>nn.Container</code> 也是個抽象個類別，泛指能夠把 module 放進去的容器。</p>

<p>其中一種 container 可將前一個 module 的輸出，傳給下一個 module 的輸入，這種 contanier 為 <code>nn.Sequential()</code> 。</p>

<p>例如，如果要將 <code>l1</code> 的輸出，傳入 <code>s1</code> ，則可以用 <code>nn.Sequential</code> 來達成。建立一個命名為 <code>n1</code> 的 sequential container ，將 <code>l1</code> 和 <code>s1</code> 串起來，方法如下：</p>

<p>``` lua</p>

<p>n1 = nn.Sequential()
n1:add(l1)
n1:add(s1)</p>

<p>```</p>

<p>其中，<code>add</code> 是將 module 加入 container 的 function。</p>

<p>其實， <code>nn.Container</code> 也是從 <code>nn.Module</code> 繼承而來的，所以它也有 <code>forward</code> ，可以進行 forward propagation ，只要呼叫了 <code>nn.Container</code> 的 <code>forward</code> ，它就會自動將裡面的 module 進行 forward ，並輸出結果。</p>

<p>如此給定 <code>x</code> 之後，只要進行一次 forward 即可取得 <code>z</code> 的結果，如下：</p>

<p>``` lua</p>

<p>x = torch.Tensor{0,1}
z = n1:forward(x)
print(z)</p>

<p>```</p>

<p>執行完後，會印出 <code>z</code> 的結果，如下：</p>

<p>``` sh</p>

<p>0.5485
 0.7267
 0.5983
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<h2 id="nncontainer--nnsequential">nn.Container &amp; nn.Sequential</h2>

<p>這邊要介紹 <code>nn.Container</code> 和 <code>nn.Sequential</code> 的程式碼。</p>

<p><code>nn.Container</code> 程式碼：<a href="https://github.com/torch/nn/blob/master/Container.lua">https://github.com/torch/nn/blob/master/Container.lua</a></p>

<p><code>nn.Sequential</code> 程式碼： <a href="https://github.com/torch/nn/blob/master/Sequential.lua">https://github.com/torch/nn/blob/master/Sequential.lua</a></p>

<p>首先，介紹 <code>nn.Container</code> ，先看 <code>init()</code> 的部分：</p>

<p>``` lua nn/Container.lua</p>

<p>local Container, parent = torch.class(‘nn.Container’, ‘nn.Module’)</p>

<p>function Container:__init(…)
    parent.__init(self, …)
    self.modules = {}
end</p>

<p>```</p>

<p>第1行處，看到 <code>nn.Container</code> 是從 <code>nn.Module</code> 繼承而來的，因此它具有 <code>nn.Module</code> 的 variable 及 function 。</p>

<p>第5行中，container 的變量 <code>modules</code> ，它是一個 lua table ，是用來存放加到 container 中的  module 。</p>

<p>至於，要如何將 module 加到 container 中？可以用 <code>add</code> 加入。 <code>add</code> 的程式碼如下： </p>

<p>``` lua nn/Container.lua</p>

<p>function Container:add(module)
    table.insert(self.modules, module)
    return self
end</p>

<p>```</p>

<p>在第2行可看到， <code>add</code> 可以把新的 module 加到 <code>modules</code> 中。</p>

<p>``` lua nn/Container.lua</p>

<p>function Container:get(index)
    return self.modules[index]
end</p>

<p>```</p>

<p>除了加進去之外，也可以用 <code>get</code> 取得 <code>modules</code> 中的某個元素，或是用 <code>size</code> 取得 <code>modules</code> 的大小，程式碼如下：</p>

<p>``` lua nn/Container.lua</p>

<p>function Container:get(index)
    return self.modules[index]
end</p>

<p>function Container:size()
    return #self.modules
end</p>

<p>```</p>

<p>再來是實作的部分，首先我們建立一個 sequential 的 container ，命名為 <code>n2</code> ，並以 <code>size</code> 取得它所含的 module 個數，如下：</p>

<p>``` lua</p>

<p>n2 = nn.Sequential()
print(n2:size())</p>

<p>```</p>

<p>剛建立時， 由於 <code>n2</code> 的 <code>modules</code> 是空的，所以 <code>size</code> 是 0。執行結果如下：</p>

<p>``` sh</p>

<p>0	</p>

<p>```</p>

<p>再來將 <code>l1</code> 和 <code>s1</code> 依序加入，再看看 <code>size</code> 的變化：</p>

<p>``` lua</p>

<p>n2:add(l1)
n2:add(s1)
print(n2:size())</p>

<p>```</p>

<p>此時已經加入了兩個 module 進去了，所以 <code>size</code> 會是 2 ，執行結果如下：</p>

<p>``` sh</p>

<p>2	</p>

<p>```</p>

<p>可以用 <code>get</code> 取得加進去的 module ，例如，想取得第一個加進去的，作法如下：</p>

<p>``` lua</p>

<p>print(n2:get(1))</p>

<p>```</p>

<p>第一個加進去的為 <code>nn.Linear</code> ，執行結果如下：</p>

<p>``` sh</p>

<p>nn.Linear(2 -&gt; 3)
{
  gradBias : DoubleTensor - size: 3
  weight : DoubleTensor - size: 3x2
  _type : torch.DoubleTensor
  output : DoubleTensor - size: 3
  gradInput : DoubleTensor - empty
  bias : DoubleTensor - size: 3
  gradWeight : DoubleTensor - size: 3x2
}</p>

<p>```</p>

<p>再來看到 <code>nn.Sequential()</code> 的程式碼。</p>

<p>``` lua nn/Sequential.lua</p>

<p>local Sequential, _ = torch.class(‘nn.Sequential’, ‘nn.Container’)</p>

<p>```</p>

<p>第一行顯示 <code>nn.Sequential</code> 繼承了 <code>nn.Container</code> 。它的 <code>init</code> 也與 <code>nn.Container</code> 共用，沒有再另外實作。</p>

<p>再來看 <code>add</code> 的程式碼。</p>

<p>``` lua nn/Sequential.lua</p>

<p>function Sequential:add(module)
   if #self.modules == 0 then
      self.gradInput = module.gradInput
   end
   table.insert(self.modules, module)
   self.output = module.output
   return self
end</p>

<p>```</p>

<p>在 <code>nn.Sequential</code> 中， <code>add</code> 除了會將 module 加到 <code>modules</code> 之外，它的 <code>output</code> 會是最後一個加進去的 module 的 <code>output</code> 。</p>

<p>以下實作，將 <code>n2</code> 的 <code>output</code> 和它裡面的兩個 module 的 <code>output</code> 都印出來看看：</p>

<p>``` lua</p>

<p>print(n2:get(1).output)
print(n2:get(2).output)
print(n2.output)</p>

<p>```</p>

<p>結果如下， <code>n2</code> 的 output 是第二個 module 的 output ：</p>

<p>```sh</p>

<p>0.1948
 0.9780
 0.3982
[torch.DoubleTensor of size 3]</p>

<p>0.5485
 0.7267
 0.5983
[torch.DoubleTensor of size 3]</p>

<p>0.5485
 0.7267
 0.5983
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<p>再來看到 <code>updateOutput</code> 的部分：</p>

<p>``` lua nn/Sequential.lua</p>

<p>function Sequential:updateOutput(input)
   local currentOutput = input
   for i=1,#self.modules do
      currentOutput = self:rethrowErrors(self.modules[i], i, ‘updateOutput’, currentOutput)
   end
   self.output = currentOutput
   return currentOutput
end</p>

<p>```</p>

<p>以上可知，在第3行的 for 迴圈中， <code>modules</code> 中的 <code>module</code> 會依序進行  <code>updateOutput</code> ，並將其 <code>output</code> 傳遞到下個 <code>module</code> 的 <code>input</code> ，而 <code>nn.Sequential</code> 的 <code>output</code> 會是最後一個 <code>module</code> 的 <code>output</code></p>

<p>假設輸入 <code>n2</code> 的 <code>x</code> 為 <script type="math/tex">[1,0]</script> ，則進行 forward propagation ，將輸出結果存到 <code>z</code> ，實作如下 ：</p>

<p>``` lua</p>

<p>x = torch.Tensor{1,0}
z = n2:forward(x)
print(z)</p>

<p>```</p>

<p>透過 updateOutput 中的 for 迴圈，它會依序跑完內部所有的 module，並得出最後結果，結果如下：</p>

<p>``` sh</p>

<p>0.5331
 0.6890
 0.5093
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<p>如果想看 <code>n2</code> 中有哪些 module ，以及它們的順序，也可以直接用 <code>print</code> 的方式，作法如下：</p>

<p>``` lua </p>

<p>print(n2)</p>

<p>```</p>

<p>結果如下：</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;nn.Sequential <span class="o">{</span>
</span><span class='line'>  <span class="o">[</span>input -<span class="p">&amp;</span>gt<span class="p">;</span> <span class="o">(</span>1<span class="o">)</span> -<span class="p">&amp;</span>gt<span class="p">;</span> <span class="o">(</span>2<span class="o">)</span> -<span class="p">&amp;</span>gt<span class="p">;</span> output<span class="o">]</span>
</span><span class='line'>  <span class="o">(</span>1<span class="o">)</span>: nn.Linear<span class="o">(</span><span class="m">2</span> -<span class="p">&amp;</span>gt<span class="p">;</span> 3<span class="o">)</span>
</span><span class='line'>  <span class="o">(</span>2<span class="o">)</span>: nn.Sigmoid
</span><span class='line'><span class="o">}</span>
</span><span class='line'><span class="o">{</span>
</span><span class='line'>  gradInput : DoubleTensor - empty
</span><span class='line'>  modules :
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>      <span class="m">1</span> :
</span><span class='line'>        nn.Linear<span class="o">(</span><span class="m">2</span> -<span class="p">&amp;</span>gt<span class="p">;</span> 3<span class="o">)</span>
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>          gradBias : DoubleTensor - size: 3
</span><span class='line'>          weight : DoubleTensor - size: 3x2
</span><span class='line'>          _type : torch.DoubleTensor
</span><span class='line'>          output : DoubleTensor - size: 3
</span><span class='line'>          gradInput : DoubleTensor - empty
</span><span class='line'>          bias : DoubleTensor - size: 3
</span><span class='line'>          gradWeight : DoubleTensor - size: 3x2
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>      <span class="m">2</span> :
</span><span class='line'>        nn.Sigmoid
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>          gradInput : DoubleTensor - empty
</span><span class='line'>          _type : torch.DoubleTensor
</span><span class='line'>          output : DoubleTensor - size: 3
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  _type : torch.DoubleTensor
</span><span class='line'>  output : DoubleTensor - size: 3
</span><span class='line'><span class="o">}</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;
</span></code></pre></td></tr></table></div></figure></p>

<p>以上結果可分成兩部分來看，第一部分是 <code>nn.Sequential</code> 裡面每個 module 的順序，從 input 到 output 之間，依次進行了哪些運算。</p>

<p>第二部份是詳細印出 <code>nn.Sequential</code> 中，有哪些成員 ，它有從 <code>nn.Module</code> 繼承而來的 <code>gradInput</code> 即 <code>output</code> ，也有從 <code>nn.Container</code> 繼承而來的 <code>modules</code> 。而 <code>modules</code> 的部分會詳細列出裡面每個 module 中有哪些 variable。</p>

<p>這部分的程式碼於 <code>tostring</code> 函式中，程式碼如下：</p>

<p>``` lua nn/Sequential.lua</p>

<p>function Sequential:<strong>tostring</strong>()
   local tab = ‘  ‘
   local line = ‘\n’
   local next = ‘ -&gt; ‘
   local str = ‘nn.Sequential’
   str = str .. ‘ {‘ .. line .. tab .. ‘[input’
   for i=1,#self.modules do
      str = str .. next .. ‘(‘ .. i .. ‘)’
   end
   str = str .. next .. ‘output]’
   for i=1,#self.modules do
      str = str .. line .. tab .. ‘(‘ .. i .. ‘): ‘ .. tostring(self.modules[i]):gsub(line, line .. tab)
   end
   str = str .. line .. ‘}’
   return str
end</p>

<p>```</p>

<h2 id="materials">Materials</h2>

<p>本次教學的完整程式碼於此：</p>

<p><a href="https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/2_nn_container_and_sequential.ipynb">https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/2_nn_container_and_sequential.ipynb</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Torch NN Tutorial 1 : NN.Module & NN.Linear]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/12/19/torch-nn-tutorial-1-nn-module/"/>
    <updated>2016-12-19T22:36:47+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/12/19/torch-nn-tutorial-1-nn-module</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>此系列講解如何用 torch 實作 neural network 。</p>

<p>本系列不講解如何安裝 torch 及 lua 的基本語法，假設讀者都已具備這些基礎知識。</p>

<p>以 torch 實作 neural network 時，最常用的套件為 <a href="https://github.com/torch/nn">nn</a>，而在 <code>nn</code> 中，建構 neural network 最基本的單位為 <a href="https://github.com/torch/nn/blob/master/Module.lua">nn.Module</a> 。 <code>nn.Module</code> 是一個抽象類別，所有建構 neural network 本身有關的 module ，都是從 <code>nn.Module</code> 所繼承而來。</p>

<p>舉個例子，如果要實作以下運算：</p>

<script type="math/tex; mode=display">

\textbf{y} = \textbf{W}\textbf{x} + \textbf{b} 

</script>

<p>假設 <script type="math/tex">\textbf{x}</script> 為 2 維的 input ，而 <script type="math/tex">\textbf{y}</script> 為 3 維的output， <script type="math/tex">\textbf{W},\textbf{b}</script> 分別為 weight 和 bias ，此兩參數皆以隨機值進行初始化。</p>

<p>使用 torch 實作此運算的方法如下：</p>

<p>首先，載入 nn 套件：</p>

<p>```lua</p>

<p>require ‘nn’</p>

<p>```</p>

<p>建立一個 Linear Module：</p>

<p>```lua</p>

<p>l1 = nn.Linear(2,3)</p>

<p>```</p>

<!--more-->

<p>其中， <a href="https://github.com/torch/nn/blob/master/Linear.lua">nn.Linear</a> 即是用來進行 <script type="math/tex">\textbf{y} = \textbf{W}\textbf{x} + \textbf{b} </script> 這類的線性運算所用的模組，它繼承了 <code>nn.Module</code> 。 而 2 和 3 分別代表了 <script type="math/tex">\textbf{x}</script> 和 <script type="math/tex">\textbf{y}</script> 的維度。 當它被建構出來時， weight 和 bias 的值會以隨機值來初始化。 </p>

<p>以上程式中，建立一個命名為 <code>l1</code> 的 module ，如果要取得它的 weight 和 bias ，可以用 <code>l1.weight</code> 和 <code>l1.bias</code> 取得，方法如下：</p>

<p>```lua</p>

<p>print(l1.weight)
print(l1.bias)</p>

<p>```</p>

<p>執行結果如下：</p>

<p>```sh</p>

<p>0.1453  0.5062
 0.0635  0.4911
-0.1080  0.1747
[torch.DoubleTensor of size 3x2]</p>

<p>0.2063
-0.1635
-0.0883
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<p>其中， size 3x2 的 tensor 為 weight, size 3 的 tensor 為 bias。</p>

<p>用此 module 可以執行運算，令 x 為一個二維向量 <script type="math/tex">[0,1]</script> ，輸入此 module ，進行 forward propagation ，也就是說，執行 <script type="math/tex">\textbf{y} = \textbf{W}\textbf{x} + \textbf{b} </script> 的運算， 並輸出結果為 <script type="math/tex">\textbf{y}</script>  ，實作如下：</p>

<p>```lua</p>

<p>x = torch.Tensor{0,1}
y = l1:forward(x)
print(y)</p>

<p>```</p>

<p>輸出結果 <code>y</code> 為一個三維向量，如下：</p>

<p>```sh</p>

<p>0.7125
 0.3276
 0.0865
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<h2 id="nnmodule--nnlinear">nn.Module &amp; nn.Linear</h2>

<p>這邊要更進一步介紹 <code>nn.Module</code> 和 <code>nn.Linear</code> 的內容是什麼。由於 torch 的程式碼相當簡潔易懂，可以直接看程式碼來了解它的功能是什麼。</p>

<p><code>nn.Module</code> 程式碼： <a href="https://github.com/torch/nn/blob/master/Module.lua">https://github.com/torch/nn/blob/master/Module.lua</a></p>

<p><code>nn.Linear</code> 程式碼： <a href="https://github.com/torch/nn/blob/master/Linear.lua">https://github.com/torch/nn/blob/master/Linear.lua</a></p>

<p>首先，介紹 <code>nn.Module</code> ，先看 <code>init()</code> 的部分：</p>

<p>```lua nn/Module.lua</p>

<p>function Module:__init()
   self.gradInput = torch.Tensor()
   self.output = torch.Tensor()
   self._type = self.output:type()
end</p>

<p>```</p>

<p><code>Module</code> 中最基本的成員有 <code>output</code> 和 <code>gradInput</code> 。
<code>output</code> 為此 <code>Module</code> 的 forward propagation 結果，而 <code>gradInput</code> 為 backward propagation 的運算結果。
這些變量一開始都會被初始化為 空的 tensor 。</p>

<p>註：本文先不講解 backward propagation 與 <code>gradInput</code> 的部分，交由之後的教學文章來解釋。</p>

<p>在 <code>Module:forward</code> 的部分，是用來進行 forward propagation的，如下：</p>

<p>```lua nn/Module.lua</p>

<p>function Module:updateOutput(input)
   return self.output
end</p>

<p>function Module:forward(input)
   return self:updateOutput(input)
end</p>

<p>```</p>

<p>先看 forward 的部分， Module 沒有運算的實作，僅單純輸出 <code>output</code> 值。如果呼叫了 forward propagation ，則從 <code>Module:updateOutput</code> 就直接輸出了 <code>output</code> 。</p>

<p>而 <code>nn.Linear</code> 則實作了 forward propagation。</p>

<p>所謂的 Linear，即是指 <script type="math/tex">\textbf{y} = \textbf{W}\textbf{x} + \textbf{b} </script> 的線性運算。</p>

<p>再來看 <code>nn.Linear</code> 的程式碼，先看 <code>init()</code> 的部分：</p>

<p>```lua nn/Linear.lua</p>

<p>local Linear, parent = torch.class(‘nn.Linear’, ‘nn.Module’)</p>

<p>function Linear:__init(inputSize, outputSize, bias)
   parent.__init(self)
   local bias = ((bias == nil) and true) or bias
   self.weight = torch.Tensor(outputSize, inputSize)
   self.gradWeight = torch.Tensor(outputSize, inputSize)
   if bias then
      self.bias = torch.Tensor(outputSize)
      self.gradBias = torch.Tensor(outputSize)
   end
   self:reset()
end</p>

<p>```</p>

<p>在第1行， <code>nn.Linear</code> 繼承了 <code>nn.Module</code> 。</p>

<p>在第3行開始可以看到，建構 Linear 所需的參數有 <code>inputSize</code> , <code>outputSize</code> 和 <code>bias</code> 。 <code>bias</code>  不一定要給，如果沒有給，則預設值會讓它是隨機的。除非 <code>bias=false</code> ，則此 Linear Module 就不會有 <code>bias</code> 。
從6~10行中，它比 <code>nn.Module</code> 多了 <code>weigt</code> 和 <code>bias</code> 這兩個變量，而 <code>reset()</code> 則是將它們初始化。</p>

<p>如果要建立一個 Linear Module，則要給定 <code>inputSize</code> 和 <code>outputSize</code> ，也就是 <script type="math/tex">\textbf{x}</script> 和 <script type="math/tex">\textbf{y}</script> 的維度。</p>

<p>假設  <script type="math/tex">\textbf{x}</script> 是二維， <script type="math/tex">\textbf{y}</script> 是三維，建立一個命名為 <code>l2</code> 的 Linear 模組：</p>

<p>```lua</p>

<p>l2 = nn.Linear(2,3)</p>

<p>```</p>

<p>用以下方法印出 l2 的 <code>weight</code> , <code>bias</code> 和 <code>output</code> ：</p>

<p>```lua</p>

<p>print(l2.weight)
print(l2.bias)
print(l2.output)</p>

<p>```</p>

<p>輸出結果如下：</p>

<p>```sh</p>

<p>-0.2863  0.5541
-0.6269  0.6557
-0.3215 -0.1648
[torch.DoubleTensor of size 3x2]</p>

<p>-0.0316
 0.4126
 0.4415
[torch.DoubleTensor of size 3]</p>

<p>[torch.DoubleTensor with no dimension]</p>

<p>```</p>

<p>其中，<code>weight</code> 和 <code>bias</code> 會被初始化隨機成 size 3x2 和 size 3 的 double tensor ，而最後一行顯示出 <code>output</code> 還是空的（with no dimension）。</p>

<p>如果想知道各個 variable 的 size ，還有個方式，就是直接用 print 的方式把它印出來，作法如下：</p>

<p>```lua</p>

<p>print(l2)</p>

<p>```</p>

<p>執行結果如下：</p>

<p>```sh</p>

<p>nn.Linear(2 -&gt; 3)
{
  gradBias : DoubleTensor - size: 3
  weight : DoubleTensor - size: 3x2
  _type : torch.DoubleTensor
  output : DoubleTensor - empty
  gradInput : DoubleTensor - empty
  bias : DoubleTensor - size: 3
  gradWeight : DoubleTensor - size: 3x2
}</p>

<p>```</p>

<p>要讓 <code>output</code> 有值，就要進行 forward propagation 。而 <code>Linear:updateOutput</code> 則是實作了 <code>Module:updateOutput</code> 中， forward propagation 運算的實際內容，程式碼如下：</p>

<p>```lua nn/Linear.lua</p>

<p>function Linear:updateOutput(input)
   if input:dim() == 1 then
      self.output:resize(self.weight:size(1))
      if self.bias then self.output:copy(self.bias) else self.output:zero() end
      self.output:addmv(1, self.weight, input)
   elseif input:dim() == 2 then
      local nframe = input:size(1)
      local nElement = self.output:nElement()
      self.output:resize(nframe, self.weight:size(1))
      if self.output:nElement() ~= nElement then
         self.output:zero()
      end
      updateAddBuffer(self, input)
      self.output:addmm(0, self.output, 1, input, self.weight:t())
      if self.bias then self.output:addr(1, self.addBuffer, self.bias) end
   else
      error(‘input must be vector or matrix’)
   end</p>

<p>return self.output
end</p>

<p>```</p>

<p>以上可以分為兩部分來看，首先是當 <code>input:dim() ==1</code> 時，也就是 <code>input</code> 的維度為 1 ，也就是一次只輸入單筆資料的時候。第一步，會先調整 <code>output</code> 的 <code>size</code> 為適當的大小，再將 <code>bias</code> 複製到 <code>output</code> ，再讓 <code>input</code> 和 <code>weight</code> 進行矩陣相乘，並和 <code>output</code> 相加，再輸出結果。</p>

<p>從數學公式上來看，輸入值 <script type="math/tex">\textbf{x}</script> 是一維的向量，進行的運算如下：</p>

<script type="math/tex; mode=display">
\textbf{W}\textbf{x} + \textbf{b}
</script>

<p>此時， <script type="math/tex">\textbf{W}</script> 放前面，而 <script type="math/tex">\textbf{x}</script> 放後面。</p>

<p>例如當輸入值向量 <script type="math/tex">[0,1]</script> 時，則矩陣運算的結果為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\begin{bmatrix}
-0.2863 & 0.5541 \\
-0.6269 & 0.6557 \\
-0.3215 & -0.1648 \\
\end{bmatrix}
\begin{bmatrix}
0 \\
1
\end{bmatrix}
+ 
\begin{bmatrix}
-0.0316 \\
 0.4126 \\
 0.4415 \\
\end{bmatrix}
=
\begin{bmatrix}
-0.2863 \times 0 + 0.5541 \times 1  -0.0316 \\
-0.6269 \times 0 + 0.6557 \times 1 + 0.4126\\
-0.3215 \times 0  -0.1648 \times 1 + 0.4415\\
\end{bmatrix}\\
&=
\begin{bmatrix}
0.5541 -0.0316 \\
0.6557 + 0.4126\\
-0.1648 + 0.4415\\
\end{bmatrix}
=
\begin{bmatrix}
  0.5225 \\
  1.0683  \\
  0.2766 \\
\end{bmatrix}
\end{align}

 %]]&gt;</script>

<p>實作以上算式，呼叫 <code>l2:forward</code> ，輸入 <code>torch.Tensor{0,1}</code> 並印出結果：</p>

<p>```lua</p>

<p>print(l2:forward(torch.Tensor{0,1}))</p>

<p>```</p>

<p>結果如下：</p>

<p>```sh</p>

<p>0.5225
 1.0683
 0.2766
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<p>這時已經進行過了 forward 運算，而 <code>output</code> 有值了，所以可以印出它的值，方法如下：</p>

<p>```lua</p>

<p>print(l2.output)</p>

<p>```</p>

<p><code>output</code> 的值也是如下：</p>

<p>```sh</p>

<p>0.5225
 1.0683
 0.2766
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<p>而當 <code>input:dim() ==2</code> 時，也就是 <code>input</code> 的維度為 2 ，也就是一次輸入多筆資料。這時需要先把 <code>weight</code> 進行轉置，再和 <code>input</code> ，並和 <code>bias</code> 相加，並將結果加到 <code>output</code> 並輸出。</p>

<p>一次輸入多筆資料的目的是為了加速運算，因為用矩陣對矩陣的相乘的方式就可以來加速。這樣同時輸入的一批資料，就稱為 batch 。</p>

<p>從公式上來看，輸入值 <script type="math/tex">\textbf{X}</script> 是二維的矩陣，則進行以下矩陣運算：</p>

<script type="math/tex; mode=display">
\textbf{X}\textbf{W}^{T} + \textbf{B}
</script>

<p>此時， <script type="math/tex">\textbf{X}</script> 放前面，而 <script type="math/tex">\textbf{W}</script> 進行轉置後放後面。</p>

<p>而 <script type="math/tex">\textbf{B}</script> 是將 <script type="math/tex">\textbf{b}</script> 轉置以後，再複製其橫排所形成的矩陣，以便和前面的矩陣相乘結果來相加。</p>

<p>例如當輸入資料有兩筆向量 <script type="math/tex">[0, 1]</script> 和 <script type="math/tex">[2, 1]</script> 時，則可以組成以下矩陣 (2x2) ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
0 &1 \\
2 &1 \\
\end{bmatrix}


 %]]&gt;</script>

<p>則矩陣運算的過程如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&
\begin{bmatrix}
0 &1 \\
2 &1 \\
\end{bmatrix}
\begin{bmatrix}
-0.2863 & -0.6269 & -0.3215 \\
 0.5541 & 0.6557 & -0.1648 \\
 \end{bmatrix}
+ 
\begin{bmatrix}
-0.0316 & 0.4126 & 0.4415 \\
-0.0316 & 0.4126 & 0.4415 
\end{bmatrix}\\
&
=
\begin{bmatrix}
   -0.2863 \times 0 +  0.5541 \times 1
&  -0.6269 \times 0 +  0.6557 \times 1
&  -0.3215 \times 0   -0.1648 \times 1\\

   -0.2863 \times 2 +  0.5541 \times 1
&  -0.6269 \times 2 +  0.6557 \times 1
&  -0.3215 \times 2   -0.1648 \times 1\\
\end{bmatrix}\\
&
+ 
\begin{bmatrix}
-0.0316 & 0.4126 & 0.4415 \\
-0.0316 & 0.4126 & 0.4415 
\end{bmatrix}\\
&
=
\begin{bmatrix}
0.5541 -0.0316
& 0.6557 +0.4126
& -0.1648 +0.4415 \\
-0.0186  -0.0316
& -0.5981 +0.4126
& -0.8079 +0.4415 \\
\end{bmatrix}\\
&
=
\begin{bmatrix}
 0.5225 & 1.0683  & 0.2766 \\
-0.0502 & -0.1855 & -0.3664 \\
\end{bmatrix}
\end{align}\\

 %]]&gt;</script>

<p>輸出結果為一個 2x3 的矩陣，每一個橫排為一個三維向量，代表著每一筆資料經過線性運算的結果。</p>

<p>實作以上算式，呼叫 <code>l2:forward</code> ，輸入由 <code>{0,1}</code> 和 <code>{2,1}</code> 這兩筆資料組成的 batch， 並印出結果：</p>

<p>
```lua</p>

<p>input={
   {0,1},
   {2,1}
}
print(l2:forward(torch.Tensor(input)))</p>

<p>```
</p>

<p>結果如下：</p>

<p>```sh</p>

<p>0.5225  1.0683  0.2766
-0.0502 -0.1855 -0.3664
[torch.DoubleTensor of size 2x3]</p>

<p>```</p>

<p>也可印出 <code>output</code> 的值：</p>

<p>```lua</p>

<p>print(l2.output)</p>

<p>```</p>

<p>結果如下：</p>

<p>```sh</p>

<p>0.5225  1.0683  0.2766
-0.0502 -0.1855 -0.3664
[torch.DoubleTensor of size 2x3]</p>

<p>```</p>

<h2 id="materials">Materials</h2>

<p>本次教學的完整程式碼於此：</p>

<p><a href="https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/1_nn_module_and_linear.ipynb">https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/1_nn_module_and_linear.ipynb</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Word2vec (Part 3 : Implementation)]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/08/29/neural-network-word2vec-part-3-implementation/"/>
    <updated>2016-08-29T11:17:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/08/29/neural-network-word2vec-part-3-implementation</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>本文接續 <a href="/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation">word2vec (part2)</a> ，介紹如何根據推導出來的 <em>backward propagation</em> 公式，從頭到尾實作一個簡易版的 <em>word2vec</em> 。</p>

<p>本例的 input layer 採用 <em>skip-gram</em> ， output layer 採用 <em>negative sampling</em></p>

<p>本例用唐詩語料庫：https://github.com/ckmarkoh/coscup_nndl/blob/master/poem.txt</p>

<p>首先，載入所需的模組</p>

<p>```python
import json
from collections import Counter, OrderedDict
import numpy as np
import random
import math</p>

<p>```</p>

<!--more-->

<h2 id="build-dictionray">Build Dictionray</h2>

<p>再來是建立字典，即將每個字給一個id來對應。</p>

<p>```python
def LearnVocabFromTrainFile():</p>

<pre><code># 開啟唐詩語料庫
f = open("poem.txt")
 
# 統計唐詩語料庫中每個字出現的頻率
vcount = Counter()
for line in f.readlines():
    for w in line.decode("utf-8").strip().split():
        vcount.update(w)
        
# 僅保留出現次數大於五的字，並按照出現次數排序
vcount_list = sorted(filter(lambda x: x[1] &gt;= 5, vcount.items())
                     , reverse=True, key=lambda x: x[1])
                     
# 建立字典，將每個字給一個id ，字為 key, id 為 value
vocab_dict = OrderedDict(map(lambda x: (x[1][0], x[0]), enumerate(vcount_list)))

# 建立詞頻統計用的字典，給定某字，可查到其出現頻率
vocab_freq_dict = OrderedDict(map(lambda x: (x[0], x[1]), vcount_list))
return vocab_dict, vocab_freq_dict
</code></pre>

<p>vocab_dict, vocab_freq_dict =  LearnVocabFromTrainFile()</p>

<p>```</p>

<p>印出字典檔，每個字對應到一個id（編號）</p>

<p>```python
for w,wid in vocab_dict.items():
    print “%s : %s”%(w,wid)</p>

<p>```</p>

<p>結果如下：</p>

<p>```sh
不 : 0
人 : 1
山 : 2
無 : 3
風 : 4
……
謏 : 5496
笮 : 5497
躠 : 5498
噆 : 5499</p>

<p>```</p>

<p>印出詞頻統計用的字典，給定某字，可查詢到其出現頻率：</p>

<p>```python
for w,wfreq in vocab_freq_dict.items():
    print “%s : %s”%(w,wfreq)</p>

<p>```</p>

<p>結果如下：</p>

<p>```sh
不 : 26426
人 : 20966
山 : 16056
無 : 15795
風 : 15618
…
謏 : 5
笮 : 5
躠 : 5
噆 : 5</p>

<p>```</p>

<h2 id="build-unigram-table">Build Unigram Table</h2>

<p>本例採用 <em>negative sampling</em> ，需要先建立 <em>unigram table</em> 以便進行 <em>negative sampling</em> 。</p>

<p>所謂的 <em>Unigram Table</em> 即是一個 <em>array</em> ，其中每個元素為某字的id，而某字的頻率，即為此id在此 <em>table</em> 中出現的次數的 0.75次方。</p>

<p>例如，id 為 5496 的字，詞頻為 5 ，則在此 <em>Unigram Table</em> 中，5496 的次數為：</p>

<script type="math/tex; mode=display">

5^{0.75} = 3.34 \approx 3

</script>

<p>由於 <em>array</em> 中的元素個數必須是整數，所以 5496 在 <em>Unigram Table</em> 中出現三次。</p>

<p>建立 <em>Unigram Table</em> 的程式碼如下：</p>

<p>```python
def InitUnigramTable(vocab_freq_dict):
    table_freq_list = map(lambda x: (x[0], int(x[1][1] ** 0.75)), enumerate(vocab_freq_dict.items()))
    table_size = sum([x[1] for x in table_freq_list])
    table = np.zeros(table_size).astype(int)
    offset = 0
    for item in table_freq_list:
        table[offset:offset + item[1]] = item[0]
        offset += item[1]</p>

<pre><code>return table
</code></pre>

<p>table = InitUnigramTable(vocab_freq_dict)</p>

<p>```</p>

<p>得出的 <em>Unigram Table</em> 如下：</p>

<p>```
[   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  <br />
0    0    0    0  … , 5495 5495 5495 5496  5496 5496 5497 5497 5497 5498 
5498 5498 5499 5499 5499]</p>

<p>```</p>

<h2 id="training-word2vec">Training word2vec</h2>

<p>```python
def train(vocab_dict, vocab_freq_dict, table):</p>

<pre><code>total_words = sum([x[1] for x in vocab_freq_dict.items()])
vocab_size = len(vocab_dict)

# 參數設定
layer1_size = 30 # hidden layer 的大小，即向量大小
window = 2 # 上下文寬度的上限
alpha_init = 0.025 # learning rate
sample = 0.001 # 用來隨機丟棄高頻字用
negative = 10 # negative sampling 的數量
ite = 2 # iteration 次數

# Weights 初始化
# syn0 : input layer 到 hidden layer 之間的 weights ，用隨機值初始化
# syn1 : hidden layer 到 output layer 之間的 weights ，用0初始化
syn0 = (0.5 - np.random.rand(vocab_size, layer1_size)) / layer1_size 
syn1 = np.zeros((layer1_size, vocab_size))

# 印出進度用
train_words = 0 # 總共訓練了幾個字
p_count = 0
avg_err = 0.
err_count = 0

for local_iter in range(ite):
    print "local_iter", local_iter
    f = open("poem.txt")
    for line in f.readlines():
        
        #用來暫存要訓練的字，一次訓練一個句子
        sen = []
        
        # 取出要被訓練的字
        for word_raw in line.decode("utf-8").strip().split():
            last_word = vocab_dict.get(word_raw, -1)
            
            # 丟棄字典中沒有的字（頻率太低）
            if last_word == -1:
                continue
            cn = vocab_freq_dict.get(word_raw)
            ran = (math.sqrt(cn / float(sample * total_words + 1))) * (sample * total_words) / cn
            
            # 根據字的頻率，隨機丟棄，頻率越高的字，越有機會被丟棄
            if ran &lt; random.random():
                continue
            train_words += 1
            
            # 將要被訓練的字加到 sen
            sen.append(last_word)
            
        # 根據訓練過的字數，調整 learning rate
        alpha = alpha_init * (1 - train_words / float(ite * total_words + 1))
        if alpha &lt; alpha_init * 0.0001:
            alpha = alpha_init * 0.0001
            
        # 逐一訓練 sen 中的字
        for a, word in enumerate(sen):
        
        		# 隨機調整 window 大小
            b = random.randint(1, window)
            for c in range(a - b, a + b + 1):
                
                # input 為 window 範圍中，上下文的某一字
                if c &lt; 0 or c == a or c &gt;= len(sen):
                    continue
                last_word = sen[c]
									
                # h_err 暫存 hidden layer 的 error 用
                h_err = np.zeros((layer1_size))
                
                # 進行 negative sampling
                for negcount in range(negative):
                
                		# positive example，從 sen 中取得，模型要輸出 1
                    if negcount == 0:
                        target_word = word
                        label = 1
                    
                    # negative example，從 table 中抽樣，模型要輸出 0 
                    else:
                        while True:
                            target_word = table[random.randint(0, len(table) - 1)]
                            if target_word not in sen:
                                break
                        label = 0
                    
                    # 模型預測結果
                    o_pred = 1 / (1 + np.exp(- np.dot(syn0[last_word, :], syn1[:, target_word])))
                    
                    # 預測結果和標準答案的差距
                    o_err = o_pred - label
                    
                    # backward propagation
                    # 此部分請參照 word2vec part2 的公式推導結果
                    
                    # 1.將 error 傳遞到 hidden layer                        
                    h_err += o_err * syn1[:, target_word]
                    
                    # 2.更新 syn1
                    syn1[:, target_word] -= alpha * o_err * syn0[last_word]
                    avg_err += abs(o_err)
                    err_count += 1
                
                # 3.更新 syn0
                syn0[last_word, :] -= alpha * h_err
                
                # 印出目前結果
                p_count += 1
                if p_count % 10000 == 0:
                    print "Iter: %s, Alpha %s, Train Words %s, Average Error: %s" \
                          % (local_iter, alpha, 100 * train_words, avg_err / float(err_count))
                    avg_err = 0.
                    err_count == 0.
                    
    # 每一個 iteration 儲存一次訓練完的模型
    model_name = "w2v_model_blog_%s.json" % (local_iter)
    print "save model: %s" % (model_name)
    fm = open(model_name, "w")
    fm.write(json.dumps(syn0.tolist(), indent=4))
    fm.close()
</code></pre>

<p>```</p>

<p>開始訓練：</p>

<p>```python
train(vocab_dict, vocab_freq_dict, table)</p>

<p>```</p>

<p>輸出結果如下，可以看到，當訓練過的字數增加時， Error 也跟著降低</p>

<p>大概要花幾十分鐘左右訓練完</p>

<p>```sh
Iter: 0, Alpha 0.0249923666923, Train Words 475200, Average Error: 0.499999254842
Iter: 0, Alpha 0.0249846739501, Train Words 954100, Average Error: 0.249998343836
Iter: 0, Alpha 0.0249771900316, Train Words 1420000, Average Error: 0.166660116256
Iter: 0, Alpha 0.0249693430813, Train Words 1908500, Average Error: 0.124949913475
Iter: 0, Alpha 0.024961329072, Train Words 2407400, Average Error: 0.0993522008349
Iter: 0, Alpha 0.0249531817368, Train Words 2914600, Average Error: 0.0787704454331
Iter: 0, Alpha 0.0249453540624, Train Words 3401900, Average Error: 0.06351951221
Iter: 0, Alpha 0.0249377801891, Train Words 3873400, Average Error: 0.0495117808015
……….</p>

<p>```</p>

<h2 id="show-result">Show Result</h2>

<p>檢視 word2vec 訓練結果的方法，即是看使用 <em>cosine similarity</em> 計算，是否能得出與某字語意相近的字。</p>

<p>```python</p>

<h1 id="section">讀取訓練好的模型</h1>
<p>f2 = open(“w2v_model_1.json”, “r”) 
w2v_model = np.array(json.loads(““.join(f2.readlines())))
f2.close()</p>

<p>vocab_dict_reversed = OrderedDict([(x[1], x[0]) for x in vocab_dict.items()])</p>

<h1 id="cosine-similarity-">計算 cosine similarity 最高的前五字</h1>
<p>def get_top(word):
    wid = vocab_dict.get(word)</p>

<pre><code># 將某字與模型中所有的字向量做內積
dot_result = np.dot(w2v_model, np.expand_dims(w2v_model[wid], axis=1))
norm = np.sqrt(np.sum(np.power(w2v_model.T, 2), axis=0))

# 計算 cosine similarity
cosine_result = np.divide(dot_result[:, 0], norm*norm[wid])

# 根據 cosine similarity 的值排序
final_result = sorted(filter(lambda x:x[0] != wid, 
                      [(x[0], x[1]) for x in enumerate(cosine_result)]),
                      key=lambda x: x[1], reverse=True)
print word

# 印出語意最接近的前五字，以及其 cosine similarity
for x in final_result[:5]:
    print vocab_dict_reversed.get(x[0]), x[1]
</code></pre>

<p>```</p>

<p>分別計算「山、峰、河、日」這四字語意最相近的字</p>

<p>```python
get_top(u”山”)
get_top(u”峰”)
get_top(u”河”)
get_top(u”日”)</p>

<p>```</p>

<p>結果如下，可看出，計算所得出語意最相近的字，實際上，語意也相近，例如，山和峰、嶺的語意都很接近。</p>

<p>```sh
山
嶺 0.854901128361
嵩 0.846620438864
峰 0.842831270385
岡 0.838129842909
嶂 0.834701215189
峰
山 0.842831270385
嶽 0.83917452917
嶺 0.8219837161
頂 0.821088331571
嶂 0.809565794884
河
湟 0.787726187693
涇 0.770652269018
淮 0.751135710239
川 0.742243126005
汾 0.740643816278
日
旦 0.869047480855
又 0.842383624714
曛 0.830549707539
夕 0.826327222048
暉 0.82616774597</p>

<p>```</p>

<p>向量加減運算後的 <em>cosine similarity</em> ，例如： 女 + 父 - 男 = 母</p>

<p>```python
def get_calculated_top(w1, w2, w3):
    wid1, wid2, wid3 = vocab_dict.get(w1), vocab_dict.get(w2), vocab_dict.get(w3)
    v1, v2, v3 = w2v_model[wid1], w2v_model[wid2], w2v_model[wid3]</p>

<pre><code># 得出加減運算後的向量
combined_vec = v1 + (v2 - v3)
dot_result = np.dot(w2v_model, np.expand_dims(combined_vec, axis=1))
norm = np.sqrt(np.sum(np.power(w2v_model.T, 2), axis=0))
cvec_norm = np.sqrt(np.sum(np.power(combined_vec, 2)))
cosine_result = np.divide(dot_result[:, 0], norm * cvec_norm)

final_result = sorted(filter(lambda x: x[0] not in [wid1, wid2, wid3],
                             [(x[0], x[1]) for x in enumerate(cosine_result)]),
                      key=lambda x: x[1], reverse=True)
print "%s + %s - %s" % (w1, w2, w3)
for x in final_result[:5]:
    print vocab_dict_reversed.get(x[0]), x[1]
</code></pre>

<p>```</p>

<p>```python
get_calculated_top(u”女”, u”父”, u”男”)</p>

<p>```</p>

<p>結果如下，如預期，運算結果的語意接近「母」：</p>

<p>```sh
女 + 父 - 男
母 0.731002049447
娥 0.707469857054
客 0.69027387716
娃 0.687831493041
侶 0.681667240226</p>

<p>```</p>
]]></content>
  </entry>
  
</feed>
