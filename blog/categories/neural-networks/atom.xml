<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Neural Networks | Mark Chang's Blog]]></title>
  <link href="http://ckmarkoh.github.io/blog/categories/neural-networks/atom.xml" rel="self"/>
  <link href="http://ckmarkoh.github.io/"/>
  <updated>2016-12-11T15:32:34+08:00</updated>
  <id>http://ckmarkoh.github.io/</id>
  <author>
    <name><![CDATA[Mark Chang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Word2vec (Part 3 : Implementation)]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/08/29/neural-network-word2vec-part-3-implementation/"/>
    <updated>2016-08-29T11:17:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/08/29/neural-network-word2vec-part-3-implementation</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>本文接續 <a href="/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation">word2vec (part2)</a> ，介紹如何根據推導出來的 <em>backward propagation</em> 公式，從頭到尾實作一個簡易版的 <em>word2vec</em> 。</p>

<p>本例的 input layer 採用 <em>skip-gram</em> ， output layer 採用 <em>negative sampling</em></p>

<p>本例用唐詩語料庫：https://github.com/ckmarkoh/coscup_nndl/blob/master/poem.txt</p>

<p>首先，載入所需的模組</p>

<p>```python
import json
from collections import Counter, OrderedDict
import numpy as np
import random
import math</p>

<p>```</p>

<!--more-->

<h2 id="build-dictionray">Build Dictionray</h2>

<p>再來是建立字典，即將每個字給一個id來對應。</p>

<p>```python
def LearnVocabFromTrainFile():</p>

<pre><code># 開啟唐詩語料庫
f = open("poem.txt")
 
# 統計唐詩語料庫中每個字出現的頻率
vcount = Counter()
for line in f.readlines():
    for w in line.decode("utf-8").strip().split():
        vcount.update(w)
        
# 僅保留出現次數大於五的字，並按照出現次數排序
vcount_list = sorted(filter(lambda x: x[1] &gt;= 5, vcount.items())
                     , reverse=True, key=lambda x: x[1])
                     
# 建立字典，將每個字給一個id ，字為 key, id 為 value
vocab_dict = OrderedDict(map(lambda x: (x[1][0], x[0]), enumerate(vcount_list)))

# 建立詞頻統計用的字典，給定某字，可查到其出現頻率
vocab_freq_dict = OrderedDict(map(lambda x: (x[0], x[1]), vcount_list))
return vocab_dict, vocab_freq_dict
</code></pre>

<p>vocab_dict, vocab_freq_dict =  LearnVocabFromTrainFile()</p>

<p>```</p>

<p>印出字典檔，每個字對應到一個id（編號）</p>

<p>```python
for w,wid in vocab_dict.items():
    print “%s : %s”%(w,wid)</p>

<p>```</p>

<p>結果如下：</p>

<p>```sh
不 : 0
人 : 1
山 : 2
無 : 3
風 : 4
……
謏 : 5496
笮 : 5497
躠 : 5498
噆 : 5499</p>

<p>```</p>

<p>印出詞頻統計用的字典，給定某字，可查詢到其出現頻率：</p>

<p>```python
for w,wfreq in vocab_freq_dict.items():
    print “%s : %s”%(w,wfreq)</p>

<p>```</p>

<p>結果如下：</p>

<p>```sh
不 : 26426
人 : 20966
山 : 16056
無 : 15795
風 : 15618
…
謏 : 5
笮 : 5
躠 : 5
噆 : 5</p>

<p>```</p>

<h2 id="build-unigram-table">Build Unigram Table</h2>

<p>本例採用 <em>negative sampling</em> ，需要先建立 <em>unigram table</em> 以便進行 <em>negative sampling</em> 。</p>

<p>所謂的 <em>Unigram Table</em> 即是一個 <em>array</em> ，其中每個元素為某字的id，而某字的頻率，即為此id在此 <em>table</em> 中出現的次數的 0.75次方。</p>

<p>例如，id 為 5496 的字，詞頻為 5 ，則在此 <em>Unigram Table</em> 中，5496 的次數為：</p>

<script type="math/tex; mode=display">

5^{0.75} = 3.34 \approx 3

</script>

<p>由於 <em>array</em> 中的元素個數必須是整數，所以 5496 在 <em>Unigram Table</em> 中出現三次。</p>

<p>建立 <em>Unigram Table</em> 的程式碼如下：</p>

<p>```python
def InitUnigramTable(vocab_freq_dict):
    table_freq_list = map(lambda x: (x[0], int(x[1][1] ** 0.75)), enumerate(vocab_freq_dict.items()))
    table_size = sum([x[1] for x in table_freq_list])
    table = np.zeros(table_size).astype(int)
    offset = 0
    for item in table_freq_list:
        table[offset:offset + item[1]] = item[0]
        offset += item[1]</p>

<pre><code>return table
</code></pre>

<p>table = InitUnigramTable(vocab_freq_dict)</p>

<p>```</p>

<p>得出的 <em>Unigram Table</em> 如下：</p>

<p>```
[   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  <br />
0    0    0    0  … , 5495 5495 5495 5496  5496 5496 5497 5497 5497 5498 
5498 5498 5499 5499 5499]</p>

<p>```</p>

<h2 id="training-word2vec">Training word2vec</h2>

<p>```python
def train(vocab_dict, vocab_freq_dict, table):</p>

<pre><code>total_words = sum([x[1] for x in vocab_freq_dict.items()])
vocab_size = len(vocab_dict)

# 參數設定
layer1_size = 30 # hidden layer 的大小，即向量大小
window = 2 # 上下文寬度的上限
alpha_init = 0.025 # learning rate
sample = 0.001 # 用來隨機丟棄高頻字用
negative = 10 # negative sampling 的數量
ite = 2 # iteration 次數

# Weights 初始化
# syn0 : input layer 到 hidden layer 之間的 weights ，用隨機值初始化
# syn1 : hidden layer 到 output layer 之間的 weights ，用0初始化
syn0 = (0.5 - np.random.rand(vocab_size, layer1_size)) / layer1_size 
syn1 = np.zeros((layer1_size, vocab_size))

# 印出進度用
train_words = 0 # 總共訓練了幾個字
p_count = 0
avg_err = 0.
err_count = 0

for local_iter in range(ite):
    print "local_iter", local_iter
    f = open("poem.txt")
    for line in f.readlines():
        
        #用來暫存要訓練的字，一次訓練一個句子
        sen = []
        
        # 取出要被訓練的字
        for word_raw in line.decode("utf-8").strip().split():
            last_word = vocab_dict.get(word_raw, -1)
            
            # 丟棄字典中沒有的字（頻率太低）
            if last_word == -1:
                continue
            cn = vocab_freq_dict.get(word_raw)
            ran = (math.sqrt(cn / float(sample * total_words + 1))) * (sample * total_words) / cn
            
            # 根據字的頻率，隨機丟棄，頻率越高的字，越有機會被丟棄
            if ran &lt; random.random():
                continue
            train_words += 1
            
            # 將要被訓練的字加到 sen
            sen.append(last_word)
            
        # 根據訓練過的字數，調整 learning rate
        alpha = alpha_init * (1 - train_words / float(ite * total_words + 1))
        if alpha &lt; alpha_init * 0.0001:
            alpha = alpha_init * 0.0001
            
        # 逐一訓練 sen 中的字
        for a, word in enumerate(sen):
        
        		# 隨機調整 window 大小
            b = random.randint(1, window)
            for c in range(a - b, a + b + 1):
                
                # input 為 window 範圍中，上下文的某一字
                if c &lt; 0 or c == a or c &gt;= len(sen):
                    continue
                last_word = sen[c]
									
                # h_err 暫存 hidden layer 的 error 用
                h_err = np.zeros((layer1_size))
                
                # 進行 negative sampling
                for negcount in range(negative):
                
                		# positive example，從 sen 中取得，模型要輸出 1
                    if negcount == 0:
                        target_word = word
                        label = 1
                    
                    # negative example，從 table 中抽樣，模型要輸出 0 
                    else:
                        while True:
                            target_word = table[random.randint(0, len(table) - 1)]
                            if target_word not in sen:
                                break
                        label = 0
                    
                    # 模型預測結果
                    o_pred = 1 / (1 + np.exp(- np.dot(syn0[last_word, :], syn1[:, target_word])))
                    
                    # 預測結果和標準答案的差距
                    o_err = o_pred - label
                    
                    # backward propagation
                    # 此部分請參照 word2vec part2 的公式推導結果
                    
                    # 1.將 error 傳遞到 hidden layer                        
                    h_err += o_err * syn1[:, target_word]
                    
                    # 2.更新 syn1
                    syn1[:, target_word] -= alpha * o_err * syn0[last_word]
                    avg_err += abs(o_err)
                    err_count += 1
                
                # 3.更新 syn0
                syn0[last_word, :] -= alpha * h_err
                
                # 印出目前結果
                p_count += 1
                if p_count % 10000 == 0:
                    print "Iter: %s, Alpha %s, Train Words %s, Average Error: %s" \
                          % (local_iter, alpha, 100 * train_words, avg_err / float(err_count))
                    avg_err = 0.
                    err_count == 0.
                    
    # 每一個 iteration 儲存一次訓練完的模型
    model_name = "w2v_model_blog_%s.json" % (local_iter)
    print "save model: %s" % (model_name)
    fm = open(model_name, "w")
    fm.write(json.dumps(syn0.tolist(), indent=4))
    fm.close()
</code></pre>

<p>```</p>

<p>開始訓練：</p>

<p>```python
train(vocab_dict, vocab_freq_dict, table)</p>

<p>```</p>

<p>輸出結果如下，可以看到，當訓練過的字數增加時， Error 也跟著降低</p>

<p>大概要花幾十分鐘左右訓練完</p>

<p>```sh
Iter: 0, Alpha 0.0249923666923, Train Words 475200, Average Error: 0.499999254842
Iter: 0, Alpha 0.0249846739501, Train Words 954100, Average Error: 0.249998343836
Iter: 0, Alpha 0.0249771900316, Train Words 1420000, Average Error: 0.166660116256
Iter: 0, Alpha 0.0249693430813, Train Words 1908500, Average Error: 0.124949913475
Iter: 0, Alpha 0.024961329072, Train Words 2407400, Average Error: 0.0993522008349
Iter: 0, Alpha 0.0249531817368, Train Words 2914600, Average Error: 0.0787704454331
Iter: 0, Alpha 0.0249453540624, Train Words 3401900, Average Error: 0.06351951221
Iter: 0, Alpha 0.0249377801891, Train Words 3873400, Average Error: 0.0495117808015
……….</p>

<p>```</p>

<h2 id="show-result">Show Result</h2>

<p>檢視 word2vec 訓練結果的方法，即是看使用 <em>cosine similarity</em> 計算，是否能得出與某字語意相近的字。</p>

<p>```python</p>

<h1 id="section">讀取訓練好的模型</h1>
<p>f2 = open(“w2v_model_1.json”, “r”) 
w2v_model = np.array(json.loads(““.join(f2.readlines())))
f2.close()</p>

<p>vocab_dict_reversed = OrderedDict([(x[1], x[0]) for x in vocab_dict.items()])</p>

<h1 id="cosine-similarity-">計算 cosine similarity 最高的前五字</h1>
<p>def get_top(word):
    wid = vocab_dict.get(word)</p>

<pre><code># 將某字與模型中所有的字向量做內積
dot_result = np.dot(w2v_model, np.expand_dims(w2v_model[wid], axis=1))
norm = np.sqrt(np.sum(np.power(w2v_model.T, 2), axis=0))

# 計算 cosine similarity
cosine_result = np.divide(dot_result[:, 0], norm*norm[wid])

# 根據 cosine similarity 的值排序
final_result = sorted(filter(lambda x:x[0] != wid, 
                      [(x[0], x[1]) for x in enumerate(cosine_result)]),
                      key=lambda x: x[1], reverse=True)
print word

# 印出語意最接近的前五字，以及其 cosine similarity
for x in final_result[:5]:
    print vocab_dict_reversed.get(x[0]), x[1]
</code></pre>

<p>```</p>

<p>分別計算「山、峰、河、日」這四字語意最相近的字</p>

<p>```python
get_top(u”山”)
get_top(u”峰”)
get_top(u”河”)
get_top(u”日”)</p>

<p>```</p>

<p>結果如下，可看出，計算所得出語意最相近的字，實際上，語意也相近，例如，山和峰、嶺的語意都很接近。</p>

<p>```sh
山
嶺 0.854901128361
嵩 0.846620438864
峰 0.842831270385
岡 0.838129842909
嶂 0.834701215189
峰
山 0.842831270385
嶽 0.83917452917
嶺 0.8219837161
頂 0.821088331571
嶂 0.809565794884
河
湟 0.787726187693
涇 0.770652269018
淮 0.751135710239
川 0.742243126005
汾 0.740643816278
日
旦 0.869047480855
又 0.842383624714
曛 0.830549707539
夕 0.826327222048
暉 0.82616774597</p>

<p>```</p>

<p>向量加減運算後的 <em>cosine similarity</em> ，例如： 女 + 父 - 男 = 母</p>

<p>```python
def get_calculated_top(w1, w2, w3):
    wid1, wid2, wid3 = vocab_dict.get(w1), vocab_dict.get(w2), vocab_dict.get(w3)
    v1, v2, v3 = w2v_model[wid1], w2v_model[wid2], w2v_model[wid3]</p>

<pre><code># 得出加減運算後的向量
combined_vec = v1 + (v2 - v3)
dot_result = np.dot(w2v_model, np.expand_dims(combined_vec, axis=1))
norm = np.sqrt(np.sum(np.power(w2v_model.T, 2), axis=0))
cvec_norm = np.sqrt(np.sum(np.power(combined_vec, 2)))
cosine_result = np.divide(dot_result[:, 0], norm * cvec_norm)

final_result = sorted(filter(lambda x: x[0] not in [wid1, wid2, wid3],
                             [(x[0], x[1]) for x in enumerate(cosine_result)]),
                      key=lambda x: x[1], reverse=True)
print "%s + %s - %s" % (w1, w2, w3)
for x in final_result[:5]:
    print vocab_dict_reversed.get(x[0]), x[1]
</code></pre>

<p>```</p>

<p>```python
get_calculated_top(u”女”, u”父”, u”男”)</p>

<p>```</p>

<p>結果如下，如預期，運算結果的語意接近「母」：</p>

<p>```sh
女 + 父 - 男
母 0.731002049447
娥 0.707469857054
客 0.69027387716
娃 0.687831493041
侶 0.681667240226</p>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Word2vec (Part 2 : Backward Propagation)]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation/"/>
    <updated>2016-07-12T09:21:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>本文接續 <a href="/blog/2016/07/12/neural-network-word2vec-part-1-overview">word2vec (part1)</a> ，介紹 <em>word2vec</em> 訓練過程的 <em>backward propagation</em> 公式推導。</p>

<p><em>word2vec</em> 的訓練過程中，輸出的結果，跟上下文有關的字，在 <em>output layer</em> 輸出為 1 ，跟上下文無關的字，在 <em>output layer</em> 輸出為 0。 在此，把跟上下文有關的，稱為 <em>positive example</em> ，而跟上下文無關的，稱為 <em>negative example</em> 。</p>

<p>根據 <a href="/blog/2016/07/12/neural-network-word2vec-part-1-overview">word2vec (part1)</a> 中提到的例子， <em>cat</em> 的向量為 <script type="math/tex">\textbf{v}_2</script> ， <em>run</em> 的向量為 <script type="math/tex">\textbf{w}_3</script> ， <em>fly</em> 的向量為 <script type="math/tex">\textbf{w}_4</script> ，由於 <em>cat</em> 的上下文有 <em>run</em> ，所以 <em>run</em> 為 <em>positive example</em> ，而 <em>cat</em> 的上下文沒有 <em>fly</em> ，所以 <em>fly</em> 為 <em>negative example</em> ，如下圖所示：</p>

<p><img src="/images/pic/pic_00187.png" alt="" /></p>

<!--more-->

<h2 id="objective-function">Objective Function</h2>

<p>訓練類神經網路需要有個目標函數，如果希望 <em>positive example</em> 輸出為 1 ， <em>negative example</em> 輸出為 0，則可以將以下函數 <script type="math/tex">J</script> 做最小化。</p>

<script type="math/tex; mode=display">

J = - \text{log}(\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{pos} }}) - \sum_{neg} \text{log} ( 1- \frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{neg} }} )

</script>

<p>其中 <script type="math/tex">\textbf{v}_I</script> 為輸入端的字向量，而 <script type="math/tex">\textbf{w}_{pos}</script> 和 <script type="math/tex">\textbf{w}_{neg}</script> 為輸出端的字向量。 <script type="math/tex">\textbf{w}_{pos}</script> 為 <em>positive example</em> ，而  <script type="math/tex">\textbf{w}_{neg}</script> 為 <em>negative example</em> 。通常，對於每筆 <script type="math/tex">\textbf{v}_I</script> 而言，會找一個 <em>positive example</em> 和多個 <em>negative example</em> ，因此用 <script type="math/tex">\sum</script> 將這些 <em>negative example</em> 算出的結果給加起來。</p>

<p>先看這公式前半部的部分：</p>

<script type="math/tex; mode=display">

-\text{log}(\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{pos} }})

</script>

<p>從以上公式得知，當 <script type="math/tex">\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{pos} }} = 0</script> 時，  <script type="math/tex">J</script> 會趨近無限大，而當 <script type="math/tex">\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{pos} }} = 1</script> 時 ，  <script type="math/tex">J</script> 會趨近 0 ，所以，降低 <script type="math/tex">J</script> 的值，會迫使  <script type="math/tex">\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{pos} }}</script> 接近 1 。</p>

<p>再來看另一部分：</p>

<script type="math/tex; mode=display">

-\text{log}(1 - \frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{neg} }})

</script>

<p>當 <script type="math/tex">\frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{neg} }} = 1</script> 時 <script type="math/tex">J</script> 會趨近無限大，反之亦然，同理，降低 <script type="math/tex">J</script> 的值，會迫使  <script type="math/tex">1- \frac{1}{1+e^{-\textbf{v}_I^T \textbf{w}_{neg} }}</script> 接近 0 。</p>

<h2 id="backward-propagation">Backward Propagation</h2>

<p>至於要怎麼調整 <script type="math/tex">\textbf{v}</script> 和  <script type="math/tex">\textbf{w}</script> 的值，才能讓 <script type="math/tex">J</script> 變小？ 就是要用到 <em>backward propagation</em> 。</p>

<h3 id="positive-example">Positive Example</h3>

<p>這邊先看 <em>positive example</em> 的部分， 輸入端為 <em>dog</em> ，輸出端為 <em>run</em> ，在進行偏微分公式推導之前，先定義一些符號，以便之後推導：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x_{13} = \textbf{v}_1 \cdot \textbf{w}_3 \\

& y_{13} = \dfrac{1}{1+e^{-x_{13}}}  \\

& J = - \text{log} (y_{13} )

\end{align}

 %]]&gt;</script>

<p>這些符號，如下圖所示， <script type="math/tex">x_{13}</script> 是 <em>output layer</em> 在 <em>sigmoid</em> 之前的值， <script type="math/tex">y_{13} </script> 是通過 <em>sigmoid</em> 後的值， <script type="math/tex">J</script> 是針對這筆 <em>positive example</em> 所算出來的 <em>cost function</em> 。</p>

<p><img src="/images/pic/pic_00188.png" alt="" /></p>

<p>如果要更新 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_3</script> 的值，讓 <script type="math/tex">J</script> 變小，就要用 <em>gradient descent</em> 的方式，過程如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{v}_1 \leftarrow \textbf{v}_1 - \eta \nabla_{\textbf{v}_1}  J  \\

& \textbf{w}_3 \leftarrow \textbf{w}_3 - \eta \nabla_{\textbf{w}_1}  J   

\end{align}

 %]]&gt;</script>

<p>想瞭解更多關於 <em>gradient descent</em> ，請參考：<a href="/blog/2015/12/23/optimization-method-adagrad">Gradient Descent &amp; AdaGrad </a></p>

<p>其中， <script type="math/tex">\eta</script> 為 <em>learning rate</em> ，為一常數，就是決定每一步要走多大，至於 <script type="math/tex">\nabla_{\textbf{v}_1} J </script> 這項要怎麼算？</p>

<p>先看看它每個維度上的值：</p>

<script type="math/tex; mode=display">

\nabla_{\textbf{v}_1} J = 

\begin{bmatrix}

\frac{\partial J}{\partial v_{11}} \\

\frac{\partial J}{\partial v_{12}} \\

\frac{\partial J}{\partial v_{13}} \\

\end{bmatrix}

</script>

<p>先看 <script type="math/tex">\frac{\partial J}{\partial v_{11}}</script> 這項，可以用 <em>chain rule</em> 把它拆開：</p>

<script type="math/tex; mode=display">

\frac{\partial J}{\partial v_{11} } = \frac{\partial J}{\partial y_{13}} \times \frac{\partial y_{13}}{\partial x_{13}}  \times \frac{\partial x_{13}}{\partial v_{11}}

</script>

<p>將 <script type="math/tex">\frac{\partial J}{\partial v_{11} }</script> 拆成 <script type="math/tex">\frac{\partial J}{\partial y_{13}}</script> 、 <script type="math/tex">\frac{\partial y_{13}}{\partial x_{13}}</script> 和 <script type="math/tex">\frac{\partial x_{13}}{\partial v_{11}}</script> 這三項。而這三項的值可分別求出來：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \frac{\partial J}{\partial y_{13}} = \frac{\partial   - \text{log} (y_{13} )}{\partial y_{13}} = \frac{-1}{y_{13}} \\ 

& \frac{\partial y_{13}}{\partial x_{13}} = \frac{\partial (\frac{1}{1+e^{-x_{13}}})}{\partial x_{13}} = \frac{1}{1+e^{-x_{13}}}( 1- \frac{1}{1+e^{-x_{13}}}) = y_{13} ( 1- y_{13}) \\

& \frac{\partial x_{13}}{\partial v_{11}} = \frac{\partial \textbf{v}_{1} \cdot \textbf{w}_3} {\partial v_{11}} = w_{31}


\end{align}

 %]]&gt;</script>

<p>代回這三項的結果到 <script type="math/tex">\frac{\partial J}{\partial v_{11} }</script> ，得出：</p>

<script type="math/tex; mode=display">

\frac{\partial J}{\partial v_{11} } = \frac{-1}{y_{13}} \times y_{13} ( 1- y_{13}) \times  w_{31} = ( y_{13} - 1)  \times w_{31}

</script>

<p>而 <script type="math/tex">\frac{\partial J}{\partial v_{12}}</script> 和 <script type="math/tex">\frac{\partial J}{\partial v_{13}}</script> 也可用同樣方式得出其值， 如下：</p>

<script type="math/tex; mode=display">

\nabla_{\textbf{v}_1} J = 

\begin{bmatrix}

\frac{\partial J}{\partial v_{11}} \\

\frac{\partial J}{\partial v_{12}} \\

\frac{\partial J}{\partial v_{13}} \\

\end{bmatrix}

=

\begin{bmatrix}

 ( y_{13} - 1)  \times w_{31} \\

 ( y_{13} - 1)  \times w_{32} \\

 ( y_{13} - 1)  \times w_{33} \\

 \end{bmatrix}

 = 

  ( y_{13} - 1)  \textbf{w}_3

</script>

<p>因此，可得出 <script type="math/tex">\textbf{v}_1</script> 要調整的量：</p>

<script type="math/tex; mode=display">

\textbf{v}_1 \leftarrow \textbf{v}_1 - \eta  ( y_{13} - 1)  \textbf{w}_3

</script>

<p>同理， <script type="math/tex">\textbf{w}_3</script> 要調整的量：</p>

<script type="math/tex; mode=display">

\textbf{w}_3 \leftarrow \textbf{w}_3 - \eta  ( y_{13} - 1)   \textbf{v}_1

</script>

<p>其中，可以把 <script type="math/tex">( y_{13} - 1)</script> 看成是，模型預測出來的，和我們所想要的值，差距多少。因為在 <em>positive example</em> 的情況下，我們希望模型輸出結果 <script type="math/tex">y_{13}</script> 為 1 。如果  <script type="math/tex">y_{13} = 1</script> 表示模型預測對了，則不需要修正 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_3</script> ，如果  <script type="math/tex">y_{13} \neq 1</script> 時，才要修正 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_3</script> 。</p>

<p>還有，之所以把這過程，稱為 <em>backward propagation</em> ，是因為可以把 <em>chain rule</em> 拆解 <script type="math/tex">\nabla_{\textbf{v}_1} J </script> 的過程，看成是將 <script type="math/tex">\frac{\partial J}{\partial y_{13} }</script> 的值， 由 <em>output layer</em> 往前傳遞，如下圖：</p>

<p><img src="/images/pic/pic_00189.png" alt="" /></p>

<p>想瞭解更多關於 <em>backward propagation</em> 的推導，請參考： <a href="/blog/2015/05/28/neural-network-backward-propagation">Backward Propagation 詳細推導過程 </a></p>

<h3 id="negative-example">Negative Example</h3>

<p>再來看看 <em>negative example</em> 的部分， 輸入端為 <em>dog</em> ，輸出端為 <em>fly</em> ，在進行偏微分公式推導之前，先定義一些符號，以便之後推導：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x_{14} = \textbf{v}_1 \cdot \textbf{w}_4 \\

& y_{14} = \dfrac{1}{1+e^{-x_{14}}}  \\

& J = - \text{log} (1 - y_{14} )

\end{align}

 %]]&gt;</script>

<p>這些符號，如下圖所示， <script type="math/tex">x_{14}</script> 是 <em>output layer</em> 在 <em>sigmoid</em> 之前的值， <script type="math/tex">y_{14} </script> 是通過 <em>sigmoid</em> 後的值， <script type="math/tex">J</script> 是針對這筆 <em>positive example</em> 所算出來的 <em>cost function</em> 。</p>

<p><img src="/images/pic/pic_00190.png" alt="" /></p>

<p>同之前 <em>positive example</em> ，如果要更新 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_4</script> 的值，讓 <script type="math/tex">J</script> 變小，就要用 <em>gradient descent</em> 的方式：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{v}_1 \leftarrow \textbf{v}_1 - \eta \nabla_{\textbf{v}_1}  J  \\

& \textbf{w}_4 \leftarrow \textbf{w}_4 - \eta \nabla_{\textbf{w}_1}  J   

\end{align}

 %]]&gt;</script>

<p>剩下的推導和 <em>positive example</em> 時，幾乎一樣，只有 <script type="math/tex">J</script> 不一樣。此處只需推導 <script type="math/tex">\frac{\partial J}{\partial y_{14}}</script> 的結果。</p>

<script type="math/tex; mode=display">

\frac{\partial J}{\partial y_{14}} = \frac{\partial   - \text{log} (1 - y_{14} )}{\partial y_{14}} = \frac{1}{1 - y_{14}} 

</script>

<p>代回此結果到 <script type="math/tex">\frac{\partial J}{\partial v_{11} }</script> ，得出：</p>

<script type="math/tex; mode=display">

\frac{\partial J}{\partial v_{11} } = \frac{1}{ 1- y_{14}} \times y_{14} ( 1- y_{14}) \times  w_{41} = ( y_{14} - 0)  \times w_{41}

</script>

<p>於是可以得出要修正的量：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{v}_1 \leftarrow \textbf{v}_1 - \eta  ( y_{14} - 0)  \textbf{w}_4 \\ 

& \textbf{w}_4 \leftarrow \textbf{w}_4 - \eta  ( y_{14} - 0)   \textbf{v}_1

\end{align}

 %]]&gt;</script>

<p>其中，可以把 <script type="math/tex">( y_{14} - 0)</script> 看成是，模型預測出來的，和我們所想要的值，差距多少。因為在 <em>negative example</em> 的情況下，我們希望模型輸出結果 <script type="math/tex">y_{14}</script> 為 0 。如果  <script type="math/tex">y_{13} = 0</script> 表示模型預測對了，則不需要修正 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_4</script> ，如果  <script type="math/tex">y_{14} \neq 0</script> 時，才要修正 <script type="math/tex">\textbf{v}_1</script> 和 <script type="math/tex">\textbf{w}_4</script> 。</p>

<h2 id="further-reading">Further Reading</h2>

<p>關於如何從無到有，不使用任何自動微分的套件，實作一個 <em>word2vec</em> ，請看：<a href="/blog/2016/08/29/neural-network-word2vec-part-3-implementation">word2vec (part3)</a></p>

<p>註： 實際上， <em>word2vec</em> 的 <em>input layer</em> 和 <em>output layer</em> 各有兩種架構。<em>input layer</em> 有 <em>cbow</em> 和 <em>skip-gram</em> 兩種，<em>output layer</em> 有 <em>hierarchical sofrmax</em> 和 <em>negative sampling</em> 兩種，本文所寫的為 <em>skip-gram</em> 搭配 <em>negative sampling</em> 的架構。關於 <em>hierarchical softmax</em> 請參考<a href="/blog/2015/05/23/-hierarchical-probabilistic-neural-networks-neural-network-language-model">hierarchical sofrmax</a> 。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Word2vec (Part 1 : Overview)]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/07/12/neural-network-word2vec-part-1-overview/"/>
    <updated>2016-07-12T09:19:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/07/12/neural-network-word2vec-part-1-overview</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>文字的語意可以用向量來表示，在上一篇 <a href="/blog/2016/07/10/nlp-vector-space-semantics"> Vector Space of Semantics </a> 中提到，如果把每種字當成一個維度，假設總共有一萬總字，那向量就會有一萬個維度。有兩種方法可降低維度，分別是 <em>singular value decomposition</em> 和 <em>word2vec</em> 。</p>

<p>本文講解 <em>word2vec</em> 的原理。 <em>word2vec</em> 流程，總結如下：</p>

<p><img src="/images/pic/pic_00191.png" alt="" /></p>

<p>首先，將文字做 <em>one-hot encoding</em> ，然後再用 <em>word2vec</em> 類神經網路計算，求出壓縮後（維度降低後）的語意向量。</p>

<!--more-->

<h2 id="one-hot-encoding">One-Hot Encoding</h2>

<p>一開始，不知道哪個字和哪個字語意相近，所以就假設每個字的語意是不相干的。也就是說，每個字的向量都是互相垂直。</p>

<p>這邊舉個比較簡單的例子，假設總字彙量只有 4 個， 分別為 <em>dog, cat, run, fly</em> ，那麼，經過 <em>one-hot encoding</em> 的結果如下：</p>

<p><img src="/images/pic/pic_00192.png" alt="" /></p>

<p>如上圖， <em>dog</em> 的向量為 (1,0,0,0) ，只有在第一個維度是 1 ，其他維度是 0 ，而 <em>cat</em> 的向量為 (0,1,0,0) 只有在第一個維度是 1 ，其他維度是 0 。</p>

<p>也就是說，每個字都有一個代表它的維度，而它 <em>one hot encoding</em> 的結果，只有在那個維度上是 1 ，其他維度都是 0 。這樣的話，任意兩個 <em>one hot encoding</em> 的向量內積結果，都會是 0 ，內積結果為 0 ，表示兩向量是垂直的。</p>

<p>註：實際應用中，字彙量即是語料庫中的單字種類，通常會有幾千個甚至一萬個以上。</p>

<h2 id="word2vec">word2vec</h2>

<p><em>word2vec</em> 的神經網路架構如下，總共有三層， <em>input layer</em> 和 <em>output layer</em> 一樣大，中間的 <em>hidden layer</em> 比較小。</p>

<p><img src="/images/pic/pic_00193.png" alt="" /></p>

<p>如上圖，總字彙量有 4 個，那麼 <em>input layer</em> 和 <em>output layer</em> 的維度為 4， 每個維度分別代表一個字。 如果想要把向量維度降至三維， <em>hidden layer</em> 的維度為 3。</p>

<p>另外要注意的是， <em>hidden layer</em> 沒有非線性的 <em>activation funciton</em> ，而 <em>output layer</em> 的 <em>activation function</em> 是 <em>sigmoid</em> ，這兩點會有什麼影響，之後會提到。</p>

<p>其中，在 <em>input layer</em> 和 <em>hidden layer</em> 之間， 有 <script type="math/tex">4 \times 3</script> 個 <em>weight</em> ，在此命名為 <script type="math/tex">\textbf{V}</script> ，而在 <em>hidden layer</em> 到 <em>output layer</em> 之間， 有 <script type="math/tex">3 \times 4</script> 個 <em>weight</em> ，在此命名為 <script type="math/tex">\textbf{W}</script> 。</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\textbf{V}=

\begin{bmatrix}

    v_{11} & v_{12} & v_{13}  \\

    v_{21} & v_{22} & v_{23}  \\

    v_{31} & v_{32} & v_{33}  \\

    v_{41} & v_{42} & v_{43}  \\

\end{bmatrix}

\mspace{30mu}

\textbf{W}^T=

\begin{bmatrix}

    w_{11} & w_{12} & w_{13}  \\

    w_{21} & w_{22} & w_{23}  \\

    w_{31} & w_{32} & w_{33}  \\

    w_{41} & w_{42} & w_{43}  \\

\end{bmatrix}

 %]]&gt;</script>

<p>由於 <em>input</em> 是 <em>one hot encoding</em> 的向量，又因為 <em>hidden layer</em> 沒有 <em>sigmoid</em> 之類的非線性 <em>activation function</em>。 輸入到類神經網路後，在 <em>hidden layer</em> 所取得的值，即是 <script type="math/tex">\textbf{V}</script> 中某個橫排的值，如下：</p>

<p><img src="/images/pic/pic_00194.png" alt="" /></p>

<p>例如，輸入的是 <em>dog</em> 的 <em>one hot encoding</em> ，只有在第 1 個維度是 1 ，與 <script type="math/tex">\textbf{V}</script> 作矩陣相乘後，在 <em>hidden layer</em> 取得的值是 <script type="math/tex">\textbf{V}</script> 中的第一個橫排： <script type="math/tex">(v_{11}, v_{12}, v_{13})</script> ，這個向量就是 <em>dog</em> 壓縮後的語意向量。運算過程如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{bmatrix}

1 & 0 & 0 & 0 

\end{bmatrix}

\begin{bmatrix}

    v_{11} & v_{12} & v_{13}  \\

    v_{21} & v_{22} & v_{23}  \\

    v_{31} & v_{32} & v_{33}  \\

    v_{41} & v_{42} & v_{43}  \\

\end{bmatrix}

= 

\begin{bmatrix}

v_{11} & v_{12} & v_{13}

\end{bmatrix}

 %]]&gt;</script>

<p>因此， <script type="math/tex">\textbf{V}</script> 中的某個橫排，就是某個字的語意向量。從反方向來看，由於 <em>output layer</em> 也是對應到字彙的 <em>one hot encoding</em> 因此， <script type="math/tex">\textbf{W}^T</script> 中的某個橫排，就是某個字的語意向量。</p>

<p>所以，一個字分別在 <script type="math/tex">\textbf{V}</script> 和 <script type="math/tex">\textbf{W}</script> 中各有一個語意向量。但通常會選擇 <script type="math/tex">\textbf{V}</script> 中的語意向量，作為 <em>word2vec</em> 的輸出結果。</p>

<h2 id="initializing-word2vec">Initializing word2vec</h2>

<p>至於如何訓練這個類神經網路？ 訓練一個類神經網路的過程，第一步就是要先將 <em>weight</em> 作初始化。初始化即是隨機給每個 <em>weight</em> 不同的數值，這些數值介於 <script type="math/tex"> -N \sim N</script> 之間。</p>

<p>因此，在還沒開始訓練之前，這些向量的方向都是隨機的，跟語意無關。</p>

<p>舉 <em>dog</em> 和 <em>cat</em> 在 <script type="math/tex">\textbf{V}</script> 中的向量，為 <script type="math/tex">\textbf{V}_1,\textbf{V}_2</script> ，以及 <em>run</em> 和 <em>fly</em> 在 <script type="math/tex">\textbf{W}</script> 中的向量 為 <script type="math/tex">\textbf{W}_3,\textbf{W}_4</script> ，為例：</p>

<p><img src="/images/pic/pic_00195.png" alt="" /></p>

<p>由於 <script type="math/tex">\textbf{V}</script> 和 <script type="math/tex">\textbf{W}</script> 都是隨機初始化的，因此 <script type="math/tex">\textbf{V}_1, \textbf{V}_2, \textbf{W}_3, \textbf{W}_4 </script> 這些向量的方向都是隨機的，跟語意無關，如下圖所示：</p>

<p><img src="/images/pic/pic_00196.png" alt="" /></p>

<h2 id="training-word2vec">Training word2vec</h2>

<p>訓練 <em>word2vec</em> 的目的，是希望讓語意向量真的跟語意有關。，在上一篇 <a href="/blog/2016/07/10/nlp-vector-space-semantics"> Vector Space of Semantics </a> 中提到，某字的語意，可從其上下文有哪些字來判斷。因此，可以用某字上下文的字，來做訓練，讓語意向量能抓到文字的語意。</p>

<p>若 <em>dog</em> 的上下文中有 <em>run</em> ， 令 <em>dog</em> 為 <em>word2vec</em> 的 <em>input</em> ， <em>run</em> 為 <em>output</em> 則輸入類神經網路後，在 <em>run</em> 的位置，在經過 <em>sigmoid</em> 之前，得到的結果是 <script type="math/tex">\textbf{V}_1</script> 和 <script type="math/tex">\textbf{W}_3</script> 的內積。經過了，<em>sigmoid</em> ，得到的值為：</p>

<script type="math/tex; mode=display">

\frac{1}{1+e^{- \textbf{V}_1 \cdot \textbf{W}_3}}

</script>

<p>由於 <em>run</em> 出現在 <em>dog</em> 的上下文中，所以要訓練類神經網路，在 <em>run</em> 位置可以輸出 1，如下：</p>

<script type="math/tex; mode=display">

\frac{1}{1+e^{- \textbf{V}_1 \cdot \textbf{W}_3}} \approx 1

</script>

<p>過程如下圖所示：</p>

<p><img src="/images/pic/pic_00197.png" alt="" /></p>

<p>根據上圖，如果要讓 <em>run</em> 的位置輸出為 1 ，則 <script type="math/tex">\textbf{V}_1</script> 和 <script type="math/tex">\textbf{W}_3</script> 的內積要越大越好。</p>

<p>內積要大，就是向量角度要越小，訓練過程中，會修正這兩個向量的角度，如下圖：</p>

<p><img src="/images/pic/pic_00198.png" alt="" /></p>

<p>上圖左方為先正之前，各向量的方向，上圖右方為修正之後的方向，其中，深藍色為修正後的，淺藍色為修正前的，畫在一起以便作比較。修正完後， <script type="math/tex">\textbf{V}_1</script> 和 <script type="math/tex">\textbf{W}_3</script> 的角度又更接近了。</p>

<p>同理，若 <em>cat</em> 的上下文中有 <em>run</em> ，則用 <em>word2vec</em> 做同樣訓練，如下圖：</p>

<p><img src="/images/pic/pic_00199.png" alt="" /></p>

<p>修正向量的角度後，<script type="math/tex">\textbf{V}_2</script> 和 <script type="math/tex">\textbf{W}_3</script> 的角度會更接近，結果如下圖：</p>

<p><img src="/images/pic/pic_00200.png" alt="" /></p>

<p>不過，以上訓練方法有個問題，就是訓練完後， <em>所有的向量都會位於同一條直線上，而無法分辨出每個字語意的差異</em> 。如果要讓 <em>word2vec</em> 學會分辨語意的差異，就需要加入反例，也就是 <em>不是出現在上下文的字</em> 。</p>

<p>如果 <em>dog</em> 的上下文中，不會出現 <em>fly</em> ， <em>fly</em> 的向量為 <script type="math/tex">\textbf{W}_4</script> ，將 <em>dog</em> 輸入類神經網路後，在 <em>fly</em> 的位置，訓練其輸出結果為 0 ，如下：</p>

<script type="math/tex; mode=display">

\frac{1}{1+e^{- \textbf{V}_1 \cdot \textbf{W}_3}} \approx 0

</script>

<p>如下圖所示：</p>

<p><img src="/images/pic/pic_00201.png" alt="" /></p>

<p>如果要讓輸出結果接近 0，則 <script type="math/tex">\textbf{V}_1</script> 和 <script type="math/tex">\textbf{W}_4</script> 的內積要越小越好，也就是說，它們之間的角度要越大越好。修正這兩個向量的角度，如下圖：</p>

<p><img src="/images/pic/pic_00202.png" alt="" /></p>

<p>同理，若 <em>cat</em> 的上下文中沒有 <em>fly</em> ，則訓練其輸出 0 ：</p>

<p><img src="/images/pic/pic_00203.png" alt="" /></p>

<p>修正  <script type="math/tex">\textbf{V}_2</script> 和 <script type="math/tex">\textbf{W}_4</script> 的夾角，如下圖：</p>

<p><img src="/images/pic/pic_00204.png" alt="" /></p>

<p>訓練後，得出的這些語意向量，語意相近的，夾角越小，語意相差越遠的，夾角越大，如下圖：</p>

<p><img src="/images/pic/pic_00205.png" alt="" /></p>

<h2 id="further-reading">Further Reading</h2>

<ol>
  <li>
    <p><em>word2vec</em> 的 <em>backward propagation</em> 公式要怎麼推導，請看：<a href="/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation">word2vec (part2 : Backward Propagation)</a></p>
  </li>
  <li>
    <p>如何從無到有，不使用任何自動微分的套件，實作一個 <em>word2vec</em> ，請看：<a href="/blog/2016/08/29/neural-network-word2vec-part-3-implementation">word2vec (part3 : Implementation)</a></p>
  </li>
</ol>

<p>註： 實際上， <em>word2vec</em> 的 <em>input layer</em> 和 <em>output layer</em> 各有兩種架構。<em>input layer</em> 有 <em>cbow</em> 和 <em>skip-gram</em> 兩種，<em>output layer</em> 有 <em>hierarchical sofrmax</em> 和 <em>negative sampling</em> 兩種，本文所寫的為 <em>skip-gram</em> 搭配 <em>negative sampling</em> 的架構。關於 <em>hierarchical softmax</em> 請參考<a href="/blog/2015/05/23/-hierarchical-probabilistic-neural-networks-neural-network-language-model">hierarchical sofrmax</a> 。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Neural Turing Machine]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/10/26/neural-network-neural-turing-machine/"/>
    <updated>2015-10-26T16:25:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/10/26/neural-network-neural-turing-machine</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p><em>Recurrent Neural Network</em> 在進行 <em>Gradient Descent</em> 的時候，會遇到所謂的 <em>Vanishing Gradient Problem</em> ，也就是說，在後面時間點的所算出的修正量，要回傳去修正較前面時間的參數值，此修正量會隨著時間傳遞而衰減。</p>

<p>為了改善此問題，可以用類神經網路模擬記憶體的構造，把前面神經元所算出的值，儲存起來。例如： <em>Long Short-term Memory (LSTM)</em> 即是模擬記憶體讀寫的構造，將某個時間點算出的值給儲存起來，等需要用它的時候再讀出來。</p>

<p>除了模擬單一記憶體的儲存與讀寫功能之外，也可以用類神經網路的構造來模擬 <em>Turing Machine</em> ，也就是說，有個 <em>Controller</em> ，可以更精確地控制，要將什麼值寫入哪一個記憶體區塊，或讀取哪一個記憶體區塊的值，這種類神經網路模型，稱為 <em>Neural Turing Machine</em> 。</p>

<p>如果可以模擬 <em>Turing Machine</em> ，即表示可以學會電腦能做的事。也就是說，這種機器學習模型可以學會電腦程式的邏輯控制與運算。</p>

<h2 id="neural-turing-machine">Neural Turing Machine</h2>

<p><em>Neural Turing Machine</em> 的架構如下：</p>

<p><img src="/images/pic/pic_00100.jpeg" alt="Neural Turing Machine" /></p>

<!--more-->

<p>可分為幾個部分：</p>

<p><strong>Input:</strong> 從外部輸入的值。</p>

<p><strong>Output:</strong> 輸出到外部的值。</p>

<p><strong>Controller:</strong> 相當於電腦的IO和CPU，可以從外部輸入值，或從記憶體讀取值，經過運算，再將算出的結果輸出去，或寫入記憶體， <em>Controller</em> 可以用 <em>feed forward neural network</em> 或者 <em>recurrent neural network</em> （相當於有register的CPU）來模擬。</p>

<p><strong>Read/Write Head:</strong> 記憶體的讀寫頭，相當於pointer ，是要被讀取或被寫入的記憶體的address。</p>

<p><strong>Memory:</strong> 記憶體，相當於電腦的RAM，同一個地址可對應到一整排的記憶體單位，就像電腦一樣，用8個bit組成的一個byte，具有同一個memory address。</p>

<p>以下細講每一部份的數學模型。</p>

<h3 id="memory">Memory</h3>

<p><em>memory</em> 是一個二維陣列。如下圖，一個 <em>memory block</em> 是由數個 <em>memory cell</em> 所構成。同一個 <em>block</em> 中的 <em>cell</em> 有相同的 <em>address</em> 。如下圖中，共有 <script type="math/tex">n</script> 個 <em>block</em> ， 每個 <em>block</em> 有 <script type="math/tex">m</script> 個 <em>cell</em> 。</p>

<p><img src="/images/pic/pic_00101.jpeg" alt="" /></p>

<p>操作 <em>Memory</em> 的動作有三種：即 <em>Read</em> ， <em>Erase</em> 和 <em>Add</em> 。</p>

<h4 id="read">Read</h4>

<p><em>Read</em> 是將記憶體裡面的值，讀出來，並傳給 <em>controller</em> 。由於記憶體有很多個 <em>memory block</em> ，至於要讀取哪個，由讀寫頭（ <em>Read/Write Head</em> ）來控制，讀寫頭為一個向量 <script type="math/tex">\textbf{w}</script> ，其數值表示要讀取記憶體位置的權重，滿足以下條件：</p>

<script type="math/tex; mode=display">

\sum_{i}w(i) = 1 \\

 0 \leq w(i) \leq 1, \forall i 

</script>

<p>讀寫頭內部各元素 <script type="math/tex">w_{i}</script> 的值介於 0 到 1 之間，且加起來的和為 1 ，這可解釋為，讀寫頭存在的位置，是用機率來表示。而讀出來的值，為記憶體區塊所儲存的值，乘上讀寫頭在此區塊 <script type="math/tex">i</script> 的機率 <script type="math/tex">w(i)</script> ，所得出之期望值，如下：</p>

<script type="math/tex; mode=display">

\textbf{r} \leftarrow \sum_{i}w(i)\textbf{M}(i) \mspace{40mu} \text{(1)}

</script>

<p>其中，<script type="math/tex">\textbf{r}</script> 為 <em>Read vector</em> ，即從記憶體讀出來的值，  <script type="math/tex">\textbf{M(i)}</script> 為記憶體 <script type="math/tex">i</script> 區塊的值， 而  <script type="math/tex">w(i)</script> 為讀寫頭 <script type="math/tex">w</script> 在區塊 <script type="math/tex">i</script> 的機率。</p>

<p>例如下圖中， <script type="math/tex">w(0) = 0.9</script> ， <script type="math/tex">w(1) = 0.1</script> ，即表示，讀寫頭在位置 0 的機率為 0.9，在位置 1 的機率為 0.1 。</p>

<p><img src="/images/pic/pic_00102.jpeg" alt="Read" /></p>

<p>將上圖中記憶體內部的值 <script type="math/tex">\textbf{M(i)}</script> ，以及讀寫頭位置的值 <script type="math/tex">w(i)</script> ，代入公式(1)，即可得出</p>

<script type="math/tex; mode=display">

\begin{bmatrix}

      r_{0}  \\[0.3em]

      r_{1}  \\[0.3em]

      r_{2}  \\[0.3em]

    \end{bmatrix}

=\begin{bmatrix}

      1*0.9+2*0.1  \\[0.3em]

      1*0.9+1*0.1  \\[0.3em]

      2*0.9+4*0.1  \\[0.3em]

    \end{bmatrix}

＝

\begin{bmatrix}

      1.1  \\[0.3em]

      1.0  \\[0.3em]

      2.2  \\[0.3em]

    \end{bmatrix}

</script>

<h4 id="erase">Erase</h4>

<p>如果要刪除記憶體內部的值，則要進行 <em>Erase</em> ，過程跟 <em>Read</em> 類似，都需要用讀寫頭 <em>w</em> 來控制。但刪除的動作，需要控制去刪除掉哪個 <em>memory cell</em> 的值，而不是一次就把整個 <em>memory block</em> 的值都刪除。所以需要另一個 <em>erase vector</em> <script type="math/tex">\textbf{e}</script> 來選擇要被刪除的 <em>cell</em> 。 <em>erase vector</em> 為一向量，如下：</p>

<script type="math/tex; mode=display">

 0 \leq e(j) \leq 1,  \mspace{10mu} 0 \leq j \leq m-1 , \mspace{10mu} \forall j 

</script>

<p>其中， <script type="math/tex">j</script> 為一個介於 0~m-1 之間的數， m 為 <em>block size</em> 。向量元素的值 <script type="math/tex">e(j)</script> 介於 0~1 之間。如果值為1，則表示要清空這個 <em>cell</em> 的值，若為 0 則表示保留 <em>cell</em> 原本的值， <em>erase</em> 的公式如下：</p>

<script type="math/tex; mode=display">

\textbf{M}(i) \leftarrow (1-w(i) \textbf{e} ) \textbf{M}(i) \mspace{40mu} \text{(2)}


</script>

<p>其中， <script type="math/tex">\textbf{w}</script> 是用來控制要清除哪個 <em>memory block</em> 而 <script type="math/tex">\textbf{e}</script> 是要控制清除這個 <em>block</em> 裡面的哪些 <em>cell</em> ，如下圖所示：</p>

<p><img src="/images/pic/pic_00103.jpeg" alt="Erase" /></p>

<p>上圖中，根據 <script type="math/tex">\textbf{w}</script> 和 <script type="math/tex">\textbf{e}</script> 這兩個向量所選擇的結果， 在 <script type="math/tex">\textbf{M}</script> 中，共有四個 <em>cell</em> 的值被削減了，分別位於左上角和左下角，用較明亮的背景色表示其位置。</p>

<p>將上圖中 <script type="math/tex">\textbf{w}</script> 、 <script type="math/tex">\textbf{e}</script> 和 <script type="math/tex">\textbf{M}</script> 中的值，代入公式(2) ，可得出此結果，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


M= 

\begin{bmatrix}

      1(1-0.9) & 2(1-0.1) & 3 & ...  \\[0.3em]

      1 & 1 & 2 & ...  \\[0.3em]

      2(1-0.9) & 4(1-0.1) & 1 & ...  \\[0.3em]

    \end{bmatrix}

=\begin{bmatrix}

      0.1 & 1.8 & 3 & ...  \\[0.3em]

      1 & 1 & 2 & ...  \\[0.3em]

      0.2 & 3.6 & 1 & ...  \\[0.3em]

    \end{bmatrix}


 %]]&gt;</script>

<h4 id="add">Add</h4>

<p>將新的值寫入記憶體的動作為 <em>add</em> 。之所以稱為 <em>add</em> （而非 <em>write</em> ）因為這個動作是會把記憶體內原本的值，再「加上」要寫入的值。至於要把哪些值加到記憶體，則需要有一個 <em>add vector</em> ，其維度和 <em>memory block</em> 的大小 <script type="math/tex">m</script> 相同。 <em>Add</em> 的公式如下：</p>

<script type="math/tex; mode=display">

\textbf{M}(i) \leftarrow  \textbf{M}(i)  + w(i) \textbf{a} \mspace{40mu} \text{(3)}

</script>

<p>過程如下圖所示：</p>

<p><img src="/images/pic/pic_00104.jpeg" alt="Add" /></p>

<p>上圖中，位於 <script type="math/tex">M</script> 的左上角，共有四個 <em>cell</em> 的值被增加了。</p>

<p>將上圖中 <script type="math/tex">\textbf{w}</script> 、 <script type="math/tex">\textbf{a}</script> 和 <script type="math/tex">\textbf{M}</script> 中的值，代入公式(3) ，可得出此結果，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


M= 

\begin{bmatrix}

      0.1+0.9 & 1.8+0.1 & 3 & ...  \\[0.3em]

      1.0+0.9 & 1.0+0.1 & 2 & ...  \\[0.3em]

      0.2 & 3.6 & 1 & ...  \\[0.3em]

    \end{bmatrix}

=\begin{bmatrix}

      1.0 & 1.9 & 3 & ...  \\[0.3em]

      1.9 & 1.1 & 2 & ...  \\[0.3em]

      0.2 & 3.6 & 1 & ...  \\[0.3em]

    \end{bmatrix}

 %]]&gt;</script>

<h3 id="controller">Controller</h3>

<p><em>Controller</em> 為控制器，它可以用類神經網路之類的機器學習模型來代替，但其實可以把它當成是黑盒子，只要可以符合下圖中所要求的 <em>input</em> 、 <em>output</em> 以及各種參數的值，就可以當 <em>controller</em> 。</p>

<p><img src="/images/pic/pic_00105.jpeg" alt="Controller" /></p>

<p>上圖中， <em>controller</em> 根據外部環境的輸入值 <em>input</em>，以及 <em>read vector</em> <script type="math/tex">\textbf{r}</script> ，經過其內部運算，會輸出 <em>output</em> 值到外在環境，還有 <em>erase vector</em> <script type="math/tex">\textbf{e}</script> 和 <em>add vector</em> <script type="math/tex">\textbf{a}</script> ，來控制記憶體的清除與寫入。但還缺少了讀寫頭向量 <script type="math/tex">\textbf{w}</script> 。</p>

<p>如果要產生讀寫頭向量 <script type="math/tex">\textbf{w}</script> ， 需要透過一連串的 <em>Addressing Mechanisms</em> 的運算，最後即可得出讀寫頭位置。而 <em>controller</em> 則負責產生出 <em>Addressing Mechanisms</em> 所需的參數。</p>

<h3 id="addressing-mechanism">Addressing Mechanism</h3>

<p><em>controller</em> 會產生五個參數來進行 <em>addressing mechanisms</em> ，這些參數分別為： <script type="math/tex">\textbf{k}, \beta, g , \textbf{s}, \gamma </script> 。其中， <script type="math/tex">\textbf{k}</script> 和 <script type="math/tex">\textbf{s}</script> 為向量，其餘參數為純量，這些參數的意義，在以下篇章會解釋，整個 <em>addressing mechanisms</em>  的過程如下圖所示。</p>

<p><img src="/images/pic/pic_00106.jpeg" alt="Addressing Mechanism" /></p>

<p>上圖中，總共有四個步驟，這四個步驟共需要用到這五種參數，經過了這一連串的過程之後，最後所產生出的 <script type="math/tex">\textbf{w}</script> 即為讀寫頭位置，如上圖左下角所示。以下細講每個步驟在做什麼。</p>

<h4 id="content-addressing">Content Addressing</h4>

<p>首先，是找出記憶體中跟參數 <em>memory key</em> <script type="math/tex">\textbf{k}</script> 值最相近的記憶體區塊。</p>

<p>讀寫頭的位置 <script type="math/tex">w</script> ，就先根據記憶體區塊中，跟 <script type="math/tex">\textbf{k}</script> 的相似度來決定，公示如下：</p>

<script type="math/tex; mode=display">

w(i) \leftarrow \frac{e^{\beta K[\textbf{k},\textbf{M}(i)] } }{ \sum_{j} e^{ \beta K[\textbf{k},\textbf{M}(j)] } }

</script>

<p>其中， <script type="math/tex">K[\textbf{k},\textbf{M}(i)]</script> 表示 <em>memory key</em> <script type="math/tex">\textbf{k}</script> 跟記憶體區塊 <script type="math/tex">M(i)</script> 的 <em>cosine similarity</em> ，即兩向量的夾角，如果 <script type="math/tex">\textbf{k}</script> 跟 <script type="math/tex">M(i)</script> 的內容越接近的話，則 <script type="math/tex"> K[\textbf{k},\textbf{M}(i)]</script> 算出來的值會越大。 最後算出來的值 <script type="math/tex">w(i)</script> ，即是 <script type="math/tex">\textbf{k}</script> 跟 <script type="math/tex">M(i)</script> 的相似度，除以記憶體內所有區塊相似度，標準化的結果。</p>

<p><em>cosine similarity</em> 的公式如下：</p>

<script type="math/tex; mode=display">

K[\textbf{u},\textbf{v} ] = \frac{ \textbf{u} \cdot \textbf{v} }{ |\textbf{u}| \cdot |\textbf{v}| } 

</script>

<p>經過了 <em>cosine similarity</em> 後，越相似的向量，值會越大，而參數 <script type="math/tex">\beta</script> 是個大於0的參數，可用來控制 <script type="math/tex">\textbf{w}</script> 內的元素值，集中與分散程度，如下圖所示：</p>

<p><img src="/images/pic/pic_00107.jpeg" alt="Content Addressing" /></p>

<p>上圖中，向量 <script type="math/tex">\textbf{k}</script> 中的值，與記憶體中第三行區塊的值最相似（用較淺色的背景表示）。但如果 <script type="math/tex">\beta</script> 很大（例如： <script type="math/tex">\beta=50</script>），算出來的 <script type="math/tex">\textbf{w}</script> 值會集中在第三個位置，也就是說，只有第三個位置的值是1，其他都是0（用較淺色的背景表示），如上圖的左下方。如果 <script type="math/tex">\beta</script> 很小（例如： <script type="math/tex">\beta=0</script>），則算出來的 <script type="math/tex">\textbf{w}</script> 值會平均分散到每個元素之中，如上圖的右下方。 </p>

<h4 id="interpolation">Interpolation</h4>

<p>讀寫頭其實也是有「記憶」的，也就是說，目前時間點的 <script type="math/tex">\textbf{w}_{t} </script> ，也可能會受到上個時間點 <script type="math/tex">\textbf{w}_{t-1}</script> 的影響，要達到這樣的效果，就是用 <em>content addressing</em> 所算出的值 <script type="math/tex">\textbf{w}_{t} </script> ，和上個時間點的讀寫頭位置 <script type="math/tex">\textbf{w}_{t-1}</script> 做 <em>interpolation</em> ，公式如下：</p>

<script type="math/tex; mode=display">

\textbf{w}_{t} \leftarrow g \textbf{w}_{t} + (1-g) \textbf{w}_{t-1} 

</script>

<p>其中，參數 <script type="math/tex">g</script> 用來表示 <script type="math/tex"> \textbf{w} </script> 有多少比例是這個時間點 <em>content addressing</em> 所算出的值，還是上個時間點的值。如下圖所示：</p>

<p><img src="/images/pic/pic_00108.jpeg" alt="Interpolation" /></p>

<p>如果 <script type="math/tex">g=1</script> ，則 <script type="math/tex">\textbf{w}</script> 的值會完全取決於這個時間點 <em>content addressing</em> 所算出的值，如上圖的左下方，若 <script type="math/tex">g=0</script> ，  <script type="math/tex">\textbf{w}</script> 會完全取決於上個時間點的值，如上圖的右下方。</p>

<h4 id="convolutional-shift">Convolutional Shift</h4>

<p>如果要讓讀寫頭的位置可以稍微往左或往右移動，這就要用 <em>Convolutional Shift</em> 來做調整。 參數 <script type="math/tex">\textbf{s}</script> 是一個向量，用 <em>convolutional shift</em> ，來將 <script type="math/tex">\textbf{w}</script> 的值往左或往右平移，公式如下：</p>

<script type="math/tex; mode=display">

w(i) \leftarrow \sum_{j} w(j) s(i-j)

</script>

<p>舉個例子，如果 <script type="math/tex">\textbf{s}</script> 中有三個元素：<script type="math/tex">s_{-1}, s_{0}, s_{1}</script> ，則 <script type="math/tex">w(i)</script> 經過了以上公式後，結果如下：</p>

<script type="math/tex; mode=display">

w(i) \leftarrow w(i+1) s(-1) + w(i)s(0) + w(i-1)s(1)

</script>

<p>根據此公式， <script type="math/tex">w(i)</script> 的值，如下圖所示：</p>

<p><img src="/images/pic/pic_00109.jpeg" alt="Convolutional" /></p>

<p>也就是說， <script type="math/tex">s_{-1}=1</script> 時，可讓原本的 <script type="math/tex">w_{i+1}</script> 往左移一格，移到 <script type="math/tex">w_{i}</script> ，若 <script type="math/tex">s_{1}=1</script> 時，可讓原本的 <script type="math/tex">w_{i-1}</script> 往右移一格，移到 <script type="math/tex">w_{i}</script> 。</p>

<p>舉個例子，如果 <script type="math/tex">s_{-1} = 1, s_{0}=0, s_{1} = 0</script> ，則  <script type="math/tex">\textbf{w}</script> 中的值，全部往左移動一格，若碰到邊界則再循環到最右邊，如下圖左方所示。 如果 <script type="math/tex">s_{-1} = 0, s_{0}=0, s_{1} = 1</script> ，則  <script type="math/tex">\textbf{w}</script> 中的值，全部往右移動一格。若 <script type="math/tex">s_{-1} = 0.5, s_{0}=0, s_{1} = 0.5</script> ，則 <script type="math/tex">\textbf{w}</script> 為往左和往右移動後的平均，如下圖右方所示。</p>

<p><img src="/images/pic/pic_00110.jpeg" alt="Convolutional Shift" /></p>

<h4 id="sharpening">Sharpening</h4>

<p>此過程是再一次調整 <script type="math/tex">\textbf{w}</script> 的集中與分散程度，公示如下：</p>

<script type="math/tex; mode=display">

w(i) \leftarrow \frac{w(i)^{\gamma}}{\sum_{j}w(j)^{\gamma}}

</script>

<p>其中， <script type="math/tex">\gamma</script> 的功能和 <em>Content Addressing</em> 中的 <script type="math/tex">\beta</script> 是一樣的，但是經過了接下來的 <em>Interpolation</em> 跟 <em>Convolutional Shift</em> 之後，<script type="math/tex">\textbf{w}</script> 裡面的集中度又會改變，所以要再重新調整一次。</p>

<p><img src="/images/pic/pic_00111.jpeg" alt="Sharpening" /></p>

<h2 id="experiment-repeat-copy">Experiment: Repeat Copy</h2>

<p>關於 <em>Neural Turing Machine</em> 的學習能力，可以參考以下例子。</p>

<p>在訓練資料中，給定一個區塊的 <em>data</em> （如下圖左上角紅色區塊）做為 <em>input data</em> ，將這個區塊複製成七份，做為 <em>output data</em> 。則 <em>Neural Turing Machine</em> 有辦法學會這個「複製」過程所需的運算程序，也就是重複跑七次輸出一樣的東西。</p>

<p><img src="/images/pic/pic_00112.png" alt="Experiment" /></p>

<p><img src="/images/pic/pic_00113.png" alt="Experiment" /></p>

<p>從上圖中，可看到讀寫頭的移動，重複走了相同的路徑，走了七次，依序將記憶體中儲存的 <em>input data</em> 的值，讀出來並輸出到 <em>output</em> 。</p>

<p>有個完整的  <em>Neural Turing Machine</em> 套件，以及此實驗的相關程式碼於：https://github.com/fumin/ntm</p>

<h2 id="reference">Reference</h2>

<p><a href="http://arxiv.org/abs/1410.5401">Alex Graves, Greg Wayne, Ivo Danihelka. Neural Turing Machines. 2014</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Recurrent Neural Network]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/06/06/neural-network-recurrent-neural-network/"/>
    <updated>2015-06-06T09:32:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/06/06/neural-network-recurrent-neural-network</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>在數位電路裡面，如果一個電路沒有 <em>latch</em> 或 <em>flip flop</em> 這類的元件，它的輸出值只會取決於目前的輸入值，和上個時間點的輸入值是無關的，這種的電路叫作 <em>combinational circuit</em> 。</p>

<p>對於類神經網路而言，如果它的值只是從輸入端一層層地依序傳到輸出端，不會再把值從輸出端傳回輸入端，這種神經元就相當於 <em>combinational circuit</em> ，也就是說它的輸出值只取決於目前時刻的輸入值，這樣的類神經網路稱為 <em>feedforward neural network</em> 。</p>

<p>如果一個電路有 <em>latch</em> 或 <em>flip flop</em> 這類的元件，它的輸出值就跟上個時間點的輸入值有關，這種的電路它稱為 <em>sequential circuit</em> 。</p>

<p>所謂的 <em>Recurrent Neural Network</em> ，是一種把輸出端再接回輸入端的類神經網路，這樣可以把上個時間點的輸出值再傳回來，記錄在神經元中，達成和 <em>latch</em> 類似的效果，使得下個時間點的輸出值，跟上個時間點有關，也就是說，這樣的神經網路是有 <em>記憶</em> 的。</p>

<h2 id="recurrent-neural-network">Recurrent Neural Network</h2>

<p>由一個簡單神經元所構成的 <em>Recurrent Neural Network</em> ，構造如下：</p>

<p><img src="/images/pic/pic_00095.png" alt="" /></p>

<p>這個神經元在 <script type="math/tex">t</script> 時間，訓練資料的輸入值為 <script type="math/tex">x_{t}</script> ，訓練資料的答案為 <script type="math/tex">y_{t}</script> ，神經元 <script type="math/tex">n</script> 的輸出值 <script type="math/tex">n_{out,t}</script> ，可用以下公式表示：</p>

<!--more-->

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{aligned}

& n_{in,t} = w_{c}x_{t}+ w_{p}n_{out,t-1} + w_{b} \\

& n_{out,t} = \frac{1}{1+e^{-n_{in,t}}} \\

\end{aligned}

 %]]&gt;</script>

<p>其中， <script type="math/tex">n_{in,t}</script> 為輸入神經元 <script type="math/tex">n</script> 的值， <script type="math/tex">w_{c}</script> 是給目前的時間(current)時，輸入值 <script type="math/tex">x_{t}</script> 的權重， <script type="math/tex">w_{p}</script> 是給上個時間點(previous)時，輸出值 <script type="math/tex">n_{out,t-1}</script> 的權重，而 <script type="math/tex">w_{b}</script> 為 <em>bias</em> 。從上圖可看出，紫色的線將神經網路的輸出端 <script type="math/tex">n_{out}</script> 連回輸入端 <script type="math/tex">n_{in}</script> ，使得於時間 <script type="math/tex">t</script> 的輸出值跟上個時間點 <script type="math/tex">t-1</script> 的輸出值有關。</p>

<p>可以把這個神經元從時間點 <script type="math/tex">0</script> 到時間點 <script type="math/tex">t</script> 的運算，展開成下圖：</p>

<p><img src="/images/pic/pic_00096.png" alt="" /></p>

<p>從上圖，最左邊開始，依序將 <script type="math/tex">x_{0},x_{1},...,x_{t}</script> 輸入神經元 <script type="math/tex">n</script> ，而依序得出的值為 <script type="math/tex">n_{out,0},n_{out,1},...,n_{out,t}</script> 。神經元 <script type="math/tex">n</script> 在時間點 <script type="math/tex">t-1</script> 的輸出值 <script type="math/tex">n_{out,t-1}</script> ，會接到時間點 <script type="math/tex">t</script> 時的輸入值 <script type="math/tex">n_{in,t}</script> 。 </p>

<h2 id="training-recurrent-neural-network">Training Recurrent Neural Network</h2>

<p>訓練 <em>recurrent neural network</em> 的方法，和訓練 <em>feedforward neural network</em> 的方法一樣，都可以用 <em>back propagation</em> 。但是在 <em>recurrent neural network</em> 中，要依據時間順序，將值從最後一個時間點，回傳到第一個時間點。</p>

<p>在時間點 <script type="math/tex">t</script> 時的 <em>cost function</em> 為：</p>

<script type="math/tex; mode=display">

J=−y_{t}log(n_{out,t})−(1−y_{t})log(1−n_{out,t})

</script>

<p>計算 <em>recurrent neural network</em> 的 <em>back propagation</em> 要分為兩部分來算，先算好時間點位於 <script type="math/tex">t</script> 的偏微分 <script type="math/tex">\dfrac{\partial J}{ \partial n_{in} } </script> 值，再依序往前算出時間點 <script type="math/tex">t</script> 之前的偏微分值，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\dfrac{\partial J}{ \partial n_{in,s} } = 

\begin{cases} 

(\dfrac{\partial J}{ \partial n_{out,s} } )

(\dfrac{ \partial n_{out,s}}{\partial n_{in,s} } ) & \text{if } s = t \\

(\dfrac{\partial J}{ \partial n_{in,s+1} } )

(\dfrac{ \partial n_{in,s+1}}{\partial n_{out,s} } )

(\dfrac{ \partial n_{out,s}}{\partial n_{in,s} } )

  & \text{otherwise}

\end{cases}

 %]]&gt;</script>

<p>其中， <script type="math/tex">s</script> 為 <script type="math/tex">0</script> 到 <script type="math/tex">t</script> 中的其中一個時間點。用 <a href="/blog/2015/05/28/neural-network-backward-propagation">Backward Propagation 詳細推導過程</a> 所提到的推導方法，可推導出 <script type="math/tex"> (\dfrac{\partial J}{ \partial n_{out,s} } )(\dfrac{ \partial n_{out,s}}{\partial n_{in,s} } )</script> 、 <script type="math/tex">(\dfrac{ \partial n_{in,s+1}}{\partial n_{out,s} } )</script> 與 <script type="math/tex">(\dfrac{ \partial n_{out,s}}{\partial n_{in,s} } )</script> 的值，並令 <script type="math/tex"> \delta_{in,s} = \dfrac{\partial J}{ \partial n_{in,s} } </script> 代入以上公式，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

\delta_{in,s}  = 

\begin{cases} 

n_{out,s} - y_{s}  & \text{if } s = t \\

\delta_{in,s+1} w_{p} n_{out,s} (1-n_{out,s})  & \text{otherwise}

\end{cases}

\end{align}

 %]]&gt;</script>

<p>此公是可分為兩部分，當 <script type="math/tex">s = t</script> 時，與  <script type="math/tex">s\neq t</script> 時。計算 <script type="math/tex">\delta_{s}</script> 的方式不同。</p>

<p>在 <script type="math/tex">s = t</script> 時， <script type="math/tex">\delta_{s}</script> 的傳遞過程就如同 <em>feedforward neural network</em> ，如下圖：</p>

<p><img src="/images/pic/pic_00097.png" alt="" /></p>

<p>若 <script type="math/tex">s\neq t</script> 時， 要算 <script type="math/tex">\delta_{in,s}</script> 之前，要先從 <script type="math/tex">s+1</script> 時間點將 <script type="math/tex">\delta_{in,s+1}</script> 傳遞過來，傳遞過程如下圖：</p>

<p><img src="/images/pic/pic_00098.png" alt="" /></p>

<p>因為需要把 <script type="math/tex">\delta</script> 從後面的時間點往前面傳，故這個過程又稱為 <em>back propagation through time</em> 。</p>

<p>於時間點 <script type="math/tex">s</script> 計算完 <script type="math/tex">\delta</script> 後，用以下公式將 <script type="math/tex">s</script> 時間點算出的偏微分值，更新到神經元的權重：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{c} \leftarrow w_{c} - 

\eta \dfrac{\partial J}{ \partial n_{in,s} } \dfrac{\partial n_{in,s}}{\partial w_{c}} \\ 

& w_{b} \leftarrow w_{b} - 

\eta \dfrac{\partial J}{ \partial n_{in,s} } \dfrac{\partial n_{in,s}}{\partial w_{b}} \\

& w_{p} \leftarrow w_{p} - 

\eta \dfrac{\partial J}{ \partial n_{in,s} } \dfrac{\partial n_{in,s}}{\partial w_{p}}  \\

\end{align}

 %]]&gt;</script>

<p>用 <a href="/blog/2015/05/28/neural-network-backward-propagation">Backward Propagation 詳細推導過程</a> ，求出 <script type="math/tex">\dfrac{\partial n_{in,s}}{\partial w_{c}}</script> 、 <script type="math/tex">\dfrac{\partial n_{in,s}}{\partial w_{b}} </script> 和 <script type="math/tex">\dfrac{\partial n_{in,s}}{\partial w_{p}}</script> 的值，並換成用 <script type="math/tex">\delta_{in,s}</script> 代替 <script type="math/tex">\dfrac{\partial J}{ \partial n_{in,s} } </script> 代入以上公式，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& w_{c} \leftarrow w_{c} - \eta \delta_{in,s} x_{s} \\ 

& w_{b} \leftarrow w_{b} - \eta \delta_{in,s} \\

& w_{p} \leftarrow w_{p} - \eta \delta_{in,s} n_{out,s-1} \\

\end{align}

 %]]&gt;</script>

<p>此過程如下圖所示：</p>

<p><img src="/images/pic/pic_00099.png" alt="" /></p>

<h2 id="implementation">Implementation</h2>

<p>再來是實作的部分，以下是個簡單的應用，用 <em>Recurrent Neural Network</em> 來預測一個字串序列中，下一個可能出現的字是什麼。例如，給定以下字串：</p>

<p>```
001001001</p>

<p>```</p>

<p>根據這個字串的特徵，如果連續出現了兩個 <em>0</em> ，可以預測下個出現的為 <em>1</em> ，若前面兩個字為 <em>10</em> 則可預測下個出現的自為 <em>0</em> ，以此類推。</p>

<p>以下為實作部分：</p>

<p>```python simple_rnn.py
from math import e                                                                                  <br />
from random import random</p>

<p>def sigmoid(x):
    return 1/float(1+e<em>*(-1</em>x))</p>

<p>def my_rand():
    return random()-0.5</p>

<p>def rnn(x):
    r = 0.05
    w_p, w_c, w_b = my_rand(),my_rand(),my_rand()
    l = len(x)
    n_in = [0]<em>l
    n_out = [0]</em>l
    for h in range(10000):
        for i in range(l-1): 
            n_in[i] = w_c * x[i] + w_p * n_out[i] + w_b 
            n_out[i+1] = sigmoid(n_in[i])
        for i in range(l-1): 
            for j in range(i+1):
                k =  (i-j)
                if j == 0:
                    d_c = n_out[k+1] - x[k+1]
                else:
                    d_c = w_p * n_out[k+1] * (1-n_out[k+1]) * d_c
                w_c = w_c - r * d_c * x [k]
                w_b = w_b - r * d_c
                w_p = w_p - r * d_c * n_out[k]</p>

<pre><code>for i in range(l-1):
    n_in[i] = w_c * x[i] + w_p * n_out[i] + w_b
    n_out[i+1] = sigmoid(n_in[i])
for w in zip(x,n_out):
    print w[0],w[1]
</code></pre>

<p>```</p>

<p>其中， <code>x</code> 為輸入的序列， <code>n_out</code> 為神經元預測的結果。進行這個演算法之前，首先，先給權重 <code>w_p, w_c, w_b</code> 的初始值用介於 <em>-0.5~0.5</em> 之間的隨機值。再來是進行訓練過程，用 <em>for loop</em> 進行了 <em>10000</em> 次的訓練，在每次的訓練過程中，先進行 <em>forward propagation</em> 依時間順序，算出每個時間點的 <code>n_out</code> 。再來是用 <em>back propagation through time</em> 來更新 <code>w_p, w_c, w_b</code> 的值。訓練完後，進行一次 <em>forward propogation</em> 用訓練過程得出的權重來預測序列的下一個字，並將預測結果印出。</p>

<p>到 <em>interactive mode</em> 執行以下程式，輸入序列 <em>001001001</em> 。</p>

<p>```</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>import smallrnn
smallrnn.rnn([0,0,1,0,0,1,0,0,1])
0 0
0 0.203697155215
1 0.93315712478
0 0.00354811782422
0 0.215230877663
1 0.945971073339
0 0.00455854449755
0 0.218600850027
1 0.949254861129</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>左側為輸入序列，右側為預測的結果，可以發現 <em>recurrent neural network</em> 可以預測出下個字可能會是 <em>0</em> 還是 <em>1</em> 。當左側為 <em>1</em> 時，右側的數字會接近於 <em>1</em> 。</p>

<h2 id="further-reading">Further Reading</h2>

<p>關於 <em>recurrent neural network</em> 可參考 coursera 課程 Geoffrey Hinton. Neural Networks for Machine Learning</p>

<p>https://www.coursera.org/course/neuralnets</p>

]]></content>
  </entry>
  
</feed>
