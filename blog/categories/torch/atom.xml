<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Torch | Mark Chang's Blog]]></title>
  <link href="http://ckmarkoh.github.io/blog/categories/torch/atom.xml" rel="self"/>
  <link href="http://ckmarkoh.github.io/"/>
  <updated>2016-12-25T17:48:00+08:00</updated>
  <id>http://ckmarkoh.github.io/</id>
  <author>
    <name><![CDATA[Mark Chang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Torch NN Tutorial 2 : NN.Container & NN.Sequential]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/12/24/torch-nn-tutorial-2-nn-container/"/>
    <updated>2016-12-24T00:24:25+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/12/24/torch-nn-tutorial-2-nn-container</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>在<a href="/blog/2016/12/19/torch-nn-tutorial-1-nn-module/">Torch NN tutorial 1 : NN.Module &amp; NN.Linear</a>中，介紹了構成 neural network 的最基本的單位，也就是 <code>nn.Module</code> ，並介紹了 <code>nn.Linear</code> 可以進行一次線性運算，但通常一個 neural network 是經由多個線性和非線性的運算構成的。要把多個 Module 串接起來，就需要用到 <code>nn.Contaner</code> 。</p>

<p>例如，有個 neural network 的輸入為 x ，輸出為 z ，中間經過了一次線性運算與一個sigmoid function ，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\textbf{y} = \textbf{W}\textbf{x} + \textbf{b}  \\
&\textbf{z} = \frac{1}{1+e^{-\textbf{y}}}
\end{align}
 %]]&gt;</script>

<p>假設 <script type="math/tex">\textbf{x}</script> 為 2 維的向量，而 <script type="math/tex">\textbf{y}</script> 為 3 維向量， 
<script type="math/tex">\textbf{W},\textbf{b}</script> 分別為 weight 和 bias ，此兩參數皆以隨機值進行初始化，
<script type="math/tex">\textbf{z}</script>  是 <script type="math/tex">\textbf{y}</script> 經過 sigmoid 非線性運算的輸出結果。</p>

<!--more-->

<p>使用 torch 實作此運算：</p>

<p>首先，載入 <code>nn</code> 套件：</p>

<p>```lua</p>

<p>require ‘nn’</p>

<p>```</p>

<p>建立以上兩個運算所需的Module，分別為 <code>nn.Linear</code> 和 <code>nn.Sigmoid</code> ，並將它們分別命名為 <code>l1</code> 和 <code>s1</code> ，如下：</p>

<p>```lua</p>

<p>l1 = nn.Linear(2,3)
s1 = nn.Sigmoid()</p>

<p>```</p>

<p>其中， <code>nn.Sigmoid</code> 為進行 sigmoid 運算所需的 module 。</p>

<p>如果要進行運算，方法如下：</p>

<p>假設 x 為一個二維向量 <script type="math/tex">[0,1]</script> ，先輸入 <code>l1</code> ，進行 forward propagation ，將運算結果傳給 <code>y</code> ，如下：</p>

<p>```lua</p>

<p>x = torch.Tensor{0,1}
y = l1:forward(x)
print(y)</p>

<p>```</p>

<p>執行完後會印出 <code>l1</code> 的輸出值，也就是 <code>y</code> 的值，是個三維的向量，
y 的值會與 <code>l1.weight</code> 和 <code>l1.bias</code> 的初始值值關， <code>y</code> 值如下：</p>

<p>```sh</p>

<p>0.1948
 0.9780
 0.3982
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<p>再來將 <code>y</code> 輸入到 <code>s1</code> 中， 進行 forward propagation，將運算結果傳給 <code>z</code> ，如下：</p>

<p>```lua</p>

<p>z = s1:forward(y)
print(z)</p>

<p>```</p>

<p>執行完後會印出 <code>z</code> 值，此值是由 <code>y</code> 的值經過 sigmoid function 的運算所得出來的， <code>z</code> 值如下：</p>

<p>```sh</p>

<p>0.5485
 0.7267
 0.5983
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<p>以上過程中，給定了 <code>x</code> 值，經過每一個 module 都要呼叫一次 <code>forward</code> ，最後才能取得 <code>z</code> 的值，
如果一個 network 是由很多個 module 所組成的，這樣要呼叫很多次 <code>forward</code> ，會很麻煩。</p>

<p>而 <code>nn.Container</code> 則是一個容器，它可以把多個 module 放進去，並自動處理這些 module 輸入與輸出值的傳遞。 <code>nn.Container</code> 也是個抽象個類別，泛指能夠把 module 放進去的容器。</p>

<p>其中一種 container 可將前一個 module 的輸出，傳給下一個 module 的輸入，這種 contanier 為 <code>nn.Sequential()</code> 。</p>

<p>例如，如果要將 <code>l1</code> 的輸出，傳入 <code>s1</code> ，則可以用 <code>nn.Sequential</code> 來達成。建立一個命名為 <code>n1</code> 的 sequential container ，將 <code>l1</code> 和 <code>s1</code> 串起來，方法如下：</p>

<p>```lua</p>

<p>n1 = nn.Sequential()
n1:add(l1)
n1:add(s1)</p>

<p>```</p>

<p>其中，<code>add</code> 是將 module 加入 container 的 function。</p>

<p>其實， <code>nn.Container</code> 也是從 <code>nn.Module</code> 繼承而來的，所以它也有 <code>forward</code> ，可以進行 forward propagation ，只要呼叫了 <code>nn.Container</code> 的 <code>forward</code> ，它就會自動將裡面的 module 進行 forward ，並輸出結果。</p>

<p>如此給定 <code>x</code> 之後，只要進行一次 forward 即可取得 <code>z</code> 的結果，如下：</p>

<p>```lua</p>

<p>x = torch.Tensor{0,1}
z = n1:forward(x)
print(z)</p>

<p>```</p>

<p>執行完後，會印出 <code>z</code> 的結果，如下：</p>

<p>```sh</p>

<p>0.5485
 0.7267
 0.5983
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<h2 id="nncontainer--nnsequential">nn.Container &amp; nn.Sequential</h2>

<p>這邊要介紹 <code>nn.Container</code> 和 <code>nn.Sequential</code> 的源碼。</p>

<p><code>nn.Container</code> 源碼：<a href="https://github.com/torch/nn/blob/master/Container.lua">https://github.com/torch/nn/blob/master/Container.lua</a></p>

<p><code>nn.Sequential</code> 源碼： <a href="https://github.com/torch/nn/blob/master/Sequential.lua">https://github.com/torch/nn/blob/master/Sequential.lua</a></p>

<p>首先，介紹 <code>nn.Container</code> ，先看 <code>init()</code> 的部分：</p>

<p>```lua nn/Container.lua</p>

<p>local Container, parent = torch.class(‘nn.Container’, ‘nn.Module’)</p>

<p>function Container:__init(…)
    parent.__init(self, …)
    self.modules = {}
end</p>

<p>```</p>

<p>第1行處，看到 <code>nn.Container</code> 是從 <code>nn.Module</code> 繼承而來的，因此它具有 <code>nn.Module</code> 的 variable 及 function 。</p>

<p>第5行中，container 的變量 <code>modules</code> ，它是一個 lua table ，是用來存放加到 container 中的  module 。</p>

<p>至於，要如何將 module 加到 container 中？可以用 <code>add</code> 加入。 <code>add</code> 的程式碼如下： </p>

<p>```lua nn/Container.lua</p>

<p>function Container:add(module)
    table.insert(self.modules, module)
    return self
end</p>

<p>```</p>

<p>在第2行可看到， <code>add</code> 可以把新的 module 加到 <code>modules</code> 中。</p>

<p>```lua nn/Container.lua</p>

<p>function Container:get(index)
    return self.modules[index]
end</p>

<p>```</p>

<p>除了加進去之外，也可以用 <code>get</code> 取得 <code>modules</code> 中的某個元素，或是用 <code>size</code> 取得 <code>modules</code> 的大小，程式碼如下：</p>

<p>```lua nn/Container.lua</p>

<p>function Container:get(index)
    return self.modules[index]
end</p>

<p>function Container:size()
    return #self.modules
end</p>

<p>```</p>

<p>再來是實作的部分，首先我們建立一個 sequential 的 container ，命名為 <code>n2</code> ，並以 <code>size</code> 取得它所含的 module 個數，如下：</p>

<p>```lua</p>

<p>n2 = nn.Sequential()
print(n2:size())</p>

<p>```</p>

<p>剛建立時， 由於 <code>n2</code> 的 <code>modules</code> 是空的，所以 <code>size</code> 是 0。執行結果如下：</p>

<p>```sh</p>

<p>0	</p>

<p>```</p>

<p>再來將 <code>l1</code> 和 <code>s1</code> 依序加入，再看看 <code>size</code> 的變化：</p>

<p>```lua</p>

<p>n2:add(l1)
n2:add(s1)
print(n2:size())</p>

<p>```</p>

<p>此時已經加入了兩個 module 進去了，所以 <code>size</code> 會是 2 ，執行結果如下：</p>

<p>```sh</p>

<p>2	</p>

<p>```</p>

<p>可以用 <code>get</code> 取得加進去的 module ，例如，想取得第一個加進去的，作法如下：</p>

<p>```lua</p>

<p>print(n2:get(1))</p>

<p>```</p>

<p>第一個加進去的為 <code>nn.Linear</code> ，執行結果如下：</p>

<p>```sh</p>

<p>nn.Linear(2 -&gt; 3)
{
  gradBias : DoubleTensor - size: 3
  weight : DoubleTensor - size: 3x2
  _type : torch.DoubleTensor
  output : DoubleTensor - size: 3
  gradInput : DoubleTensor - empty
  bias : DoubleTensor - size: 3
  gradWeight : DoubleTensor - size: 3x2
}</p>

<p>```</p>

<p>再來看到 <code>nn.Sequential()</code> 的程式碼。</p>

<p>```lua nn/Sequential.lua</p>

<p>local Sequential, _ = torch.class(‘nn.Sequential’, ‘nn.Container’)</p>

<p>```</p>

<p>第一行顯示 <code>nn.Sequential</code> 繼承了 <code>nn.Container</code> 。它的 <code>init</code> 也與 <code>nn.Container</code> 共用，沒有再另外實作。</p>

<p>再來看 <code>add</code> 的程式碼。</p>

<p>```lua nn/Sequential.lua</p>

<p>function Sequential:add(module)
   if #self.modules == 0 then
      self.gradInput = module.gradInput
   end
   table.insert(self.modules, module)
   self.output = module.output
   return self
end</p>

<p>```</p>

<p>在 <code>nn.Sequential</code> 中， <code>add</code> 除了會將 module 加到 <code>modules</code> 之外，它的 <code>output</code> 會是最後一個加進去的 module 的 <code>output</code> 。</p>

<p>以下實作，將 <code>n2</code> 的 <code>output</code> 和它裡面的兩個 module 的 <code>output</code> 都印出來看看：</p>

<p>```lua</p>

<p>print(n2:get(1).output)
print(n2:get(2).output)
print(n2.output)</p>

<p>```</p>

<p>結果如下， <code>n2</code> 的 output 是第二個 module 的 output ：</p>

<p>```sh</p>

<p>0.1948
 0.9780
 0.3982
[torch.DoubleTensor of size 3]</p>

<p>0.5485
 0.7267
 0.5983
[torch.DoubleTensor of size 3]</p>

<p>0.5485
 0.7267
 0.5983
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<p>再來看到 <code>updateOutput</code> 的部分：</p>

<p>```lua nn/Sequential.lua</p>

<p>function Sequential:updateOutput(input)
   local currentOutput = input
   for i=1,#self.modules do
      currentOutput = self:rethrowErrors(self.modules[i], i, ‘updateOutput’, currentOutput)
   end
   self.output = currentOutput
   return currentOutput
end</p>

<p>```</p>

<p>以上可知，在第3行的 for 迴圈中， <code>modules</code> 中的 <code>module</code> 會依序進行  <code>updateOutput</code> ，並將其 <code>output</code> 傳遞到下個 <code>module</code> 的 <code>input</code> ，而 <code>nn.Sequential</code> 的 <code>output</code> 會是最後一個 <code>module</code> 的 <code>output</code></p>

<p>假設輸入 <code>n2</code> 的 <code>x</code> 為 <script type="math/tex">[1,0]</script> ，則進行 forward propagation ，將輸出結果存到 <code>z</code> ，實作如下 ：</p>

<p>```lua</p>

<p>x = torch.Tensor{1,0}
z = n2:forward(x)
print(z)</p>

<p>```</p>

<p>透過 updateOutput 中的 for 迴圈，它會依序跑完內部所有的 module，並得出最後結果，結果如下：</p>

<p>```sh</p>

<p>0.5331
 0.6890
 0.5093
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<p>如果想看 <code>n2</code> 中有哪些 module ，以及它們的順序，也可以直接用 <code>print</code> 的方式，作法如下：</p>

<p>```lua </p>

<p>print(n2)</p>

<p>```</p>

<p>結果如下：</p>

<p>```sh</p>

<p>nn.Sequential {
  [input -&gt; (1) -&gt; (2) -&gt; output]
  (1): nn.Linear(2 -&gt; 3)
  (2): nn.Sigmoid
}
{
  gradInput : DoubleTensor - empty
  modules : 
    {
      1 : 
        nn.Linear(2 -&gt; 3)
        {
          gradBias : DoubleTensor - size: 3
          weight : DoubleTensor - size: 3x2
          _type : torch.DoubleTensor
          output : DoubleTensor - size: 3
          gradInput : DoubleTensor - empty
          bias : DoubleTensor - size: 3
          gradWeight : DoubleTensor - size: 3x2
        }
      2 : 
        nn.Sigmoid
        {
          gradInput : DoubleTensor - empty
          _type : torch.DoubleTensor
          output : DoubleTensor - size: 3
        }
    }
  _type : torch.DoubleTensor
  output : DoubleTensor - size: 3
}</p>

<p>​```</p>

<p>以上結果可分成兩部分來看，第一部分是 <code>nn.Sequential</code> 裡面每個 module 的順序，從 input 到 output 之間，依次進行了哪些運算。</p>

<p>第二部份是詳細印出 <code>nn.Sequential</code> 中，有哪些成員 ，它有從 <code>nn.Module</code> 繼承而來的 <code>gradInput</code> 即 <code>output</code> ，也有從 <code>nn.Container</code> 繼承而來的 <code>modules</code> 。而 <code>modules</code> 的部分會詳細列出裡面每個 module 有哪些成員。</p>

<p>這部分的程式碼於 <code>tostring</code> 函式中，程式碼如下：</p>

<p>```lua nn/Sequential.lua</p>

<p>function Sequential:<strong>tostring</strong>()
   local tab = ‘  ‘
   local line = ‘\n’
   local next = ‘ -&gt; ‘
   local str = ‘nn.Sequential’
   str = str .. ‘ {‘ .. line .. tab .. ‘[input’
   for i=1,#self.modules do
      str = str .. next .. ‘(‘ .. i .. ‘)’
   end
   str = str .. next .. ‘output]’
   for i=1,#self.modules do
      str = str .. line .. tab .. ‘(‘ .. i .. ‘): ‘ .. tostring(self.modules[i]):gsub(line, line .. tab)
   end
   str = str .. line .. ‘}’
   return str
end</p>

<p>```</p>

<h2 id="materials">Materials</h2>

<p>本次教學的完整程式碼於此：</p>

<p><a href="https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/2_nn_container_and_sequential.ipynb">https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/2_nn_container_and_sequential.ipynb</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Torch NN Tutorial 1 : NN.Module & NN.Linear]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/12/19/torch-nn-tutorial-1-nn-module/"/>
    <updated>2016-12-19T22:36:47+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/12/19/torch-nn-tutorial-1-nn-module</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>此系列講解如何用 torch 實作 neural network 。</p>

<p>本系列不講解如何安裝 torch 及 lua 的基本語法，假設讀者都已具備這些基礎知識。</p>

<p>以 torch 實作 neural network 時，最常用的套件為 <a href="https://github.com/torch/nn">nn</a>，而在 <code>nn</code> 中，建構 neural network 最基本的單位為 <a href="https://github.com/torch/nn/blob/master/Module.lua">nn.Module</a> 。 <code>nn.Module</code> 是一個抽象類別，所有建構 neural network 本身有關的 module ，都是從 <code>nn.Module</code> 所繼承而來。</p>

<p>舉個例子，如果要實作以下運算：</p>

<script type="math/tex; mode=display">

\textbf{y} = \textbf{W}\textbf{x} + \textbf{b} 

</script>

<p>假設 <script type="math/tex">\textbf{x}</script> 為 2 維的 input ，而 <script type="math/tex">\textbf{y}</script> 為 3 維的output， <script type="math/tex">\textbf{W},\textbf{b}</script> 分別為 weight 和 bias ，此兩參數皆以隨機值進行初始化。</p>

<p>使用 torch 實作此運算的方法如下：</p>

<p>首先，載入 nn 套件：</p>

<p>```lua</p>

<p>require ‘nn’</p>

<p>```</p>

<p>建立一個 Linear Module：</p>

<p>```lua</p>

<p>l1 = nn.Linear(2,3)</p>

<p>```</p>

<!--more-->

<p>其中， <a href="https://github.com/torch/nn/blob/master/Linear.lua">nn.Linear</a> 即是用來進行 <script type="math/tex">\textbf{y} = \textbf{W}\textbf{x} + \textbf{b} </script> 這類的線性運算所用的模組，它繼承了 <code>nn.Module</code> 。 而 2 和 3 分別代表了 <script type="math/tex">\textbf{x}</script> 和 <script type="math/tex">\textbf{y}</script> 的維度。 當它被建構出來時， weight 和 bias 的值會以隨機值來初始化。 </p>

<p>以上程式中，建立一個命名為 <code>l1</code> 的 module ，如果要取得它的 weight 和 bias ，可以用 <code>l1.weight</code> 和 <code>l1.bias</code> 取得，方法如下：</p>

<p>```lua</p>

<p>print(l1.weight)
print(l1.bias)</p>

<p>```</p>

<p>執行結果如下：</p>

<p>```sh</p>

<p>0.1453  0.5062
 0.0635  0.4911
-0.1080  0.1747
[torch.DoubleTensor of size 3x2]</p>

<p>0.2063
-0.1635
-0.0883
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<p>其中， size 3x2 的 tensor 為 weight, size 3 的 tensor 為 bias。</p>

<p>用此 module 可以執行運算，令 x 為一個二維向量 <script type="math/tex">[0,1]</script> ，輸入此 module ，進行 forward propagation ，也就是說，執行 <script type="math/tex">\textbf{y} = \textbf{W}\textbf{x} + \textbf{b} </script> 的運算， 並輸出結果為 <script type="math/tex">\textbf{y}</script>  ，實作如下：</p>

<p>```lua</p>

<p>x = torch.Tensor{0,1}
y = l1:forward(x)
print(y)</p>

<p>```</p>

<p>輸出結果 <code>y</code> 為一個三維向量，如下：</p>

<p>```sh</p>

<p>0.7125
 0.3276
 0.0865
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<h2 id="nnmodule--nnlinear">nn.Module &amp; nn.Linear</h2>

<p>這邊要更進一步介紹 <code>nn.Module</code> 和 <code>nn.Linear</code> 的內容是什麼。由於 torch 的源碼相當簡潔易懂，可以直接看源碼來了解它的功能是什麼。</p>

<p><code>nn.Module</code> 源碼： <a href="https://github.com/torch/nn/blob/master/Module.lua">https://github.com/torch/nn/blob/master/Module.lua</a></p>

<p><code>nn.Linear</code> 源碼： <a href="https://github.com/torch/nn/blob/master/Linear.lua">https://github.com/torch/nn/blob/master/Linear.lua</a></p>

<p>首先，介紹 <code>nn.Module</code> ，先看 <code>init()</code> 的部分：</p>

<p>```lua nn/Module.lua</p>

<p>function Module:__init()
   self.gradInput = torch.Tensor()
   self.output = torch.Tensor()
   self._type = self.output:type()
end</p>

<p>```</p>

<p><code>Module</code> 中最基本的成員有 <code>output</code> 和 <code>gradInput</code> 。
<code>output</code> 為此 <code>Module</code> 的 forward propagation 結果，而 <code>gradInput</code> 為 backward propagation 的運算結果。
這些變量一開始都會被初始化為 空的 tensor 。</p>

<p>註：本文先不講解 backward propagation 與 <code>gradInput</code> 的部分，交由之後的教學文章來解釋。</p>

<p>在 <code>Module:forward</code> 的部分，是用來進行 forward propagation的，如下：</p>

<p>```lua nn/Module.lua</p>

<p>function Module:updateOutput(input)
   return self.output
end</p>

<p>function Module:forward(input)
   return self:updateOutput(input)
end</p>

<p>```</p>

<p>先看 forward 的部分， Module 沒有運算的實作，僅單純輸出 <code>output</code> 值。如果呼叫了 forward propagation ，則從 <code>Module:updateOutput</code> 就直接輸出了 <code>output</code> 。</p>

<p>而 <code>nn.Linear</code> 則實作了 forward propagation。</p>

<p>所謂的 Linear，即是指 <script type="math/tex">\textbf{y} = \textbf{W}\textbf{x} + \textbf{b} </script> 的線性運算。</p>

<p>再來看 <code>nn.Linear</code> 的程式碼，先看 <code>init()</code> 的部分：</p>

<p>```lua nn/Linear.lua</p>

<p>local Linear, parent = torch.class(‘nn.Linear’, ‘nn.Module’)</p>

<p>function Linear:__init(inputSize, outputSize, bias)
   parent.__init(self)
   local bias = ((bias == nil) and true) or bias
   self.weight = torch.Tensor(outputSize, inputSize)
   self.gradWeight = torch.Tensor(outputSize, inputSize)
   if bias then
      self.bias = torch.Tensor(outputSize)
      self.gradBias = torch.Tensor(outputSize)
   end
   self:reset()
end</p>

<p>```</p>

<p>在第1行， <code>nn.Linear</code> 繼承了 <code>nn.Module</code> 。</p>

<p>在第3行開始可以看到，建構 Linear 所需的參數有 <code>inputSize</code> , <code>outputSize</code> 和 <code>bias</code> 。 <code>bias</code>  不一定要給，如果沒有給，則預設值會讓它是隨機的。除非 <code>bias=false</code> ，則此 Linear Module 就不會有 <code>bias</code> 。
從6~10行中，它比 <code>nn.Module</code> 多了 <code>weigt</code> 和 <code>bias</code> 這兩個變量，而 <code>reset()</code> 則是將它們初始化。</p>

<p>如果要建立一個 Linear Module，則要給定 <code>inputSize</code> 和 <code>outputSize</code> ，也就是 <script type="math/tex">\textbf{x}</script> 和 <script type="math/tex">\textbf{y}</script> 的維度。</p>

<p>假設  <script type="math/tex">\textbf{x}</script> 是二維， <script type="math/tex">\textbf{y}</script> 是三維，建立一個命名為 <code>l2</code> 的 Linear 模組：</p>

<p>```lua</p>

<p>l2 = nn.Linear(2,3)</p>

<p>```</p>

<p>用以下方法印出 l2 的 <code>weight</code> , <code>bias</code> 和 <code>output</code> ：</p>

<p>```lua</p>

<p>print(l2.weight)
print(l2.bias)
print(l2.output)</p>

<p>```</p>

<p>輸出結果如下：</p>

<p>```sh</p>

<p>-0.2863  0.5541
-0.6269  0.6557
-0.3215 -0.1648
[torch.DoubleTensor of size 3x2]</p>

<p>-0.0316
 0.4126
 0.4415
[torch.DoubleTensor of size 3]</p>

<p>[torch.DoubleTensor with no dimension]</p>

<p>```</p>

<p>其中，<code>weight</code> 和 <code>bias</code> 會被初始化隨機成 size 3x2 和 size 3 的 double tensor ，而最後一行顯示出 <code>output</code> 還是空的（with no dimension）。</p>

<p>如果想知道各個 variable 的 size ，還有個方式，就是直接用 print 的方式把它印出來，作法如下：</p>

<p>```lua</p>

<p>print(l2)</p>

<p>```</p>

<p>執行結果如下：</p>

<p>```sh</p>

<p>nn.Linear(2 -&gt; 3)
{
  gradBias : DoubleTensor - size: 3
  weight : DoubleTensor - size: 3x2
  _type : torch.DoubleTensor
  output : DoubleTensor - empty
  gradInput : DoubleTensor - empty
  bias : DoubleTensor - size: 3
  gradWeight : DoubleTensor - size: 3x2
}</p>

<p>```</p>

<p>要讓 <code>output</code> 有值，就要進行 forward propagation 。而 <code>Linear:updateOutput</code> 則是實作了 <code>Module:updateOutput</code> 中， forward propagation 運算的實際內容，程式碼如下：</p>

<p>```lua nn/Linear.lua</p>

<p>function Linear:updateOutput(input)
   if input:dim() == 1 then
      self.output:resize(self.weight:size(1))
      if self.bias then self.output:copy(self.bias) else self.output:zero() end
      self.output:addmv(1, self.weight, input)
   elseif input:dim() == 2 then
      local nframe = input:size(1)
      local nElement = self.output:nElement()
      self.output:resize(nframe, self.weight:size(1))
      if self.output:nElement() ~= nElement then
         self.output:zero()
      end
      updateAddBuffer(self, input)
      self.output:addmm(0, self.output, 1, input, self.weight:t())
      if self.bias then self.output:addr(1, self.addBuffer, self.bias) end
   else
      error(‘input must be vector or matrix’)
   end</p>

<p>return self.output
end</p>

<p>```</p>

<p>以上可以分為兩部分來看，首先是當 <code>input:dim() ==1</code> 時，也就是 <code>input</code> 的維度為 1 ，也就是一次只輸入單筆資料的時候。第一步，會先調整 <code>output</code> 的 <code>size</code> 為適當的大小，再將 <code>bias</code> 複製到 <code>output</code> ，再讓 <code>input</code> 和 <code>weight</code> 進行矩陣相乘，並和 <code>output</code> 相加，再輸出結果。</p>

<p>從數學公式上來看，輸入值 <script type="math/tex">\textbf{x}</script> 是一維的向量，進行的運算如下：</p>

<script type="math/tex; mode=display">
\textbf{W}\textbf{x} + \textbf{b}
</script>

<p>此時， <script type="math/tex">\textbf{W}</script> 放前面，而 <script type="math/tex">\textbf{x}</script> 放後面。</p>

<p>例如當輸入值向量 <script type="math/tex">[0,1]</script> 時，則矩陣運算的結果為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\begin{bmatrix}
-0.2863 & 0.5541 \\
-0.6269 & 0.6557 \\
-0.3215 & -0.1648 \\
\end{bmatrix}
\begin{bmatrix}
0 \\
1
\end{bmatrix}
+ 
\begin{bmatrix}
-0.0316 \\
 0.4126 \\
 0.4415 \\
\end{bmatrix}
=
\begin{bmatrix}
-0.2863 \times 0 + 0.5541 \times 1  -0.0316 \\
-0.6269 \times 0 + 0.6557 \times 1 + 0.4126\\
-0.3215 \times 0  -0.1648 \times 1 + 0.4415\\
\end{bmatrix}\\
&=
\begin{bmatrix}
0.5541 -0.0316 \\
0.6557 + 0.4126\\
-0.1648 + 0.4415\\
\end{bmatrix}
=
\begin{bmatrix}
  0.5225 \\
  1.0683  \\
  0.2766 \\
\end{bmatrix}
\end{align}

 %]]&gt;</script>

<p>實作以上算式，呼叫 <code>l2:forward</code> ，輸入 <code>torch.Tensor{0,1}</code> 並印出結果：</p>

<p>```lua</p>

<p>print(l2:forward(torch.Tensor{0,1}))</p>

<p>```</p>

<p>結果如下：</p>

<p>```sh</p>

<p>0.5225
 1.0683
 0.2766
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<p>這時已經進行過了 forward 運算，而 <code>output</code> 有值了，所以可以印出它的值，方法如下：</p>

<p>```lua</p>

<p>print(l2.output)</p>

<p>```</p>

<p><code>output</code> 的值也是如下：</p>

<p>```sh</p>

<p>0.5225
 1.0683
 0.2766
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<p>而當 <code>input:dim() ==2</code> 時，也就是 <code>input</code> 的維度為 2 ，也就是一次輸入多筆資料。這時需要先把 <code>weight</code> 進行轉置，再和 <code>input</code> ，並和 <code>bias</code> 相加，並將結果加到 <code>output</code> 並輸出。</p>

<p>一次輸入多筆資料的目的是為了加速運算，因為用矩陣對矩陣的相乘的方式就可以來加速。這樣同時輸入的一批資料，就稱為 batch 。</p>

<p>從公式上來看，輸入值 <script type="math/tex">\textbf{X}</script> 是二維的矩陣，則進行以下矩陣運算：</p>

<script type="math/tex; mode=display">
\textbf{X}\textbf{W}^{T} + \textbf{B}
</script>

<p>此時， <script type="math/tex">\textbf{X}</script> 放前面，而 <script type="math/tex">\textbf{W}</script> 進行轉置後放後面。</p>

<p>而 <script type="math/tex">\textbf{B}</script> 是將 <script type="math/tex">\textbf{b}</script> 轉置以後，再複製其橫排所形成的矩陣，以便和前面的矩陣相乘結果來相加。</p>

<p>例如當輸入資料有兩筆向量 <script type="math/tex">[0, 1]</script> 和 <script type="math/tex">[2, 1]</script> 時，則可以組成以下矩陣 (2x2) ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
0 &1 \\
2 &1 \\
\end{bmatrix}


 %]]&gt;</script>

<p>則矩陣運算的過程如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&
\begin{bmatrix}
0 &1 \\
2 &1 \\
\end{bmatrix}
\begin{bmatrix}
-0.2863 & -0.6269 & -0.3215 \\
 0.5541 & 0.6557 & -0.1648 \\
 \end{bmatrix}
+ 
\begin{bmatrix}
-0.0316 & 0.4126 & 0.4415 \\
-0.0316 & 0.4126 & 0.4415 
\end{bmatrix}\\
&
=
\begin{bmatrix}
   -0.2863 \times 0 +  0.5541 \times 1
&  -0.6269 \times 0 +  0.6557 \times 1
&  -0.3215 \times 0   -0.1648 \times 1\\

   -0.2863 \times 2 +  0.5541 \times 1
&  -0.6269 \times 2 +  0.6557 \times 1
&  -0.3215 \times 2   -0.1648 \times 1\\
\end{bmatrix}\\
&
+ 
\begin{bmatrix}
-0.0316 & 0.4126 & 0.4415 \\
-0.0316 & 0.4126 & 0.4415 
\end{bmatrix}\\
&
=
\begin{bmatrix}
0.5541 -0.0316
& 0.6557 +0.4126
& -0.1648 +0.4415 \\
-0.0186  -0.0316
& -0.5981 +0.4126
& -0.8079 +0.4415 \\
\end{bmatrix}\\
&
=
\begin{bmatrix}
 0.5225 & 1.0683  & 0.2766 \\
-0.0502 & -0.1855 & -0.3664 \\
\end{bmatrix}
\end{align}\\

 %]]&gt;</script>

<p>輸出結果為一個 2x3 的矩陣，每一個橫排為一個三維向量，代表著每一筆資料經過線性運算的結果。</p>

<p>實作以上算式，呼叫 <code>l2:forward</code> ，輸入由 <code>{0,1}</code> 和 <code>{2,1}</code> 這兩筆資料組成的 batch， 並印出結果：</p>

<p>
```lua</p>

<p>input={
   {0,1},
   {2,1}
}
print(l2:forward(torch.Tensor(input)))</p>

<p>```
</p>

<p>結果如下：</p>

<p>```sh</p>

<p>0.5225  1.0683  0.2766
-0.0502 -0.1855 -0.3664
[torch.DoubleTensor of size 2x3]</p>

<p>```</p>

<p>也可印出 <code>output</code> 的值：</p>

<p>```lua</p>

<p>print(l2.output)</p>

<p>```</p>

<p>結果如下：</p>

<p>```sh</p>

<p>0.5225  1.0683  0.2766
-0.0502 -0.1855 -0.3664
[torch.DoubleTensor of size 2x3]</p>

<p>```</p>

<h2 id="materials">Materials</h2>

<p>本次教學的完整程式碼於此：</p>

<p><a href="https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/1_nn_module_and_linear.ipynb">https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/1_nn_module_and_linear.ipynb</a></p>

]]></content>
  </entry>
  
</feed>
