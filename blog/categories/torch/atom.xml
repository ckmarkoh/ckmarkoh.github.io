<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Torch | Mark Chang's Blog]]></title>
  <link href="http://ckmarkoh.github.io/blog/categories/torch/atom.xml" rel="self"/>
  <link href="http://ckmarkoh.github.io/"/>
  <updated>2017-05-31T01:09:05+08:00</updated>
  <id>http://ckmarkoh.github.io/</id>
  <author>
    <name><![CDATA[Mark Chang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Torch NN Tutorial 6 : Backward Propagation ( Part 3 : nn.Module )]]></title>
    <link href="http://ckmarkoh.github.io/blog/2017/01/01/torch-nn-tutorial-6-backward-propagation/"/>
    <updated>2017-01-01T18:36:08+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2017/01/01/torch-nn-tutorial-6-backward-propagation</id>
    <content type="html"><![CDATA[<h2 id="backward-propagation-in-nnmodule">Backward Propagation in nn.Module</h2>

<p>本文接續 <a href="/blog/2017/01/01/torch-nn-tutorial-4-backward-propagation/">Torch NN Tutorial 4: Backward Propagation (part 1)</a> ，與 <a href="/blog/2017/01/01/torch-nn-tutorial-5-backward-propagation/">Torch NN Tutorial 5: Backward Propagation (part 2)</a> 講解 <code>nn.Module</code> 中， 與 backward propagation 有關的程式碼。</p>

<p><code>nn.Module</code> 程式碼：<a href="https://github.com/torch/nn/blob/master/Module.lua">https://github.com/torch/nn/blob/master/Module.lua</a></p>

<p>與 backward propagation 有關的，主要有以下三個運算 ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\text{1.} \mspace{20mu} \Delta_\textbf{W} = 0 \\
&   \mspace{40mu} \Delta_\textbf{b} = 0 \\
&\text{2.} \mspace{20mu} \Delta_\textbf{W} =  \frac{\partial J}{\partial \textbf{W}} =  \frac{\partial J}{\partial \textbf{y}}  \frac{\partial \textbf{y}}{\partial \textbf{W}}  \\ 
&   \mspace{40mu} \Delta_\textbf{b} =  \frac{\partial J}{\partial \textbf{b}} =  \frac{\partial J}{\partial \textbf{y}}  \frac{\partial \textbf{y}}{\partial \textbf{b}}  \\
&\text{3.} \mspace{20mu} \textbf{W} \leftarrow \textbf{W}  - \eta \Delta_\textbf{W} \\ 
&   \mspace{40mu} \textbf{b} \leftarrow \textbf{b} - \eta \Delta_\textbf{b} \\
\end{align}
 %]]&gt;</script>

<p>而這三個運算，分別對應到以下三個 function ：</p>

<ol>
  <li>
    <p>zeroGradParameters</p>
  </li>
  <li>
    <p>backward</p>
  </li>
  <li>
    <p>updateParameters</p>
  </li>
</ol>

<p>本文將詳細講解這三個 function 的程式碼內容。</p>

<p>註：</p>

<p>1.本文的 <script type="math/tex">\textbf{y}</script> 同前兩篇文（Torch NN Tutorial 4~5）中的 <script type="math/tex">\bar{y}</script> 。</p>

<p>2.本文中的 <script type="math/tex">\textbf{W}</script> 為矩陣，而 <script type="math/tex">\textbf{x}</script> ，<script type="math/tex">\textbf{y}</script> 為向量。</p>

<!--more-->

<h2 id="zerogradparameters">1.zeroGradParameters</h2>

<p>此步驟是將 <script type="math/tex">\Delta_\textbf{W}</script> 和 <script type="math/tex">\Delta_\textbf{b}</script> 歸零。</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \Delta_\textbf{W} = 0 \\
& \Delta_\textbf{b} = 0 \\
\end{align}
 %]]&gt;</script>

<p>在程式中， <script type="math/tex">\Delta_\textbf{W}</script> 和 <script type="math/tex">\Delta_\textbf{b}</script> 的變數是 <code>gradWeight</code> 和 <code>gradBias</code> 。可用 <code>zeroGradParameters</code> 將它們歸零，程式碼如下：</p>

<p>``` lua nn/Module.lua</p>

<p>function Module:zeroGradParameters()
   local _,gradParams = self:parameters()
   if gradParams then
      for i=1,#gradParams do
         gradParams[i]:zero()
      end
   end
end</p>

<p>```</p>

<p>在第五行中，可看到 <code>gradParams[i]:zero()</code> ，即是將所有的 <code>gradParams</code> ，包含 <code>gradWeight</code> 及 <code>gradBias</code> 歸零。</p>

<p>由於 <code>nn.Module</code> 本身沒有 <code>gradWeight</code> 和 <code>gradBias</code> ，如果要取得它們，就要透過 <code>self:parameters()</code> 來取得。
<code>parameters</code> 的程式碼如下：</p>

<p>``` lua nn/Module.lua</p>

<p>function Module:parameters()
   if self.weight and self.bias then
      return {self.weight, self.bias}, {self.gradWeight, self.gradBias}
   elseif self.weight then
      return {self.weight}, {self.gradWeight}
   elseif self.bias then
      return {self.bias}, {self.gradBias}
   else
      return
   end
end</p>

<p>```</p>

<p>從第二行開始，可看到，如果繼承 <code>nn.Module</code> 的類別有實作 <code>weight</code> ，則回傳 <code>weight</code> 和 <code>gradWeight</code> ，以此類推。</p>

<p>至於歸零的作用，因為在建立 Module 的時候， <code>gradWeight</code> 和 <code>gradBias</code> 並沒有被設為什麼值，它有可能是任意數。舉個例子，建立一個 <code>nn.Linear</code> ，命名為 <code>l1</code> ， input size 為 2， output size 為 3，一開始什麼都沒做，就直接印出  <code>gradWeight</code> 和 <code>gradBias</code> ，程式如下：</p>

<p>``` lua</p>

<p>l1 = nn.Linear(2,3)
print(l1.gradWeight)
print(l1.gradBias)</p>

<p>```</p>

<p>得出結果如下，它們可能是任意數：</p>

<p>``` sh</p>

<p>-1.2882e-231 -1.2882e-231
 2.1403e+161  3.9845e+252
  7.7075e-43   4.5713e-71
[torch.DoubleTensor of size 3x2]</p>

<p>-1.2882e-231
-1.2882e-231
  1.1659e-28
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<p>如果用 <code>zeroGradParameters</code> 將它們歸零，程式碼如下：</p>

<p>``` lua</p>

<p>l1:zeroGradParameters()
print(l1.gradWeight)
print(l1.gradBias)</p>

<p>```</p>

<p>結果如下，可看到它們都歸零了：</p>

<p>``` sh</p>

<p>0  0
  0  0
  0  0
[torch.DoubleTensor of size 3x2]</p>

<p>0
  0
  0
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<h2 id="backward">2.backward</h2>

<p>再來看到 <code>backward</code> 的部分， <code>backward</code> 個功能由兩個 <code>function</code> 來執行，分別是 <code>updateGradInput</code> 及 <code>accGradParameters</code> 。</p>

<p>``` lua nn/Module.lua</p>

<p>function Module:backward(input, gradOutput, scale)
   scale = scale or 1
   self:updateGradInput(input, gradOutput)
   self:accGradParameters(input, gradOutput, scale)
   return self.gradInput
end</p>

<p>``` </p>

<p>為何要分這兩部分？從數學公式上來看，如果 <code>input</code> 是 <script type="math/tex">\textbf{x}</script> ， <code>output</code> 是 <script type="math/tex">\textbf{y}</script> ，loss function 為 <script type="math/tex">J</script>   ，則：</p>

<script type="math/tex; mode=display">
 \textbf{y}=\textbf{W}\textbf{x}+\textbf{b}
</script>

<p><code>accGradParameters</code> 是負責計算以下兩項，也就是 <code>gradWeight</code> 和 <code>gradBias</code> 的值。</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \Delta_\textbf{W} =  \frac{\partial J}{\partial \textbf{W}} =  \frac{\partial J}{\partial \textbf{y}}  \frac{\partial \textbf{y}}{\partial \textbf{W}}  \\ 
& \Delta_\textbf{b} =  \frac{\partial J}{\partial \textbf{b}} =  \frac{\partial J}{\partial \textbf{y}}  \frac{\partial \textbf{y}}{\partial \textbf{b}}  \\
\end{align}
 %]]&gt;</script>

<p>除了以上兩項以外，如果 input 前面還有其他 layer 的話，就需計算 <script type="math/tex"> \frac{\partial J}{\partial \textbf{x}}</script> ，並將此值往前傳遞。</p>

<p><code>updateGradInput</code> 是負責計算 <script type="math/tex">\frac{\partial J}{\partial \textbf{x}}</script> 的值，也就是 <code>gradInput</code> 的值，如下：</p>

<script type="math/tex; mode=display">

\frac{\partial J}{\partial \textbf{x}} = \frac{\partial J}{\partial \textbf{y}}\frac{\partial \textbf{y}}{\partial \textbf{x}}

</script>

<p>在 <code>nn.Module</code> 中，這兩個 function 皆沒實作，需由繼承 <code>nn.Module</code> 的類別來實作。</p>

<p>``` lua nn/Module.lua</p>

<p>function Module:updateGradInput(input, gradOutput)
   return self.gradInput
end</p>

<p>function Module:accGradParameters(input, gradOutput, scale)
end</p>

<p>```</p>

<p><code>nn.Linear</code> 則實作了這兩個 function。</p>

<p><code>nn.Linear</code> 程式碼：<a href="https://github.com/torch/nn/blob/master/Linear.lua">https://github.com/torch/nn/blob/master/Linear.lua</a></p>

<h3 id="updategradinput">updateGradInput</h3>

<p>先看 <code>updateGradInput</code> 的程式碼：</p>

<p>``` lua nn/Linear.lua</p>

<p>function Linear:updateGradInput(input, gradOutput)
   if self.gradInput then</p>

<pre><code>  local nElement = self.gradInput:nElement()
  self.gradInput:resizeAs(input)
  if self.gradInput:nElement() ~= nElement then
     self.gradInput:zero()
  end
  if input:dim() == 1 then
     self.gradInput:addmv(0, 1, self.weight:t(), gradOutput)
  elseif input:dim() == 2 then
     self.gradInput:addmm(0, 1, gradOutput, self.weight)
  end

  return self.gradInput    end end
</code></pre>

<p>```</p>

<p>此處可分為兩部分來看，在 <a href="/blog/2016/12/19/torch-nn-tutorial-1-nn-module/">Torch NN Tutorial 1 : NN.Module &amp; NN.Linear</a> 曾講到，當 <code>input:dim() == 1</code> 時，是一次輸入一筆資料。</p>

<p>計算 <code>gradInput</code> 公式如下：</p>

<script type="math/tex; mode=display">

\frac{\partial J}{\partial \textbf{x}} = \frac{\partial J}{\partial \textbf{y}}\frac{\partial \textbf{y}}{\partial \textbf{x}}

</script>

<p>公式中的 <script type="math/tex">\frac{\partial J}{\partial \textbf{y}}</script> ，在程式中是 loss function 的 <code>gradInput</code> ，並從 <code>updateGradInput</code> 的參數 <code>gradOutput</code> 輸入。</p>

<p>假設 output size 為 3， input size 為 2，則在 forward propagation 時，<script type="math/tex">\textbf{y}</script> 是由以下公式算出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \textbf{y} = \textbf{W}\textbf{x} + \textbf{b} \\
& \Rightarrow
\begin{bmatrix}
y_{1} \\
y_{2} \\ 
y_{3} \\
\end{bmatrix}
= 
\begin{bmatrix}
w_{11} & w_{12} \\
w_{21} & w_{22} \\
w_{31} & w_{32} \\
\end{bmatrix}
\begin{bmatrix}
x_{1}
x_{2}
\end{bmatrix}
+
\begin{bmatrix}
b_{1} \\
b_{2} \\
b_{3} \\
\end{bmatrix} \\
= 
&\begin{bmatrix}
w_{11}x_{1}+w_{12}x_{2}+b_{1} \\
w_{21}x_{1}+w_{22}x_{2}+b_{2} \\
w_{31}x_{1}+w_{32}x_{2}+b_{3} \\
\end{bmatrix} \\
\end{align}
 %]]&gt;</script>

<p>而 <script type="math/tex"> \frac{\partial \textbf{y}}{\partial \textbf{x}} </script>  的結果如下，</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}
& \frac{\partial \textbf{y}}{\partial \textbf{x}} 
 = 
\begin{bmatrix}
\frac{\partial y_{1}}{\partial x_{1}} & \frac{\partial y_{1}}{\partial x_{2}} \\
\frac{\partial y_{2}}{\partial x_{1}} & \frac{\partial y_{2}}{\partial x_{2}} \\
\frac{\partial y_{3}}{\partial x_{1}} & \frac{\partial y_{3}}{\partial x_{2}} \\
\end{bmatrix} \\
&= 
\begin{bmatrix}
\frac{w_{11}x_{1}+w_{12}x_{2}+b_{1}}{\partial x_{1}} & \frac{w_{11}x_{1}+w_{12}x_{2}+b_{1}}{\partial x_{2}} \\
\frac{w_{21}x_{1}+w_{22}x_{2}+b_{2}}{\partial x_{1}} & \frac{w_{21}x_{1}+w_{22}x_{2}+b_{2}}{\partial x_{2}} \\
\frac{w_{31}x_{1}+w_{32}x_{2}+b_{3}}{\partial x_{1}} & \frac{w_{31}x_{1}+w_{32}x_{2}+b_{3}}{\partial x_{2}} \\
\end{bmatrix} \\
&= 
\begin{bmatrix}
w_{11} & w_{12} \\
w_{21} & w_{22} \\
w_{31} & w_{32} \\
\end{bmatrix}\\
&= \textbf{W}
\end{align}
 %]]&gt;</script>

<p>因此， <script type="math/tex">\frac{\partial J}{\partial \textbf{x}}</script> 的值為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\frac{\partial J}{\partial \textbf{x}} 
=\begin{bmatrix}
\frac{\partial J}{\partial x_{1}} \\
\frac{\partial J}{\partial x_{2}}
\end{bmatrix} \\
&=\begin{bmatrix}
\frac{\partial J}{\partial y_{1}}\frac{\partial y_{1}}{\partial x_{1}} +
\frac{\partial J}{\partial y_{2}}\frac{\partial y_{2}}{\partial x_{1}} +
\frac{\partial J}{\partial y_{3}}\frac{\partial y_{3}}{\partial x_{1}} 
\\
\frac{\partial J}{\partial y_{1}}\frac{\partial y_{1}}{\partial x_{2}} +
\frac{\partial J}{\partial y_{2}}\frac{\partial y_{2}}{\partial x_{2}} +
\frac{\partial J}{\partial y_{3}}\frac{\partial y_{3}}{\partial x_{2}} 
\\
\end{bmatrix} \\

&=
\begin{bmatrix}
\frac{\partial y_{1}}{\partial x_{1}} & \frac{\partial y_{1}}{\partial x_{2}} \\
\frac{\partial y_{2}}{\partial x_{1}} & \frac{\partial y_{2}}{\partial x_{2}} \\
\frac{\partial y_{3}}{\partial x_{1}} & \frac{\partial y_{3}}{\partial x_{2}} \\
\end{bmatrix} ^{T}
\begin{bmatrix}
\frac{\partial J}{\partial y_{1}} \\
\frac{\partial J}{\partial y_{2}} \\ 
\frac{\partial J}{\partial y_{3}} \\
\end{bmatrix}\\

&=
\textbf{W}^{T}\frac{\partial J}{\partial \textbf{y}}
\end{align}\\
 %]]&gt;</script>

<p>從以上結果得知，要先將 <code>weight</code> 轉置，再和 <code>gradOutput</code> 做 矩陣-向量 乘積。如同程式碼中所寫 <code>self.gradInput:addmv(0, 1, self.weight:t(), gradOutput)</code></p>

<p>如果 <script type="math/tex">W</script> 和 <script type="math/tex">\frac{\partial J}{\partial \textbf{y}}</script> 的值分別如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \textbf{W} =
\begin{bmatrix}
 0.1453 & 0.5062 \\
 0.0635 & 0.4911 \\
-0.1080 & 0.1747 \\
\end{bmatrix} \\
& \frac{\partial J}{\partial \textbf{y}} = 
\begin{bmatrix}
1 \\
2 \\
3 \\ 
\end{bmatrix}
\end{align}
 %]]&gt;</script>

<p>則， <script type="math/tex">\frac{\partial J}{\partial \textbf{x}}</script> 的值為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\frac{\partial J}{\partial \textbf{x}} 
=\textbf{W}^{T}\frac{\partial J}{\partial \textbf{y}}\\
&=
\begin{bmatrix}
 0.1453 & 0.0635 & -0.1080 \\ 
 0.5062 & 0.4911 & 0.1747 \\
 \end{bmatrix}
\begin{bmatrix}
1 \\
2 \\
3 \\ 
\end{bmatrix}\\
&=
\begin{bmatrix}
 0.1453 \times 1 & 0.0635 \times 2  & -0.1080 \times 3  \\ 
 0.5062 \times 1 & 0.4911 \times 2  & 0.1747 \times 3  \\
\end{bmatrix} \\
&=
\begin{bmatrix}
-0.0516 \\
 2.0126 \\
\end{bmatrix}\\
\end{align}
 %]]&gt;</script>

<p>執行 <code>updateGradInput</code>  ，程式碼如下：</p>

<p>``` lua</p>

<p>dj_dy = torch.Tensor{1,2,3}
x = torch.Tensor{4,5}
l1:updateGradInput( x,dj_dy )
print(l1.gradInput)</p>

<p>```</p>

<p>結果如下：</p>

<p>``` sh</p>

<p>-0.0516
 2.0126
[torch.DoubleTensor of size 2]</p>

<p>```</p>

<p>以上程式碼，輸入 <code>x</code> 只是為了滿足 <code>input:dim() == 1</code> 的條件， 而 <code>x</code> 的數值是不會影響結果的。</p>

<p>至於 <code>input:dim() == 2</code> 的情況，推導方式同上，在此不詳細推導。</p>

<p>再來看 <code>accGradParameters</code> 的程式碼：</p>

<h3 id="accgradparameters">accGradParameters</h3>

<p>``` lua nn/Linear.lua</p>

<p>function Linear:accGradParameters(input, gradOutput, scale)
   scale = scale or 1
   if input:dim() == 1 then
      self.gradWeight:addr(scale, gradOutput, input)
      if self.bias then self.gradBias:add(scale, gradOutput) end
   elseif input:dim() == 2 then
      self.gradWeight:addmm(scale, gradOutput:t(), input)
      if self.bias then
         – update the size of addBuffer if the input is not the same size as the one we had in last updateGradInput
         updateAddBuffer(self, input)
         self.gradBias:addmv(scale, gradOutput:t(), self.addBuffer)
      end
   end
end</p>

<p>```</p>

<p>先看到 <code>input:dim() == 1</code> 的情形，計算 <code>gradWeight</code> 的公式如下：</p>

<script type="math/tex; mode=display">

 \Delta_\textbf{W} =  \frac{\partial J}{\partial \textbf{W}} =  \frac{\partial J}{\partial \textbf{y}}  \frac{\partial \textbf{y}}{\partial \textbf{W}}  

</script>

<p>公式中的 <script type="math/tex">\frac{\partial J}{\partial \textbf{y}}</script> ，在程式中是 loss function 的 <code>gradInput</code> ，並從 <code>updateGradInput</code> 的參數 <code>gradOutput</code> 輸入。</p>

<p>假設 output size 為 3， input size 為 2，則 <script type="math/tex"> \frac{\partial \textbf{y}}{\partial \textbf{W}} </script>  的結果如下，</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}
& \frac{\partial \textbf{y}}{\partial \textbf{W}} 
 = 
\begin{bmatrix}
\frac{\partial y_{1}}{\partial w_{11}} & \frac{\partial y_{1}}{\partial w_{12}} \\
\frac{\partial y_{2}}{\partial w_{21}} & \frac{\partial y_{2}}{\partial w_{22}} \\
\frac{\partial y_{3}}{\partial w_{31}} & \frac{\partial y_{3}}{\partial w_{32}} \\
\end{bmatrix} \\
&= 
\begin{bmatrix}
\frac{w_{11}x_{1}+w_{12}x_{2}+b_{1}}{\partial w_{11}} & \frac{w_{11}x_{1}+w_{12}x_{2}+b_{1}}{\partial w_{12}} \\
\frac{w_{21}x_{1}+w_{22}x_{2}+b_{2}}{\partial w_{21}} & \frac{w_{21}x_{1}+w_{22}x_{2}+b_{2}}{\partial w_{22}} \\
\frac{w_{31}x_{1}+w_{32}x_{2}+b_{3}}{\partial w_{31}} & \frac{w_{31}x_{1}+w_{32}x_{2}+b_{3}}{\partial w_{32}} \\
\end{bmatrix} \\
&= 
\begin{bmatrix}
x_{1} & x_{2} \\
x_{1} & x_{2} \\
x_{1} & x_{2} \\
\end{bmatrix}\\
\end{align}
 %]]&gt;</script>

<p>因此， <script type="math/tex"> \frac{\partial J}{\partial \textbf{W}} </script> 的值為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \frac{\partial J}{\partial \textbf{W}} 
 = 
\begin{bmatrix}
\frac{\partial J}{\partial y_{1}} \frac{\partial y_{1}}{\partial w_{11}} &
\frac{\partial J}{\partial y_{1}} \frac{\partial y_{1}}{\partial w_{12}} \\
\frac{\partial J}{\partial y_{2}} \frac{\partial y_{2}}{\partial w_{21}} &
\frac{\partial J}{\partial y_{2}} \frac{\partial y_{2}}{\partial w_{22}} \\
\frac{\partial J}{\partial y_{3}} \frac{\partial y_{3}}{\partial w_{31}} &
\frac{\partial J}{\partial y_{3}} \frac{\partial y_{3}}{\partial w_{32}} \\
\end{bmatrix} \\
&= 
\begin{bmatrix}
\frac{\partial J}{\partial y_{1}} x_{1} &
\frac{\partial J}{\partial y_{1}} x_{2} \\
\frac{\partial J}{\partial y_{2}} x_{1} &
\frac{\partial J}{\partial y_{2}} x_{2} \\
\frac{\partial J}{\partial y_{3}} x_{1} &
\frac{\partial J}{\partial y_{3}} x_{2} \\
\end{bmatrix} \\
&= 
\begin{bmatrix}
\frac{\partial J}{\partial y_{1}} \\
\frac{\partial J}{\partial y_{2}} \\
\frac{\partial J}{\partial y_{3}} \\
\end{bmatrix}
\begin{bmatrix}
x_{1} &  x_{2} \\
\end{bmatrix}\\
&= \frac{\partial J}{\partial \textbf{y}} \textbf{x}^{T}
\end{align}
 %]]&gt;</script>

<p>如果 <script type="math/tex">\frac{\partial J}{\partial \textbf{y}}</script> 的值同前例， <script type="math/tex">\textbf{x}</script> 的值如下：</p>

<script type="math/tex; mode=display">
\textbf{x} =
\begin{bmatrix}
4 \\
5 \\
\end{bmatrix} 
</script>

<p>則， <script type="math/tex">\frac{\partial J}{\partial \textbf{W}}</script> 的值為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}
& \frac{\partial J}{\partial \textbf{W}} 
= \frac{\partial J}{\partial \textbf{y}}\textbf{x}^{T}\\
& = 
\begin{bmatrix}
1 \\
2 \\
3 \\ 
\end{bmatrix}
\begin{bmatrix}
4 & 5 
\end{bmatrix}\\ 
&=
\begin{bmatrix}
1 \times 4 & 1 \times 5 \\
2 \times 4 & 2 \times 5 \\
3 \times 4 & 3 \times 5 \\ 
\end{bmatrix}\\
&=
\begin{bmatrix}
4 & 5 \\
8 & 10 \\
12 & 15 \\ 
\end{bmatrix}\\
\end{align}
 %]]&gt;</script>

<p>計算 <code>gradBias</code> 的公式如下：</p>

<script type="math/tex; mode=display">

 \Delta_\textbf{W} =  \frac{\partial J}{\partial \textbf{b}} =  \frac{\partial J}{\partial \textbf{y}}  \frac{\partial \textbf{y}}{\partial \textbf{b}}  

</script>

<p>而 <script type="math/tex"> \frac{\partial \textbf{y}}{\partial \textbf{b}} </script>  這項的結果如下，</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \frac{\partial \textbf{y}}{\partial \textbf{b}} 
= 
\begin{bmatrix}
\frac{\partial y_{1}}{\partial b_{1}} \\
\frac{\partial y_{2}}{\partial b_{2}} \\
\frac{\partial y_{3}}{\partial b_{3}} \\
\end{bmatrix} \\
&= 
\begin{bmatrix}
\frac{w_{11}x_{1}+w_{12}x_{2}+b_{1}}{\partial b_{1}} \\
\frac{w_{21}x_{1}+w_{22}x_{2}+b_{2}}{\partial b_{2}} \\
\frac{w_{31}x_{1}+w_{32}x_{2}+b_{3}}{\partial b_{3}} \\
\end{bmatrix} \\
&= 
\begin{bmatrix}
1 \\
1 \\
1 \\
\end{bmatrix}\\
\end{align}
 %]]&gt;</script>

<p>因此， <script type="math/tex"> \frac{\partial J}{\partial \textbf{b}} </script> 的值為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \frac{\partial J}{\partial \textbf{b}} 
 = 
\begin{bmatrix}
\frac{\partial J}{\partial y_{1}} \frac{\partial y_{1}}{\partial b_{1}} \\
\frac{\partial J}{\partial y_{1}} \frac{\partial y_{1}}{\partial b_{2}} \\
\frac{\partial J}{\partial y_{2}} \frac{\partial y_{2}}{\partial b_{3}} \\
\end{bmatrix} \\
&= 
\begin{bmatrix}
\frac{\partial J}{\partial y_{1}} \\
\frac{\partial J}{\partial y_{2}} \\
\frac{\partial J}{\partial y_{3}} \\
\end{bmatrix} \\
&= \frac{\partial J}{\partial \textbf{y}} \\
\end{align}
 %]]&gt;</script>

<p>因此， <script type="math/tex"> \frac{\partial J}{\partial \textbf{b}} </script> 的值，和 <script type="math/tex">\frac{\partial J}{\partial \textbf{y}}</script> 的值相同。</p>

<p>執行 <code>accGradParameters</code>  ，程式碼如下：</p>

<p>``` lua</p>

<p>dj_dy = torch.Tensor{1,2,3}
x = torch.Tensor{4,5}
print( l1:accGradParameters( x,dj_dy ))
print(l1.gradWeight)
print(l1.gradBias)</p>

<p>```</p>

<p>結果如下：</p>

<p>``` sh</p>

<p>4   5
  8  10
 12  15
[torch.DoubleTensor of size 3x2]</p>

<p>1
 2
 3
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<p>至於 <code>input:dim() == 2</code> 的情況，推導方式同上，在此不詳細推導。</p>

<h2 id="updateparameters">3.updateParameters</h2>

<p>此函數在 <code>nn.Module</code> 中即有實作其內容，它所進行的運算相當簡單，就是更新 <script type="math/tex">\textbf{W}</script> 和 <script type="math/tex">\textbf{b}</script> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \textbf{W} \leftarrow \textbf{W}  - \eta \Delta_\textbf{W} \\ 
& \textbf{b} \leftarrow \textbf{b} - \eta \Delta_\textbf{b} \\
\end{align}
 %]]&gt;</script>

<p>程式碼如下：</p>

<p>``` lua  nn/Module.lua</p>

<p>function Module:updateParameters(learningRate)
   local params, gradParams = self:parameters()
   if params then
      for i=1,#params do
         params[i]:add(-learningRate, gradParams[i])
      end
   end
end</p>

<p>```</p>

<p>其中，公式中的 <script type="math/tex">\eta</script> 即為程式碼中的 <code>learningRate</code> ，而程式中的第五行，則是負責將 <code>weight</code> 和 <code>bias</code> 更新。 其中， <code>weight</code> 和 <code>bias</code> 由第二行的 <code>parameters</code> 函數所取得，回傳到 <code>param</code> 中，而 <code>gradWeight</code> 和 <code>gradBias</code> 也用同樣方式取得，回傳到 <code>gradParams</code> 中。</p>

<p>假設 <script type="math/tex">\eta = 0.02</script> ，<script type="math/tex">\textbf{W} , \textbf{b} , \frac{\partial J}{\partial \textbf{W}} ,  \frac{\partial J}{\partial \textbf{b}}</script> 的值分別如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}
&\textbf{W}=
\begin{bmatrix}
 0.1453 & 0.5062 \\
 0.0635 & 0.4911 \\
-0.1080 & 0.1747 \\
\end{bmatrix} \\
&\textbf{b}=
\begin{bmatrix}
 0.2063 \\
-0.1635 \\
-0.0883 \\
\end{bmatrix} \\
& \frac{\partial J}{\partial \textbf{W}} = 
\begin{bmatrix}
4 & 5 \\
8 & 10 \\
12 & 15 \\ 
\end{bmatrix}\\
& \frac{\partial J}{\partial \textbf{b}} = 
\begin{bmatrix}
1 \\
2 \\
3 \\ 
\end{bmatrix}\\
\end{align}

 %]]&gt;</script>

<p>更新 <script type="math/tex">\textbf{W}</script> 和 <script type="math/tex">\textbf{b}</script> ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \textbf{W}  - \eta \Delta_\textbf{W} \\
& = 
\begin{bmatrix}
 0.1453 & 0.5062 \\
 0.0635 & 0.4911 \\
-0.1080 & 0.1747 \\
\end{bmatrix} -
0.02
\begin{bmatrix}
4 & 5 \\
8 & 10 \\
12 & 15 \\ 
\end{bmatrix}\\
& = 
\begin{bmatrix}
 0.0653 & 0.4062 \\
-0.0965 & 0.2911 \\
-0.3480 & -0.1253 \\
\end{bmatrix} \\

& \textbf{b}  - \eta \Delta_\textbf{b} \\
& = 
\begin{bmatrix}
 0.2063 \\
-0.1635 \\
-0.0883 \\
\end{bmatrix} -
0.02
\begin{bmatrix}
1 \\
2 \\
3 \\ 
\end{bmatrix}\\
&=
\begin{bmatrix}
 0.1863 \\
-0.2035 \\ 
-0.1483 \\
\end{bmatrix} 
\end{align}
 %]]&gt;</script>

<p>印出 <code>l1</code> 的 <code>weight</code> 和 <code>bias</code> ，執行 <code>updateParameters</code> ，比較執行前後的 <code>weight</code> 和 <code>bias</code> 差異，程式如下：</p>

<p>``` lua</p>

<p>print(l1.weight)
print(l1.bias)
l1:updateParameters(0.02)
print(l1.weight)
print(l1.bias)</p>

<p>```</p>

<p>結果如下：</p>

<p>``` sh</p>

<p>0.1453  0.5062
 0.0635  0.4911
-0.1080  0.1747
[torch.DoubleTensor of size 3x2]</p>

<p>0.2063
-0.1635
-0.0883
[torch.DoubleTensor of size 3]</p>

<p>0.0653  0.4062
-0.0965  0.2911
-0.3480 -0.1253
[torch.DoubleTensor of size 3x2]</p>

<p>0.1863
-0.2035
-0.1483
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<h2 id="materials">Materials</h2>

<p>本次教學的完整程式碼於此：</p>

<p><a href="https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/6_backward_propagation_part_3.ipynb">https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/6_backward_propagation_part_3.ipynb
</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Torch NN Tutorial 5 : Backward Propagation ( Part 2 : nn.Criterion )]]></title>
    <link href="http://ckmarkoh.github.io/blog/2017/01/01/torch-nn-tutorial-5-backward-propagation/"/>
    <updated>2017-01-01T15:17:57+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2017/01/01/torch-nn-tutorial-5-backward-propagation</id>
    <content type="html"><![CDATA[<h2 id="backward-propagation-in-nncriterion">Backward Propagation in nn.Criterion</h2>

<p>本文接續 <a href="/blog/2017/01/01/torch-nn-tutorial-4-backward-propagation/">Torch NN Tutorial 4: Backward Propagation (part 1)</a> ，講解 <code>nn.Criterion</code> 中，與 backward propagation 相關的程式碼。</p>

<p>Criterion 的 <code>forward</code>  是負責計算 loss funciton <script type="math/tex">J</script> 的值，而 <code>backward</code> 則是計算 <script type="math/tex">\frac{\partial J}{\partial \bar{y}}</script> 的值。其中， <script type="math/tex">\bar{y}</script> 為模型預測出的結果。</p>

<p><code>nn.Criterion</code> 程式碼：<a href="https://github.com/torch/nn/blob/master/Criterion.lua">https://github.com/torch/nn/blob/master/Criterion.lua</a></p>

<p><code>backward</code> 的部分，如下：</p>

<p>``` lua nn/Criterion.lua</p>

<p>function Criterion:backward(input, target)
   return self:updateGradInput(input, target)
end</p>

<p>function Criterion:updateGradInput(input, target)
end</p>

<p>```</p>

<p><code>nn.Criterion</code> 的 <code>backward</code> 只做一件事，也就是 <code>updateGradInput</code> ，而  <code>updateGradInput</code> 的運算內容則由繼承它的類別來實作。</p>

<!--more-->

<h2 id="backward-propagation-in-nnmsecriterion">Backward Propagation in nn.MSECriterion</h2>

<p>本文舉 <code>nn.MSECriterion</code> 為例。</p>

<p><code>nn.MSECriterion</code> 程式碼：<a href="https://github.com/torch/nn/blob/master/MSECriterion.lua">https://github.com/torch/nn/blob/master/MSECriterion.lua</a></p>

<p><code>updateGradInput</code> 的部分，如下：</p>

<p>``` lua nn/MSECriterion.lua</p>

<p>function MSECriterion:updateGradInput(input, target)
   input.THNN.MSECriterion_updateGradInput(
      input:cdata(),
      target:cdata(),
      self.gradInput:cdata(),
      self.sizeAverage
   )
   return self.gradInput
end</p>

<p>```</p>

<p>此部分的運算由 C 來實作，運算完後，回傳結果 <code>gradInput</code> 。</p>

<p>C 程式碼： <a href="https://github.com/torch/nn/blob/master/lib/THNN/generic/MSECriterion.c">https://github.com/torch/nn/blob/master/lib/THNN/generic/MSECriterion.c</a></p>

<p>``` c nn/lib/THNN/generic/MSECriterion.c</p>

<p>void THNN_(MSECriterion_updateGradInput)(
          THNNState *state,
          THTensor *input,
          THTensor *target,
          THTensor *gradInput,
          bool sizeAverage)
{
  THNN_CHECK_NELEMENT(input, target);</p>

<p>real norm = (sizeAverage ? 2./((real)THTensor_(nElement)(input)) : 2.);</p>

<p>THTensor_(resizeAs)(gradInput, input);
  TH_TENSOR_APPLY3(real, gradInput, real, input, real, target,
    <em>gradInput_data = norm * (</em>input_data - *target_data);
  );
}</p>

<p>```</p>

<p>主要的運算是由第13行的 <code>TH_TENSOR_APPLY3</code> 來執行，這個 function 的意思是，執行某個 function 在三個 tensor 上。而要執行的 function 是 <code>*gradInput_data = norm * (*input_data - *target_data)</code> ，它需要 tensor ，分別是 <code>input_data</code> ， <code>target_data</code> 和 <code>gradInput_data</code> 。</p>

<p>從數學公式上來看，公式如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \frac{\partial J}{\partial \bar{y} } = \frac{ \partial (\bar{y} -y)^2  }{\partial \bar{y}} \\
& = 2(\bar{y} -y)
\end{align}
 %]]&gt;</script>

<p>其中， <script type="math/tex"> \frac{\partial J}{\partial \bar{y} } </script> 為 <code>gradInput_data</code> ， <script type="math/tex">\bar{y}</script> 為 <code>input_data</code> ，而 <script type="math/tex">y</script> 為 <code>target_data</code> 。  <script type="math/tex">2</script> 為常數 <code>norm</code> 。</p>

<p>如果 <script type="math/tex"> \bar{y} </script> 和 <script type="math/tex">y</script> 皆為向量，則 <script type="math/tex"> \frac{\partial J}{\partial \bar{y} } </script> 則是針對 <script type="math/tex"> \bar{y} </script> 中的每個元素來分別計算，如果 <code>sizeAverage</code> 為 true 的話，<code>norm</code> 還要除以向量的長度。舉以下例子，<script type="math/tex"> \bar{y} = [1,1,1], y=[1,2,3] </script>  則 ：</p>

<script type="math/tex; mode=display">
\frac{\partial J}{\partial \bar{y} } = 
\begin{bmatrix}
\frac{\partial J}{\partial \bar{y}_{1} } \\
\frac{\partial J}{\partial \bar{y}_{2} } \\ 
\frac{\partial J}{\partial \bar{y}_{3} } \\
\end{bmatrix} 
=
\frac{2}{3}
\begin{bmatrix}
\bar{y}_{1} - y_{1} \\
\bar{y}_{2} - y_{2} \\
\bar{y}_{3} - y_{3} \\
\end{bmatrix} 
=
\frac{2}{3}
\begin{bmatrix}
1 - 1 \\
1 - 2 \\
1 - 3 \\
\end{bmatrix} 
=
\begin{bmatrix}
0.0000 \\
-0.6667 \\
-1.3333 \\
\end{bmatrix}
</script>

<p>其中， <script type="math/tex">\bar{y}_{1}</script> 代表 <script type="math/tex">\bar{y}</script> 中的第一的元素，而前面乘上 <script type="math/tex">\frac{2}{3}</script> 是因為 <code>sizeAverage</code> 為 <code>true</code> ，所以微分的結果要除以向量長度 3。</p>

<p>以 <code>backward</code> 執行以上運算，程式如下：</p>

<p>``` lua</p>

<p>y_ = torch.Tensor{
    {1},{1},{1}
}</p>

<p>y = torch.Tensor{
    {1},{2},{3}
}</p>

<p>c1 = nn.MSECriterion()
c1:backward(y_,y)
print(c1.gradInput)</p>

<p>```</p>

<p>結果如下：</p>

<p>``` sh</p>

<p>0.0000
-0.6667
-1.3333
[torch.DoubleTensor of size 3x1]</p>

<p>```</p>

<p>如果 <code>sizeAverage</code> 為 <code>false</code> 的話， 則不需再除以 <code>input</code> 的大小， <script type="math/tex"> \frac{\partial J}{\partial \bar{y} } </script> 的結果如下：</p>

<script type="math/tex; mode=display">
\frac{\partial J}{\partial \bar{y} } = 
\begin{bmatrix}
\frac{\partial J}{\partial \bar{y}_{1} } \\
\frac{\partial J}{\partial \bar{y}_{2} } \\ 
\frac{\partial J}{\partial \bar{y}_{3} } \\
\end{bmatrix} 
=
2
\begin{bmatrix}
\bar{y}_{1} - y_{1} \\
\bar{y}_{2} - y_{2} \\
\bar{y}_{3} - y_{3} \\
\end{bmatrix} 
=
2
\begin{bmatrix}
1 - 1 \\
1 - 2 \\
1 - 3 \\
\end{bmatrix} 
=
\begin{bmatrix}
0 \\
-2 \\
-4 \\
\end{bmatrix}
</script>

<p>將 <code>sizeAverage</code> 設為 <code>false</code> 的方法，即是在建立 <code>nn.MSECriterion</code> 時，輸入 <code>false</code> ，方法如下：</p>

<p>``` lua</p>

<p>c2 = nn.MSECriterion(false)
c2:backward(y_,y)
print(c2.gradInput)</p>

<p>```</p>

<p>以 <code>backward</code> 執行運算，結果如下：</p>

<p>``` sh</p>

<p>0
-2
-4
[torch.DoubleTensor of size 3x1]</p>

<p>```</p>

<h2 id="materials">Materials</h2>

<p>本次教學的完整程式碼於此：</p>

<p><a href="https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/5_backward_propagation_part_2.ipynb">https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/5_backward_propagation_part_2.ipynb
</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Torch NN Tutorial 4: Backward Propagation ( Part 1 : Overview & Linear Regression )]]></title>
    <link href="http://ckmarkoh.github.io/blog/2017/01/01/torch-nn-tutorial-4-backward-propagation/"/>
    <updated>2017-01-01T14:00:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2017/01/01/torch-nn-tutorial-4-backward-propagation</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>本文以 Linear Regression 為例，介紹 Torch nn 如何進行 Backward Propagation。</p>

<p>Linear Regression 是以機器學習的方式，學出以下函數：</p>

<script type="math/tex; mode=display">

y = wx+b

</script>

<p>以 Gradient Descent 的方式來進行 Linear regression 的流程如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\text{1.} \mspace{20mu} \text{initialize } w \text{ and } b \\
&\text{2.} \mspace{20mu} \text{for i=0 ; i < max_epoch; i++} \\
&\text{3.} \mspace{40mu} \bar{y} = wx+b \\
&\text{4.} \mspace{40mu} J = (\bar{y} - y )^2 \\
&\text{5.} \mspace{40mu} \Delta_w = 0 \\
&   \mspace{60mu} \Delta_b = 0 \\
&\text{6.} \mspace{40mu} \frac{\partial J}{\partial \bar{y}} = 2(\bar{y} - y) \\
&\text{7.} \mspace{40mu} \Delta_w =  \frac{\partial J}{\partial w} =  \frac{\partial J}{\partial \bar{y}}  \frac{\partial y}{\partial w} =  \frac{\partial J}{\partial \bar{y}} \times x  \\ 
&   \mspace{60mu} \Delta_b =  \frac{\partial J}{\partial b} =  \frac{\partial J}{\partial \bar{y}}  \frac{\partial y}{\partial b}  = \frac{\partial J}{\partial \bar{y}} \times 1 \\
&\text{8.} \mspace{40mu} w \leftarrow w  - \eta \Delta_w \\ 
&   \mspace{60mu} b \leftarrow b - \eta \Delta_b \\
\end{align}
 %]]&gt;</script>

<p>其中 <script type="math/tex">x</script> 為 training data， <script type="math/tex">y</script> 為 golden value ， <script type="math/tex">\bar{y}</script> 為 predicted value， <script type="math/tex">w</script> 和 <script type="math/tex">b</script> 分別為 weight 和 bias 。 max_epoch 為 for 迴圈執行次數。</p>

<p>以下詳細講解整個流程，並實作之。</p>

<!--more-->

<h2 id="linear-regression-by-gradient-descent">Linear Regression by Gradient Descent</h2>

<p>首先，載入 <code>nn</code> 套件，並產生 Training Data <script type="math/tex">x</script> 及 <script type="math/tex">y</script> ：</p>

<p>``` lua</p>

<p>require ‘nn’
x = torch.linspace(1,3,3):resize(3,1)
y = x*2+1
print(x)
print(y)</p>

<p>```</p>

<p>以上程式，假設 <script type="math/tex">y</script> 是由 <script type="math/tex">y = 2 x + 1 </script> 產生出來的。而訓練的目標，是要讓 <script type="math/tex">w=2, b=1</script> 。</p>

<p>產生出來的 <code>x</code> 為 <code>[1,2,3]</code> ， <code>y</code> 為 <code>[3,5,7]</code> 如下：</p>

<p>``` sh</p>

<p>1
 2
 3
[torch.DoubleTensor of size 3x1]</p>

<p>3
 5
 7
[torch.DoubleTensor of size 3x1]</p>

<p>```</p>

<p>為了方便以 batch 計算，將 <code>x</code> 及 <code>y</code> 調整成 3x1 的大小。</p>

<p>建立完 training data 之後，可以開始進行 Linear Regression。</p>

<p>第一步，將 <script type="math/tex">w</script> 和 <script type="math/tex">b</script> 以隨機值初始化。</p>

<script type="math/tex; mode=display"> \text{1.}  \mspace{20mu} \text{initialize } w \text{ and } b </script>

<p>建立 <code>nn.Linear</code> ，命名為 <code>l1</code> ，如下： </p>

<p>``` lua</p>

<p>l1 = nn.Linear(1,1)
print(l1.weight)
print(l1.bias)</p>

<p>```</p>

<p>一開始， <code>weight</code> 和 <code>bias</code> 會以隨機值初始化，結果如下：</p>

<p>``` sh</p>

<p>0.2055
[torch.DoubleTensor of size 1x1]</p>

<p>0.7159
[torch.DoubleTensor of size 1]</p>

<p>```</p>

<p>再來是建立 loss function，命名為 <code>c1</code> ：</p>

<p>``` sh</p>

<p>c1 = nn.MSECriterion()</p>

<p>```</p>

<p>由於 loss function 為 Mimimum Square Error，所以 criterion 採用 <code>nn.MSECriterion</code></p>

<p>再來，這裡先講解 for 迴圈中進行的運算。</p>

<p>第三步算 linear 的 forward propagation ：</p>

<script type="math/tex; mode=display">\text{3.} \mspace{20mu} \bar{y} = wx+b </script>

<p>將 <script type="math/tex">x,w,b</script> 的數值帶入以上公式，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \bar{y} = wx+b\\
& =
0.2055
\begin{bmatrix}
1 \\
2 \\
3 \\
\end{bmatrix}
+ 0.7159 \\
& =
\begin{bmatrix}
 0.2055 \times 1 + 0.7159 \\
 0.2055 \times 2 + 0.7159 \\
 0.2055 \times 3 + 0.7159 \\
\end{bmatrix} \\
& =
\begin{bmatrix}
 0.9214 \\
 1.1269 \\
 1.3325 \\
\end{bmatrix}
\end{align}
 %]]&gt;</script>

<p>程式碼如下：</p>

<p>``` lua</p>

<p>y_ = l1:forward(x)
print(y_)</p>

<p>```</p>

<p>結果如下：</p>

<p>``` sh</p>

<p>0.9214
 1.1269
 1.3325
[torch.DoubleTensor of size 3x1]</p>

<p>```</p>

<p>第四步，計算 coss function 的 forward propagation ：</p>

<script type="math/tex; mode=display">\text{4.} \mspace{20mu} J = (\bar{y} - y )^2 </script>

<p>將 <script type="math/tex">y,\bar{y}</script> 的數值帶入以上公式，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& J = (\bar{y} - y)^2\\
& =
\begin{bmatrix}
(0.9214 - 3)^2 \\
(1.1269 - 5)^2 \\
(1.3325 - 7)^2 \\
\end{bmatrix} \\
& =
\begin{bmatrix}
 4.3206 \\
 15.0009 \\
 32.1206 \\
\end{bmatrix}
\end{align}
 %]]&gt;</script>

<p>如果輸入的值有多個維度，則 loss function 會將個維度的值，加起來平均，結果如下：</p>

<script type="math/tex; mode=display">

J = \frac{4.3206 + 15.0009 + 32.1206}{3} = 17.147313377985 

</script>

<p>計算 <script type="math/tex">J</script> 值的程式碼如下：</p>

<p>``` lua</p>

<p>j = c1:forward(y_,y)
print(j)</p>

<p>```</p>

<p>loss 值 <code>j</code> 如下：</p>

<p>``` sh</p>

<p>17.147313377985 </p>

<p>```</p>

<p>再來，要進行 backward propagation。</p>

<p>第五步，先將 <script type="math/tex"> \Delta_w </script> 和 <script type="math/tex"> \Delta_b </script> 歸零：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\text{5.} \mspace{40mu} \Delta_w = 0 \\
&   \mspace{60mu} \Delta_b = 0 \\
\end{align}
 %]]&gt;</script>

<p><script type="math/tex"> \Delta_w </script> 和 <script type="math/tex"> \Delta_b </script> 在程式中所對應的值，是 <code>l1</code> 的 <code>gradWeight</code> 和 <code>gradBias</code> 。在還沒歸零之前，這兩變數可能是任意值，印出這兩數的值，程式如下：</p>

<p>``` lua</p>

<p>print(l1.gradWeight)
print(l1.gradBias)</p>

<p>```</p>

<p>結果如下：</p>

<p>``` sh</p>

<p>1e-154 *
 -1.4917
[torch.DoubleTensor of size 1x1]</p>

<p>1e-154 *
-1.4917
[torch.DoubleTensor of size 1]</p>

<p>```</p>

<p>可用函式 <code>zeroGradParameters</code> 將這兩個值歸零，程式如下：</p>

<p>``` lua</p>

<p>l1:zeroGradParameters()
print(l1.gradWeight)
print(l1.gradBias)</p>

<p>```</p>

<p>結果為0，表示已歸零：</p>

<p>``` sh</p>

<p>0
[torch.DoubleTensor of size 1x1]</p>

<p>0
[torch.DoubleTensor of size 1]</p>

<p>```</p>

<p>第六步，計算 <script type="math/tex">\frac{\partial J}{\partial \bar{y}}</script> 的值。</p>

<script type="math/tex; mode=display"> \text{6.} \mspace{40mu} \frac{\partial J}{\partial \bar{y}} = 2(\bar{y} - y) </script>

<p>將 <script type="math/tex">y,\bar{y}</script> 的數值帶入以上公式，得出：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \frac{\partial J}{\partial \bar{y}} = 2(\bar{y} - y)\\
& =
\begin{bmatrix}
2(0.9214 - 3) \\
2(1.1269 - 5) \\
2(1.3325 - 7) \\
\end{bmatrix} \\
& =
\begin{bmatrix}
-1.3857 \\
-2.5820 \\
-3.7784 \\
\end{bmatrix}
\end{align}
 %]]&gt;</script>

<p>由於 <script type="math/tex">\bar{y}</script> 有三筆資料，每筆資料都會各算出一個微分結果。
程式中，計算此值的方式即是呼叫 <code>c1</code> 的 <code>backward</code> ，如下：</p>

<p>``` lua</p>

<p>dj_dy_ =c1:backward(y<em>,y)
print(dj_dy</em>)</p>

<p>```</p>

<p>得出的結果即是 <script type="math/tex">\frac{\partial J}{\partial \bar{y} } </script> ，結果如下：</p>

<p>``` sh </p>

<p>-1.3857
-2.5820
-3.7784
[torch.DoubleTensor of size 3x1]</p>

<p>```</p>

<p>第七步，計算 <script type="math/tex"> \Delta_w </script> 和 <script type="math/tex"> \Delta_b </script> 的值：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\text{7.} \mspace{40mu} \Delta_w =  \frac{\partial J}{\partial w} =  \frac{\partial J}{\partial \bar{y}}  \frac{\partial y}{\partial w} =  \frac{\partial J}{\partial \bar{y}} \times x  \\ 
&   \mspace{60mu} \Delta_b =  \frac{\partial J}{\partial b} =  \frac{\partial J}{\partial \bar{y}}  \frac{\partial y}{\partial b}  = \frac{\partial J}{\partial \bar{y}} \times 1 \\
\end{align}
 %]]&gt;</script>

<p>先看 <script type="math/tex">\Delta_w </script> 的數值，將 <script type="math/tex">x</script> 的值，以及先前算出的 <script type="math/tex"> \frac{\partial J}{\partial \bar{y}}</script> 值代入，即可算出它：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& \Delta_w =  \frac{\partial J}{\partial \bar{y}} \times x \\
&=
\begin{bmatrix}
-1.3857 \times 1\\
-2.5820 \times 2\\
-3.7784 \times 3\\
\end{bmatrix}  \\
&=
\begin{bmatrix}
-1.3857 \\
-5.1640 \\
-11.3352 \\
\end{bmatrix}
\end{align}
 %]]&gt;</script>

<p>如果輸值為有多筆資料，則 <script type="math/tex">\Delta_w</script> 的最終結果會將每筆的結果累加起來，如下：</p>

<script type="math/tex; mode=display">
\Delta_w = -1.3857 + -5.1640 + -11.3352 = -17.8849
</script>

<p>而 <script type="math/tex">\Delta_b</script> 的值是把 <script type="math/tex"> \frac{\partial J}{\partial \bar{y}}</script> 中的每筆資料結果累積起來。</p>

<script type="math/tex; mode=display">
\Delta_b = -1.3857 + -2.5820 + -3.7784 = -7.7461
</script>

<p>以程式來計算此兩值。呼叫 <code>l1</code> 的 <code>backward</code> ，輸入 <script type="math/tex">\frac{\partial J}{\partial \bar{y} }</script> 到 <code>backward</code> 後，它與  <script type="math/tex">\frac{\partial \bar{y}}{\partial w}</script> 和 <script type="math/tex">\frac{\partial \bar{y}}{\partial b}</script> 相乘後結果分別為 <script type="math/tex">\Delta_w</script> 和 <script type="math/tex">\Delta_b</script> ，此兩數分別儲存於 <code>gradWeight</code> 和 <code>gradBias</code> 。</p>

<p>``` lua</p>

<p>l1:backward(x, dj_dy_ )
print(l1.gradWeight)
print(l1.gradBias)</p>

<p>```</p>

<p>結果如下：</p>

<p>``` sh</p>

<p>-17.8849
[torch.DoubleTensor of size 1x1]</p>

<p>-7.7461
[torch.DoubleTensor of size 1]</p>

<p>```</p>

<p>第八步，更新 weight 和 bias ，公式如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}
&\text{8.} \mspace{40mu} w \leftarrow w  - \eta \Delta_w \\ 
&   \mspace{60mu} b \leftarrow b - \eta \Delta_b \\
\end{align}

 %]]&gt;</script>

<p>其中， <script type="math/tex">eta</script>  是指 learning rate 。令 <script type="math/tex">eta=0.2</script> ，更新 <script type="math/tex">w</script> 和 <script type="math/tex">b</script> ，數值如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
& w  - \eta \Delta_w 
= 0.2055 - 0.02 \times (- 17.8849) = 0.5632 \\
& b - \eta \Delta_b 
= 0.7159 - 0.02 \times (-7.7461) = 0.8708 \\
\end{align}
 %]]&gt;</script>

<p>程式中，更新 <code>l1</code> 的 <code>weight</code> 和 <code>bias</code> 可以用 <code>updateParameters</code> ，它的輸入即是 <script type="math/tex">eta</script> 。令 <script type="math/tex">eta=0.2</script> ，程式如下：</p>

<p>``` lua</p>

<p>l1:updateParameters(0.02)
print(l1.weight)
print(l1.bias)</p>

<p>```</p>

<p>更新完後印出 <code>weight</code> 和 <code>bias</code> 的值，結果如下：</p>

<p>``` sh</p>

<p>0.5632
[torch.DoubleTensor of size 1x1]</p>

<p>0.8708
[torch.DoubleTensor of size 1]</p>

<p>```</p>

<p>把第三到第八步的 for 迴圈，串連起來。此 for 迴圈連續跑 500 次，每 50 次印出一次結果，程式碼如下：</p>

<p>``` lua</p>

<p>for i = 1,500 do
    y_ = l1:forward(x)
    j = c1:forward(y<em>,y)
    l1:zeroGradParameters()
    dj_dy</em> =c1:backward(y<em>,y) 
    l1:backward(x,dj_dy</em>)
    l1:updateParameters(0.02)
    if i%50 == 0 then
        print(“i:” .. tostring(i)
           .. “ loss:” .. tostring(j) 
           .. “ weight:” .. tostring(l1.weight[1][1])
           .. “ bias:” .. tostring(l1.bias[1]) .. “\n”)
    end
end</p>

<p>```</p>

<p>執行此程式，在訓練完 500 次以後，<code>loss</code> 會接近 0 ，而 <code>weight</code> 和 <code>bias</code> 會接近 2 和 1 了。結果如下：</p>

<p>``` sh</p>

<p>i:50 loss:0.015879173659737 weight:1.8543434015475 bias:1.3310995564975</p>

<p>i:100 loss:0.0098066687579948 weight:1.885537390716 bias:1.2602004152583</p>

<p>…  </p>

<p>i:450 loss:0.00033602903432248 weight:1.9788119352928 bias:1.0481654513272</p>

<p>i:500 loss:0.00020752499774989 weight:1.9833490852405 bias:1.0378514430405</p>

<p>```  </p>

<h2 id="materials">Materials</h2>

<p>本次教學的完整程式碼於此：</p>

<p><a href="https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/4_backward_propagation_part_1.ipynb">https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/4_backward_propagation_part_1.ipynb
</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Torch NN Tutorial 3 : NN.Criterion & NN.MSECriterion]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/12/25/torch-nn-tutorial-3-nn-dot-criterion/"/>
    <updated>2016-12-25T18:48:14+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/12/25/torch-nn-tutorial-3-nn-dot-criterion</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>torch 的 <code>nn.Module</code> 是組成 neural network 的部分，但是要訓練一個 neural network ，就需要有 loss function 。而 <code>nn.Criterion</code> 就是用來計算 loss function 的值。<code>nn.Criterion</code> 是個抽象類別，所有種類的 loss function 都繼承於它。</p>

<p>例如， loss funciton 用 Minimum Square Error 時， <script type="math/tex">\bar{y}</script> 為模型預測出來的數值， <script type="math/tex">y</script> 為正確的數值 ，則 loss function <script type="math/tex"> L (y, \bar{y}) </script> 的公式如下：</p>

<script type="math/tex; mode=display">

l( y , \bar{y}) = (y  - \bar{y} )^2 

</script>

<p>在 torch 中，負責計算 Minimum Square Error 的 criterion 為 <code>nn.MSECriterion</code> 。</p>

<!-- more -->

<p>實作部份如下，首先，載入 <code>nn</code> 模組：</p>

<p>``` lua</p>

<p>require ‘nn’</p>

<p>```</p>

<p>建立 <code>nn.MSECriterion</code> ，命名為 <code>c1</code> ，如下：</p>

<p>``` lua</p>

<p>c1 = nn.MSECriterion()</p>

<p>```</p>

<p>假設模型預測出的數值 <script type="math/tex"> \bar{y} = 5</script> ，正確數值 <script type="math/tex">y=3</script> ，則：</p>

<script type="math/tex; mode=display">

l( y , \bar{y}) = (y  - \bar{y} )^2  = (3-5)^2 = 4

</script>

<p>實作如下，將 <code>y_ = 5</code> 與 <code>y = 3</code> 兩數值傳入 <code>c1</code> ，進行 forward propagation ，得出 loss function <code>l</code> 的數值。</p>

<p>``` lua</p>

<p>y_ = torch.Tensor{5}
y = torch.Tensor{3}
l = c1:forward(y_,y)
print(l)</p>

<p>```</p>

<p>執行結果， <code>l = 4</code> ，如下：</p>

<p>``` sh</p>

<p>4	</p>

<p>```</p>

<h2 id="nncriterion--nnmsecriterion">nn.Criterion &amp; nn.MSECriterion</h2>

<p>這邊講解 <code>nn.Criterion</code> 以及 <code>nn.MSECriterion</code> 的程式碼：</p>

<p><code>nn.Criterion</code> 程式碼：<a href="https://github.com/torch/nn/blob/master/Criterion.lua">https://github.com/torch/nn/blob/master/Criterion.lua</a></p>

<p><code>nn.MSECriterion</code> 程式碼： <a href="https://github.com/torch/nn/blob/master/MSECriterion.lua">https://github.com/torch/nn/blob/master/MSECriterion.lua</a></p>

<p>首先看到 <code>nn.Criterion</code> 的部分，先從 <code>init</code> 開始。</p>

<p>``` lua nn/Criterion.lua</p>

<p>local Criterion = torch.class(‘nn.Criterion’)</p>

<p>function Criterion:__init()
   self.gradInput = torch.Tensor()
   self.output = 0
end</p>

<p>```</p>

<p>從以上程式碼得知， <code>nn.Criterion</code> 雖然有 <code>output</code> 和 <code>gradInput</code> 這兩個變量，但它並沒有繼承 <code>nn.Module</code> 。</p>

<p>再來看到 forward propagation 的部分， <code>nn.Criterion</code> 的 <code>forward</code> 和 <code>nn.Module</code> 的不一樣，因為它的輸入有 <code>input</code> 和 <code>target</code> 這兩個變量，如下：</p>

<p>因為 loss function 的 forward propagation 需要有 input 和 target 這兩個數值才算得出來，而 module 中的 forward propagation 部分，通常只需要 input 的數值即可。</p>

<p>``` lua nn/Criterion.lua</p>

<p>function Criterion:forward(input, target)
   return self:updateOutput(input, target)
end</p>

<p>```</p>

<p>再來看到 <code>nn.MSECriterion</code> 的程式碼。</p>

<p>``` lua nn/MSECriterion.lua</p>

<p>function MSECriterion:__init(sizeAverage)
   parent.__init(self)
   if sizeAverage ~= nil then
     self.sizeAverage = sizeAverage
   else
     self.sizeAverage = true
   end
end</p>

<p>```</p>

<p>其中， <code>sizeAverage</code> 是設定要不要將 output 根據 input 的維度來做平均，預設值為 <code>true</code> 。</p>

<p>舉個例子，<code>y2_</code> 和 <code>y2</code> 分別為 input 與 target ，分別建立 <code>c2</code> 與 <code>c3</code> 兩個 <code>nn.MSECriterion</code> ， <code>c2</code> 的 <code>sizeAverage</code> 為 <code>true</code> ，而 <code>c3</code> 的 <code>sizeAverage</code> 為 <code>false</code> 。</p>

<p>``` lua</p>

<p>y2_ = torch.Tensor{5,5}
y2 = torch.Tensor{3,3}
c2 = nn.MSECriterion(true)
c3 = nn.MSECriterion(false)</p>

<p>```</p>

<p>將 <code>y2_</code> 和 <code>y2</code> 都輸入 <code>c2</code> 和 <code>c3</code> ，進行 forward propagation ，比較其結果差異，程式碼如下：</p>

<p>``` lua</p>

<p>print(c2:forward(y2<em>,y2))
print(c3:forward(y2</em>,y2))</p>

<p>```</p>

<p>則輸出結果如下：</p>

<p>``` sh</p>

<p>4	
8</p>

<p>```</p>

<p>以上， 4 是 <code>sizeAverage</code> 為 <code>true</code> 的結果，也就是將 <code>y2_</code> 和 <code>y2</code> 的每個元素相減後再平均，如下：</p>

<script type="math/tex; mode=display">

\frac{(5-3)^2 + (5-3)^2 }{2} = 4

</script>

<p>而 8 是 <code>sizeAverage</code> 為 <code>false</code> 的結果，也就是將 <code>y2_</code> 和 <code>y2</code> 的每個元素相減，但沒平均，如下：</p>

<script type="math/tex; mode=display">

(5-3)^2 + (5-3)^2  = 8

</script>

<p>這裡要注意的是，不管 <code>sizeAverage</code> 是 <code>ture</code> 或 <code>false</code> ，也不論 input 的維度有多少，output 都是一維的純量。</p>

<p>再來看到 <code>updateOutput</code> 的程式碼：</p>

<p>``` lua nn/Criterion.lua</p>

<p>function MSECriterion:updateOutput(input, target)
   self.output_tensor = self.output_tensor or input.new(1)
   input.THNN.MSECriterion_updateOutput(
      input:cdata(),
      target:cdata(),
      self.output_tensor:cdata(),
      self.sizeAverage
   )
   self.output = self.output_tensor[1]
   return self.output
end</p>

<p>``` </p>

<p>此部分的核心運算是用 C 來加速，在第三行會呼叫 C 的函數 <code>MSECriterion_updateOutput</code> ，再將結果存在 <code>output_tensor</code> ，而 lua 則從 <code>output_tensor</code> 中取出結果，傳到 <code>output</code> 。</p>

<p>C 的程式碼在 <a href="https://github.com/torch/nn/blob/master/lib/THNN/generic/MSECriterion.c">MSECriterion.c</a> 檔案中：</p>

<p>``` c  nn/lib/THNN/generic/MSECriterion.c</p>

<p>void THNN_(MSECriterion_updateOutput)(
          THNNState *state,
          THTensor *input,
          THTensor *target,
          THTensor *output,
          bool sizeAverage)
{
  THNN_CHECK_NELEMENT(input, target);
  THNN_CHECK_DIM_SIZE(output, 1, 0, 1);</p>

<p>real sum = 0;</p>

<p>TH_TENSOR_APPLY2(real, input, real, target,
    real z = (<em>input_data - *target_data);
    sum += z</em>z;
  );</p>

<p>if (sizeAverage)
    sum /= THTensor_(nElement)(input);</p>

<p>THTensor_(set1d)(output, 0, sum);
}</p>

<p>```</p>

<p>在第 14 ~ 15 行中可看到將 <code>input_data</code> 與 <code>target_data</code> 相減後得出 <code>z</code> ，並將 <code>z</code> 平方後累加，得到 <code>sum</code> ，而在 18 ~ 19 行中， <code>sum</code> 會根依序 <code>sizeAverage</code> 的值，來決定要不要除以所有元素的個數。</p>

<p>最後一行則將 <code>sum</code> 的值存到 <code>output</code> 中，這個 <code>output</code> 所對應到 lua 中的 <code>output_tensor</code> 。</p>

<p>關於 <code>updateGradInput</code> 以及 backward propagation 的部分，將在下回介紹。</p>

<h2 id="materials">Materials</h2>

<p>本次教學的完整程式碼於此：</p>

<p><a href="https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/3_nn_criterion_and_msecriterion.ipynb">https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/3_nn_criterion_and_msecriterion.ipynb</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Torch NN Tutorial 2 : NN.Container & NN.Sequential]]></title>
    <link href="http://ckmarkoh.github.io/blog/2016/12/24/torch-nn-tutorial-2-nn-container/"/>
    <updated>2016-12-24T00:24:25+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2016/12/24/torch-nn-tutorial-2-nn-container</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>在<a href="/blog/2016/12/19/torch-nn-tutorial-1-nn-module/">Torch NN tutorial 1 : NN.Module &amp; NN.Linear</a>中，介紹了構成 neural network 的最基本的單位，也就是 <code>nn.Module</code> ，並介紹了 <code>nn.Linear</code> 可以進行一次線性運算，但通常一個 neural network 是經由多個線性和非線性的運算構成的。要把多個 Module 串接起來，就需要用到 <code>nn.Contaner</code> 。</p>

<p>例如，有個 neural network 的輸入為 x ，輸出為 z ，中間經過了一次線性運算與一個sigmoid function ，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
&\textbf{y} = \textbf{W}\textbf{x} + \textbf{b}  \\
&\textbf{z} = \frac{1}{1+e^{-\textbf{y}}}
\end{align}
 %]]&gt;</script>

<p>假設 <script type="math/tex">\textbf{x}</script> 為 2 維的向量，而 <script type="math/tex">\textbf{y}</script> 為 3 維向量， 
<script type="math/tex">\textbf{W},\textbf{b}</script> 分別為 weight 和 bias ，此兩參數皆以隨機值進行初始化，
<script type="math/tex">\textbf{z}</script>  是 <script type="math/tex">\textbf{y}</script> 經過 sigmoid 非線性運算的輸出結果。</p>

<!--more-->

<p>使用 torch 實作此運算：</p>

<p>首先，載入 <code>nn</code> 套件：</p>

<p>``` lua</p>

<p>require ‘nn’</p>

<p>```</p>

<p>建立以上兩個運算所需的Module，分別為 <code>nn.Linear</code> 和 <code>nn.Sigmoid</code> ，並將它們分別命名為 <code>l1</code> 和 <code>s1</code> ，如下：</p>

<p>``` lua</p>

<p>l1 = nn.Linear(2,3)
s1 = nn.Sigmoid()</p>

<p>```</p>

<p>其中， <code>nn.Sigmoid</code> 為進行 sigmoid 運算所需的 module 。</p>

<p>如果要進行運算，方法如下：</p>

<p>假設 x 為一個二維向量 <script type="math/tex">[0,1]</script> ，先輸入 <code>l1</code> ，進行 forward propagation ，將運算結果傳給 <code>y</code> ，如下：</p>

<p>``` lua</p>

<p>x = torch.Tensor{0,1}
y = l1:forward(x)
print(y)</p>

<p>```</p>

<p>執行完後會印出 <code>l1</code> 的輸出值，也就是 <code>y</code> 的值，是個三維的向量，
y 的值會與 <code>l1.weight</code> 和 <code>l1.bias</code> 的初始值值關， <code>y</code> 值如下：</p>

<p>``` sh</p>

<p>0.1948
 0.9780
 0.3982
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<p>再來將 <code>y</code> 輸入到 <code>s1</code> 中， 進行 forward propagation，將運算結果傳給 <code>z</code> ，如下：</p>

<p>``` lua</p>

<p>z = s1:forward(y)
print(z)</p>

<p>```</p>

<p>執行完後會印出 <code>z</code> 值，此值是由 <code>y</code> 的值經過 sigmoid function 的運算所得出來的， <code>z</code> 值如下：</p>

<p>``` sh</p>

<p>0.5485
 0.7267
 0.5983
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<p>以上過程中，給定了 <code>x</code> 值，經過每一個 module 都要呼叫一次 <code>forward</code> ，最後才能取得 <code>z</code> 的值，
如果一個 network 是由很多個 module 所組成的，這樣要呼叫很多次 <code>forward</code> ，會很麻煩。</p>

<p>而 <code>nn.Container</code> 則是一個容器，它可以把多個 module 放進去，並自動處理這些 module 輸入與輸出值的傳遞。 <code>nn.Container</code> 也是個抽象個類別，泛指能夠把 module 放進去的容器。</p>

<p>其中一種 container 可將前一個 module 的輸出，傳給下一個 module 的輸入，這種 contanier 為 <code>nn.Sequential()</code> 。</p>

<p>例如，如果要將 <code>l1</code> 的輸出，傳入 <code>s1</code> ，則可以用 <code>nn.Sequential</code> 來達成。建立一個命名為 <code>n1</code> 的 sequential container ，將 <code>l1</code> 和 <code>s1</code> 串起來，方法如下：</p>

<p>``` lua</p>

<p>n1 = nn.Sequential()
n1:add(l1)
n1:add(s1)</p>

<p>```</p>

<p>其中，<code>add</code> 是將 module 加入 container 的 function。</p>

<p>其實， <code>nn.Container</code> 也是從 <code>nn.Module</code> 繼承而來的，所以它也有 <code>forward</code> ，可以進行 forward propagation ，只要呼叫了 <code>nn.Container</code> 的 <code>forward</code> ，它就會自動將裡面的 module 進行 forward ，並輸出結果。</p>

<p>如此給定 <code>x</code> 之後，只要進行一次 forward 即可取得 <code>z</code> 的結果，如下：</p>

<p>``` lua</p>

<p>x = torch.Tensor{0,1}
z = n1:forward(x)
print(z)</p>

<p>```</p>

<p>執行完後，會印出 <code>z</code> 的結果，如下：</p>

<p>``` sh</p>

<p>0.5485
 0.7267
 0.5983
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<h2 id="nncontainer--nnsequential">nn.Container &amp; nn.Sequential</h2>

<p>這邊要介紹 <code>nn.Container</code> 和 <code>nn.Sequential</code> 的程式碼。</p>

<p><code>nn.Container</code> 程式碼：<a href="https://github.com/torch/nn/blob/master/Container.lua">https://github.com/torch/nn/blob/master/Container.lua</a></p>

<p><code>nn.Sequential</code> 程式碼： <a href="https://github.com/torch/nn/blob/master/Sequential.lua">https://github.com/torch/nn/blob/master/Sequential.lua</a></p>

<p>首先，介紹 <code>nn.Container</code> ，先看 <code>init()</code> 的部分：</p>

<p>``` lua nn/Container.lua</p>

<p>local Container, parent = torch.class(‘nn.Container’, ‘nn.Module’)</p>

<p>function Container:__init(…)
    parent.__init(self, …)
    self.modules = {}
end</p>

<p>```</p>

<p>第1行處，看到 <code>nn.Container</code> 是從 <code>nn.Module</code> 繼承而來的，因此它具有 <code>nn.Module</code> 的 variable 及 function 。</p>

<p>第5行中，container 的變量 <code>modules</code> ，它是一個 lua table ，是用來存放加到 container 中的  module 。</p>

<p>至於，要如何將 module 加到 container 中？可以用 <code>add</code> 加入。 <code>add</code> 的程式碼如下： </p>

<p>``` lua nn/Container.lua</p>

<p>function Container:add(module)
    table.insert(self.modules, module)
    return self
end</p>

<p>```</p>

<p>在第2行可看到， <code>add</code> 可以把新的 module 加到 <code>modules</code> 中。</p>

<p>``` lua nn/Container.lua</p>

<p>function Container:get(index)
    return self.modules[index]
end</p>

<p>```</p>

<p>除了加進去之外，也可以用 <code>get</code> 取得 <code>modules</code> 中的某個元素，或是用 <code>size</code> 取得 <code>modules</code> 的大小，程式碼如下：</p>

<p>``` lua nn/Container.lua</p>

<p>function Container:get(index)
    return self.modules[index]
end</p>

<p>function Container:size()
    return #self.modules
end</p>

<p>```</p>

<p>再來是實作的部分，首先我們建立一個 sequential 的 container ，命名為 <code>n2</code> ，並以 <code>size</code> 取得它所含的 module 個數，如下：</p>

<p>``` lua</p>

<p>n2 = nn.Sequential()
print(n2:size())</p>

<p>```</p>

<p>剛建立時， 由於 <code>n2</code> 的 <code>modules</code> 是空的，所以 <code>size</code> 是 0。執行結果如下：</p>

<p>``` sh</p>

<p>0	</p>

<p>```</p>

<p>再來將 <code>l1</code> 和 <code>s1</code> 依序加入，再看看 <code>size</code> 的變化：</p>

<p>``` lua</p>

<p>n2:add(l1)
n2:add(s1)
print(n2:size())</p>

<p>```</p>

<p>此時已經加入了兩個 module 進去了，所以 <code>size</code> 會是 2 ，執行結果如下：</p>

<p>``` sh</p>

<p>2	</p>

<p>```</p>

<p>可以用 <code>get</code> 取得加進去的 module ，例如，想取得第一個加進去的，作法如下：</p>

<p>``` lua</p>

<p>print(n2:get(1))</p>

<p>```</p>

<p>第一個加進去的為 <code>nn.Linear</code> ，執行結果如下：</p>

<p>``` sh</p>

<p>nn.Linear(2 -&gt; 3)
{
  gradBias : DoubleTensor - size: 3
  weight : DoubleTensor - size: 3x2
  _type : torch.DoubleTensor
  output : DoubleTensor - size: 3
  gradInput : DoubleTensor - empty
  bias : DoubleTensor - size: 3
  gradWeight : DoubleTensor - size: 3x2
}</p>

<p>```</p>

<p>再來看到 <code>nn.Sequential()</code> 的程式碼。</p>

<p>``` lua nn/Sequential.lua</p>

<p>local Sequential, _ = torch.class(‘nn.Sequential’, ‘nn.Container’)</p>

<p>```</p>

<p>第一行顯示 <code>nn.Sequential</code> 繼承了 <code>nn.Container</code> 。它的 <code>init</code> 也與 <code>nn.Container</code> 共用，沒有再另外實作。</p>

<p>再來看 <code>add</code> 的程式碼。</p>

<p>``` lua nn/Sequential.lua</p>

<p>function Sequential:add(module)
   if #self.modules == 0 then
      self.gradInput = module.gradInput
   end
   table.insert(self.modules, module)
   self.output = module.output
   return self
end</p>

<p>```</p>

<p>在 <code>nn.Sequential</code> 中， <code>add</code> 除了會將 module 加到 <code>modules</code> 之外，它的 <code>output</code> 會是最後一個加進去的 module 的 <code>output</code> 。</p>

<p>以下實作，將 <code>n2</code> 的 <code>output</code> 和它裡面的兩個 module 的 <code>output</code> 都印出來看看：</p>

<p>``` lua</p>

<p>print(n2:get(1).output)
print(n2:get(2).output)
print(n2.output)</p>

<p>```</p>

<p>結果如下， <code>n2</code> 的 output 是第二個 module 的 output ：</p>

<p>```sh</p>

<p>0.1948
 0.9780
 0.3982
[torch.DoubleTensor of size 3]</p>

<p>0.5485
 0.7267
 0.5983
[torch.DoubleTensor of size 3]</p>

<p>0.5485
 0.7267
 0.5983
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<p>再來看到 <code>updateOutput</code> 的部分：</p>

<p>``` lua nn/Sequential.lua</p>

<p>function Sequential:updateOutput(input)
   local currentOutput = input
   for i=1,#self.modules do
      currentOutput = self:rethrowErrors(self.modules[i], i, ‘updateOutput’, currentOutput)
   end
   self.output = currentOutput
   return currentOutput
end</p>

<p>```</p>

<p>以上可知，在第3行的 for 迴圈中， <code>modules</code> 中的 <code>module</code> 會依序進行  <code>updateOutput</code> ，並將其 <code>output</code> 傳遞到下個 <code>module</code> 的 <code>input</code> ，而 <code>nn.Sequential</code> 的 <code>output</code> 會是最後一個 <code>module</code> 的 <code>output</code></p>

<p>假設輸入 <code>n2</code> 的 <code>x</code> 為 <script type="math/tex">[1,0]</script> ，則進行 forward propagation ，將輸出結果存到 <code>z</code> ，實作如下 ：</p>

<p>``` lua</p>

<p>x = torch.Tensor{1,0}
z = n2:forward(x)
print(z)</p>

<p>```</p>

<p>透過 updateOutput 中的 for 迴圈，它會依序跑完內部所有的 module，並得出最後結果，結果如下：</p>

<p>``` sh</p>

<p>0.5331
 0.6890
 0.5093
[torch.DoubleTensor of size 3]</p>

<p>```</p>

<p>如果想看 <code>n2</code> 中有哪些 module ，以及它們的順序，也可以直接用 <code>print</code> 的方式，作法如下：</p>

<p>``` lua </p>

<p>print(n2)</p>

<p>```</p>

<p>結果如下：</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;nn.Sequential <span class="o">{</span>
</span><span class='line'>  <span class="o">[</span>input -<span class="p">&amp;</span>gt<span class="p">;</span> <span class="o">(</span>1<span class="o">)</span> -<span class="p">&amp;</span>gt<span class="p">;</span> <span class="o">(</span>2<span class="o">)</span> -<span class="p">&amp;</span>gt<span class="p">;</span> output<span class="o">]</span>
</span><span class='line'>  <span class="o">(</span>1<span class="o">)</span>: nn.Linear<span class="o">(</span><span class="m">2</span> -<span class="p">&amp;</span>gt<span class="p">;</span> 3<span class="o">)</span>
</span><span class='line'>  <span class="o">(</span>2<span class="o">)</span>: nn.Sigmoid
</span><span class='line'><span class="o">}</span>
</span><span class='line'><span class="o">{</span>
</span><span class='line'>  gradInput : DoubleTensor - empty
</span><span class='line'>  modules :
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>      <span class="m">1</span> :
</span><span class='line'>        nn.Linear<span class="o">(</span><span class="m">2</span> -<span class="p">&amp;</span>gt<span class="p">;</span> 3<span class="o">)</span>
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>          gradBias : DoubleTensor - size: 3
</span><span class='line'>          weight : DoubleTensor - size: 3x2
</span><span class='line'>          _type : torch.DoubleTensor
</span><span class='line'>          output : DoubleTensor - size: 3
</span><span class='line'>          gradInput : DoubleTensor - empty
</span><span class='line'>          bias : DoubleTensor - size: 3
</span><span class='line'>          gradWeight : DoubleTensor - size: 3x2
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>      <span class="m">2</span> :
</span><span class='line'>        nn.Sigmoid
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>          gradInput : DoubleTensor - empty
</span><span class='line'>          _type : torch.DoubleTensor
</span><span class='line'>          output : DoubleTensor - size: 3
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  _type : torch.DoubleTensor
</span><span class='line'>  output : DoubleTensor - size: 3
</span><span class='line'><span class="o">}</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;
</span></code></pre></td></tr></table></div></figure></p>

<p>以上結果可分成兩部分來看，第一部分是 <code>nn.Sequential</code> 裡面每個 module 的順序，從 input 到 output 之間，依次進行了哪些運算。</p>

<p>第二部份是詳細印出 <code>nn.Sequential</code> 中，有哪些成員 ，它有從 <code>nn.Module</code> 繼承而來的 <code>gradInput</code> 即 <code>output</code> ，也有從 <code>nn.Container</code> 繼承而來的 <code>modules</code> 。而 <code>modules</code> 的部分會詳細列出裡面每個 module 中有哪些 variable。</p>

<p>這部分的程式碼於 <code>tostring</code> 函式中，程式碼如下：</p>

<p>``` lua nn/Sequential.lua</p>

<p>function Sequential:<strong>tostring</strong>()
   local tab = ‘  ‘
   local line = ‘\n’
   local next = ‘ -&gt; ‘
   local str = ‘nn.Sequential’
   str = str .. ‘ {‘ .. line .. tab .. ‘[input’
   for i=1,#self.modules do
      str = str .. next .. ‘(‘ .. i .. ‘)’
   end
   str = str .. next .. ‘output]’
   for i=1,#self.modules do
      str = str .. line .. tab .. ‘(‘ .. i .. ‘): ‘ .. tostring(self.modules[i]):gsub(line, line .. tab)
   end
   str = str .. line .. ‘}’
   return str
end</p>

<p>```</p>

<h2 id="materials">Materials</h2>

<p>本次教學的完整程式碼於此：</p>

<p><a href="https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/2_nn_container_and_sequential.ipynb">https://github.com/ckmarkoh/torch_nn_tutorials/blob/master/2_nn_container_and_sequential.ipynb</a></p>
]]></content>
  </entry>
  
</feed>
