<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Information Retrieval | Mark Chang's Blog]]></title>
  <link href="http://ckmarkoh.github.io/blog/categories/information-retrieval/atom.xml" rel="self"/>
  <link href="http://ckmarkoh.github.io/"/>
  <updated>2017-05-12T23:47:33+08:00</updated>
  <id>http://ckmarkoh.github.io/</id>
  <author>
    <name><![CDATA[Mark Chang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Boolean Retrieval]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/02/09/-information-retrieval-boolean-retrieval/"/>
    <updated>2015-02-09T00:27:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/02/09/-information-retrieval-boolean-retrieval</id>
    <content type="html"><![CDATA[<h2 id="information-retrieval">Information Retrieval</h2>

<p>所謂的資訊檢索（ <em>Information Retrieval</em> )，就是從大量非結構的資料，例如網頁，根據某些關鍵字，找出具有此關鍵字的文件。例如，搜尋引擎，即是一種資訊檢索的應用。</p>

<p>資訊檢索的演算法，其實跟我們要在某本書中，找尋某個字的方法差不多。</p>

<p>例如我們想在 <em>Introduction to Information Retrieval</em> 這本教科書中，找到 <em>informational queries</em> 這個詞在哪一頁，如果從第一頁開始，一個字一個字慢慢找，要比對成千上萬個字才能找到。</p>

<p>但如果我們翻到書本後面的 <em>Index</em> （如下） ，即可很快找到 <em>informational queries</em> 這個字是在第 <em>432</em> 頁。</p>

<p>```
…
information gain, 285 
information need, 5, 152 
information retrieval, 1 
informational queries, 432 
inner product, 121 
…</p>

<p>```</p>

<p>所以，資訊檢索，最核心的概念就是建立 <em>Index</em> ，這樣就可以快速找到哪些文件中具有某個關鍵字。</p>

<!--more-->

<h2 id="boolean-retrieval">Boolean Retrieval</h2>

<p>最基本的資訊檢索方法，就是 <em>Boolean Retrieval</em> 。它用 <em>boolean logic</em> 的概念，來建立 <em>index</em> 和執行 <em>query</em> 。</p>

<p>舉個例子，假設有三篇文劍，內容分別如下：</p>

<table>
  <thead>
    <tr>
      <th>文件</th>
      <th>內容</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>D1</td>
      <td>The way to avoid linearly scanning is to index the documents in advance.</td>
    </tr>
    <tr>
      <td>D2</td>
      <td>The model views each document as just a set of words.</td>
    </tr>
    <tr>
      <td>D3</td>
      <td>We will discuss and model these size assumption.</td>
    </tr>
  </tbody>
</table>

<p>根據這些文字，我們可以建立 <em>Term-Document Matrix</em> ，記錄哪個字出現在哪篇文章，如下：</p>

<table>
  <thead>
    <tr>
      <th>Term</th>
      <th>D1</th>
      <th>D2</th>
      <th>D3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>way</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>avoid</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>linear</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>scan</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>index</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>document</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>advance</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>model</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>view</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>set</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>word</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>discuss</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <td>size</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <td>assumption</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>此表格用 <em>boolean</em> 值來表示某個字出現在哪幾篇文章，例如， <em>way</em> 出現在文件 <em>D1</em> ，而沒出現在 <em>D2</em> 和 <em>D3</em> ，所以它的值為 <em>1 0 0</em> 。</p>

<p>為了避免此表格過大，建立過多無意義的 <em>index</em> ，因此運用了以下兩個原則：</p>

<pre><code>1. 忽略 *the* , *a* 這些高頻字（ 這種字稱為 *stop word* ）

2. 將有形態變化的字轉為原型，例如把 *views* 轉為 *view* （ 此過程稱為 *stemming* ） 
</code></pre>

<p>建立了 <em>Term-Document Matrix</em> 之後，就可以用關鍵字來查詢</p>

<p><strong>1.查詢 <em>way</em> 這個字，可得出 <em>way</em> 的值為：</strong></p>

<script type="math/tex; mode=display">

way = [1,0,0] 

</script>

<p>所以 <em>way</em> 出現在 <em>D1</em> 。</p>

<p><strong>2.查詢沒有 <em>way</em> 這個字的，可用以下的 <em>boolean</em> 運算得出結果：</strong></p>

<script type="math/tex; mode=display">

\neg way = \neg ([1,0,0]) = [0 ,1, 1]

</script>

<p>所以沒有 <em>way</em> 的文件為 <em>D2</em> 、 <em>D3</em> 。</p>

<p><strong>3.查詢包含 <em>document</em> 和 <em>model</em> 這兩個字都有文件：</strong></p>

<script type="math/tex; mode=display">

document \wedge model  = [1,1,0] \wedge [0,1,1] = [0 ,1, 0]

</script>

<p>得出結果為 <em>D2</em> 。</p>

<p><strong>4.查詢有 <em>avoid</em> 或 <em>view</em> 其中一個字的文件：</strong></p>

<script type="math/tex; mode=display">

avoid \vee view  = [1,0,0] \vee [0,1,0] = [1 ,1, 0]

</script>

<p>得出結果為 <em>D1</em> 和 <em>D2</em> 。</p>

<h2 id="reference">Reference</h2>

<p>本文參考教科書 <em>Introduction to Information Retrieval</em></p>

<p>http://nlp.stanford.edu/IR-book/</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TF-IDF]]></title>
    <link href="http://ckmarkoh.github.io/blog/2014/04/14/natural-language-processing-tf-idf/"/>
    <updated>2014-04-14T10:12:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2014/04/14/natural-language-processing-tf-idf</id>
    <content type="html"><![CDATA[<h2 id="introduction">1.Introduction</h2>

<p>所謂的 <em>TF-IDF</em> , 是用來找出一篇文章中, 足以代表這篇文章的關鍵字的方法</p>

<p>例如, 有一篇新聞, 是 <em>nltk</em> 的 <em>Reuters Corpus</em> 中的文章, 這篇文章被歸類在 <em>grain</em> , <em>ship</em> 這兩種類別下, 文章的內容如下：</p>

<blockquote>
  <p>GRAIN SHIPS LOADING AT PORTLAND 
There were three grain ships loading and two ships were waiting to load at Portland , according to the Portland Merchants Exchange .</p>
</blockquote>

<p>假設不知道什麼是 <em>TF-IDF</em>, 先用人工判別法試看看, 這篇新聞的關鍵字, 應該是 <em>Portland</em> , <em>ship</em> , <em>grain</em> 之類的字, 而不會是 <em>to</em> , <em>at</em> 這種常常出現的字</p>

<p>為什麼呢？因為 <em>to</em> 或 <em>at</em> 雖然在這篇文章中出現較多次, 但其他文章中也常有這些字, 所謂的關鍵的字, 應該是在這篇文章中出現較多次, 且在其他文章中比較少出現的字</p>

<p>所以,如果要在一篇文章中, 尋找這樣的關鍵字, 要考慮以下兩個要素:</p>

<!--more-->

<h4 id="term-frequency-tf-">1.這個字在這篇文章中出現的頻率( <em>Term-Frequency TF</em> )</h4>

<h4 id="inverse-document-frequency-idf-">2.在所有的文章中,有幾篇文章有這個字( <em>Inverse-Document-Frequency IDF</em> )</h4>

<p>這就是所謂的 IF-IDF</p>

<p>公式如下：</p>

<script type="math/tex; mode=display">

TF\text{-}IDF

\mspace{5mu}=\mspace{5mu} TF \times IDF

\mspace{5mu}=\mspace{5mu} n_{w}^{d} \times log_{2} \frac{N}{N_{w}}

</script>

<p>其中, <script type="math/tex">n_{w}^{d}</script> 表示在文章 <script type="math/tex">d</script> 中, 文字 <script type="math/tex">w</script> 出現的次數, <script type="math/tex">N</script> 表示總共有幾篇文章, <script type="math/tex">N_{w}</script> 表示有文字 <script type="math/tex">w</script> 的文章有幾篇</p>

<p>例如以上 <em>Reuters Corpus</em> 文章的例子, <em>Portland</em> 在這篇文章中出現了 <em>3</em> 次, 所以 <script type="math/tex">n_{w}^{d} =3 </script> , 而 <em>Reuters Corpus</em>  , 總共有 <em>10788</em> 篇文章, <script type="math/tex">N = 10788</script> , 在這些文章中, 有 <em>Portland</em> 這個字的文章, 有 <em>28</em> 篇, 所以 <script type="math/tex">N_{w} = 28.0</script> , 把這些數值帶入公式, 得出</p>

<script type="math/tex; mode=display">

TF\text{-}IDF_{Portland} \mspace{5mu}=\mspace{5mu}  3 \times log_{2} \frac{10788}{28} \mspace{5mu}  \approx \mspace{5mu} 25.769

</script>

<p>如果是 <em>to</em> 這個字, 在這篇文章出現 <em>2</em> 次, 但是總共有 <em>6944</em> 篇文章有這個字, 所以算出來的 <em>TF-IDF</em>  是</p>

<script type="math/tex; mode=display">

TF\text{-}IDF_{Portland} \mspace{5mu}=\mspace{5mu}  2 \times log_{2} \frac{10788}{6944} \mspace{5mu}  \approx \mspace{5mu} 1.271

</script>

<p>就這樣, 根據公式算出 <em>TF-IDF</em> 值的大小, 值越大的, 越可以作為代表這篇文章的關鍵字</p>

<h2 id="implementation">2.Implementation</h2>

<p>首先來瞭解一下 <em>nltk</em> 的 <em>Reuters Corpus</em>  和本文中的範例文章  </p>

<p>先到 <em>python</em> 的 <em>Interactive Mode</em> 載入 <code>reuters</code> 模組</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>from nltk.corpus import reuters</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>本文中的範例文章, 在 <em>Reuters Corpus</em>  裡面的 <em>Id</em> 為 <code>training/3386</code></p>

<p>可以把文章內容印出來看看</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>fileid = ‘training/3386’
“ “.join(reuters.words(fileids=fileid))
‘GRAIN SHIPS LOADING AT PORTLAND There were three grain ships loading and two ships were \
waiting to load at Portland , according to the Portland Merchants Exchange .’</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>也可以看一下這篇文章的分類, 是被分在 <em>grain</em> 和 <em>ship</em>  這兩個類別</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>reuters.categories(fileids=fileid)
[‘grain’, ‘ship’]</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>再來是實作 <em>TF_IDF</em> 的公式, </p>

<p>開一個新的檔案, 命名為 <em>tf_idf.py</em> , 並貼上以下程式碼</p>

<p>```python tf_idf.py
from nltk.corpus import reuters
from math import log 
from nltk import WordNetLemmatizer
wnl=WordNetLemmatizer()</p>

<p>_DN = float(len(reuters.fileids()))
_RTS = {k:[wnl.lemmatize(w.lower()) for w in reuters.words(k)] for k in reuters.fileids()}</p>

<p>def idf(w):
    w = wnl.lemmatize(w.lower())
    return log( _DN / sum([float(w in _RTS[k]) for k in _RTS.keys() ]) , 2)</p>

<p>def tf(w,f):
    w = wnl.lemmatize(w.lower())
    return sum([float(w == x) for x in _RTS[f] ]) </p>

<p>def tf_idf(w,f):
    return tf(w,f)*idf(w)</p>

<p>def run(f):
    word_tfidf = [(w,tf_idf(w,f)) for w in set(_RTS[f])] 
    for w,t in sorted(word_tfidf, key = lambda x : x[1], reverse=True):
        print “%-15s %.10f”%(w,t)</p>

<p>```</p>

<p>其中, <code>_DN</code> 是 <em>Reuters Corpus</em>  中, 所有文章的數量, </p>

<p>而 <code>_RTS = {k:[wnl.lemmatize(w.lower()) for w in reuters.words(k)] for k in reuters.fileids()}</code> </p>

<p>是先把所有文章中的字都轉成小寫, 並轉換成單數, 例如把 <em>SHIPS</em>, <em>ships</em> , <em>ship</em> 這三種字, 都轉成同一種字 <em>ship</em></p>

<p>剩下的 <em>function</em>  <code>idf(w)</code> , <code>tf(w,f)</code> , <code>tf_idf(w,f)</code> 就是 <em>TF-IDF</em> 公式, 而 <code>run(f)</code> 是把一篇文章每一個字都去跑 <em>TF-IDF</em> , 並印出結果</p>

<p>再來到 <em>python</em> 的 <em>Interactive Mode</em> 載入 <em>tf_idf.py</em> 這個檔案</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>import tf_idf as ti</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>這可能需要花十幾秒甚至數分鐘的時間, 因為程式要把所有大小寫和單複數不同的字, 都轉成同樣的字, 要花一些時間</p>

<p>再來, 別忘了先前看的那篇文章 <em>id</em> 是什麼, 因為等一下還會用到</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>fileid = ‘training/3386’</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>算一下 <em>Portland</em> 這個字在那篇文章中的 <em>TF</em> , <em>IDF</em> 以及 <em>TF-IDF</em></p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ti.tf(‘Portland’,fileid)
3.0</p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ti.idf(‘Portland’)
8.589784884178</p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ti.tf_idf(‘Portland’,fileid)
25.769354652534</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>還有, <em>to</em> 這個字在那篇文章中的 <em>TF</em> , <em>IDF</em> 以及 <em>TF-IDF</em></p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ti.tf(‘to’,fileid)
2.0</p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ti.idf(‘to’)
0.6355885737911242</p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ti.tf_idf(‘to’,fileid)
1.2711771475822484</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>再來,把文章中每個字的 <em>TF-IDF</em> 都算出來, 並做排序</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ti.run(fileid)
portland        25.7693546525
ship            17.9126251546
loading         15.9417501031
grain           10.3270402590
load            8.7532836165
waiting         7.6967000881
merchant        7.4198598827
according       5.0979317878
were            4.9210037345
there           3.5817565104
exchange        3.4056179602
at              3.3358878942
three           3.0888007761
two             2.4827546741
to              1.2711771476
and             0.6732655874
the             0.6341349766
,               0.2614305201
.               0.1459533027</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>以上結果顯示, <em>Portland</em> , <em>ship</em> , <em>grain</em> 之類的關鍵字, 所算出來的 <em>TF-IDF</em> 值都比較高, 而 <em>to</em> , <em>at</em> , 或其他標點符號, 算出來的值就較低, 因此 <em>TF-IDF</em> 可以把文章中的關鍵字區分出來</p>

<h2 id="reference">Reference</h2>

<p>本文使用 nltk 的 <em>Reuters Corpus</em> 做為範例, 若想知道 <em>Reuters Corpus</em> 要怎麼用, 請到以下網址</p>

<p>http://nltk.googlecode.com/svn/trunk/doc/book/ch02.html</p>

<p>另外, 本文的公式與觀念, 參考至以下這門 <em>coursera</em> 的線上課程 </p>

<h4 id="dr-gautam-shroff--web-intelligence-and-big-data">Dr. Gautam Shroff.  Web Intelligence and Big Data</h4>

<p>https://www.coursera.org/course/bigdata</p>

]]></content>
  </entry>
  
</feed>
