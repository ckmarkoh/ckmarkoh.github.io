<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machine_learning | Mark Chang's Blog]]></title>
  <link href="http://ckmarkoh.github.io/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://ckmarkoh.github.io/"/>
  <updated>2016-12-10T21:43:24+08:00</updated>
  <id>http://ckmarkoh.github.io/</id>
  <author>
    <name><![CDATA[Mark Chang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[機器學習 -- Model Selection]]></title>
    <link href="http://ckmarkoh.github.io/blog/2014/04/17/machine-learning-model-selection/"/>
    <updated>2014-04-17T07:15:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2014/04/17/machine-learning-model-selection</id>
    <content type="html"><![CDATA[<h2 id="motivation">1.Motivation</h2>

<p>本文接續先前提到的 <em>Overfitting and Regularization</em></p>

<p><a href="/blog/2014/04/13/machine-learning-overfitting-and-regularization">Machine Learning – Overfitting and Regularization</a></p>

<p>探討如何避免 <em>Overfitting</em> 並選出正確的 <em>Model</em></p>

<p>因為 <em>Overfitting</em> 的緣故, 所以無法用 <script type="math/tex">E_{in}</script> 來選擇要用哪個 <em>Model</em> </p>

<p>在上一篇文章中, 可以把 <script type="math/tex">E_{out}</script> 最小的 <em>Model</em> 當成是最佳的 <em>Model</em> , 但是在現實生活的應用中, 無法這樣選擇, 因為, <strong>在訓練 <em>Model</em> 時,無法事先知道 <em>Testing Data</em> 的預測結果是什麼</strong> ,所以就不可能用 <script type="math/tex">E_{out}</script> 來選擇 <em>Model</em> </p>

<p>既然這樣, 要怎麼辦呢？ 既然不可以用 <script type="math/tex">E_{in}</script> 來選擇 <em>Model</em> , 又無法事先算出 <script type="math/tex">E_{out}</script></p>

<!--more-->

<h2 id="validation-set">2.Validation Set</h2>

<p>例如, 用高次多項式做 <em>Linear Regression</em> , 假設只有一群 <em>Training Data</em> , 如下圖藍色點, 沒有 <em>Testing Data</em> , 要怎麼辦呢？</p>

<p><img src="/images/pic/pic_00035.png" alt="data1" /></p>

<p>有個解決方法, 就是在 <em>Training Data</em> 中, 隨機選取某一部份 <strong>當作</strong> <em>Testing Data</em> , 在訓練過程中不去使用, 這些 <em>Data</em> 稱為 <em>Validation Set</em> ,如下圖紫色的點</p>

<p><img src="/images/pic/pic_00036.png" alt="data2" /></p>

<p>用圖中藍色的點, 訓練出一個 <em>Model</em> , 如下圖 </p>

<p><img src="/images/pic/pic_00037.png" alt="data_model" /></p>

<p>訓練完之後, 再用 <em>Validation Set</em>  算 <em>Error</em> , 這個 <em>Error</em>  稱作 <em>Validation Error</em> , <script type="math/tex">E_{val}</script> ,可用於挑選最佳的 <em>Model</em> </p>

<p>至於 <em>Validation Set</em>  要挑多少 <em>Data</em> ? , 挑太多會導致 <em>Training Data</em> 的量大減少太多, 而無法訓練出準確的 <em>Model</em> , 但挑太少則會使得 <em>Validation Error</em> 被少數的點給 <em>Bias</em> , 所以, 通常是選取 <em>Training Data</em> 的 <script type="math/tex">\frac{1}{3}</script> ~ <script type="math/tex">\frac{1}{5}</script> 左右的量, 作為 <em>Validation Set</em> </p>

<h2 id="implementation-of-validation-set">3.Implementation of Validation Set</h2>

<p>接著來實作, 先來看看 <em>Training Data</em> 要怎麼切割</p>

<p>開新的檔案 <em>mdselect.py</em> 並貼上以下程式碼</p>

<p>```python mdselect.py
import numpy as np
import matplotlib.pyplot as plt
from operator import itemgetter</p>

<p>data_tr=[
(0.9310,-0.3209), (-0.3103,-1.1853), (-0.7241,0.8071), (0.6552,-1.1979), (-1.0000,0.9587),
(-0.5172,0.1218), (0.3793,1.0747), (-0.7931,0.9202), (0.1724,0.9259), (-0.2414,-0.8748),
(0.5862,-0.0751), (0.2414,0.9573), (1.0000,-0.2715), (-0.6552,0.3408), (-0.3793,-1.0831),
(0.0345,-0.2877), (0.5172,-0.0876), (0.3103,0.5422), (-0.9310,0.7560), (0.7931,-0.2613),
(0.8621,-1.0038), (-0.1724,-0.3660), (-0.0345,-0.5762), (0.1034,0.4364), (-0.8621,0.5780),
(-0.5862,0.1347), (-0.1034,-1.1036), (0.7241,-1.0032), (-0.4483,-0.5247), (0.4483,0.6258),
]
data_ts=[
(0.1034,0.4559), (0.5172,0.6431), (-0.1724,-1.1199), (0.7931,-0.9601), (0.7241,-1.4629),
(-0.6552,1.1571), (-0.0345,-0.3840), (0.2414,0.9064), (0.5862,-0.2830), (-1.0000,0.8299),
(0.1724,1.1434), (0.3103,0.7773), (-0.3103,-1.3973), (-0.9310,0.5383), (-0.4483,-0.6886),
(-0.5172,-0.2233), (-0.2414,-0.7119), (-0.1034,-0.3853), (-0.7241,0.9869), (-0.7931,0.9888),
(0.6552,-0.8112), (-0.8621,0.9862), (-0.3793,-1.0019), (0.3793,0.6254), (0.0345,-0.1150),
(1.0000,-0.0712), (0.8621,-0.8452), (0.4483,0.0301), (-0.5862,-0.4771), (0.9310,-0.7827),
]</p>

<p>def data_split(s1,s2):
    return data_tr[:s1]+data_tr[s2:] , data_tr[s1:s2]</p>

<p>def plot_data( d_tr=None,d_val=None, d_ts=None, d_m=None, title=’’):
    plt.ion()
    fig, ax = plt.subplots()
    for d,c in [(d_tr,’bo’),(d_val,’mo’),(d_ts,’ro’)]:
        if d != None:
            ax.plot(map(itemgetter(0),d), map(itemgetter(1),d), c )
    if d_m != None:
        ax.plot(np.array(map(itemgetter(0),d_m)), np.array(map(itemgetter(1),d_m)), ‘k–’)
    ax.set_xlim((-1, 1))
    ax.set_ylim((-2, 2))
    ax.set_title(title)
    plt.show()</p>

<p>```</p>

<p>其中 , <code>data_tr</code> 和 <code>data_t</code> 分別是 <em>Training Data</em> 和 <em>Testing Data</em> , 這些 <em>Data</em> 都已經先隨機洗牌過,因此用 <em>Index</em> 的順序抽出的 <em>Data</em> 都已經是隨機的, 不會依序剛好抽到一筆連續的 <em>Data</em> , 而 <code>data_split</code> 是用來切割 <em>Data</em> 用的 <em>function</em> , <code>plot_data</code> 是畫圖用的</p>

<p>到 <em>python</em> 的 <em>interactive mode</em> 載入模組</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>import mdselect as ms</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>用 <code>data_split(s1,s2)</code> 就可以把 <em>Training Data</em> 分開, 例如我想要把第 <em>1~10</em> 筆資料抽出來, 作為 <em>Validation Set</em> , 方法如下 </p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>dtr,dval=ms.data_split(0,10)
dtr
[(0.5862, -0.0751), (0.2414, 0.9573), (1.0, -0.2715), (-0.6552, 0.3408),    \
(-0.3793, -1.0831), (0.0345, -0.2877), (0.5172, -0.0876), (0.3103, 0.5422), \
(-0.931, 0.756), (0.7931, -0.2613), (0.8621, -1.0038), (-0.1724, -0.366),   \
(-0.0345, -0.5762), (0.1034, 0.4364), (-0.8621, 0.578), (-0.5862, 0.1347),  \
(-0.1034, -1.1036), (0.7241, -1.0032), (-0.4483, -0.5247), (0.4483, 0.6258)]</p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <blockquote>
      <p>dval
[(0.931, -0.3209), (-0.3103, -1.1853), (-0.7241, 0.8071), (0.6552, -1.1979), \
(-1.0, 0.9587), (-0.5172, 0.1218), (0.3793, 1.0747), (-0.7931, 0.9202),      \
(0.1724, 0.9259), (-0.2414, -0.8748)]</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>其中 <code>dtr</code> 是 <em>Training Data</em> , <code>dval</code> 是 <em>Validation Set</em></p>

<p>再來, 用 <code>plot_data</code> 把資料畫出來</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ms.plot_data(d_tr=dtr,d_val=dval)</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p><img src="/images/pic/pic_00038.png" alt="i0" /></p>

<p>再來, 就是用 <em>Training Data</em> 來訓練 <em>Model</em> , 用 <em>Validation Set</em> 計算 <em>Validation Error</em></p>

<p>到 <em>mdselect.py</em> 貼上以下程式碼</p>

<p>```python mdselect.py</p>

<p>def model_train(order, split=None, plot=True, show_eout=False):
    if split != None :
        d_tr, d_val  = data_split(split[0],split[1])
        d_ary = [ d_tr, d_val, data_ts ] 
    else :
        d_ary = [ data_tr, data_ts ]</p>

<pre><code>X_ary = map(lambda d : np.matrix(map( lambda x : 
                          map(pow, [x]*(order+1), range(order+1)), map(itemgetter(0), d )))
                       , d_ary )
Y_ary = map(lambda d : np.matrix(map(itemgetter(1), d)) , d_ary )
w = np.linalg.pinv( X_ary[0] )*Y_ary[0].T 
y_ary = map(lambda X : (X*w).T , X_ary )
E_ary = map(lambda Y,y : np.average(np.square(Y - y)) , Y_ary , y_ary )
        
p_model = sorted( reduce(add ,map(lambda i : zip(map(itemgetter(0), d_ary[i]), 
                    map(lambda j : y_ary[i][0,j], range(y_ary[i].shape[1]))) 
                  , range(len(d_ary)-1))), key=itemgetter(0))
if plot != False:
    if split != None:  d_val = d_ary[1] 
    else: d_val= None
    if show_eout == True: d_out = d_ary[-1]
    else: d_out = None 
    plot_data(d_ary[0], d_val , d_out, p_model,"order=%s"%(order)) 
return E_ary
</code></pre>

<p>```</p>

<p>第一個參數是 <code>order</code> 是多項式的次數, 第二個參數是 <code>split</code> 就是要切出來做 <em>Validation Set</em> 的 <em>Data</em> ,  預設為 <code>split=None</code> 表示把所有的 <em>Training Data</em> 都當成 <em>Training Data</em> , 剩下的參數, <code>plot</code> 選擇是否要畫圖, <code>show_eout</code> 選擇是否要在圖上顯示 <script type="math/tex">E_{out}</script></p>

<p>修改完 <em>mdselect.py</em> 要重新載入</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>reload(ms)
&lt;module ‘mdselect’ from ‘mdselect.py’&gt;</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>如果是用六次多項式來訓練, 而 <em>Validation Set</em> 是第 <em>1~10</em> 筆資料, 用法如下</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ms.model_train(order=6, split=(0,10))
[0.085523976119494666, 0.34656284169000939, 0.16791141763767087]</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>這個 <em>function</em> 會回傳三個參數, 依序為 <script type="math/tex">E_{in},E_{val},E_{out}</script> , 並會自動畫出以下圖形</p>

<p><img src="/images/pic/pic_00039.png" alt="i1_no_eout" /></p>

<p>如果要在圖上顯示  <script type="math/tex">E_{out}</script> , 則設定參數 <code>show_eout=True</code> , 如下</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ms.model_train(order=6, split=(0,10), show_eout=True )
[0.085523976119494666, 0.34656284169000939, 0.16791141763767087]</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p><img src="/images/pic/pic_00040.png" alt="i1_has_eout" /></p>

<p>接著, 要找出 <script type="math/tex">E_{val}</script> 最小的 <em>Model</em> 是哪一個, 需要依序用不同的 <em>Order</em> 來訓練不同的 <em>Model</em> </p>

<p>再到  <em>mdselect.py</em>  貼上以下程式碼</p>

<p>```python mdselect.py</p>

<p>def model_select(split=None,o1=3,o2=11):
    result = map(lambda o : (o, model_train(o, split, plot=False)), range(o1,o2))
    rmin = min(result,key=lambda x : x[1][1])
    for order,err in result:
        if split != None:
            print “Order:%s, Ein:%.5f, Eval:%.5f, Eout:%.5f”%(order,err[0],err[1],err[2])
        else:
            print “Order:%s, Ein:%.5f, Eout:%.5f”%(order,err[0],err[1])
    print “Min Result:”
    if split != None:
        print “Order:%s, Ein:%.5f, Eval:%.5f, Eout:%.5f”%(rmin[0],rmin[1][0],rmin[1][1],rmin[1][2])
    else:
        print “Order:%s, Ein:%.5f, Eout:%.5f”%(rmin[0],rmin[1][0],rmin[1][1])
    plot_model_select(result,split,o1,o2)</p>

<p>def plot_model_select(result,split,o1,o2):
    x=[order for order,_ in result]
    d=[err for _,err in result]
    fig, ax = plt.subplots()
    if split != None:
        tp_ary = [(0,’bo’,’b–’,’Ein’),(1,’mo’,’m–’,’Eval’),(2,’ro’,’r–’,’Eout’)]
    else:
        tp_ary = [(0,’bo’,’b–’,’Ein’),(1,’ro’,’r–’,’Eout’)]
    for tp in tp_ary : 
        ax.plot(x, map(itemgetter(tp[0]),d) , tp[2],label = tp[3])
        ax.plot(x, map(itemgetter(tp[0]),d) , tp[1])
    ax.set_xlim((o1,o2))
    ax.set_ylim((0,1))
    ax.set_xlabel(‘Order’)
    ax.set_ylabel(‘Error’)
    plt.legend()
    plt.show()</p>

<p>```</p>

<p>修改完 <em>mdselect.py</em> 重新載入</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>reload(ms)
&lt;module ‘mdselect’ from ‘mdselect.py’&gt;</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>這個 <em>function</em> 會依序從 <script type="math/tex">Order = 3</script> 訓練到 <script type="math/tex">Order = 10</script> , 並把 <script type="math/tex">E_{in},E_{val},E_{out}</script> 的值畫成圖表, 以及找出 <script type="math/tex">E_{val}</script> 最小的 <em>Order</em> 是哪一個</p>

<p>例如用第 <em>1~10</em> 筆為 <em>Validation Set</em> ,選出最佳的 <em>Model</em> ,如下</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ms.model_select(split=(0,10))
Order:3, Ein:0.24487, Eval:0.55254, Eout:0.44307
Order:4, Ein:0.23451, Eval:0.58775, Eout:0.44250
Order:5, Ein:0.08565, Eval:0.32730, Eout:0.16432
Order:6, Ein:0.08552, Eval:0.34656, Eout:0.16791
Order:7, Ein:0.06510, Eval:0.13649, Eout:0.13285
Order:8, Ein:0.06497, Eval:0.15669, Eout:0.13948
Order:9, Ein:0.06431, Eval:0.14018, Eout:0.13009
Order:10, Ein:0.05805, Eval:0.99262, Eout:0.42279
Min Result:
Order:7, Ein:0.06510, Eval:0.13649, Eout:0.13285</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>結果顯示, 在 <script type="math/tex">Order = 7</script> 時 , 有最小的 <script type="math/tex">E_{val}</script> , <script type="math/tex">E_{out}</script> 也為最小, 程式畫出圖表如下</p>

<p><img src="/images/pic/pic_00041.png" alt="plot1" /></p>

<p>圖中有三條線, 其中紫色的線為 <script type="math/tex">E_{val}</script> , 這條線的趨勢和紅色的線 <script type="math/tex">E_{out}</script> 類似, 所以可以用來選擇最佳的 <em>Model</em></p>

<p>可以把 <script type="math/tex">Order = 7</script> 的 <em>Model</em> 也畫出來看看, 如下</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ms.model_train(7, split=(0,10), show_eout=True)
[0.065103000120556462, 0.13648767336843776, 0.13284787573861817]</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p><img src="/images/pic/pic_00042.png" alt="i2_has_eout" /></p>

<h2 id="train-again-">4.Train Again !</h2>

<p>由於選出 <em>Validation Set</em> 會使得原本的 <em>Training Data</em> 變少, 但是用更多的 <em>Training Data</em> 來訓練, 是有可能使 <script type="math/tex">E_{out}</script> 降得更低</p>

<p>所以, 在選好 <em>Model</em> 以後, 可以把 <em>Validation Set</em> 也併入 <em>Training Data</em> , 再訓練一次, 這樣就有可能把 <script type="math/tex">E_{out}</script> 降低</p>

<p>用 <code>model_train</code> 訓練剛才得出的最佳 <em>Order</em> , 也就是 <script type="math/tex">Order = 7</script> , 但這次用參數 <code>split=None</code> , 就是不要切割出 <em>Validation Set</em> </p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ms.model_train(7, split=None, show_eout=True )
[0.071946203316513205, 0.096047913185552308]</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p><img src="/images/pic/pic_00043.png" alt="i3_has_eout" /></p>

<p>得出以上結果, 可知, 用所有的 <em>Training Data</em> 做訓練, 結果為 <script type="math/tex">E_{out} \approx 0.09605 </script> 比起剛剛, 有切出 <em>Validation Set</em> 的 <script type="math/tex">E_{out} \approx 0.13285</script> 下降一些</p>

<p>所以經由 <em>Validation Set</em> 找出了參數 <em>Order</em> 以後, 再用全部的 <em>Training Data</em> 都訓練過一次, 的確可以把 <script type="math/tex">E_{out}</script> 降低</p>

<p>來看看用全部的 <em>Training Data</em> 訓練來挑 <em>Order</em> 參數, 誰的 <script type="math/tex">E_{out}</script> 最小,用 <code>model_select</code> 參數輸入 <code>split=None</code> , 結果如下</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ms.model_select(split=None)
Order:3, Ein:0.34432, Eout:0.43882
Order:4, Ein:0.34271, Eout:0.43519
Order:5, Ein:0.13194, Eout:0.12439
Order:6, Ein:0.12855, Eout:0.12578
Order:7, Ein:0.07195, Eout:0.09605
Order:8, Ein:0.06991, Eout:0.10288
Order:9, Ein:0.06987, Eout:0.10224
Order:10, Ein:0.06754, Eout:0.10167
Min Result:
Order:7, Ein:0.07195, Eout:0.09605</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p><img src="/images/pic/pic_00044.png" alt="plot2" /></p>

<p>這次運氣很好, 用 <em>Validation Set</em> 就成功挑出了 <script type="math/tex">E_{out}</script> 最小的 <em>Order</em> 參數, 但事實上, 未必每次運氣都這麼好</p>

<h2 id="v-fold-cross-validation">5.V-fold Cross Validation</h2>

<p>運氣不好的時候, 選的 <em>Validation Set</em> , 和 <script type="math/tex">E_{out}</script> 的分佈型態有所差距, 以至於用 <script type="math/tex">E_{val}</script> 無法找出 <script type="math/tex">E_{out}</script> 最小的<em>Model</em>  </p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ms.model_select(split=(15,25))
Order:3, Ein:0.45906, Eval:0.13305, Eout:0.45137
Order:4, Ein:0.45786, Eval:0.12820, Eout:0.44651
Order:5, Ein:0.10279, Eval:0.31029, Eout:0.12523
Order:6, Ein:0.09748, Eval:0.30769, Eout:0.12489
Order:7, Ein:0.03825, Eval:0.17898, Eout:0.09523
Order:8, Ein:0.02848, Eval:0.22228, Eout:0.11895
Order:9, Ein:0.02838, Eval:0.23299, Eout:0.12376
Order:10, Ein:0.02832, Eval:0.25718, Eout:0.12957
Min Result:
Order:4, Ein:0.45786, Eval:0.12820, Eout:0.44651</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p><img src="/images/pic/pic_00045.png" alt="plot3" /></p>

<p>由上圖紫色的線看到, <script type="math/tex">E_{val}</script> 的趨勢和 <script type="math/tex">E_{in} , E_{out}</script> 很不一樣, 把 <script type="math/tex">Ordr = 4 </script> 的 <em>Model</em> 畫出來, 看看出了什麼問題</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ms.model_train(4, split=(15,25), show_eout=True)
[0.45785961181234375, 0.12819567955706512, 0.44650895269797458]</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>從下圖可知, 我們運氣真的很不好, 因為挑到的點的 <em>y</em> 值都在中間, 所以會選出這樣的 <em>Model</em></p>

<p><img src="/images/pic/pic_00046.png" alt="i4_has_out" /></p>

<p>那這種情況要怎麼避免呢？</p>

<p>有種方法叫作 <em>V-fold Cross Validation</em> , 就是把 <em>Training Data</em> 切成 <script type="math/tex">V</script> 份, 總共訓練 <script type="math/tex">V</script> 次, 每次訓練從 <script type="math/tex">V</script> 份中挑一份作為 <em>Validation Set</em> , 剩下的 <script type="math/tex">V-1</script> 份當作 <em>Training set</em> , 最後再把每一次計算所得出的 <em>Error</em> 平均起來, 詳細過程如下</p>

<p>把 <em>Training Data</em>  <script type="math/tex">X</script> 切成  <script type="math/tex">X_{1},X_{2},X_{3},...,X_{V-1},X_{V}</script> , 先挑一份作為 <em>Validation Set</em></p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \text{Training Set: }\mspace{10mu} X_{1},...,X{i-1},X_{i+1},...,X_{V} \\[5pt]

& \text{Validation Set: }\mspace{10mu} X_{i} \\[5pt]

\end{align}

 %]]&gt;</script>

<p>得出 <em>Validation Error</em> : <script type="math/tex">E_{val_{i}}</script></p>

<p>再挑一份不同的 <em>Data</em> 為 <em>Validation Set</em> , 如下</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \text{Training Set: }\mspace{10mu} X_{1},...,X{j-1},X_{j+1},...,X_{V} \\[5pt]

& \text{Validation Set: }\mspace{10mu} X_{j}, \mspace{10mu} \text{where } j \neq i \\[5pt]

\end{align}

 %]]&gt;</script>

<p>得出 <em>Validation Error</em> : <script type="math/tex">E_{val_{j}}</script></p>

<p>這樣重複進行 <script type="math/tex">V</script> 次, 直到每一份 <script type="math/tex">X_{i}</script> 都當過 <em>Validation Set</em> , 再把所有算出來的 <em>Validation Error</em> 平均起來</p>

<script type="math/tex; mode=display">

 E_{val} = \frac{1}{V}\sum_{i=1}^{v} E_{val_{i}}

</script>

<p>這樣可避免  <em>Validation Error</em>  受到運氣不好的 <em>Validation Set</em> 的影響, 而選出不好的 <em>Model</em></p>

<h2 id="implementation-of-v-fold-cv">Implementation of V-fold CV</h2>

<p>再到  <em>mdselect.py</em>  貼上以下程式碼</p>

<p>```python mdselect.py</p>

<p>def v_fold(o1=3,o2=11): 
    result = map(lambda o : (o,np.average(np.matrix(
                            map(lambda s : model_train(o,split=(s<em>6,(s+1)</em>6), plot=False), range(5)))
                            , axis=0)), range(o1,o2))
    rmin = min(result, key = lambda x : x[1][0,1])
    for order,err in result:
        print “Order:%s, Ein:%.5f, Eval:%.5f, Eout:%.5f”%(order,err[0,0],err[0,1],err[0,2])
    print “Min Result:”
    print “Order:%s, Ein:%.5f, Eval:%.5f, Eout:%.5f”%(rmin[0],rmin[1][0,0],rmin[1][0,1],rmin[1][0,2])
    plot_vfold(result,o1,o2)</p>

<p>def plot_vfold(result,o1,o2):
    x=[order for order,_ in result]
    y_ary = map(lambda i : [err[0,i] for _,err in result] , [0,1,2])
    tp_ary = [(0,’bo’,’b–’,’Ein’),(1,’mo’,’m–’,’Eval’),(2,’ro’,’r–’,’Eout’)]
    fig, ax = plt.subplots()
    for tp in tp_ary : 
        ax.plot(x, y_ary[tp[0]] , tp[2],label = tp[3])
        ax.plot(x, y_ary[tp[0]] , tp[1])
    ax.set_xlim((o1,o2))
    ax.set_ylim((0,1))
    ax.set_xlabel(‘Order’)
    ax.set_ylabel(‘Error’)
    plt.legend()
    plt.show()</p>

<p>```</p>

<p>其中 <code>v_fold</code> 就是 <em>V-fold Cross Validation</em> 演算法 , 在這 <em>function</em> 中, <script type="math/tex">V=5</script> 也就是把 <em>Training Data</em> 切成五份 , 並把其中一份挑出來做 <em>Validation Set</em></p>

<p>修改完 <em>mdselect.py</em> 重新載入</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>reload(ms)
&lt;module ‘mdselect’ from ‘mdselect.py’&gt;</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>執行 <code>v_fold</code> ,結果如下</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ms.v_fold()
Order:3, Ein:0.33048, Eval:0.48405, Eout:0.45466
Order:4, Ein:0.32709, Eval:0.51738, Eout:0.45618
Order:5, Ein:0.12095, Eval:0.27391, Eout:0.14044
Order:6, Ein:0.11037, Eval:0.45183, Eout:0.17004
Order:7, Ein:0.06660, Eval:0.12957, Eout:0.10250
Order:8, Ein:0.06446, Eval:0.13500, Eout:0.11171
Order:9, Ein:0.06402, Eval:0.18829, Eout:0.12179
Order:10, Ein:0.06035, Eval:0.50482, Eout:0.18569
Min Result:
Order:7, Ein:0.06660, Eval:0.12957, Eout:0.10250</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p><code>v_fold</code> 找出了 <script type="math/tex">Order = 7</script>  時, 有最小的 <script type="math/tex">E_{val}</script> , 也有最小的 <script type="math/tex">E_{out}</script> </p>

<p><img src="/images/pic/pic_00047.png" alt="plot4" /></p>

<p>以上圖表顯示 , 紫色的線和紅色的線, 趨勢也不會差太遠, <script type="math/tex">E_{val}</script> 也可用於選擇  <script type="math/tex">E_{out}</script> 最小的 <em>Model</em></p>

<p>事實上, <em>V-fold Cross Validation</em> 未必每次都能找出  <script type="math/tex">E_{out}</script> 最小的 <em>Model</em> , 因為它是由各種不同的結果所平均起來的, 也會受到較差結果者的影響, 但至少比較不會因為 <strong>運氣不好</strong> 而挑到不好的 <em>Validation Set</em> </p>

<h2 id="reference">6.Reference</h2>

<p>本文參考至以下兩門 <em>Coursera</em> 線上課程</p>

<h4 id="andrew-ng-machine-learning">1.Andrew Ng. Machine Learning</h4>

<p>https://www.coursera.org/course/ml</p>

<h4 id="machine-learning-foundations">2.林軒田 機器學習基石 (Machine Learning Foundations)</h4>

<p>https://www.coursera.org/course/ntumlone</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[機器學習 -- Overfitting and Regularization]]></title>
    <link href="http://ckmarkoh.github.io/blog/2014/04/13/machine-learning-overfitting-and-regularization/"/>
    <updated>2014-04-13T06:46:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2014/04/13/machine-learning-overfitting-and-regularization</id>
    <content type="html"><![CDATA[<h2 id="overfitting">1.Overfitting</h2>

<p>所謂 <em>Overfitting</em> 指的就是過度訓練, 意思就是說機器學習所學到的 <em>Hypothesis</em> 過度貼近 <em>Training Data</em> , 而導致和 </p>

<p><em>Testing Data</em> 的時候, <em>Error</em>  變得更大</p>

<p>假設有一筆資料如下圖, 藍色的為 <em>Training Data</em> , 紅色的為 <em>Testing Data</em> ,</p>

<p><img src="/images/pic/pic_00020.png" alt="input" /></p>

<p>想要用高次多項式的 <em>Hypothesis</em> ,<script type="math/tex">h(w)</script>,  做 <em>Linear Regression</em> </p>

<script type="math/tex; mode=display">

	h(w)=w_{0}x^{0}+w_{1}x^{1}+w_{2}x^{2}+...+w_{n}x^{n}

</script>

<p>其中, <script type="math/tex">w</script> 是 <em>weight</em>, <script type="math/tex">n</script> 表示這個多項式的次數 ( <em>Order</em> )</p>

<!--more-->

<p>如果多項式的 <em>Order</em> 不夠大, 則無法使 <em>Hypothesis</em> 貼近 <em>Training data</em>  , 如下圖</p>

<p><img src="/images/pic/pic_00021.png" alt="osmall" /></p>

<p>如果 <em>Order</em> 大小適中, 則可以使 <em>Hypothesis</em> 貼近 <em>Training data</em> 以及 <em>Testing Data</em>, 此時, <script type="math/tex">E_{in}</script> 和 <script type="math/tex">E_{out}</script> 都會降低, 如下圖</p>

<p><img src="/images/pic/pic_00022.png" alt="omedium" /></p>

<p>如果 <em>Order</em> 太大,  則會使 <em>Hypothesis</em> 過度貼近 <em>Training data</em>  , 因此遠離 <em>Testing Data</em>, 造成 <script type="math/tex">E_{out}</script> 變大, 如下圖</p>

<p><img src="/images/pic/pic_00023.png" alt="olarge" /></p>

<p>由此可知,  <em>Order</em> 並不是越大, 就可以讓 <em>Hypothesis</em> 越準確, 當 <em>Order</em> 太大的時候, 造成 <script type="math/tex">E_{out}</script> 變大, 這種情形稱作 <em>Overfitting</em></p>

<p>接著來實際操作看看 <em>Overfitting</em> 這種情形</p>

<h2 id="implementation-1">2.Implementation 1</h2>

<p>首先, 開一個新的檔案, 命名為 <em>overfitting.py</em></p>

<p>載入必要的模組, 並輸入data, 如下</p>

<p>```python overfitting.py
import numpy as np
import matplotlib.pyplot as plt</p>

<p>x=np.matrix([[-1.        , -0.93103448, -0.86206897, -0.79310345, -0.72413793, -0.65517241, 
           -0.5862069 , -0.51724138, -0.44827586, -0.37931034, -0.31034483, -0.24137931,
           -0.17241379, -0.10344828, -0.03448276,  0.03448276,  0.10344828,  0.17241379,
           0.24137931,  0.31034483,  0.37931034,  0.44827586,  0.51724138,  0.5862069 ,
           0.65517241,  0.72413793,  0.79310345,  0.86206897,  0.93103448,  1.        , ]])
y_train=np.matrix([[ 0.72679128,  0.88352371,  0.55848839,  0.9960148 ,  0.27727561,  1.58193644,
            0.35519674,  0.40919248, -0.66450448, -1.02347355, -0.71433077, -0.97857498,
           -0.9542627 , -0.85186192, -0.00210849, -0.00559543,  0.6545823 ,  0.82926143,
            0.3728542 ,  1.60336863,  1.20548029, -0.20721056,  0.44713523, -0.49832341,
           -0.34765828, -1.51883285, -0.95758709, -0.83135465, -0.90942741, -0.10016318, ]])
y_test=np.matrix([[ 0.79521635,  0.32523979,  0.63212171,  1.60522123,  0.72400525,  1.33408882,
          -0.42555819, -0.19726661, -0.66041197, -0.65470685, -0.93661018, -0.87634342,
          -0.84363868, -0.95689774, -0.1376653 ,  0.40842111, -0.20794503,  0.15057061,
           0.50331016,  1.54413185,  0.01230807,  0.38623098,  0.32021572, -0.02133113,
          -0.28643186, -0.91730531, -0.65369342, -0.68990553, -0.73800708, -0.56659495, ]])</p>

<p>```</p>

<p>再來, 新增兩個function如下</p>

<p>```python overfitting.py</p>

<p>def plot_data( y_model, title=’’):
    plt.ion()
    fig, ax = plt.subplots()
    ax.plot(np.array([x[0,i]for i in range(x.shape[1])]) ,
            np.array([y_model[0,i]for i in range(y_model.shape[1])]) ,
            ‘k–’)
    ax.plot(x, y_train, ‘bo’ )
    ax.plot(x, y_test, ‘ro’ )
    ax.set_xlim((-1, 1)) 
    ax.set_ylim((-2, 2)) 
    ax.set_title(title)
    plt.show()</p>

<p>def linear_regression(order):
    X = np.matrix([[x[0,j]<em>*i for i in range(order) ] for j in range(x.shape[1])])
    w =  np.linalg.pinv( X )</em>y_train.T 
    y_model = (X*w).T
    e_in = np.average(np.square(y_train - y_model))
    e_out = np.average(np.square(y_test - y_model))
    status_str = “Order=%s, Ein=%.6f, Eout=%.6f”%(order,e_in,e_out)
    print status_str 
    plot_data(y_model , status_str) </p>

<p>```</p>

<p>其中,  <code>plot_data( y_model, title='')</code> 是用來畫圖的 <em>function</em> ,而 <code>linear_regression(order)</code> 是用來做 <em>Linear Regression</em> 的 <em>function</em> , 參數 <code>order</code> 即為多項式的次數</p>

<p>然後, 到 <em>interactive mode</em> 載入剛剛的檔案</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>import overfitting as ovf</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>試試看不同 <em>Order</em> 的多項式, 得出的 <em>Hypothesis</em> 有什麼不一樣</p>

<p>例如, 輸入 <code>8</code> 和 <code>15</code> 這兩種 <em>Order</em> , 所得出的結果如下</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ovf.linear_regression(8)
Order=8, Ein=0.128044, Eout=0.175504</p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ovf.linear_regression(15)
Order=15, Ein=0.111843, Eout=0.208729</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>所得出的圖形分別如下</p>

<p><img src="/images/pic/pic_00024.png" alt="o8" /></p>

<p><img src="/images/pic/pic_00025.png" alt="o15" /></p>

<p>多嘗試幾種不同的 <em>Order</em> 看看</p>

<p>然後將不同 <em>Order</em> 所得出的 <script type="math/tex">E_{in}</script> 和 <script type="math/tex">E_{out}</script> , 列成表格</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{array}{c|c}


Order & 4 & 6 & 7 & 8 & 9 & 10 & 12 & 15 & 20 & 30 \\ \hline 

E_{in} & 0.4845 & 0.1949 & 0.1835 & 0.1280 & 0.1276 & 0.1254 & 0.1164 & 0.1118 & 0.0824 & 0.0000 \\

E_{out} & 0.3433 & 0.1815 & 0.1910 & 0.1755 & 0.1730 & 0.1847 & 0.2026 & 0.2087 & 0.1974 & 0.2103 

 

\end{array}

 %]]&gt;</script>

<p>並根據這些資料來畫圖表</p>

<p><img src="/images/pic/pic_00026.png" alt="plot1" /></p>

<p>以上結果顯示, 當 <em>Order = 9</em> 時, 有最小的 <script type="math/tex">E_{out}</script> , 通常會選擇 <script type="math/tex">E_{out}</script> 最小的 <em>Hypothesis</em> , 為成最佳的 <em>Hypothesis</em> </p>

<p>把 <em>Order</em> 由小逐漸增大的過程, 做成動畫, 如下：</p>

<p><img src="/images/pic/pic_00027.gif" alt="oanim" /></p>

<h2 id="regularization">3.Regularization</h2>

<p>如果我們避免 <em>Overfitting</em> , 除了從 <em>Order</em> 較低的 <em>Hypothesis</em> 逐一嘗試之外, 還有一種方式叫做 <em>Regularization</em></p>

<p><em>Regularization</em> 的概念就是, 限制 <em>weight</em> 的平方和, 為某一個常數</p>

<script type="math/tex; mode=display">

	h(w)=w_{0}x^{0}+w_{1}x^{1}+w_{2}x^{2}+...w_{n}x^{n} \\ \text{subject to} \mspace{10mu} 

  \sum_{i=0}^{n}w_{i}^{2} \leq C


</script>

<p>為什麼這樣就可以避免 <em>Over Fitting</em> ？</p>

<p>例如,如果要用一個三次的多項式 <script type="math/tex">h_{3}</script> 做為 <em>Hypothesis</em> </p>

<script type="math/tex; mode=display">

	h_{3}(w)=w_{0}x^{0}+w_{1}x^{1}+w_{2}x^{2}+w_{3}x^{3}

</script>

<p>可以把 <script type="math/tex">h_{3}</script> 看成是一個 <script type="math/tex">n</script> 次的多項式, 但是次數大於 <em>3</em> 的 <em>weight</em> 都等於 <em>0</em> , 如下</p>

<script type="math/tex; mode=display">

	h_{3}(w)=w_{0}x^{0}+w_{1}x^{1}+w_{2}x^{2}+...+w_{n}x^{n} \\ 

  \text{subject to} \mspace{10mu} 

  w_{4}=w_{5}=...=w_{n}=0

</script>

<p>可以把 <script type="math/tex">h_{3}</script> 推廣成這樣</p>

<script type="math/tex; mode=display">

	h_{3}'(w)=w_{0}x^{0}+w_{1}x^{1}+w_{2}x^{2}+...+w_{n}x^{n} \\ 

  \text{subject to} \mspace{10mu} 

  \sum_{i=0}^{n} \left[  w_{i} \neq 0 \right]  \leq 3+1

</script>

<p>由以上式子可知, <script type="math/tex"> h_{3}(w) \subset h_{3}'(w)</script> </p>

<p>但是對於 <script type="math/tex"> \left[  w_{i} \neq 0 \right] </script> 的最佳化問題, 是 <em>NP-Hard</em> , 所以就改成平方和</p>

<script type="math/tex; mode=display">

	h_{3}''(w)=w_{0}x^{0}+w_{1}x^{1}+w_{2}x^{2}+...w_{n}x^{n} \\ \text{subject to} \mspace{10mu} 

  \sum_{i=0}^{n}w_{i}^{2} \leq C


</script>

<p>因此, 可以用 <em>Regularization</em> 來避免多項式的 <em>weight</em> 平方和過大, 間接降低多項式的次數</p>

<p>此推導過程參考於 <em>Coursera</em> 線上課程 <a href="https://www.coursera.org/course/ntumlone">機器學習基石</a></p>

<p>至於如何將 <em>Regularization</em> 用於 <em>Linear Regression</em> , 我們先來看看 <em>Linear Regression</em> 的 <em>Weight</em> 是如何計算的,</p>

<script type="math/tex; mode=display">

w=(X^{T}X)^{-1}X^{T}y

</script>

<p>加上 <em>Regularization</em> 以後會變成這樣, 其中 <script type="math/tex">\lambda=\frac{1}{C}</script></p>

<script type="math/tex; mode=display">

w=(X^{T}X+\lambda I)^{-1}X^{T}y

</script>

<p>在此不推導此公式, 請參考 <em>Coursera</em> 線上課程 <a href="!https://www.coursera.org/course/ml">Machine Learning</a></p>

<h2 id="implementation-2">4.Implementation 2</h2>

<p>來實作 <em>Regularization</em> , 看看它如何避免 <em>overfitting</em> 的發生</p>

<p>新增一個 <em>function</em> 到 <em>overfitting.py</em> </p>

<p>```python</p>

<p>def regularization(C):
    order=30
    X = np.matrix([[x[0,j]<em>*i for i in range(order) ] for j in range(x.shape[1])])
    w = ( np.linalg.pinv( X.T</em>X + (1./C)<em>np.identity(X.shape[1]) )</em>X.T )<em>y_train.T
    y_model = (X</em>w).T
    e_in = np.average(np.square(y_train - y_model))
    e_out = np.average(np.square(y_test - y_model))
    status_str = “C=%s, Ein=%.6f, Eout=%.6f”%(C,e_in,e_out)
    print status_str
    plot_data( y_model, status_str )</p>

<p>```</p>

<p><code>regularization(C)</code> 是用來做 <em>Regularized Linear Regression</em> 的 <em>function</em> , <em>C</em> 可以用來控制 <em>overfitting</em> 的程度, <em>C</em> 越小 , <em>overfitting</em> 的程度越低</p>

<p>接著到 <em>interactive mode</em> 重新載入  <em>overfitting.py</em> </p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>reload(ovf)
&lt;module ‘overfitting’ from ‘overfitting.py’&gt;</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>試試看不同 <em>C</em> , 得出的 <em>Hypothesis</em> 有什麼不一樣</p>

<p>例如, 輸入 <code>10</code>,<code>50</code> , <code>1000</code> 和 <code>10000000</code>, 所得出的結果如下</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ovf.regularization(10)
C=10, Ein=0.325034, Eout=0.250901</p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ovf.regularization(50)
C=50, Ein=0.228926, Eout=0.173973</p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ovf.regularization(200)
C=200, Ein=0.167971, Eout=0.152158</p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ovf.regularization(1000)
C=1000, Ein=0.126571, Eout=0.164675</p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ovf.regularization(10000000)
C=10000000, Ein=0.111289, Eout=0.203883</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>所得出的圖形分別如下</p>

<p><img src="/images/pic/pic_00028.png" alt="c10" /></p>

<p><img src="/images/pic/pic_00029.png" alt="c50" /></p>

<p><img src="/images/pic/pic_00030.png" alt="c200" /></p>

<p><img src="/images/pic/pic_00031.png" alt="c1000" /></p>

<p><img src="/images/pic/pic_00032.png" alt="c10000000" /></p>

<p>從以上結果顯示, <script type="math/tex">C</script> 如果太小, 和多項式的 <em>Order</em> 太小的結果一樣, 都無法貼近 <em>Training Data</em> , 而 <script type="math/tex">C</script> 過大的結果, 也會產生 <em>overfitting</em> , 過度貼近 <em>Training Data</em> 使得 <script type="math/tex">E_{in}</script> 變大</p>

<p>多嘗試幾種不同的 <em>C</em> 看看</p>

<p>然後將不同 <em>C</em> 所得出的 <script type="math/tex">E_{in}</script> 和 <script type="math/tex">E_{out}</script> , 列成表格</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{array}{c|c}


C & 10 & 50 & 100 & 200 & 500 & 1000 & 3000 & 2\times 10^{4} & 5\times 10^{5} & 1\times 10^{7}  \\ \hline 

E_{in} & 0.3250 & 0.2289 & 0.1973 & 0.1680 & 0.1384 & 0.1266 & 0.1196 & 0.1161 & 0.1142 & 0.1113 \\

E_{out} & 0.2509 & 0.1740 & 0.1591 & 0.1522 & 0.1554 & 0.1647 & 0.1795 & 0.1933 & 0.2020 & 0.2039


\end{array}

 %]]&gt;</script>

<p>並根據這些資料來畫圖表</p>

<p><img src="/images/pic/pic_00033.png" alt="plot1" /></p>

<p>以上結果顯示, 當 <em>C = 200</em> 時, 有最小的 <script type="math/tex">E_{out}</script> , 通常會選擇 <script type="math/tex">E_{out}</script> 最小的 <em>Hypothesis</em> , 為成最佳的 <em>Hypothesis</em> </p>

<p>把 <em>C</em> 由小逐漸增大的過程, 做成動畫, 如下：</p>

<p><img src="/images/pic/pic_00034.gif" alt="canim" /></p>

<p>註：</p>

<ol>
  <li>
    <p>理論上, 當 <em>C</em> 趨近於無限大 <script type="math/tex">E_{in}</script> 應該要趨近於 <em>0</em> , 但根據以上結果 <script type="math/tex">E_{in}</script> 仍然無法趨近於 <em>0</em> , 這可能是由於 <code>numpy</code> 這個 <em>package</em> ,在計算 <em>pseudo inverse matrix</em> 的時候產生的, 使得 <script type="math/tex">E_{in}</script> 無法趨近於 <em>0</em></p>
  </li>
  <li>
    <p>為了避免計算 <em>pseudo inverse matrix</em> 的誤差, 或許可以改用 <em>Gradient descent</em> 的方式做最佳化</p>
  </li>
</ol>

<h2 id="reference">5.Reference</h2>

<p>本文參考至以下兩門 <em>Coursera</em> 線上課程</p>

<h4 id="andrew-ng-machine-learning">1.Andrew Ng. Machine Learning</h4>

<p>https://www.coursera.org/course/ml</p>

<h4 id="machine-learning-foundations">2.林軒田 機器學習基石 (Machine Learning Foundations)</h4>

<p>https://www.coursera.org/course/ntumlone</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[機器學習 -- Perceptron Algorithm]]></title>
    <link href="http://ckmarkoh.github.io/blog/2014/03/15/machine-learning-perceptron-algorithm/"/>
    <updated>2014-03-15T11:28:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2014/03/15/machine-learning-perceptron-algorithm</id>
    <content type="html"><![CDATA[<h2 id="introduction">1.Introduction</h2>

<p>這次來講一下機器學習(Machine Learning)</p>

<p>簡而言之,Machine Learning是一種讓機器根據已知的data,預測出未知的data情形如何</p>

<p>現在,來看看一種簡單的Machine Learning演算法</p>

<p>叫作Perceotron Algorithm</p>

<p>Peceptron Algorithm要做的事</p>

<p>就是要讓電腦學習,怎樣畫一條線,把兩群不同的資料分開</p>

<!--more-->

<h2 id="load-data">2.Load Data</h2>

<p>接著來看看要怎麼實作Perceptron Algorithm</p>

<p>首先,開啟新的檔案 <em>perceptron.py</em> 並載入模組</p>

<p>```python perceptron.py
import numpy as np
import matplotlib.pyplot as plt </p>

<p>```</p>

<p>給定一筆資料</p>

<p>```python perceptron.py</p>

<p>X1 = np.array([-0.62231486, -0.96251306,  0.42269922, -1.452746  , -0.66915783,
               -0.35716016,  0.49505163, -1.8117848 ,  0.53376487, -1.86923838,
                0.71434306, -0.4055084 ,  0.82887254,  0.81221287,  1.44280951,
               -0.45599278, -1.16715888,  1.08913131, -1.61470741,  1.61113001,
               -1.4532688 ,  1.04872588, -1.52312195, -1.62831727, -0.25191539])</p>

<p>X2 = np.array([-1.67427011, -1.81046748,  1.20384694, -0.41572751,  0.66851908,
               -1.75435288, -1.57532207, -1.22329618, -0.84375819,  0.52873296,
               -1.10837773,  0.04612922,  0.67696196,  0.84618152, -0.77362548,
                0.99153072,  1.7896494 , -0.38343121, -0.21337742,  0.64754817,
                0.36719101,  0.23132427,  1.07029963,  1.62919909, -1.53920827])</p>

<p>Y = np.array([  1.,  -1.,  -1.,  -1.,  -1.,
                1.,   1.,  -1.,   1.,  -1.,
                1.,  -1.,   1.,   1.,   1., 
               -1.,  -1.,   1.,  -1.,   1., 
               -1.,   1.,  -1.,  -1.,   1.])</p>

<p>```</p>

<p>其中 <code>X1</code> 是資料的X座標, <code>X2</code> 是資料的Y座標, <code>Y</code>是資料的分類的結果, <strong>不是Y座標</strong></p>

<p>然後寫個function 把data畫出來   </p>

<p>```python perceptron.py<br />
def plot_data(filename = ‘data0’):
    fig = plt.figure()
    ax = fig.add_subplot(111)
    plt.scatter(X1[Y &gt;= 0], X2[Y &gt;= 0], s = 80, c = ‘b’, marker = “o”)
    plt.scatter(X1[Y &lt;  0], X2[Y  &lt; 0], s = 80, c = ‘r’, marker = “^”)
    ax.set_xlim(xl1, xl2)
    ax.set_ylim(yl1, yl2)
    fig.set_size_inches(6, 6)
    plt.show()   </p>

<p>```</p>

<p>到interactive mode執行</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>import perceptron as pct
pct.plot_data()</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p><img src="http://lh3.googleusercontent.com/-OOCOyCqpkZk/UyVNYCrnbXI/AAAAAAAAAk4/OUhvS3dnKjM/s480-no/data0.png" /></p>

<p>這個圖把藍色( <script type="math/tex">y = 1</script> ),和紅色( <script type="math/tex">y = -1</script> )兩群資料呈現出來</p>

<h2 id="draw-line">3.Draw Line</h2>

<p>再來,看看怎麼畫一條線</p>

<p>先在 <em>perceptron.py</em> 檔案新增一個畫線的function</p>

<p>我們要畫的這條線有兩個參數,分別是 <script type="math/tex">w1,w2</script></p>

<p>這條線的直線方程式是: </p>

<script type="math/tex; mode=display">

w_{1}x_{1}+w_{2}x_{2}=0

</script>

<p>藉由改變 <script type="math/tex">w1,w2</script> 的數值,我們可以控制這條線的斜率</p>

<p>*註:</p>

<p>事實上若只有<script type="math/tex">w1,w2</script>只可以改變線的斜率,必須再另外再加個常數相才可改變線的位置</p>

<p>本文提到的是較簡化的模型,故省略常數項*</p>

<p>```python perceptron.py
def plot_data_and_line(w0,w1,w2):
    w1,w2 = float(w1),float(w2)
    if w2 != 0 :
        y1,y2 = (-w1<em>(xl1))/w2, (-w1</em>(xl2))/w2
        vx1,vy1 = [xl1,xl2,xl2,xl1,xl1], [y1,y2,yl2,yl2,y1]
        vx2,vy2 = [xl1,xl2,xl2,xl1,xl1], [y1,y2,yl1,yl1,y1]
    elif w1 != 0:
        vx1,vy1 = [xl2,0,0,xl2,xl2], [yl1,yl1,yl2,yl2,yl1]
        vx2,vy2 = [xl1,0,0,xl1,xl1], [yl1,yl1,yl2,yl2,yl1]
    else:
        print “ERROR, Invalid w1 and w2.”
        return;
    if  w2 &gt; 0 or ( w2 == 0 and w1 &gt; 0):
        c1,c2 = ‘b’,’r’
    else:
        c1,c2 = ‘r’,’b’
    fig = plt.figure()
    ax = fig.add_subplot(111)
    plt.scatter(X1[Y &gt; 0], X2[Y &gt; 0], s = 80, c = ‘b’, marker = “o”)
    plt.scatter(X1[Y&lt;= 0], X2[Y&lt;= 0], s = 80, c = ‘r’, marker = “^”)
    plt.fill(vx1, vy1, c1, alpha = 0.25)
    plt.fill(vx2, vy2, c2, alpha = 0.25)
    ax.set_title((“w1 = %s, w2 = %s”)%( w1, w2))
    ax.set_xlim(xl1, xl2)
    ax.set_ylim(yl1, yl2)
    fig.set_size_inches(6, 6)
    plt.show()</p>

<p>```</p>

<p>程式碼有點多,但其實是為了處理某些special case,例如 ` w2 != 0 :` 是用來判斷斜率是否為無限大</p>

<p>到interactive mode載入模組</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>reload(pct)
&lt;module ‘perceptron’ from ‘perceptron.py’&gt;</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>接著你可以試著輸入不同的參數,讓電腦畫出不同的線,例如：</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>pct.plot_data_and_line(1,1)
pct.plot_data_and_line(1,2)
pct.plot_data_and_line(1,0.5)
pct.plot_data_and_line(-1,-1)</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p><img src="https://lh5.googleusercontent.com/-BrKFzf5Qb3U/UyVN_fa8lxI/AAAAAAAAAlc/zZgKHlAfcqY/s480-no/line1.png" /></p>

<p><img src="https://lh6.googleusercontent.com/-7EGWFYaEYHI/UyVN_YyzCVI/AAAAAAAAAlY/kb7lkv_7tTI/s480-no/line2.png" /></p>

<p><img src="https://lh5.googleusercontent.com/-d3FrWKfweuU/UyVN_FAVP1I/AAAAAAAAAlQ/FLkYmbRlqo0/s480-no/line3.png" /></p>

<p><img src="https://lh3.googleusercontent.com/-uAA0pRCn7A4/UyVN_vNFk6I/AAAAAAAAAlg/4VsP_MW_QVY/s480-no/line4.png" /></p>

<p>這樣就可以畫出不一樣的線了</p>

<p>改變數字大小,可控制斜率</p>

<p>改變正負號,可以把紅色或藍色的區域反轉過來</p>

<h2 id="perceptron-algorithm">4. Perceptron Algorithm</h2>

<p>接下來,我們要讓電腦自己去學習,該怎麼樣畫一條線把資料分開</p>

<p>Peceptron Algorithm 的概念是這樣</p>

<p>先隨便給一條線,然後再根據這條線分類錯誤做修正</p>

<p>我們先給定<script type="math/tex">w1=1,w2=1</script>畫出一條線,如下圖</p>

<p><img src="https://lh5.googleusercontent.com/-BrKFzf5Qb3U/UyVN_fa8lxI/AAAAAAAAAlc/zZgKHlAfcqY/s480-no/line1.png" /></p>

<p>從上圖中分類錯誤的點中,任意挑一個點 <script type="math/tex">x</script> , </p>

<p>用<script type="math/tex">x</script> 到原點的向量 <script type="math/tex">X</script> ,來修正直線的法向量 <script type="math/tex">W</script></p>

<p>要怎麼修正呢？要看 <script type="math/tex">x</script> 的類別 <script type="math/tex">y</script> 來決定, 調整 <script type="math/tex">W</script>的方向</p>

<p>公式如下：</p>

<script type="math/tex; mode=display">

W\rightarrow W+y\cdot X

</script>

<p>用這個方法就可以改變直線的斜率,把分類錯誤的點 <script type="math/tex">x</script> ,歸到正確的一類,</p>

<p>舉例：</p>

<h4 id="y--1-">1.藍色( <script type="math/tex">y = 1</script> )分類錯誤</h4>

<p><img src="https://lh5.googleusercontent.com/-Jd3-UGYnBb8/UyVggqQjbaI/AAAAAAAAAl8/2vVq2f86GmM/s480-no/pct1_blue_0.png" /></p>

<p>修正後如下</p>

<p><img src="https://lh4.googleusercontent.com/-at-hDPDZwcw/UyVgglyyCrI/AAAAAAAAAmI/x8LDeM-5YB8/s480-no/pct1_blue_1.png" /></p>

<h4 id="y---1-">2.紅色( <script type="math/tex">y = -1</script> )分類錯誤</h4>

<p><img src="https://lh4.googleusercontent.com/-xvvwhbXXjHQ/UyVgjop2GdI/AAAAAAAAAm8/2xH3SYZRlbU/s480-no/pct1_red_0.png" /></p>

<p>修正後如下</p>

<p><img src="https://lh4.googleusercontent.com/-zkoQhjr47aQ/UyVghZHlAgI/AAAAAAAAAmQ/pQSK0S388zk/s480-no/pct1_red_1.png" /></p>

<p>以上結果顯示,<script type="math/tex">W</script> 修正過後, <em>有可能會把原本分類正確的點,分類到錯的一類</em> </p>

<p>所以可能要重複修正好幾次,直到把所有的點都分類正確為止</p>

<p>所以Perceptron的演算法如下：</p>

<p><em>(1) 任選一條線</em> </p>

<p><em>(2) 從這條線中選一個分類錯誤的點x</em></p>

<p><em>(3) 由點x修正線的斜率</em></p>

<p><em>(4) 重複(2)直到所有的點都分類正確</em></p>

<p>我們把這個演算法寫到 <em>perceptron.py</em> 裡</p>

<p>```python perceptron.py
def learn_perceptron(times=1000):
    w1,w2 = 1,1
    for i in range(times):
        ERR = (w1<em>X1+w2</em>X2) * Y &lt; 0
        if len(filter(bool,ERR)) &gt; 0:
           err_x1,err_x2,err_y = X1[ERR][0],X2[ERR][0],Y[ERR][0]
           w1,w2 = (w1+err_y<em>err_x1),(w2+err_y</em>err_x2)
        else: 
           print “Complete!”
           break;
    plot_data_and_line(w1,w2)</p>

<p>```</p>

<p>寫好之後,到interactive mode載入模組後,</p>

<p>其中times是迴圈的最大重複次數,</p>

<p>如果超過這個次數後還沒有完全分類正確,則演算法停止</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>reload(pct)
&lt;module ‘perceptron’ from ‘perceptron.py’&gt;</p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <blockquote>
      <p>pct.learn_perceptron()
Iteration:1
Iteration:2
Iteration:3
Iteration:4
Iteration:5
Complete!</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>總共執行了五次迴圈,所有的分類就都正確了</p>

<p>執行結果如下：</p>

<p><img src="http://lh4.googleusercontent.com/-LX6ouAzk-wE/UyVggokMcpI/AAAAAAAAAmA/4gNrnGYwlpM/s480-no/learn_1000.png" /></p>

<p>如果想要看看每一次迴圈中修正 <script type="math/tex">W</script> 的情形</p>

<p>以下提供更詳盡過程分解</p>

<p>但程式碼有點長</p>

<p>```python perceptron.py
def learn_and_plot():
    w1,w2 = 1.,1.
    plt.ion()
    fig = plt.figure()
    plt.show()
    complete=False
    i=0
    while True:
        plt.clf()
        ax = fig.add_subplot(111)
        ERR = (w1<em>X1+w2</em>X2) * Y &lt; 0
        if i &gt; 0 and (not complete):
            pwa1,pwa2 = pw1+err_y<em>err_x1,pw2+err_y</em>err_x2
            if err_y &gt;=0:
                eva1,eva2=[err_x1,pwa1], [err_x2,pwa2]
                ac=’b’
            else: 
                eva1,eva2=[err_x1,pw1], [err_x2,pw2]
                ac=’r’
            evb1,evb2=[pwa1,pw1], [pwa2,pw2]
            ax.arrow(0, 0, err_x1, err_x2, head_width=0.05, alpha=0.5, fc=ac, ec=ac )
            ax.arrow(0, 0, pw1, pw2, head_width=0.05, alpha=0.5, fc=’b’, ec=’b’)
            ax.arrow(0, 0, pwa1, pwa2, head_width=0.05, alpha=0.5, fc=’b’, ec=’b’)
            plt.text(err_x1/2., err_x2/2., ‘X’, color=’k’, size=20, fontweight=’bold’)
            plt.text(pw1/2., pw2/2., ‘W’, color=’k’, size=20, fontweight=’bold’)
            plt.text(pwa1/2., pwa2/2.,’W+yX’, color=’k’, size=20, fontweight=’bold’)
            plt.plot(eva1, eva2, alpha=0.3, color=’k’)
            plt.plot(evb1, evb2, alpha=0.3, color=’k’)
            plt.plot([xl1,xl2],[y1,y2],alpha=0.2, color=’b’)
            plt.fill(pvx1, pvy1, c1, alpha = 0.1)
            plt.fill(pvx2, pvy2, c2, alpha = 0.1)
        if w2 != 0 :
            y1,y2 = (-w1<em>(xl1))/w2, (-w1</em>(xl2))/w2
            vx1,vy1 = [xl1,xl2,xl2,xl1,xl1], [y1,y2,yl2,yl2,y1]
            vx2,vy2 = [xl1,xl2,xl2,xl1,xl1], [y1,y2,yl1,yl1,y1]
        elif w1 != 0:
            vx1,vy1 = [xl2,0,0,xl2,xl2], [yl1,yl1,yl2,yl2,yl1]
            vx2,vy2 = [xl1,0,0,xl1,xl1], [yl1,yl1,yl2,yl2,yl1]
        else:
            print “ERROR, Invalid w1 and w2.”
            return;
        if  w2 &gt; 0 or (w2 == 0 and w1 &gt; 0):
            c1,c2 = ‘b’,’r’
        else:      <br />
            c1,c2 = ‘r’,’b’
        plt.scatter(X1[Y &gt; 0], X2[Y &gt; 0], s = 80, c = ‘b’, marker = “o”)
        plt.scatter(X1[Y&lt;= 0], X2[Y&lt;= 0], s = 80, c = ‘r’, marker = “^”)
        ax.set_title( (“Iteration:%s, w1 = %.5f, w2 = %.5f”)%(i, w1, w2) )
        ax.set_xlim(xl1, xl2)
        ax.set_ylim(yl1, yl2)
        if not complete:
            plt.show()
            fig.set_size_inches(6, 6)
            raw_input(“Please press ENTER to update the result of iteration.”)
        plt.fill(vx1, vy1, c1, alpha = 0.4)
        plt.fill(vx2, vy2, c2, alpha = 0.4)
        pw1,pw2,pvx1,pvy1,pvx2,pvy2 = w1,w2,vx1,vy1,vx2,vy2
        #—-Perceptron Algorithm——-# 
        ERR = (w1<em>X1+w2</em>X2) * Y &lt; 0
        if len(filter(bool,ERR)) &gt; 0:
           err_x1,err_x2,err_y = X1[ERR][0],X2[ERR][0],Y[ERR][0]
           w1,w2 = (w1+err_y<em>err_x1),(w2+err_y</em>err_x2)
        else:
           if complete:
               ax.set_title((“Complete! Iteration:%s, w1 = %.5f, w2 = %.5f”)%(i-1, w1, w2))
               plt.show()
               fig.set_size_inches(6, 6)
               print “Complete!”
               break 
           else:
              complete=True
        #———————–#
        plt.show()
        fig.set_size_inches(6, 6)
        i=i+1
        raw_input(“Please press ENTER to start the next iteration.”)</p>

<p>```</p>

<p>然後,到interactive mode跑看看這個function</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>reload(pct)
&lt;module ‘perceptron’ from ‘perceptron.py’&gt;</p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <blockquote>
      <p>pct.learn_and_plot()
Please press ENTER to update the result of iteration.
Please press ENTER to start the next iteration.
….</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>進行完每次計算時,程式會把圖畫出來</p>

<p>然後程式會暫停,要按下ENTER才能繼續進行下一步</p>

<p>執行結果如下：</p>

<p><img src="https://lh5.googleusercontent.com/-0pliSS4phQs/UyVmQGxv6DI/AAAAAAAAAp8/-OdPv9EsFAM/s480-no/pct_motion_1.gif" /></p>

<h2 id="further-reading">5. Further Reading</h2>

<p>以上是Perceptron Algorithm很簡單的介紹</p>

<p>但其實Perceptron Algorithm有其他變化形式</p>

<p>例如,如果有noise存在的時候,本篇介紹的演算法就無法求得最佳解</p>

<p>就要用其他的變化形式來處理</p>

<p>想要看更多相關介紹,請到coursera上線上課程</p>

<h4 id="andrew-ng-machine-learning">1.Andrew Ng. Machine Learning</h4>

<p>https://www.coursera.org/course/ml</p>

<h4 id="machine-learning-foundations">2.林軒田 機器學習基石 (Machine Learning Foundations)</h4>

<p>https://www.coursera.org/course/ntumlone</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[機器學習 -- Logistic Regression Model (3D)]]></title>
    <link href="http://ckmarkoh.github.io/blog/2014/03/15/logisti-regression-model/"/>
    <updated>2014-03-15T01:47:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2014/03/15/logisti-regression-model</id>
    <content type="html"><![CDATA[<h2 id="introduction">1.Introduction</h2>

<p>這次來講一下機器學習(Machine Learning)的Logistic Model</p>

<p>搭上3D的風潮,讓我們來看看3D的Logistic Model畫出來會是如何</p>

<p>至於機器學習（Machine Learning）是什麼？</p>

<p>簡而言之,Machine Learning是一種讓機器根據已知的data,預測出未知的data情形如何</p>

<p>Logistic Regression是一種機器學習的Model,可以用來處理分類問題</p>

<p>從input data的feature,可以判斷出output data該歸到那一類</p>

<p>Logistic Regression的model如下</p>

<script type="math/tex; mode=display">

h(x)=\frac{1}{1+e^{-W^{T} X }}

</script>

<p>其中 <script type="math/tex">h(x)</script> 是hypothesis,藉由hypothesis,可以用已知的data來預知未知的data</p>

<p><script type="math/tex">X</script>是input data, <script type="math/tex">W</script> 是weight,這兩者皆是矩陣</p>

<!--more-->

<script type="math/tex; mode=display">

X=

\begin{bmatrix}

	x_{0}\\

  x_{1}\\

  x_{2}\\

\end{bmatrix},

W=

\begin{bmatrix}

	w_{0}\\

  w_{1}\\

  w_{2}\\

\end{bmatrix}, \mspace{20mu} W^{T} X =w_{0}x_{0}+w_{1}x_{1}+w_{2}x_{2} 

</script>

<p>用error function <script type="math/tex">E_{in}</script> 的gradient,調整 <script type="math/tex">W</script> ,可以讓h(x)根據data建立出一個的model,作出準確的預測</p>

<script type="math/tex; mode=display">

\bigtriangledown E_{in}(W)=\frac{1}{1+e^{-y W^{T} X }} \cdot (-yX) \\

W\leftarrow  W-\eta \bigtriangledown E_{in}(W)

</script>

<p>其中,<script type="math/tex">y</script> 是實際的結果,藉此可比對 <script type="math/tex">h(x)</script> 預測出的結果,和實際結果是否相等</p>

<p><script type="math/tex">\eta</script> 是學習速度,可以調整演算法收斂的速度</p>

<p>在此不提詳細理論,</p>

<p>若想更深入了解,請看coursea機器學習的線上課程 </p>

<h2 id="draw-3d-logistic-model">2. Draw 3D Logistic Model</h2>

<p>接著來畫畫看3D的Logistic Model是什麼樣子</p>

<p>首先,開一個新的檔案 <em>plot_logistic.py</em> ,載入以下模組</p>

<p>```python plot_logistic.py
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import numpy as np</p>

<p>```</p>

<p>然後寫個function來畫圖</p>

<p>```python plot_logistic.py
def plot_logistic(w0,w1,w2):
    fig = plt.figure()
    ax = fig.gca(projection=’3d’)
    X1, X2 = np.mgrid[-2:2:0.25, -2:2:0.25]
    X0 = np.ones(X1.shape)
    Z=np.divide(1,1+np.exp(-1*(np.multiply(w0,X0)+np.multiply(w1,X1)+np.multiply(w2,X2)) ))
    surf = ax.plot_wireframe(X1, X2, Z, cstride=1, rstride=1)
    ax.set_title((“w0=%s, w1=%s, w2=%s”)%(w0,w1,w2))
    plt.show()</p>

<p>```</p>

<p>這個function可以讓我們輸入不同的weight,畫出不一樣的圖</p>

<p>其中 <code>w0,w1,w2</code> 是weight,</p>

<p><code>Z=np.divide(1,1+np.exp(np.multiply(w0,X0)+np.multiply(w1,X1)+np.multiply(w2,X2)))</code></p>

<p>是logistic Model</p>

<p>接著到interactive mode來跑剛剛寫的function</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>import plot_logistic as pl</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>首先,用不同的w0畫畫看</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>pl.plot_logistic(0,1,1)
pl.plot_logistic(2,1,1)
pl.plot_logistic(-2,1,1)</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p><img src="http://lh3.googleusercontent.com/-jCPyrHFtLTw/UyP_03T4BkI/AAAAAAAAAcc/cPuCsRbEs2I/w480-h320-no/logistic1.png" /></p>

<p><img src="http://lh3.googleusercontent.com/-FihSXpokG6c/UyP_05hcLTI/AAAAAAAAAcY/iQ13UbUUFqA/w480-h320-no/logistic2.png" /></p>

<p><img src="http://lh4.googleusercontent.com/-IxWybG4zcBE/UyP_140FJYI/AAAAAAAAAc8/TdMEnA7m81A/w480-h320-no/logistic3.png" /></p>

<p>再改變不同的w1來畫</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>pl.plot_logistic(0,1,1)
pl.plot_logistic(0,2,1)
pl.plot_logistic(0,5,1)</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p><img src="http://lh3.googleusercontent.com/-jCPyrHFtLTw/UyP_03T4BkI/AAAAAAAAAcc/cPuCsRbEs2I/w480-h320-no/logistic1.png" /></p>

<p><img src="http://lh5.googleusercontent.com/-ElJA7s-qR5w/UyP_1oAHn3I/AAAAAAAAAdA/33SZMCbcrUY/w480-h320-no/logistic4.png" /></p>

<p><img src="http://lh4.googleusercontent.com/-NGiKStaFfJE/UyP_1nbKXiI/AAAAAAAAAcs/oBSw0gc7djE/w480-h320-no/logistic5.png" /></p>

<p>改變w2的結果和改變w1的結果呈對稱,在此省略</p>

<h2 id="learning-by-logistic-model">3. Learning by Logistic Model</h2>

<p>接著來看看如何用logistic model來進行機器學習</p>

<p>開一個新的檔案 <em>logistic_model.py</em> ,並載入必要模組</p>

<p>```python logistic_model.py
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt 
import numpy as np</p>

<p>```</p>

<p>假設有這些data</p>

<p>```python logistic_model.py
X0=np.array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
              1.,  1.])
X1=np.array([ 0.56084642,  1.3977301 ,  1.37559576,  1.25336531, -1.90734535,
              1.41981271,  0.96873065, -1.95225824, -0.65156755,  0.84391794,
              1.37463405, -1.90385285, -1.9961716 , -1.53774446,  0.62736126])</p>

<p>X2=np.array([-0.54631195,  1.9883947 , -1.77695809,  1.74944549, -0.99031191,
              1.6854447 , -1.78734606, -0.46925818, -1.72241882, -0.68654333,
              1.70881411,  1.22240194, -1.87704629,  1.10556042, -1.892201  ])</p>

<p>Y=np.array([-0.2041112 , -0.10358391, -2.69506218,  0.01788122,  4.42147236,
            -0.35981413, -1.88860354,  4.87603575,  1.39744193, -0.86841621,
            -0.25309822,  5.96338706,  3.9784108 ,  5.14938121, -1.27926322])</p>

<p>```</p>

<p>接著,寫出畫圖的function</p>

<p>```python logistic_model.py
def plot_data(filename=’data1’):
    fig = plt.figure()
    ax = fig.gca(projection=’3d’)
    surf = ax.scatter(X1[Y&gt;=0], X2[Y&gt;=0], np.ones(X1[Y&gt;=0].shape) ,c=’b’,marker=’o’)
    surf = ax.scatter(X1[Y&lt;0], X2[Y&lt;0], np.zeros(X1[Y&lt;0].shape) ,c=’r’,marker=’^’)
    plt.show()</p>

<p>```</p>

<p>接著到interactive mode來跑剛剛寫的function</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>import logistic_model as lm
lm.plot_data()</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p><img src="http://lh5.googleusercontent.com/-03OAk99aDVQ/UyQTVzt5JiI/AAAAAAAAAd8/uDW14eX_vi0/w480-h320-no/data1.png" /></p>

<p>再來看看,把剛剛的model加進來看看會怎樣</p>

<p>在 <em>logistic_model.py</em> 加入一個function</p>

<p>和剛剛的類似,但這個function有加入logistic model</p>

<p>```python logistic_model.py
def plot_data_and_model(w0,w1,w2):
    fig = plt.figure()
    ax = fig.gca(projection=’3d’)
    surf = ax.scatter(X1[Y&gt;=0], X2[Y&gt;=0], np.ones(X1[Y&gt;=0].shape) ,c=’b’,marker=’o’)
    surf = ax.scatter(X1[Y&lt;0], X2[Y&lt;0], np.zeros(X1[Y&lt;0].shape) ,c=’r’,marker=’^’)
    ##—Logistic Model—- 
    GX1, GX2 = np.mgrid[-2:2:0.25, -2:2:0.25]
    GX0 = np.ones(GX1.shape)
    Z = np.divide(1,1+np.exp(-1*(np.multiply(w0,GX0)+np.multiply(w1,GX1)+np.multiply(w2,GX2))))
    surf = ax.plot_wireframe(GX1, GX2, Z, cstride=1, rstride=1)
    ##———————
    plt.show()</p>

<p>```</p>

<p>到interactive mode重新載入 <em>logistic_model.py</em> ,再畫畫看</p>

<p>```</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>reload(lm)
&lt;module ‘logistic_model’ from ‘logistic_model.py’&gt;</p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <blockquote>
      <p>lm.plot_data_and_model(1,1,1)</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p><img src="" /></p>

<p>看起來,這個model跟data好像差得很遠</p>

<p>沒關係,我們用h(x)的gradient來調整w0,w1,w2</p>

<script type="math/tex; mode=display">

\bigtriangledown E_{in}(W)=\frac{1}{1+e^{-y W^{T} X }} \cdot (-y X) \\

W\leftarrow  W-\eta \bigtriangledown E_{in}(W)

</script>

<p>接著我們要用以上公式來調整 <script type="math/tex">W</script> ,達到最佳化</p>

<p>在 <em>logistic_model.py</em> 加入以下function</p>

<p>```python logistic_model.py
def learning_logistic(times):
    w0,w1,w2=1,1,1
    for i in range(times):
        eta = 0.1
        temp_err = np.divide(1,1+np.exp((Y)<em>(np.multiply(w0,X0)+np.multiply(w1,X1)+np.multiply(w2,X2))))
        e0 = eta</em>np.average(temp_err<em>(-Y</em>X0))
        e1 = eta<em>np.average(temp_err</em>(-Y<em>X1))
        e2 = eta</em>np.average(temp_err<em>(-Y</em>X2))
        w0 = w0-e0 
        w1 = w1-e1 
        w2 = w2-e2 
    plot_data_and_model(w0,w1,w2)</p>

<p>```</p>

<p>簡而言之,這個function用一個迴圈,逐次來調整w0,w1,w2,把error降低</p>

<p>times是這個迴圈要跑幾次,</p>

<p>eta是學習速率,如果eta越小,就越慢收斂</p>

<p>但eta太大,有可能會錯過最佳解</p>

<p>接著,重新載入修改好的程式</p>

<p>分別讓迴圈跑1次,10次到10000次來看看,結果如何</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>reload(ml)
&lt;module ‘logistic_model’ from ‘logistic_model.py’&gt;</p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ml.learning_logistic(1)
ml.learning_logistic(10)
ml.learning_logistic(100)
ml.learning_logistic(1000)
ml.learning_logistic(10000)</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>結果如下：</p>

<p><img src="http://lh5.googleusercontent.com/-7zj7cOhJPEI/UyQUdiwAp9I/AAAAAAAAAf0/nwRWG2AQHi4/w480-h320-no/model_1.png" /></p>

<p><img src="http://lh4.googleusercontent.com/-NAOxS1hvShE/UyQUdmlxMRI/AAAAAAAAAgI/461egO-xJWw/w480-h320-no/model_10.png" /></p>

<p><img src="http://lh3.googleusercontent.com/-SdDWeaxWkwA/UyQUePGcZOI/AAAAAAAAAfw/Mmdzl_eWWlU/w480-h320-no/model_100.png" /></p>

<p><img src="http://lh4.googleusercontent.com/-BDGOyehak7A/UyQUeoJNGoI/AAAAAAAAAgE/pcR_NCORpd4/w480-h320-no/model_1000.png" /></p>

<p><img src="http://lh6.googleusercontent.com/-ZndSShsS7ZU/UyQUfHgvlpI/AAAAAAAAAgA/BEwdIT-QwVc/w480-h320-no/model_10000.png" /></p>

<p>動畫版：</p>

<p><img src="http://lh3.googleusercontent.com/-QBkEkySQAEI/UyVEwjNnd-I/AAAAAAAAAi8/kvoOH4-g514/w480-h320-no/model_motion.gif" /></p>

<p>很明顯地,我們可以看到機器正在學習,依照data來把model做調整</p>

<p>model可以符合大部分的data都可以達到準確預測</p>

<p>但還是有些例外,例如,右上角的藍色的點,不符合model的預測</p>

<p>這種data不符合大多數data的情形,就可能noice了</p>

<h2 id="further-reading">4. Further Reading</h2>

<p>以上是Logistic Model很簡單的實作</p>

<p>想要學習Machine Learning的理論與應用</p>

<p>請到coursera上線上課程</p>

<p>主要有以下兩門課,雖然難易度不同,但都講得很好</p>

<h4 id="andrew-ng-machine-learning">1.Andrew Ng. <em>Machine Learning</em></h4>

<p>Andrew Y. Ng 是Stanford University的知名教授,也是Google Brain開發團隊的成員之一</p>

<p>這門課基本概念講得很清楚,基本的概念以及應用都有提到,淺顯易懂,適合一般大眾</p>

<p>https://www.coursera.org/course/ml</p>

<h4 id="machine-learning-foundations">2.林軒田 <em>機器學習基石 (Machine Learning Foundations)</em></h4>

<p>林軒田教授是台大資工系的教授,也曾經帶領學生參加資料探勘比賽KDDCUP獲得世界冠軍</p>

<p>這門課有很多數學理論推導,作業多,難度頗高,適合想要成為機器學習的高手的人</p>

<p>https://www.coursera.org/course/ntumlone</p>

]]></content>
  </entry>
  
</feed>
