<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machine Learning | Mark Chang's Blog]]></title>
  <link href="http://ckmarkoh.github.io/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://ckmarkoh.github.io/"/>
  <updated>2017-01-01T19:52:48+08:00</updated>
  <id>http://ckmarkoh.github.io/</id>
  <author>
    <name><![CDATA[Mark Chang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Gradient Descent & AdaGrad]]></title>
    <link href="http://ckmarkoh.github.io/blog/2015/12/23/optimization-method-adagrad/"/>
    <updated>2015-12-23T17:14:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2015/12/23/optimization-method-adagrad</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>在機器學習的過程中，常需要將 <em>Cost Function</em> 的值減小，需由最佳化的方法來達成。本文介紹 <em>Gradient Descent</em> 和 <em>AdaGrad</em> 兩種常用的最佳化方法。</p>

<h2 id="gradient-descent">Gradient Descent</h2>

<p><em>Gradient Descent</em> 的公式如下：</p>

<script type="math/tex; mode=display">

\textbf{x}_{t+1} \leftarrow \textbf{x}_{t} - \eta \textbf{g}_{t}

</script>

<p>其中， <script type="math/tex">\eta</script> 為 <em>Learning Rate</em> ， <script type="math/tex">\textbf{x} </script> 為最佳化時要調整的參數， <script type="math/tex">\textbf{g}</script> 為最佳化目標函數對 <script type="math/tex">\textbf{x}</script> 的梯度。 <script type="math/tex">\textbf{x}_{t}</script> 為調整之前的 <script type="math/tex">\textbf{x} </script> ，<script type="math/tex">\textbf{x}_{t+1}</script> 為調整之後的 <script type="math/tex">\textbf{x} </script> 。</p>

<p>舉個例子，如果目標函數為 <script type="math/tex"> f(x,y) = y^2 - x^2  </script> ，起始參數為 <script type="math/tex">(x,y) = (0.001,4)</script> ，則畫出來的圖形如下圖，曲面為目標函數，紅色的點為起始參數：</p>

<p><img src="/images/pic/pic_00126.png" alt="" /></p>

<!--more-->

<p>可藉由改變 <script type="math/tex">(x,y)</script> 來讓 <script type="math/tex">f(x,y)</script> 的值減小。 <em>Gradient Descent</em> 所走的方向為梯度最陡的方向，若 <script type="math/tex">eta=0.3</script> 則 ：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& 	x \leftarrow  x - \eta  \dfrac{\partial f(x,y)}{\partial x}  \\

&  y \leftarrow  y - \eta  \dfrac{\partial f(x,y)}{\partial y} \\

\end{align}

 %]]&gt;</script>

<p>求出微分後得：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& 	x \leftarrow  x - \eta  \times (-2x)  \\

&  y \leftarrow  y - \eta  \times 2y \\

\end{align}

 %]]&gt;</script>

<p>代入數值，得：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x = 0.001 - 0.3 \times (-2) \times 0.001 = 0.0016 \\

& y = 4 - 0.3 \times 2 \times 4 = 1.6 \\

\end{align}

 %]]&gt;</script>

<p>更新完後的結果如下：</p>

<p><img src="/images/pic/pic_00127.png" alt="" /></p>

<p>從上圖可看出，紅點移動到比較低的地方，即 <script type="math/tex">f(x,y)</script> 變小了。</p>

<p>經過了數次改變 <script type="math/tex">(x,y)</script> 值的循環之後，<script type="math/tex">f(x,y)</script> 的值會越變越小，紅點移動的路徑如下圖所示：</p>

<p><img src="/images/pic/pic_00118.png" alt="" /></p>

<p>動畫版：</p>

<p><img src="/images/pic/pic_00119.gif" alt="" /></p>

<p>從上圖可發現，紅色的點會卡在 <script type="math/tex">(0,0)</script> 附近（也就是Saddle Point），過了一陣子後才會繼續往下滾。</p>

<h2 id="adagrad">AdaGrad</h2>

<p><em>Gradient Descent</em> 的缺點有：</p>

<p>(1) <em>Learning Rate</em> 不會隨著時間而減少</p>

<p>(2) <em>Learning Rate</em> 在每個方向是固定的</p>

<p>以上的(1)會使得在越接近近目標函數最小值時，越容易走過頭，(2)則會容易卡在目標函數的Saddle Point。</p>

<p>因為 <em>Gradient Descent</em> 只考慮目前的 <em>Gradient</em> ，如果可以利用過去時間在各個方向的 <em>Gradient</em> ，來調整現在時間點在各個方向的 <em>Learning Rate</em> ，則可避免以上兩種情型發生。</p>

<p><em>AdaGrad</em> 的公式如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \textbf{G}_{t} = \sum_{n=0}^{t} \textbf{g}_{n}^{2} \\

& \textbf{x}_{t+1} \leftarrow \textbf{x}_{t} - \frac{\eta}{\sqrt{\textbf{G}_{t}}} \textbf{g}_{t} \\

\end{align}

 %]]&gt;</script>

<p>其中，<script type="math/tex"> \textbf{G}_{t} </script> 為過去到現在所有時間點所有的 <script type="math/tex">\textbf{g}</script> 的平方和。由於  <script type="math/tex">\textbf{x}</script> ， <script type="math/tex">\textbf{g}</script>和 <script type="math/tex">\textbf{G}</script> 皆為向量，設 <script type="math/tex">x_{i}</script> ， <script type="math/tex">g_{i}</script> 和 <script type="math/tex">G_{i}</script> 各為其元素，則公式可寫成：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& G_{i,t} = \sum_{n=0}^{t} g_{i,n}^{2} \\

& x_{i,t+1} \leftarrow x_{i,t} - \frac{\eta}{\sqrt{G_{i,t}}} g_{i,t} \\

\end{align}

 %]]&gt;</script>

<p>這公式可修正以上兩個 <em>Gradient Descent</em> 的缺點：</p>

<p>1.若時間越久，則 <em>Gradient</em> 平方和越大，使得 <em>Learning Rate</em> 越小，這樣就可以讓 <em>Learning Rate</em> 隨著時間減少，而在接近目標函數的最小值時，比較不會走過頭。</p>

<p>2.若某方向從過去到現在時間點 <em>Gradient</em> 平方和越小，則 <em>Learning Rate</em> 要越大。（直覺上來講，過去時間點 <em>Gradient</em> 越小的方向，在未來可能越重要，這種概念有點類似<a href="/blog/2014/04/14/natural-language-processing-tf-idf">tf-idf</a>，在越少文檔中出現的詞，可能越重要。）由於各方向的 <em>Learning Rate</em> 不同，比較不會卡在 <em>Saddle Point</em> 。</p>

<p>前述例子，起始參數為 <script type="math/tex">(x,y) = (0.001,4)</script> ，則畫出來的圖形如下圖，曲面為目標函數，藍點為起始參數：</p>

<p><img src="/images/pic/pic_00128.png" alt="" /></p>

<p>用 <em>AdaGrad</em> 來更新 <script type="math/tex">(x,y)</script> 的值，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& 	x_{t+1} \leftarrow  x_{t} - \frac{\eta}{\sqrt{\sum_{n=0}^{t}  

(\dfrac{\partial f(x_{n},y_{n})}{\partial x_{n}} )^{2} }} 

\dfrac{\partial f(x_{t},y_{t})}{\partial x_{t}}  \\


&  y_{t+1} \leftarrow  y_{t} - \frac{\eta}{\sqrt{\sum_{n=0}^{t}  

(\dfrac{\partial f(x_{n},y_{n})}{\partial y_{n}} )^{2}  }} 

\dfrac{\partial f(x_{t},y_{t})}{\partial y_{t}} \\

\end{align}

 %]]&gt;</script>

<p>化簡後得：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& 	x_{t+1} \leftarrow  x_{t} - \frac{\eta}{\sqrt{\sum_{n=0}^{t}  

( -2x_{n} )^{2} }} 

( -2x_{t} ) \\


&  y_{t+1} \leftarrow  y_{t} - \frac{\eta}{\sqrt{\sum_{n=0}^{t}  

( 2y_{n} )^{2} }} 

( 2y_{t} ) \\

\end{align}

 %]]&gt;</script>

<p>由於 <em>AdaGrad</em> 的 <em>Learning Rate</em> 會隨時間減小，所以初始化時可以給它較大的值，此例中，設 <script type="math/tex">\eta = 1.0</script></p>

<p>代入 <script type="math/tex">(x,y)</script> 的數值，如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x = 0.001 -  \frac{1.0}{\sqrt{  ( (-2) \times 0.001 )^2  }} \times (-2) \times 0.001 = 1.001 \\

& x = 4 -  \frac{1.0}{\sqrt{  ( 2 \times 4 )^2  }} \times 2 \times 4 = 3 \\

\end{align}

 %]]&gt;</script>

<p>更新圖上的藍點，如下圖：</p>

<p><img src="/images/pic/pic_00129.png" alt="" /></p>

<p>再往下走一步， <script type="math/tex">(x,y)</script> 的值如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& x = 1.001 -  \frac{1.0}{\sqrt{  ( (-2) \times 0.001 )^2  + ( (-2) \times 1.001 )^2 }} \times (-2) \times 1.001 = 2.001 \\

& x = 3 -  \frac{1.0}{\sqrt{  ( 2 \times 4 )^2 +  ( 2 \times 3 )^2  }} \times 2 \times 3 = 2.4 \\

\end{align}

 %]]&gt;</script>

<p>更新圖上的藍點，如下圖：</p>

<p><img src="/images/pic/pic_00130.png" alt="" /></p>

<p>經過了數次改變 <script type="math/tex">(x,y)</script> 值的循環之後，<script type="math/tex">f(x,y)</script> 的值會越變越小，藍點移動的路徑如下圖所示：</p>

<p><img src="/images/pic/pic_00123.png" alt="" /></p>

<p>動畫版：</p>

<p><img src="/images/pic/pic_00124.gif" alt="" /></p>

<p>由此可以發現， <em>AdaGrad</em> 不會卡在 <em>Saddle Point</em> 。</p>

<p>將 <em>Gradient Descent</em> 和 <em>AdaGrad</em> 畫在同一張圖上，比較兩者差異：</p>

<p><img src="/images/pic/pic_00125.gif" alt="" /></p>

<h2 id="implementation">Implementation</h2>

<p>再來進入實作的部分：</p>

<p>首先,開啟新的檔案 adagrad.py 並貼上以下程式碼</p>

<p>```python adagrad.py
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
import matplotlib.pyplot as plt
import numpy as np</p>

<p>def func(x,y):
  return (y<strong>2-x</strong>2)</p>

<p>def func_grad(x,y):
  return (-2<em>x, 2</em>y)</p>

<p>def plot_func(xt,yt,c=’r’):
  fig = plt.figure()
  ax = fig.gca(projection=’3d’,
        elev=35., azim=-30)
  X, Y = np.meshgrid(np.arange(-5, 5, 0.25), np.arange(-5, 5, 0.25))
  Z = func(X,Y) 
  surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, 
    cmap=cm.coolwarm, linewidth=0.1, alpha=0.3)
  ax.set_zlim(-50, 50)
  ax.scatter(xt, yt, func(xt,yt),c=c, marker=’o’ )
  ax.set_title(“x=%.5f, y=%.5f, f(x,y)=%.5f”%(xt,yt,func(xt,yt))) 
  plt.show()
  plt.close()</p>

<p>def run_grad():
  xt = 0.001 
  yt = 4 
  eta = 0.3 
  plot_func(xt,yt,’r’)
  for i in range(20):
    gx, gy = func_grad(xt, yt)
    xt = xt - eta<em>gx
    yt = yt - eta</em>gy
    if xt &lt; -5 or yt &lt; -5 or xt &gt; 5 or yt &gt; 5:
      break
    plot_func(xt,yt,’r’)</p>

<p>def run_adagrad():
  xt = 0.001
  yt = 4 
  eta = 1.0 
  Gxt = 0
  Gyt = 0
  plot_func(xt,yt,’b’)
  for i in range(20):
    gxt,gyt = func_grad(xt, yt)
    Gxt += gxt<strong>2
    Gyt += gyt</strong>2
    xt = xt - eta<em>(1./(Gxt<strong>0.5))*gxt
    yt = yt - eta*(1./(Gyt</strong>0.5))</em>gyt
    if xt &lt; -5 or yt &lt; -5 or xt &gt; 5 or yt &gt; 5:
      break
    plot_func(xt,yt,’b’)</p>

<p>```</p>

<p>其中， <code>func(x,y)</code> 為目標函數，<code>func_grad(x,y)</code> 為目標函數的 <em>gradient</em> ，而 <code>plot_func(xt,yt,c='r')</code> 可畫出目標函數的曲面， <code>run_grad()</code> 用來執行 <em>Gradient Descent</em> ， <code>run_adagrad()</code> 用來執行 <em>AdaGrad</em> 。 <code>xt</code> 和 <code>yt</code> 對應到前例的 <script type="math/tex">(x,y)</script> ，而 <code>eta</code> 為 <em>Learning Rate</em> 。 <code>for i in range(20)</code> 表示最多會跑20個迴圈，而 <code>if xt &lt; -5 or yt &lt; -5 or xt &gt; 5 or yt &gt; 5</code> 表示，如果 <code>xt</code> 和 <code>yt</code> 超出邊界，則會先結束迴圈。</p>

<p>到 python console 執行：</p>

<p>```sh</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>import adagrad</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>執行 <em>Gradient Descent</em> ，指令如下：</p>

<p>```sh</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>adagrad.run_grad()</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="/images/pic/pic_00126.png" alt="" /></p>

<p><img src="/images/pic/pic_00127.png" alt="" /></p>

<p>以此類推</p>

<p>執行 <em>Adagrad</em> ，指令如下：</p>

<p>```sh</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>adagrad.run_adagrad()</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>則程式會逐一畫出整個過程：</p>

<p><img src="/images/pic/pic_00128.png" alt="" /></p>

<p><img src="/images/pic/pic_00129.png" alt="" /></p>

<p><img src="/images/pic/pic_00130.png" alt="" /></p>

<p>以此類推</p>

<h2 id="reference">Reference</h2>

<h4 id="notes-on-adagrad">Notes on AdaGrad</h4>

<p>http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf</p>

<h4 id="visualizing-optimization-algos">Visualizing Optimization Algos</h4>

<p>http://imgur.com/a/Hqolp</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Brown Clustering]]></title>
    <link href="http://ckmarkoh.github.io/blog/2014/10/25/natural-language-processing-brown-clustering/"/>
    <updated>2014-10-25T13:31:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2014/10/25/natural-language-processing-brown-clustering</id>
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p><em>Clustering</em> 是一種非監督式的機器學習方法。所謂非監督式的學習方法，即是不需要事先提供人工標記好的語料庫給機器學習的演算法。可以直接從未標示的語料庫中，根據既有的特徵來做分類。</p>

<p>例如，可以根據某字的前面或後面有哪些字，來決定哪些字屬於同一類。給一語料庫如下：</p>

<blockquote>

</blockquote>

<p>The dog runs.</p>

<p>A dog jumps.</p>

<p>The dog jumps.</p>

<p>A cat runs.</p>

<p>The cat jumps.</p>

<p>The cat runs.</p>

<blockquote>

</blockquote>

<p>根據這些例句，可以把 <em>cat</em> 和 <em>dog</em> 歸在同一類，因為它們前面的字是 <em>the</em> 或 <em>a</em> ，同理，也可以把 <em>run</em> 和 <em>jump</em> 歸在同一類。</p>

<h2 id="defining-the-formulation">Defining the Formulation</h2>

<p>現在來定義一下，進行這種分類所需要的數學公式。</p>

<!--more-->

<p>假設總共有 <script type="math/tex">V</script> 個字彙， <script type="math/tex">V=\{w_{1},w_{2},...,w_{t}\}</script> ，和一個分類函數 <script type="math/tex">C:V \rightarrow \{1,2,...k\}</script> 。 <script type="math/tex">C</script> 把 <script type="math/tex">V</script> 中的單字分類成 <script type="math/tex">k</script> 類， <script type="math/tex">k \leq t</script> 。</p>

<p>例如， <script type="math/tex">w_{1}</script> 被分類到類別 <script type="math/tex">3</script> ，則 <script type="math/tex">C(w_{1}) = 3</script> 。</p>

<p>給定語料庫中的句子 <script type="math/tex">S = w_{1}, w_{2}, ... ,w_{n} </script> ，則可以計算這個句子出現的機率，為：</p>

<script type="math/tex; mode=display">

p(w_{1},w_{2},...,w_{n}) = \prod_{i=1}^{n}  e(w_{i}\mid C(w_{i})) \times q(C(w_{i}) \mid C(w_{i-1}))


</script>

<p>其中，<script type="math/tex">e(w_{i}\mid C(w_{i}))</script>  為，在類別 <script type="math/tex"> C(w_{i}) </script> 中，出現 <script type="math/tex">w_{i}</script> 的機率。 </p>

<p>而 <script type="math/tex">q(C(w_{i}) \mid C(w_{i-1}))</script> 為，若此字的類別為 <script type="math/tex"> C(w_{i}) </script> ，則前一個字的類別為 <script type="math/tex">C(w_{i-1})</script> 的機率。</p>

<p>根據先前例子中的語料庫，假設已經分類完成，分類 <script type="math/tex">(1)</script> 如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&  C(\text{the})=C(\text{a})=1, \mspace{10mu}

 C(\text{dog})=C(\text{cat})=2, \mspace{10mu}

 C(\text{run})=C(\text{jump})=3 \mspace{10mu} & (1)

\end{align}

 %]]&gt;</script>

<p>並且，可計算出 <script type="math/tex">e(w_{i}\mid C(w_{i}))</script> 和 <script type="math/tex">q(C(w_{i}) \mid C(w_{i-1}))</script> 的機率。 例如， <script type="math/tex">e(\text{the}\mid C(\text{the}) )</script> 的機率為：</p>

<script type="math/tex; mode=display">

e(\text{the}\mid C(\text{the}) )= 

e(\text{the}\mid 1 )= 

\frac{count(\text{the})}{count(\text{the})+count(\text{a})} = 

\frac{4}{4+2} = \frac{2}{3}

</script>

<p>其中，<script type="math/tex">count(\text{the})</script> 為 <em>the</em> 在語料庫中出現的次數。 而 <script type="math/tex">q(C(\text{dog}) \mid C(\text{the}))</script> 的機率為：</p>

<script type="math/tex; mode=display">


q(C(\text{dog}) \mid C(\text{the})) = q(2 \mid 1) = \frac{count(1, 2 )}{count(1)} = \frac{6}{6} = 1


</script>

<p>其中，<script type="math/tex">count(1,2)</script> 為，類別為 <script type="math/tex">2</script> 的字，出現在類別為 <script type="math/tex">1</script> 的字後面的機會。</p>

<p>給定語料庫中的句子 <em>The dog runs</em> ，那麼可以算出此句的機率為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& p(\text{the}, \text{dog}, \text{run}) \\

& = e(the \mid C(\text{the})) \times e(dog \mid C(\text{dog})) \times e(run \mid C(\text{run})) \\ 

& \times q(C(\text{the}) \mid 0 ) \times q(C(\text{dog}) \mid C(\text{the}) )  \times q(C(\text{run}) \mid C(\text{dog}) ) \\

& = e(the \mid 1) \times e(dog \mid 2) \times e(run \mid 3)  \times q(1 \mid 0 ) \times q(2 \mid 1 )  \times q( 3 \mid 2 ) \\

& = \frac{2}{3} \times \frac{1}{2} \times \frac{1}{2}  \times 1  \times 1  \times 1  \\

& = \frac{1}{6}

\end{align} 

 %]]&gt;</script>

<p>其中，把第一個字 <em>the</em> 的前一個字，歸類為第 <script type="math/tex">0</script> 類，即可求出 <script type="math/tex">q(C(\text{the}) \mid 0 ) = 1</script> 。</p>

<p>再給一個分類，分類 <script type="math/tex">(2)</script>，這次隨便分，把 <em>the</em> 和 <em>dog</em> 分成一類，把 <em>a</em> 和 <em>cat</em> 分成一類，再把 <em>run</em> 和 <em>jump</em> 分成一類，分類如下：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& C(\text{the})=C(\text{dog})=1, \mspace{10mu}

 C(\text{a})=C(\text{cat})=2, \mspace{10mu}

 C(\text{run})=C(\text{jump})=3 \mspace{10mu} & (2)

\end{align}

 %]]&gt;</script>

<p>給定語料庫中的句子 <em>The dog runs</em> ，那麼可以算出此句的機率為：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& p(\text{the}, \text{dog}, \text{run}) \\

& = e(the \mid C(\text{the})) \times e(dog \mid C(\text{dog})) \times e(run \mid C(\text{run})) \\ 

& \times q(C(\text{the}) \mid 0 ) \times q(C(\text{dog}) \mid C(\text{the}) )  \times q(C(\text{run}) \mid C(\text{dog}) ) \\

& = e(the \mid 1) \times e(dog \mid 1) \times e(run \mid 3)  \times q(1 \mid 0 ) \times q(1 \mid 1 )  \times q( 3 \mid 1 ) \\

& = \frac{4}{7} \times \frac{3}{7} \times \frac{1}{2}  \times \frac{4}{6}  \times \frac{2}{7}  \times \frac{1}{2}  \\

& = \frac{4}{343}

\end{align} 

 %]]&gt;</script>

<p>從以上例子得知，用語料庫的句子  <em>The dog runs</em> 來算機率，分類 <script type="math/tex">(1)</script> 得出的機率比分類 <script type="math/tex">(2)</script> 高。若要得到較理想的分類，可以用語料庫的句子算出的機率，做最佳化，機率較高者，為較理想的分類。</p>

<p>至於，把語料庫中 <script type="math/tex">t</script> 個單字 分組成 <script type="math/tex">k</script> 個類別的過程如何？</p>

<p>首先，把 <script type="math/tex">t</script> 個單字個別分成 <script type="math/tex">t</script> 組。</p>

<p>再從這些組中挑兩組合併起來，使其得出機率為最大值。合併完後，共有 <script type="math/tex">t-1</script> 組。</p>

<p>再來，從這些 <script type="math/tex">t-1</script> 組中，再挑兩組合併，以此類推，直到剩下 <script type="math/tex">k</script> 組。</p>

<p>由於此種演算法的時間複雜度相當高，為<script type="math/tex">O(t^{5})</script> ，<em>[Brown et al., 1992]</em> 提出的 <em>hierarchical clustering</em> 的概念，可有效降低時間複雜度，有興趣者請看延伸閱讀。</p>

<h2 id="implementation">Implementation</h2>

<p>首先，建立一個程式檔，命名為 <em>bcluster.py</em></p>

<p>```python bcluster.py
W_CORPUS = [
  [‘the’,’dog’,’run’], [‘a’,’dog’,’jump’],
  [‘the’,’dog’,’jump’], [‘a’,’cat’,’run’],
  [‘the’,’cat’,’jump’], [‘the’,’cat’,’run’],
]</p>

<p>def word_count(c):
  wcount = {} 
  for s in c:
    for w in s:
      wcount.update({w:wcount.get(w,0) + 1.0})
  return wcount</p>

<p>def choose_merge(v, w_corpus, w_count):
  max_p, max_v = 0.0, []
  for i in range(len(v)):
    for j in range(i+1,len(v)):
      s1 = [x for x in v]
      s2 = [ s1.pop(i)+s1.pop(j-1) ]+[x for x in s1]
      p = calculate_prob(s2, w_corpus, w_count)
      if p &gt; max_p:
        max_p, max_v = p, s2 
  return max_v           </p>

<p>def calculate_prob(v, w_corpus, w_count):
  w_class = { w:str(i) for i,s1 in enumerate([‘0’]+v) for w in s1}
  c_corpus = [[w_class.get(w) for w in s] for s in w_corpus]<br />
  c_count = word_count(c_corpus)
  gran_count = word_count([[“_“.join(s[i:i+2]) for i in range(len(s))] for s in c_corpus])
  p = 1.0;
  for s in w_corpus:
    for i in range(1,len(s)):
      p = p<em>e(s[i], w_class, w_count, c_count)</em>q(s[i], s[i-1], w_class, c_count, gran_count)
  return p </p>

<p>def e(w, w_class, w_count, c_count):
  return w_count[w] / c_count[w_class[w]] </p>

<p>def q(w, wp, w_class, c_count, gran_count):
  return gran_count[“%s_%s”%(w_class[wp],w_class[w])] / c_count[w_class[w]]</p>

<p>def bcluster(k, corpus):
  w_count = word_count(corpus)
  w_corpus = [ [‘0’]+ s for s in corpus]
  v = [[w] for w in w_count.keys()]
  while len(v) &gt; k:
  	print v
    v = choose_merge(v, w_corpus, w_count)
  return v</p>

<p>```</p>

<p>其中，<code>W_CORPUS</code> 為語料庫，<code>bcluster(k, corpus)</code> 為主要執行 <em>cluster</em> 的函數，參數 <code>k</code> 為總共要分成幾組。 </p>

<p>到 python interactive mode 載入模組</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>from bcluster import bcluster,W_CORPUS</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>根據語料庫中的文字，分成三組，程式印出逐漸將6個字分成3組的過程，最後一行為最終結果：</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>from bcluster import bcluster,W_CORPUS
bcluster(3,W_CORPUS)
[[‘a’], [‘jump’], [‘run’], [‘the’], [‘dog’], [‘cat’]]
[[‘a’, ‘the’], [‘jump’], [‘run’], [‘dog’], [‘cat’]]
[[‘dog’, ‘cat’], [‘a’, ‘the’], [‘jump’], [‘run’]]
[[‘jump’, ‘run’], [‘dog’, ‘cat’], [‘a’, ‘the’]]</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<h2 id="reference">Reference</h2>

<p>本文參考至coursera線上課程</p>

<h4 id="michael-collins-natural-language-processing">Michael Collins. Natural Language Processing</h4>

<p>https://www.coursera.org/course/nlangp</p>

<p>Brown Clustering 出處：</p>

<h4 id="brown-et-al-class-based-n-gram-models-of-natural-language-1992">Brown et al. Class-Based n-gram Models of Natural Language, 1992</h4>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[R Data Splitting]]></title>
    <link href="http://ckmarkoh.github.io/blog/2014/08/23/r-createdatapartition/"/>
    <updated>2014-08-23T06:46:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2014/08/23/r-createdatapartition</id>
    <content type="html"><![CDATA[<p>在做機器學習演算法時，常常需要把資料分成 <em>training set</em> 和 <em>validation set</em> 這兩個資料組。</p>

<p>但是，要如何切割，才可以讓具有不同 <em>label</em> 的資料，在這兩個資料組中，平均分佈？</p>

<p>用統計語言 <em>R</em> 處理 <em>iris</em> 資料組為例。<em>iris</em> 的資料如下：</p>

<table>
  <thead>
    <tr>
      <th>index</th>
      <th>Sepal.Length</th>
      <th>Sepal.Width</th>
      <th>Petal.Length</th>
      <th>Petal.Width</th>
      <th>Species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <td>2</td>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>……</td>
    </tr>
    <tr>
      <td>51</td>
      <td>7.0</td>
      <td>3.2</td>
      <td>4.7</td>
      <td>1.4</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <td>52</td>
      <td>6.4</td>
      <td>3.2</td>
      <td>4.5</td>
      <td>1.5</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>……</td>
    </tr>
    <tr>
      <td>101</td>
      <td>6.3</td>
      <td>3.3</td>
      <td>6.0</td>
      <td>2.5</td>
      <td>virginica</td>
    </tr>
    <tr>
      <td>102</td>
      <td>5.8</td>
      <td>2.7</td>
      <td>5.1</td>
      <td>1.9</td>
      <td>virginica</td>
    </tr>
    <tr>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>……</td>
    </tr>
  </tbody>
</table>

<!--more-->

<p><em>iris</em> 的中文意思是 <em>鳶尾花</em> 。這筆資料中，給出了 <em>Sepal</em> (萼片）和 <em>Petal</em> （花瓣）的長寬，以及 <em>Species</em> （物種）。有了這筆資料，就可以把花瓣和萼片的長度，當成是 <em>feature</em> ，而把物種當成 <em>label</em> ，就可以用機器學習的演算法，以花瓣和萼片的長度，來推測物種是什麼。</p>

<p>首先，到 <em>R console</em> 中，直接輸入 <code>iris</code> ，就會顯示出以下資料，如下：</p>

<p>```R</p>

<blockquote>
  <p>iris
    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species
1            5.1         3.5          1.4         0.2     setosa
2            4.9         3.0          1.4         0.2     setosa
3            4.7         3.2          1.3         0.2     setosa
4            4.6         3.1          1.5         0.2     setosa
5            5.0         3.6          1.4         0.2     setosa
6            5.4         3.9          1.7         0.4     setosa
7            4.6         3.4          1.4         0.3     setosa
8            5.0         3.4          1.5         0.2     setosa</p>
</blockquote>

<p>```</p>

<p>用 <code>table</code> 指令，可以統計一下每個物種的個數，各有幾種。</p>

<p>```R</p>

<blockquote>
  <p>table(iris$Species)</p>
</blockquote>

<pre><code>setosa versicolor  virginica 
    50         50         50 
</code></pre>

<p>```</p>

<p>在這資料組中，共有三個物種，每個物種都有50筆資料。由於每種物種的個數比為1:1:1，希望在切割完後的 <em>training set</em> 和 <em>validation set</em> 中，各物種的個數比也都為1:1:1。</p>

<p>由於這個資料已經先根據 <em>species</em> 排序過了，不可以直接取一個 <em>index</em> 把資料切成兩半。</p>

<p>如下，假設要取前120筆資料作為<em>training set</em> 而後30筆資料作為 <em>validation set</em> ，語法如下。</p>

<p>```R</p>

<blockquote>
  <p>trainSet &lt;- iris[1:120,]
valSet &lt;- iris[121:150,]</p>
</blockquote>

<p>```</p>

<p>用 <code>table</code> 來統計這兩組資料中 <em>species</em> 的分佈，會發現以下情形：</p>

<p>```R</p>

<blockquote>
  <p>table(trainSet$Species)</p>
</blockquote>

<pre><code>setosa versicolor  virginica 
    50         50         20 
</code></pre>

<blockquote>
  <p>table(valSet$Species)</p>
</blockquote>

<pre><code>setosa versicolor  virginica 
     0          0         30 
</code></pre>

<p>```</p>

<p>在 <code>trainSet</code> 中，大部分的 <em>species</em> 為 <em>setosa</em> 和 <em>versicolor</em> ，而在 <code>valSet</code> 中，只有 <em>virginica</em> ，各物種的分佈不平均。</p>

<h2 id="split-by-sample">Split by sample</h2>

<p><code>sample</code> 是隨機在一個數字範圍中，取出任意數字（不重複或可重複）。例如在 1~10 中隨機取五個不重複的數字，可用以下方法：</p>

<p>```R</p>

<blockquote>
  <p>sample(1:10, size=5)
[1] 8 7 9 3 4</p>
</blockquote>

<p>```</p>

<p>由於 <code>sample</code> 產生的結果是隨機的，所以要先設定一下 <em>seed</em> ，以便讓每次產生的結果都一樣。</p>

<p>```R</p>

<blockquote>
  <p>set.seed(123456)</p>
</blockquote>

<p>```</p>

<p>可以隨機在 <em>iris</em> 的 <em>index</em> 範圍中，隨機取出 120 個數字作為 <em>training set</em> 的 <em>index</em>，而剩下沒取到的 30 個數字的作為 <em>validation set</em> 的 <em>index</em> ，作法如下：</p>

<p>```R</p>

<blockquote>
  <p>trainIndex &lt;- sample(nrow(iris), size=120)
trainSet &lt;- iris[trainIndex,]
valSet &lt;- iris[-trainIndex,]</p>
</blockquote>

<p>```</p>

<p>其中， <code>trainIndex</code> 是要取出來作為 <em>training set</em> 的 <em>index</em> 。用 <code>table</code> 後統計結果如下：</p>

<p>```R</p>

<blockquote>
  <p>table(trainSet$Species)</p>
</blockquote>

<pre><code>setosa versicolor  virginica 
    41         35         44 
</code></pre>

<blockquote>
  <p>table(valSet$Species)</p>
</blockquote>

<pre><code>setosa versicolor  virginica 
     9         15          6 
</code></pre>

<p>```</p>

<p>看起來分佈比較平均了，但是仍有點誤差，還不是1:1:1。</p>

<h2 id="split-by-createdatapartition">Split by createDataPartition</h2>

<p>如果要讓各個物種能夠在 <em>training set</em> 及 <em>validation set</em> 中，達到1:1:1的平均分佈，可以用 <code>createDataPartition</code>。</p>

<p>首先，要載入模組 <code>caret</code>。</p>

<p>```R</p>

<blockquote>
  <p>library(caret)</p>
</blockquote>

<p>```</p>

<p>用 <code>createDataPartition</code> 根據 <em>iris</em> 的 <em>species</em> 種類，隨機取出120筆資料，作為 <em>training set</em> 的 <em>index</em> ，作法如下：</p>

<p>```R</p>

<blockquote>
  <p>trainIndex &lt;- createDataPartition(iris$Species, p = .8, list=FALSE)
trainSet &lt;- iris[trainIndex,]
valSet &lt;- iris[-trainIndex,]</p>
</blockquote>

<p>```</p>

<p>其中， <code>iris$Species</code> 表示要根據 <em>iris</em> 的 <em>species</em> 來取 <em>index</em>， 而 <code>p = .8</code> 表示要取出 0.8<em>150 筆資料，而 <code>list=FALSE</code> 表示產生出的結果不是 *list</em> ，而是 <em>vector</em> 。</p>

<p>用 <code>table</code> 來統計一下分割的結果，物種以1:1:1的平均分佈在 <code>trainSet</code> 和 <code>valSet</code> 。</p>

<p>```R</p>

<blockquote>
  <p>table(trainSet$Species)</p>
</blockquote>

<pre><code>setosa versicolor  virginica 
    40         40         40 
</code></pre>

<blockquote>
  <p>table(valSet$Species)</p>
</blockquote>

<pre><code>setosa versicolor  virginica 
    10         10         10 
</code></pre>

<p>```</p>

<h2 id="reference">Reference</h2>

<p>關於 createDataPartition 可參考以下網址：</p>

<p>http://www.inside-r.org/packages/cran/caret/docs/createDataPartition</p>

<p>關於 Data splitting 的其他方法，可參考以下網址：</p>

<p>http://topepo.github.io/caret/splitting.html</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Log-Linear Model]]></title>
    <link href="http://ckmarkoh.github.io/blog/2014/04/28/natural-language-processing-log-linear-model/"/>
    <updated>2014-04-28T11:00:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2014/04/28/natural-language-processing-log-linear-model</id>
    <content type="html"><![CDATA[<h2 id="introduction">1. Introduction</h2>

<p>在機器學習中有一種用於分類的演算法, 叫作 <em>Logistic Regression</em> , 可以把東西分成兩類</p>

<p>而在自然語言處理的應用, 常常需要處理多類別的分類問題, 像是 <em>Part of speech Tagging</em> 就是把一個字詞分類到名詞, 動詞, 形容詞, 之類的問題</p>

<p>如果二元分類的 <em>Logistic Regression</em> , 推廣到多種類別分類, 就可以處理這種分類問題</p>

<p>首先, 把二元分類的 <em>Logistic Regression</em> 公式, 稍做調整, 如下  </p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&p(y=true|X) = \frac{1}{1+e^{-W \cdot X }} 

= \frac{ e^{\frac{W \cdot X}{2}} }{ e^{\frac{W \cdot X}{2}}+e^{\frac{-W \cdot X}{2}}  } \\[12pt]

&p(y=false|X) = \frac{e^{-W \cdot X }}{1+e^{-W \cdot X }} 

= \frac{ e^{\frac{-W \cdot X}{2}} }{ e^{\frac{W \cdot X}{2}}+e^{\frac{-W \cdot X}{2}}  } \\

\end{align}

 %]]&gt;</script>

<p>針對多類別的  <em>Logistic Regression</em> , 叫作 <em>Multinomial logistic regression</em> , 如果總共有 <script type="math/tex">k</script> 的類別, 每個類別的 <em>label</em> 為 <script type="math/tex">c_{i} , i \in k </script> , 則公式如下</p>

<!--more-->

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&p(y=c_{1}|X) = \frac{ e^{W_{c_{1}} \cdot X} }{ \sum_{i=1}^{k} e^{W_{c_{i}} \cdot X} } \\[12pt]

&p(y=c_{2}|X) = \frac{ e^{W_{c_{2}} \cdot X} }{ \sum_{i=1}^{k} e^{W_{c_{i}} \cdot X} } \\[12pt]

&...\\[12pt]

&p(y=c_{k}|X) = \frac{ e^{W_{c_{k}} \cdot X} }{ \sum_{i=1}^{k} e^{W_{c_{i}} \cdot X} } \\


\end{align}

 %]]&gt;</script>

<p>在自然語言處理中, 由於 <em>feature value</em> , 也就是 <script type="math/tex">X</script> , 通常不是數字, 例如 <em>前面幾個字的 Tag</em> 之類的, 這時就要用 <em>feature function</em> 把 <em>feature value</em> 轉成數字</p>

<p>所謂的 <em>feature function</em> , 就像是一個檢查器, 去檢查 <em>input data</em> 是否滿足某個 <em>feature</em> , 滿足的話則輸出 <em>1</em> , 不滿足者輸出 <em>0</em> , 以下為一個<em>feature function</em> 的例子</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


f_{j}(c,x) =

\begin{cases} 

1 & \text{if}\mspace{15mu} tag_{i-1} = VB \mspace{15mu}\text{and} \mspace{15mu} c=NN \\

0 & \text{otherwise}

\end{cases}

 %]]&gt;</script>

<p>其中 , <script type="math/tex">tag_{i-1}</script> 是前一個字的 <em>Tag</em> , 而 <script type="math/tex">c</script> 為這個字的類別, 如果這個字的類別是 <script type="math/tex">NN</script> , 且前一個字的 <em>Tag</em> 為 <script type="math/tex">VB</script> , 則 <script type="math/tex">f_{j}=1</script> , 若不滿足這些條件, 則 <script type="math/tex">f_{j}=0</script></p>

<p>加入 <em>feature function</em> 以後 , 原本的 <script type="math/tex">W_{c_{i}} \cdot X</script> 變為  <script type="math/tex">\sum_{i=0}^{N}w_{c_{i}}f_{i}(c,x)</script> ,  <em>Multinomial logistic regression</em> 的公式變為這樣, 也就是所謂的 <em>Log-Linear Model</em></p>

<script type="math/tex; mode=display">

p(y=c_{i}|X) = \frac{ e^{ \sum_{i=0}^{N}w_{c_{i}}f_{i}(c,x) } }{ \sum_{j=1}^{k} e^{\sum_{j=0}^{N}w_{c_{j}}f_{j}(c,x)} } \\[12pt]

</script>

<p>再來, 要怎麼訓練這個 <em>Model</em> 呢？</p>

<p><em>Training</em> 是一個求最佳解的過程, 要找到一組 <em>Weight</em> 可以使得 <script type="math/tex">\sum_{i}p(Y^{(i)} \mid X^{(i)})</script> 為最大值, 公式為</p>

<script type="math/tex; mode=display">

\mathop{\arg\,\max}\limits_{w} \sum_{i} p(Y^{(i)} \mid X^{(i)})  


</script>

<p>由於有時 <em>feature function</em> 的數量會太多, 容易導致 <em>Overfitting</em> , 為了避免此現象, 所以會減掉 <script type="math/tex">\alpha R(w)</script> 以進行 <em>Regularization</em></p>

<script type="math/tex; mode=display">

\mathop{\arg\,\max}\limits_{w} \sum_{i} p(Y^{(i)} \mid X^{(i)})  -\alpha R(w)


</script>

<p>另外, 由於此最佳化後產生的結果, 會有最大的 <em>Entropy</em> , 故 <em>Log-Linear Model</em> 又稱為 <em>Maxmum Entropy Model</em> , 在此做不推導, 欲知詳情請看 <em>Berger et al. (1996). A maximum entropy approach to natural language processing.</em></p>

<h2 id="example">2. Example</h2>

<p>舉個例子, 如何用 <em>feature function</em> 算出 <em>Tagging</em> 的機率值</p>

<p>假設現在要對以下句子進行 <em>Part of Speech Tagging</em> , 現在已經進行到了 <strong><em>race</em></strong> 這個字</p>

<script type="math/tex; mode=display">

\text{Secretariat/}NNP \text{ is/}BEZ \text{ expected/}VBN \text{ to/}TO \text{ race/}\textbf{??} \text{ tomorrow/} 

</script>

<p>總共用了以下六種 <em>feature function</em> </p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&f_{1}(c,x) =

\begin{cases} 

1 & \text{if}\mspace{15mu} word_{i} = \text{'race'} \mspace{15mu}\text{and} \mspace{15mu} c=NN \\

0 & \text{otherwise} 

\end{cases} \\[12pt]


&f_{2}(c,x) =

\begin{cases} 

1 & \text{if}\mspace{15mu} tag_{i-1} = TO \mspace{15mu}\text{and} \mspace{15mu} c=VB \\

0 & \text{otherwise} 

\end{cases} \\[12pt]


&f_{3}(c,x) =

\begin{cases} 

1 & \text{if}\mspace{15mu} suffix(word_{i}) = \text{'ing'} \mspace{15mu}\text{and} \mspace{15mu} c=VBG \\

0 & \text{otherwise} 

\end{cases} \\[12pt]


&f_{4}(c,x) =

\begin{cases} 

1 & \text{if}\mspace{15mu} isLowerCase(word_{i}) \mspace{15mu}\text{and} \mspace{15mu} c=VB \\

0 & \text{otherwise} 

\end{cases} \\[12pt]


&f_{5}(c,x) =

\begin{cases} 

1 & \text{if}\mspace{15mu} word_{i} = \text{'race'} \mspace{15mu}\text{and} \mspace{15mu} c=VB \\

0 & \text{otherwise} 

\end{cases} \\[12pt]


&f_{6}(c,x) =

\begin{cases} 

1 & \text{if}\mspace{15mu} tag_{i-1} = TO \mspace{15mu}\text{and} \mspace{15mu} c=NN \\

0 & \text{otherwise} 

\end{cases} 

\end{align}

 %]]&gt;</script>

<p>現在要求 <strong><em>race</em></strong> 這個字的 <em>Tag</em> 是 <em>NN</em> 還是 <em>VB</em> , 代入以上六個 <em>feature function</em> , 得出結</p>

<p>果於下表</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


    \begin{array}{|c|c|} 

    \hline

		   &   & f1 & f2 & f3 & f4 & f5 & f6 \\ \hline

    VB & f & 0  & 1  & 0  & 1  & 1  & 0  \\ \hline

    VB & w & 0  & 0.8& 0  &0.01& 0.1& 0  \\ \hline

    NN & f & 1  & 0  & 0  & 0  & 0  & 1  \\ \hline

    NN & w &0.8 & 0  & 0  & 0  & 0  &-1.3  \\ \hline

		\end{array}

 %]]&gt;</script>

<p>其中 <script type="math/tex">f</script> 是 <em>feature function</em> 算出來的值, <script type="math/tex">w</script> 是 <em>weight</em> , 這個值通常是針對 <em>Training Data</em> 做最佳化得出來的值, <em>weight</em> 越大則表示 <em>feature</em> 所占的比重越重</p>

<p>接著把 <script type="math/tex">w_{i}f_{i}(c,x)</script> 的值帶入公式</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

&P(NN \mid x) = \frac{e^{0.8+(-1.3)}}{e^{0.8+(-1.3)}+e^{0.8+0.01+0.1}} = 0.2 \\[12pt]

&P(VB \mid x) = \frac{e^{0.8+0.01+0.1}}{e^{0.8+(-1.3)}+e^{0.8+0.01+0.1}} = 0.8 \\

\end{align}

 %]]&gt;</script>

<p>算出結果 <script type="math/tex">0.8>0.2</script> , 所以 <strong><em>race</em></strong> 的 <em>Tag</em> 為 <em>VB</em></p>

<h2 id="implementation">3. Implementation</h2>

<p>接著來實作用 <em>Log-Linear Mode</em> 進行 <em>Part of Speech Tagging</em></p>

<p>這次要用 <em>python nltk</em>  的 <code>MaxentClassifier</code> 來實作</p>

<p>首先, 開一個新的檔案 <em>loglinear.py</em> 貼上以下程式碼</p>

<p>```python loglinear.py
import nltk
import operator</p>

<p>class LogLinearTagger(nltk.TaggerI):</p>

<pre><code>def __init__(self,training_corpus):
    self.classifier = None
    self.training_corpus = training_corpus

def train(self):
    self.classifier = nltk.MaxentClassifier.train(
                reduce(operator.add, 
                    map(lambda tagged_sent :
                        self.sent_to_feature(tagged_sent)
                        ,self.training_corpus)),algorithm='megam' )

def sent_to_feature(self,tagged_sent):
    return  map(lambda (i, elem) : 
                    apply( lambda token , tag : 
                         (self.extract_features(token, i, tag), elem[1])
                        ,zip(*tagged_sent))
                    ,enumerate(tagged_sent))

def tag_sentence(self, sentence_tag):
    if self.classifier == None:
        self.train()
    return apply (lambda sentence : 
                zip(sentence,
                reduce(lambda x,y:  
                    apply(operator.add,
                        [x,[self.classifier.classify(self.extract_features(sentence, y[0], x))]])
                    , enumerate(sentence), []))
                ,[map(operator.itemgetter(0),sentence_tag)])

def evaluate(self,test_sents):
    return apply(lambda result_list : 
                sum(result_list)/float(len(result_list))
                , [reduce(operator.add,
                    map(lambda line:
                        map(lambda tag : int(tag[0] == tag[1])
                            , zip(map(operator.itemgetter(1),line),
                                  map(operator.itemgetter(1),self.tag_sentence(line))))
                       ,test_sents))])

def extract_features(self, sentence, i, history):
    features = {}
    features["this-word"] =  sentence[i]
    if i == 0:
        features["prev-tag"] = "&lt;START&gt;"
    else:
        features["prev-tag"] = history[i-1]
    return features     
</code></pre>

<p>```</p>

<p>其中, <code>extract_features</code> 是用於把 <em>input sentence</em> 的 <em>feature</em> 取出來,  例如這次用到的 <em>feature</em> 有目前這個字是什麼 <code>"this-word"</code> ,和前一個字的 <em>Tag</em> 是什麼 <code>"prev-tag"</code> </p>

<p>取出 <em>feature</em> 後 , <code>MaxentClassifier</code> 會自動根據這些 <em>feature</em> 產生 <em>feature function</em> </p>

<p>接下來到 <em>python</em> 的 <em>interactive mode</em> 載入檔案</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>from loglinear import LogLinearTagger</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>這次要用 <em>brown corpus</em> 的 <em>category</em> , <code>news</code> 的前 <em>100</em> 句來當作 <em>Tranining Data</em> ,第 <em>100~200</em> 句當作 <em>Test Data</em> , 先輸入以下程式碼</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>from nltk.corpus import brown
brown_tagged_sents = brown.tagged_sents(categories=’news’)
train_sents = brown_tagged_sents[:100]
test_sents = brown_tagged_sents[100:200]</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>接著用 <em>Training Data</em> 建立一個 <em>LogLinearTagger</em> 的 <em>class</em></p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>classifier = LogLinearTagger(train_sents)</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>在開始訓練之前, 我們先挑其中的一句, 看一下格式, 是已經 <em>Tag</em> 好的句子 , 我們以 <code>train_sents[31]</code> 為例</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>train_sents[31]
[(‘His’, ‘PP$’), (‘petition’, ‘NN’), (‘charged’, ‘VBD’), (‘mental’, ‘JJ’), \
(‘cruelty’, ‘NN’), (‘.’, ‘.’)]</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>看一下這句可以產生出哪些 <em>Feature</em></p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>for x in classifier.sent_to_feature(train_sents[31]):
…     print x
… 
({‘prev-tag’: ‘<start>', 'this-word': 'His'}, 'PP$')
({'prev-tag': 'PP$', 'this-word': 'petition'}, 'NN')
({'prev-tag': 'NN', 'this-word': 'charged'}, 'VBD')
({'prev-tag': 'VBD', 'this-word': 'mental'}, 'JJ')
({'prev-tag': 'JJ', 'this-word': 'cruelty'}, 'NN')
({'prev-tag': 'NN', 'this-word': '.'}, '.')</start></p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>例如第一個字, <code>'His'</code> , 它的 <em>feature</em> 有 <code>'prev-tag': '&lt;START&gt;'</code> 和 <code>'this-word': 'His'</code> , <em>Tag</em> 的結果為 </p>

<p><code>'PP$'</code> , 由於第一個字前面已經沒有字了, 也沒有 <em>Tag</em> 了, 所以我們用 <code>&lt;START&gt;</code> 來表示</p>

<p>再來就是要訓練 <em>classifier</em>, 執行 <code>classifier.train()</code> 就可以開始訓練, 但要花一點時間</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>classifier.train()
Scanning file…2268 train, 0 dev, 0 test, reading…done
optimizing with lambda = 0
it 1   dw 5.348e-01 pp 4.19728e+00 er 0.79850
it 2   dw 3.179e+00 pp 3.32097e+00 er 0.82760
it 3   dw 1.037e+00 pp 2.92326e+00 er 0.67549
it 4   dw 9.602e-01 pp 2.72106e+00 er 0.63933
it 5   dw 1.345e+00 pp 2.41257e+00 er 0.54012
it 6   dw 1.378e+00 pp 2.16177e+00 er 0.46429
……</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>如果出現以下錯誤訊息, 表示你沒安裝 <em>megan</em></p>

<p>```python
    raise LookupError(‘\n\n%s\n%s\n%s’ % (div, msg, div))
LookupError: </p>

<p>===========================================================================
NLTK was unable to find the megam file!
Use software specific configuration paramaters or set the MEGAM environment variable.</p>

<p>For more information, on megam, see:
    <a href="http://www.cs.utah.edu/~hal/megam/">http://www.cs.utah.edu/~hal/megam/</a>
===========================================================================</p>

<p>```</p>

<p>請到 http://www.umiacs.umd.edu/~hal/megam/version0_91/ 下載 <em>megan</em></p>

<p>如果你是 <em>linux</em> 的使用者, 可直接下載執行檔, 放到 <code>/home/xxxxxx/bin/</code> 資料夾 ( 若你是使用 <em>Mac</em> 或 <em>Window$</em> , 則需要下載 <em>source code</em> 自行編譯</p>

<p>或者你可以把 <em>loglinear.py</em> 中的 <code>MaxentClassifier</code> 的 ` algorithm=’megam’ ` 去掉 , 變成這樣</p>

<p>```python loglinear.py
        self.classifier = nltk.MaxentClassifier.train(
                    reduce(operator.add, 
                        map(lambda tagged_sent :
                            self.sent_to_feature(tagged_sent)
                            ,self.training_corpus)) )</p>

<p>```</p>

<p>但這會導致訓練速度變得很慢</p>

<p>訓練好之後, 可以用 <em>Test Data</em> 看看結果如何 , 先挑一句, 以 <code>test_sents[10]</code> 為例</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>test_sents[10]
[(‘<code>', '</code>’), (‘You’, ‘PPSS’), (‘take’, ‘VB’), (‘out’, ‘RP’), (‘of’, ‘IN’), \
(‘circulation’, ‘NN’), (‘many’, ‘AP’), (‘millions’, ‘NNS’), (‘of’, ‘IN’), \
(‘dollars’, ‘NNS’), (“’’”, “’’”), (‘.’, ‘.’)]</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>把這個句子放到訓練好的 <code>classifier</code> , 用它來 <em>Tag</em> , 比較一下跟原本的 <em>tag</em> 有何不同 </p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>classifier.tag_sentence(test_sents[10])
[(‘<code>', '</code>’), (‘You’, ‘VB’), (‘take’, ‘VB’), (‘out’, ‘RP’), (‘of’, ‘IN’), \
(‘circulation’, ‘JJ’), (‘many’, ‘AP’), (‘millions’, ‘NNS’), (‘of’, ‘IN’), \
(‘dollars’, ‘JJ’), (“’’”, “’’”), (‘.’, ‘.’)]</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>先用肉眼觀察, 我們發現 <code>classifier</code> 所得出的 <em>Tag</em> 有些和原本的一樣, 有些不一樣, 表示 <code>classifier</code> 有些字 <em>Tag</em> 錯了</p>

<p>可以用程式來算準確率, 用 <code>classifier.evaluate</code> ,  但注意的是, <em>input argument</em> 不是 <em>sentence</em> , 而是 <em>list of sentence</em> , 所以 <em>input argument</em> 要用 <code>[test_sents[10]]</code> , 如下</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>classifier.evaluate([test_sents[10]])
0.75</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>算出來後準確度是 <em>0.75</em> , 也就是說有 <em>75%</em> 的 <em>Tag</em> 是正確的</p>

<p>再來把所有的 <em>Test Data</em> 都做 <em>Evaluation</em> 看看</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>classifier.evaluate(test_sents)
0.6910327241818954</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>得的準確率約為 <em>69.1%</em></p>

<p>這樣的準確率不是很理想, 原因是因為 <em>100</em> 句的 <em>Training Data</em> 實在是太少了</p>

<p>有興趣者可以試試看, 取 <em>2000</em> 句的 <em>Training Data</em> , 準確度應該會大幅提昇, 但是要花很久的時間訓練</p>

<h2 id="furtuer-reading">3. Furtuer Reading</h2>

<p>本文參考至這本教科書</p>

<p><a href="http://www.amazon.com/Speech-Language-Processing-2nd-Edition/dp/0131873210">Speech and Language Processing</a></p>

<p>以及台大資工系 陳信希教授的 自然語言處理 課程講義</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Model Selection]]></title>
    <link href="http://ckmarkoh.github.io/blog/2014/04/17/machine-learning-model-selection/"/>
    <updated>2014-04-17T07:15:00+08:00</updated>
    <id>http://ckmarkoh.github.io/blog/2014/04/17/machine-learning-model-selection</id>
    <content type="html"><![CDATA[<h2 id="motivation">1.Motivation</h2>

<p>本文接續先前提到的 <em>Overfitting and Regularization</em></p>

<p><a href="/blog/2014/04/13/machine-learning-overfitting-and-regularization">Machine Learning – Overfitting and Regularization</a></p>

<p>探討如何避免 <em>Overfitting</em> 並選出正確的 <em>Model</em></p>

<p>因為 <em>Overfitting</em> 的緣故, 所以無法用 <script type="math/tex">E_{in}</script> 來選擇要用哪個 <em>Model</em> </p>

<p>在上一篇文章中, 可以把 <script type="math/tex">E_{out}</script> 最小的 <em>Model</em> 當成是最佳的 <em>Model</em> , 但是在現實生活的應用中, 無法這樣選擇, 因為, <strong>在訓練 <em>Model</em> 時,無法事先知道 <em>Testing Data</em> 的預測結果是什麼</strong> ,所以就不可能用 <script type="math/tex">E_{out}</script> 來選擇 <em>Model</em> </p>

<p>既然這樣, 要怎麼辦呢？ 既然不可以用 <script type="math/tex">E_{in}</script> 來選擇 <em>Model</em> , 又無法事先算出 <script type="math/tex">E_{out}</script></p>

<!--more-->

<h2 id="validation-set">2.Validation Set</h2>

<p>例如, 用高次多項式做 <em>Linear Regression</em> , 假設只有一群 <em>Training Data</em> , 如下圖藍色點, 沒有 <em>Testing Data</em> , 要怎麼辦呢？</p>

<p><img src="/images/pic/pic_00035.png" alt="data1" /></p>

<p>有個解決方法, 就是在 <em>Training Data</em> 中, 隨機選取某一部份 <strong>當作</strong> <em>Testing Data</em> , 在訓練過程中不去使用, 這些 <em>Data</em> 稱為 <em>Validation Set</em> ,如下圖紫色的點</p>

<p><img src="/images/pic/pic_00036.png" alt="data2" /></p>

<p>用圖中藍色的點, 訓練出一個 <em>Model</em> , 如下圖 </p>

<p><img src="/images/pic/pic_00037.png" alt="data_model" /></p>

<p>訓練完之後, 再用 <em>Validation Set</em>  算 <em>Error</em> , 這個 <em>Error</em>  稱作 <em>Validation Error</em> , <script type="math/tex">E_{val}</script> ,可用於挑選最佳的 <em>Model</em> </p>

<p>至於 <em>Validation Set</em>  要挑多少 <em>Data</em> ? , 挑太多會導致 <em>Training Data</em> 的量大減少太多, 而無法訓練出準確的 <em>Model</em> , 但挑太少則會使得 <em>Validation Error</em> 被少數的點給 <em>Bias</em> , 所以, 通常是選取 <em>Training Data</em> 的 <script type="math/tex">\frac{1}{3}</script> ~ <script type="math/tex">\frac{1}{5}</script> 左右的量, 作為 <em>Validation Set</em> </p>

<h2 id="implementation-of-validation-set">3.Implementation of Validation Set</h2>

<p>接著來實作, 先來看看 <em>Training Data</em> 要怎麼切割</p>

<p>開新的檔案 <em>mdselect.py</em> 並貼上以下程式碼</p>

<p>```python mdselect.py
import numpy as np
import matplotlib.pyplot as plt
from operator import itemgetter</p>

<p>data_tr=[
(0.9310,-0.3209), (-0.3103,-1.1853), (-0.7241,0.8071), (0.6552,-1.1979), (-1.0000,0.9587),
(-0.5172,0.1218), (0.3793,1.0747), (-0.7931,0.9202), (0.1724,0.9259), (-0.2414,-0.8748),
(0.5862,-0.0751), (0.2414,0.9573), (1.0000,-0.2715), (-0.6552,0.3408), (-0.3793,-1.0831),
(0.0345,-0.2877), (0.5172,-0.0876), (0.3103,0.5422), (-0.9310,0.7560), (0.7931,-0.2613),
(0.8621,-1.0038), (-0.1724,-0.3660), (-0.0345,-0.5762), (0.1034,0.4364), (-0.8621,0.5780),
(-0.5862,0.1347), (-0.1034,-1.1036), (0.7241,-1.0032), (-0.4483,-0.5247), (0.4483,0.6258),
]
data_ts=[
(0.1034,0.4559), (0.5172,0.6431), (-0.1724,-1.1199), (0.7931,-0.9601), (0.7241,-1.4629),
(-0.6552,1.1571), (-0.0345,-0.3840), (0.2414,0.9064), (0.5862,-0.2830), (-1.0000,0.8299),
(0.1724,1.1434), (0.3103,0.7773), (-0.3103,-1.3973), (-0.9310,0.5383), (-0.4483,-0.6886),
(-0.5172,-0.2233), (-0.2414,-0.7119), (-0.1034,-0.3853), (-0.7241,0.9869), (-0.7931,0.9888),
(0.6552,-0.8112), (-0.8621,0.9862), (-0.3793,-1.0019), (0.3793,0.6254), (0.0345,-0.1150),
(1.0000,-0.0712), (0.8621,-0.8452), (0.4483,0.0301), (-0.5862,-0.4771), (0.9310,-0.7827),
]</p>

<p>def data_split(s1,s2):
    return data_tr[:s1]+data_tr[s2:] , data_tr[s1:s2]</p>

<p>def plot_data( d_tr=None,d_val=None, d_ts=None, d_m=None, title=’’):
    plt.ion()
    fig, ax = plt.subplots()
    for d,c in [(d_tr,’bo’),(d_val,’mo’),(d_ts,’ro’)]:
        if d != None:
            ax.plot(map(itemgetter(0),d), map(itemgetter(1),d), c )
    if d_m != None:
        ax.plot(np.array(map(itemgetter(0),d_m)), np.array(map(itemgetter(1),d_m)), ‘k–’)
    ax.set_xlim((-1, 1))
    ax.set_ylim((-2, 2))
    ax.set_title(title)
    plt.show()</p>

<p>```</p>

<p>其中 , <code>data_tr</code> 和 <code>data_t</code> 分別是 <em>Training Data</em> 和 <em>Testing Data</em> , 這些 <em>Data</em> 都已經先隨機洗牌過,因此用 <em>Index</em> 的順序抽出的 <em>Data</em> 都已經是隨機的, 不會依序剛好抽到一筆連續的 <em>Data</em> , 而 <code>data_split</code> 是用來切割 <em>Data</em> 用的 <em>function</em> , <code>plot_data</code> 是畫圖用的</p>

<p>到 <em>python</em> 的 <em>interactive mode</em> 載入模組</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>import mdselect as ms</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>用 <code>data_split(s1,s2)</code> 就可以把 <em>Training Data</em> 分開, 例如我想要把第 <em>1~10</em> 筆資料抽出來, 作為 <em>Validation Set</em> , 方法如下 </p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>dtr,dval=ms.data_split(0,10)
dtr
[(0.5862, -0.0751), (0.2414, 0.9573), (1.0, -0.2715), (-0.6552, 0.3408),    \
(-0.3793, -1.0831), (0.0345, -0.2877), (0.5172, -0.0876), (0.3103, 0.5422), \
(-0.931, 0.756), (0.7931, -0.2613), (0.8621, -1.0038), (-0.1724, -0.366),   \
(-0.0345, -0.5762), (0.1034, 0.4364), (-0.8621, 0.578), (-0.5862, 0.1347),  \
(-0.1034, -1.1036), (0.7241, -1.0032), (-0.4483, -0.5247), (0.4483, 0.6258)]</p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <blockquote>
      <p>dval
[(0.931, -0.3209), (-0.3103, -1.1853), (-0.7241, 0.8071), (0.6552, -1.1979), \
(-1.0, 0.9587), (-0.5172, 0.1218), (0.3793, 1.0747), (-0.7931, 0.9202),      \
(0.1724, 0.9259), (-0.2414, -0.8748)]</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>其中 <code>dtr</code> 是 <em>Training Data</em> , <code>dval</code> 是 <em>Validation Set</em></p>

<p>再來, 用 <code>plot_data</code> 把資料畫出來</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ms.plot_data(d_tr=dtr,d_val=dval)</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p><img src="/images/pic/pic_00038.png" alt="i0" /></p>

<p>再來, 就是用 <em>Training Data</em> 來訓練 <em>Model</em> , 用 <em>Validation Set</em> 計算 <em>Validation Error</em></p>

<p>到 <em>mdselect.py</em> 貼上以下程式碼</p>

<p>```python mdselect.py</p>

<p>def model_train(order, split=None, plot=True, show_eout=False):
    if split != None :
        d_tr, d_val  = data_split(split[0],split[1])
        d_ary = [ d_tr, d_val, data_ts ] 
    else :
        d_ary = [ data_tr, data_ts ]</p>

<pre><code>X_ary = map(lambda d : np.matrix(map( lambda x : 
                          map(pow, [x]*(order+1), range(order+1)), map(itemgetter(0), d )))
                       , d_ary )
Y_ary = map(lambda d : np.matrix(map(itemgetter(1), d)) , d_ary )
w = np.linalg.pinv( X_ary[0] )*Y_ary[0].T 
y_ary = map(lambda X : (X*w).T , X_ary )
E_ary = map(lambda Y,y : np.average(np.square(Y - y)) , Y_ary , y_ary )
        
p_model = sorted( reduce(add ,map(lambda i : zip(map(itemgetter(0), d_ary[i]), 
                    map(lambda j : y_ary[i][0,j], range(y_ary[i].shape[1]))) 
                  , range(len(d_ary)-1))), key=itemgetter(0))
if plot != False:
    if split != None:  d_val = d_ary[1] 
    else: d_val= None
    if show_eout == True: d_out = d_ary[-1]
    else: d_out = None 
    plot_data(d_ary[0], d_val , d_out, p_model,"order=%s"%(order)) 
return E_ary
</code></pre>

<p>```</p>

<p>第一個參數是 <code>order</code> 是多項式的次數, 第二個參數是 <code>split</code> 就是要切出來做 <em>Validation Set</em> 的 <em>Data</em> ,  預設為 <code>split=None</code> 表示把所有的 <em>Training Data</em> 都當成 <em>Training Data</em> , 剩下的參數, <code>plot</code> 選擇是否要畫圖, <code>show_eout</code> 選擇是否要在圖上顯示 <script type="math/tex">E_{out}</script></p>

<p>修改完 <em>mdselect.py</em> 要重新載入</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>reload(ms)
&lt;module ‘mdselect’ from ‘mdselect.py’&gt;</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>如果是用六次多項式來訓練, 而 <em>Validation Set</em> 是第 <em>1~10</em> 筆資料, 用法如下</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ms.model_train(order=6, split=(0,10))
[0.085523976119494666, 0.34656284169000939, 0.16791141763767087]</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>這個 <em>function</em> 會回傳三個參數, 依序為 <script type="math/tex">E_{in},E_{val},E_{out}</script> , 並會自動畫出以下圖形</p>

<p><img src="/images/pic/pic_00039.png" alt="i1_no_eout" /></p>

<p>如果要在圖上顯示  <script type="math/tex">E_{out}</script> , 則設定參數 <code>show_eout=True</code> , 如下</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ms.model_train(order=6, split=(0,10), show_eout=True )
[0.085523976119494666, 0.34656284169000939, 0.16791141763767087]</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p><img src="/images/pic/pic_00040.png" alt="i1_has_eout" /></p>

<p>接著, 要找出 <script type="math/tex">E_{val}</script> 最小的 <em>Model</em> 是哪一個, 需要依序用不同的 <em>Order</em> 來訓練不同的 <em>Model</em> </p>

<p>再到  <em>mdselect.py</em>  貼上以下程式碼</p>

<p>```python mdselect.py</p>

<p>def model_select(split=None,o1=3,o2=11):
    result = map(lambda o : (o, model_train(o, split, plot=False)), range(o1,o2))
    rmin = min(result,key=lambda x : x[1][1])
    for order,err in result:
        if split != None:
            print “Order:%s, Ein:%.5f, Eval:%.5f, Eout:%.5f”%(order,err[0],err[1],err[2])
        else:
            print “Order:%s, Ein:%.5f, Eout:%.5f”%(order,err[0],err[1])
    print “Min Result:”
    if split != None:
        print “Order:%s, Ein:%.5f, Eval:%.5f, Eout:%.5f”%(rmin[0],rmin[1][0],rmin[1][1],rmin[1][2])
    else:
        print “Order:%s, Ein:%.5f, Eout:%.5f”%(rmin[0],rmin[1][0],rmin[1][1])
    plot_model_select(result,split,o1,o2)</p>

<p>def plot_model_select(result,split,o1,o2):
    x=[order for order,_ in result]
    d=[err for _,err in result]
    fig, ax = plt.subplots()
    if split != None:
        tp_ary = [(0,’bo’,’b–’,’Ein’),(1,’mo’,’m–’,’Eval’),(2,’ro’,’r–’,’Eout’)]
    else:
        tp_ary = [(0,’bo’,’b–’,’Ein’),(1,’ro’,’r–’,’Eout’)]
    for tp in tp_ary : 
        ax.plot(x, map(itemgetter(tp[0]),d) , tp[2],label = tp[3])
        ax.plot(x, map(itemgetter(tp[0]),d) , tp[1])
    ax.set_xlim((o1,o2))
    ax.set_ylim((0,1))
    ax.set_xlabel(‘Order’)
    ax.set_ylabel(‘Error’)
    plt.legend()
    plt.show()</p>

<p>```</p>

<p>修改完 <em>mdselect.py</em> 重新載入</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>reload(ms)
&lt;module ‘mdselect’ from ‘mdselect.py’&gt;</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>這個 <em>function</em> 會依序從 <script type="math/tex">Order = 3</script> 訓練到 <script type="math/tex">Order = 10</script> , 並把 <script type="math/tex">E_{in},E_{val},E_{out}</script> 的值畫成圖表, 以及找出 <script type="math/tex">E_{val}</script> 最小的 <em>Order</em> 是哪一個</p>

<p>例如用第 <em>1~10</em> 筆為 <em>Validation Set</em> ,選出最佳的 <em>Model</em> ,如下</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ms.model_select(split=(0,10))
Order:3, Ein:0.24487, Eval:0.55254, Eout:0.44307
Order:4, Ein:0.23451, Eval:0.58775, Eout:0.44250
Order:5, Ein:0.08565, Eval:0.32730, Eout:0.16432
Order:6, Ein:0.08552, Eval:0.34656, Eout:0.16791
Order:7, Ein:0.06510, Eval:0.13649, Eout:0.13285
Order:8, Ein:0.06497, Eval:0.15669, Eout:0.13948
Order:9, Ein:0.06431, Eval:0.14018, Eout:0.13009
Order:10, Ein:0.05805, Eval:0.99262, Eout:0.42279
Min Result:
Order:7, Ein:0.06510, Eval:0.13649, Eout:0.13285</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>結果顯示, 在 <script type="math/tex">Order = 7</script> 時 , 有最小的 <script type="math/tex">E_{val}</script> , <script type="math/tex">E_{out}</script> 也為最小, 程式畫出圖表如下</p>

<p><img src="/images/pic/pic_00041.png" alt="plot1" /></p>

<p>圖中有三條線, 其中紫色的線為 <script type="math/tex">E_{val}</script> , 這條線的趨勢和紅色的線 <script type="math/tex">E_{out}</script> 類似, 所以可以用來選擇最佳的 <em>Model</em></p>

<p>可以把 <script type="math/tex">Order = 7</script> 的 <em>Model</em> 也畫出來看看, 如下</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ms.model_train(7, split=(0,10), show_eout=True)
[0.065103000120556462, 0.13648767336843776, 0.13284787573861817]</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p><img src="/images/pic/pic_00042.png" alt="i2_has_eout" /></p>

<h2 id="train-again-">4.Train Again !</h2>

<p>由於選出 <em>Validation Set</em> 會使得原本的 <em>Training Data</em> 變少, 但是用更多的 <em>Training Data</em> 來訓練, 是有可能使 <script type="math/tex">E_{out}</script> 降得更低</p>

<p>所以, 在選好 <em>Model</em> 以後, 可以把 <em>Validation Set</em> 也併入 <em>Training Data</em> , 再訓練一次, 這樣就有可能把 <script type="math/tex">E_{out}</script> 降低</p>

<p>用 <code>model_train</code> 訓練剛才得出的最佳 <em>Order</em> , 也就是 <script type="math/tex">Order = 7</script> , 但這次用參數 <code>split=None</code> , 就是不要切割出 <em>Validation Set</em> </p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ms.model_train(7, split=None, show_eout=True )
[0.071946203316513205, 0.096047913185552308]</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p><img src="/images/pic/pic_00043.png" alt="i3_has_eout" /></p>

<p>得出以上結果, 可知, 用所有的 <em>Training Data</em> 做訓練, 結果為 <script type="math/tex">E_{out} \approx 0.09605 </script> 比起剛剛, 有切出 <em>Validation Set</em> 的 <script type="math/tex">E_{out} \approx 0.13285</script> 下降一些</p>

<p>所以經由 <em>Validation Set</em> 找出了參數 <em>Order</em> 以後, 再用全部的 <em>Training Data</em> 都訓練過一次, 的確可以把 <script type="math/tex">E_{out}</script> 降低</p>

<p>來看看用全部的 <em>Training Data</em> 訓練來挑 <em>Order</em> 參數, 誰的 <script type="math/tex">E_{out}</script> 最小,用 <code>model_select</code> 參數輸入 <code>split=None</code> , 結果如下</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ms.model_select(split=None)
Order:3, Ein:0.34432, Eout:0.43882
Order:4, Ein:0.34271, Eout:0.43519
Order:5, Ein:0.13194, Eout:0.12439
Order:6, Ein:0.12855, Eout:0.12578
Order:7, Ein:0.07195, Eout:0.09605
Order:8, Ein:0.06991, Eout:0.10288
Order:9, Ein:0.06987, Eout:0.10224
Order:10, Ein:0.06754, Eout:0.10167
Min Result:
Order:7, Ein:0.07195, Eout:0.09605</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p><img src="/images/pic/pic_00044.png" alt="plot2" /></p>

<p>這次運氣很好, 用 <em>Validation Set</em> 就成功挑出了 <script type="math/tex">E_{out}</script> 最小的 <em>Order</em> 參數, 但事實上, 未必每次運氣都這麼好</p>

<h2 id="v-fold-cross-validation">5.V-fold Cross Validation</h2>

<p>運氣不好的時候, 選的 <em>Validation Set</em> , 和 <script type="math/tex">E_{out}</script> 的分佈型態有所差距, 以至於用 <script type="math/tex">E_{val}</script> 無法找出 <script type="math/tex">E_{out}</script> 最小的<em>Model</em>  </p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ms.model_select(split=(15,25))
Order:3, Ein:0.45906, Eval:0.13305, Eout:0.45137
Order:4, Ein:0.45786, Eval:0.12820, Eout:0.44651
Order:5, Ein:0.10279, Eval:0.31029, Eout:0.12523
Order:6, Ein:0.09748, Eval:0.30769, Eout:0.12489
Order:7, Ein:0.03825, Eval:0.17898, Eout:0.09523
Order:8, Ein:0.02848, Eval:0.22228, Eout:0.11895
Order:9, Ein:0.02838, Eval:0.23299, Eout:0.12376
Order:10, Ein:0.02832, Eval:0.25718, Eout:0.12957
Min Result:
Order:4, Ein:0.45786, Eval:0.12820, Eout:0.44651</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p><img src="/images/pic/pic_00045.png" alt="plot3" /></p>

<p>由上圖紫色的線看到, <script type="math/tex">E_{val}</script> 的趨勢和 <script type="math/tex">E_{in} , E_{out}</script> 很不一樣, 把 <script type="math/tex">Ordr = 4 </script> 的 <em>Model</em> 畫出來, 看看出了什麼問題</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ms.model_train(4, split=(15,25), show_eout=True)
[0.45785961181234375, 0.12819567955706512, 0.44650895269797458]</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>從下圖可知, 我們運氣真的很不好, 因為挑到的點的 <em>y</em> 值都在中間, 所以會選出這樣的 <em>Model</em></p>

<p><img src="/images/pic/pic_00046.png" alt="i4_has_out" /></p>

<p>那這種情況要怎麼避免呢？</p>

<p>有種方法叫作 <em>V-fold Cross Validation</em> , 就是把 <em>Training Data</em> 切成 <script type="math/tex">V</script> 份, 總共訓練 <script type="math/tex">V</script> 次, 每次訓練從 <script type="math/tex">V</script> 份中挑一份作為 <em>Validation Set</em> , 剩下的 <script type="math/tex">V-1</script> 份當作 <em>Training set</em> , 最後再把每一次計算所得出的 <em>Error</em> 平均起來, 詳細過程如下</p>

<p>把 <em>Training Data</em>  <script type="math/tex">X</script> 切成  <script type="math/tex">X_{1},X_{2},X_{3},...,X_{V-1},X_{V}</script> , 先挑一份作為 <em>Validation Set</em></p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \text{Training Set: }\mspace{10mu} X_{1},...,X{i-1},X_{i+1},...,X_{V} \\[5pt]

& \text{Validation Set: }\mspace{10mu} X_{i} \\[5pt]

\end{align}

 %]]&gt;</script>

<p>得出 <em>Validation Error</em> : <script type="math/tex">E_{val_{i}}</script></p>

<p>再挑一份不同的 <em>Data</em> 為 <em>Validation Set</em> , 如下</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}

& \text{Training Set: }\mspace{10mu} X_{1},...,X{j-1},X_{j+1},...,X_{V} \\[5pt]

& \text{Validation Set: }\mspace{10mu} X_{j}, \mspace{10mu} \text{where } j \neq i \\[5pt]

\end{align}

 %]]&gt;</script>

<p>得出 <em>Validation Error</em> : <script type="math/tex">E_{val_{j}}</script></p>

<p>這樣重複進行 <script type="math/tex">V</script> 次, 直到每一份 <script type="math/tex">X_{i}</script> 都當過 <em>Validation Set</em> , 再把所有算出來的 <em>Validation Error</em> 平均起來</p>

<script type="math/tex; mode=display">

 E_{val} = \frac{1}{V}\sum_{i=1}^{v} E_{val_{i}}

</script>

<p>這樣可避免  <em>Validation Error</em>  受到運氣不好的 <em>Validation Set</em> 的影響, 而選出不好的 <em>Model</em></p>

<h2 id="implementation-of-v-fold-cv">Implementation of V-fold CV</h2>

<p>再到  <em>mdselect.py</em>  貼上以下程式碼</p>

<p>```python mdselect.py</p>

<p>def v_fold(o1=3,o2=11): 
    result = map(lambda o : (o,np.average(np.matrix(
                            map(lambda s : model_train(o,split=(s<em>6,(s+1)</em>6), plot=False), range(5)))
                            , axis=0)), range(o1,o2))
    rmin = min(result, key = lambda x : x[1][0,1])
    for order,err in result:
        print “Order:%s, Ein:%.5f, Eval:%.5f, Eout:%.5f”%(order,err[0,0],err[0,1],err[0,2])
    print “Min Result:”
    print “Order:%s, Ein:%.5f, Eval:%.5f, Eout:%.5f”%(rmin[0],rmin[1][0,0],rmin[1][0,1],rmin[1][0,2])
    plot_vfold(result,o1,o2)</p>

<p>def plot_vfold(result,o1,o2):
    x=[order for order,_ in result]
    y_ary = map(lambda i : [err[0,i] for _,err in result] , [0,1,2])
    tp_ary = [(0,’bo’,’b–’,’Ein’),(1,’mo’,’m–’,’Eval’),(2,’ro’,’r–’,’Eout’)]
    fig, ax = plt.subplots()
    for tp in tp_ary : 
        ax.plot(x, y_ary[tp[0]] , tp[2],label = tp[3])
        ax.plot(x, y_ary[tp[0]] , tp[1])
    ax.set_xlim((o1,o2))
    ax.set_ylim((0,1))
    ax.set_xlabel(‘Order’)
    ax.set_ylabel(‘Error’)
    plt.legend()
    plt.show()</p>

<p>```</p>

<p>其中 <code>v_fold</code> 就是 <em>V-fold Cross Validation</em> 演算法 , 在這 <em>function</em> 中, <script type="math/tex">V=5</script> 也就是把 <em>Training Data</em> 切成五份 , 並把其中一份挑出來做 <em>Validation Set</em></p>

<p>修改完 <em>mdselect.py</em> 重新載入</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>reload(ms)
&lt;module ‘mdselect’ from ‘mdselect.py’&gt;</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p>執行 <code>v_fold</code> ,結果如下</p>

<p>```python</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ms.v_fold()
Order:3, Ein:0.33048, Eval:0.48405, Eout:0.45466
Order:4, Ein:0.32709, Eval:0.51738, Eout:0.45618
Order:5, Ein:0.12095, Eval:0.27391, Eout:0.14044
Order:6, Ein:0.11037, Eval:0.45183, Eout:0.17004
Order:7, Ein:0.06660, Eval:0.12957, Eout:0.10250
Order:8, Ein:0.06446, Eval:0.13500, Eout:0.11171
Order:9, Ein:0.06402, Eval:0.18829, Eout:0.12179
Order:10, Ein:0.06035, Eval:0.50482, Eout:0.18569
Min Result:
Order:7, Ein:0.06660, Eval:0.12957, Eout:0.10250</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>```</p>

<p><code>v_fold</code> 找出了 <script type="math/tex">Order = 7</script>  時, 有最小的 <script type="math/tex">E_{val}</script> , 也有最小的 <script type="math/tex">E_{out}</script> </p>

<p><img src="/images/pic/pic_00047.png" alt="plot4" /></p>

<p>以上圖表顯示 , 紫色的線和紅色的線, 趨勢也不會差太遠, <script type="math/tex">E_{val}</script> 也可用於選擇  <script type="math/tex">E_{out}</script> 最小的 <em>Model</em></p>

<p>事實上, <em>V-fold Cross Validation</em> 未必每次都能找出  <script type="math/tex">E_{out}</script> 最小的 <em>Model</em> , 因為它是由各種不同的結果所平均起來的, 也會受到較差結果者的影響, 但至少比較不會因為 <strong>運氣不好</strong> 而挑到不好的 <em>Validation Set</em> </p>

<h2 id="reference">6.Reference</h2>

<p>本文參考至以下兩門 <em>Coursera</em> 線上課程</p>

<h4 id="andrew-ng-machine-learning">1.Andrew Ng. Machine Learning</h4>

<p>https://www.coursera.org/course/ml</p>

<h4 id="machine-learning-foundations">2.林軒田 機器學習基石 (Machine Learning Foundations)</h4>

<p>https://www.coursera.org/course/ntumlone</p>
]]></content>
  </entry>
  
</feed>
