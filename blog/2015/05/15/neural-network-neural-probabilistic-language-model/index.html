
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Neural Probabilistic Language Model - Mark Chang's Blog</title>
  <meta name="author" content="Mark Chang">

  
  <meta name="description" content="Introduction 傳統的語言模型 Ngram ，只考慮一個句子中的前面幾個字，來預測下個字是什麼。 當使用較長的 Ngram 的時候，則在測試資料中，找到和訓練資料相同的機率，就會大幅降低，甚至使機率為0，此種現象稱為 curse of dimensionality 。這類問題，傳統上可用 &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://ckmarkoh.github.io/blog/2015/05/15/neural-network-neural-probabilistic-language-model/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Mark Chang's Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

<!-- MathJax Configuration -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/SVG"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-50146000-2', 'auto');
  ga('send', 'pageview');

</script>

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Mark Chang's Blog</a></h1>
  
    <h2>Machine Learning, Deep Learning and Python</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="ckmarkoh.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About Me</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Neural Probabilistic Language Model</h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2015-05-15T02:38:00+08:00'><span class='date'><span class='date-month'>May</span> <span class='date-day'>15</span><span class='date-suffix'>th</span>, <span class='date-year'>2015</span></span> <span class='time'>2:38 am</span></time>
        
      </p>
    
  </header>


<div class="entry-content"><h2 id="introduction">Introduction</h2>

<p>傳統的語言模型 <em>Ngram</em> ，只考慮一個句子中的前面幾個字，來預測下個字是什麼。</p>

<p>當使用較長的 <em>Ngram</em> 的時候，則在測試資料中，找到和訓練資料相同的機率，就會大幅降低，甚至使機率為0，此種現象稱為 <em>curse of dimensionality</em> 。這類問題，傳統上可用 <a href="/blog/2014/03/28/equations-for-nlp-ngram-smoothing"><em>Smoothing, Interpolation</em> 或 <em>Backoff</em></a>來處理。</p>

<p>另一種對付 <em>curse of dimensionality</em> 的方法，可以用 <em>Distributional Semantics</em> 的概念，即把詞彙的語意用向量來表示。在訓練資料中，根據這個詞和鄰近周圍的詞的關係，計算出向量中各個維度的值，得出來的值即可表示這個詞的語意。例如，假設在訓練資料中出現 <em>The cat is walking in the bedroom</em> 和 <em>A dog was running in a room</em> 這兩個句子，計算出 <em>dog</em> 和 <em>cat</em> 語意向量中的詞，可得出這兩個詞在語意上是相近的。</p>

<p>所謂的 <em>Neural Probabilistic Language Model</em> ，即是用類神經網路，從語料庫中計算出各個詞彙的語意向量值。</p>

<h2 id="neural-probabilistic-language-model">Neural Probabilistic Language Model</h2>

<p>給定訓練資料為 <script type="math/tex"> w_{1} \cdots w_{T} </script> 一連串的字，每個字都可包含於詞庫 <script type="math/tex">V</script> 中，即 <script type="math/tex">w_{t} \in V</script> ，然後，用此資料訓練出語言模型：</p>

<script type="math/tex; mode=display">


f(w_{t},...,w_{t-n+1}) = P(w_{t} \mid w_{1}^{t-1} )


</script>

<!--more-->

<p>其中， <script type="math/tex"> P(w_{t} \mid w_{1}^{t-1} ) </script> 為，給定第 <script type="math/tex">w_{1}</script> 到 <script type="math/tex">w_{t-1}</script> 的條件下，出現 <script type="math/tex">w_{t}</script> 的機率，為了簡化計算，通常只考慮 <script type="math/tex">w_{t}</script> 前面 <script type="math/tex">n-1</script> 個字，故語言模型的函數為 <script type="math/tex">f(w_{t},...,w_{t-n+1})</script> 。</p>

<p>可用以下兩部分來組成這個式子：</p>

<p>1.建立一個矩陣 <script type="math/tex">C</script> 其維度為 <script type="math/tex">\mid V\mid \times m</script> 。此矩陣中的每一列，即代表詞庫 <script type="math/tex">V</script> 中的每個字所對應之維度為 <script type="math/tex">m</script> 的語意向量 ，例如， <script type="math/tex">w_{t}</script> 的語意向量為 <script type="math/tex">C(w_{t}) </script> 。</p>

<p>2.語料庫中句子的某個字，跟其前面 <script type="math/tex">n-1</script> 個字的語意向量 <script type="math/tex">( C(w_{t-1}), ..., C(w_{t-n+1}) )</script> 有關， 用函數 <script type="math/tex">g</script> 來表示字 <script type="math/tex">w_{t}</script> 與前面 <script type="math/tex">n-1</script> 個字的關係，公式如下：</p>

<script type="math/tex; mode=display">

P(w_{t} \mid w_{1}^{t-1} ) = g(w_{t},w_{t-1}, ..., w_{t-n+1})


</script>

<p>用類神經網路，將 <script type="math/tex">( C(w_{t-1}), ..., C(w_{t-n+1}) )</script> 當成 <em>input</em> ， <script type="math/tex">w_{t}</script> 當成 <em>output</em> ，若 <script type="math/tex">w_{t}=i</script> 時，求 <script type="math/tex">P(w_{t}=i \mid w_{1}^{t-1} )</script> 的機率，如下圖：</p>

<p><img src="/images/pic/pic_00076.png" alt="Neural Network of Neural Probabilistic Language Model" /></p>

<p>其中，此類神經網路有個 <em>tanh</em> 函數的 <em>hidden layer</em> ， <em>input layer</em> 和 <em>hidden layer</em> 都連結到 <em>output layer</em> ，而 <em>output layer</em> 為一個 <em>softmax</em> 函數，將輸出做正規化，即是將所有 <em>output</em> 的值轉化後，其和為 <em>1</em> ， <em>softmax</em> 函數如下：</p>

<script type="math/tex; mode=display">

P(w_{t} \mid w_{t-1} , ... , w_{t-n+1} ) = \frac{e^{y_{w_{t}}}}{\sum_{i} e^{y_{i}}}  

</script>

<p><script type="math/tex">y</script> 為此類神經網路在 <em>softmax</em> 做正規化之前，所算出的值：</p>

<script type="math/tex; mode=display">


y = b+ W\times x + U \times tanh(d+H\times x)

</script>

<p>其中， <script type="math/tex">b</script> 和 <script type="math/tex">d</script> 為 <em>bias</em> ， <script type="math/tex">x</script> 為 <script type="math/tex">( C(w_{t-1}), ..., C(w_{t-n+1}) )</script>，而 <script type="math/tex">H</script> 、 <script type="math/tex">U</script> 與 <script type="math/tex">W</script> 為類神經網路從上一層傳到下一層時，各個值所佔的權重比例。此公式中各矩陣（或向量）維度如下圖所示：</p>

<p><img src="/images/pic/pic_00077.png" alt="" /></p>

<p>在類神經網路中所對應的位置如下圖所示：</p>

<p><img src="/images/pic/pic_00078.png" alt="" /></p>

<p>於此類神經網路中，可經由訓練而調整的參數為 <script type="math/tex">\theta </script> ，如下：</p>

<script type="math/tex; mode=display">

\theta = (b,d,W,U,H,C)

</script>

<p>從 <script type="math/tex">b,d,W,U,H,C</script> 的維度，可得出共有 <script type="math/tex">\mid V\mid (1 + nm + h) + h(1 + (n − 1)m)</script> 個參數可調整。</p>

<h2 id="training-the-neural-probabilistic-language-model">Training the Neural Probabilistic Language Model</h2>

<p>要訓練此類神經網路，即是對 <script type="math/tex">P(w_{t} \mid w_{1}^{t-1} )</script> 做最佳化，並作 <em>regularization</em> 以防 <em>overfitting</em> ，即讓以下公式 <script type="math/tex">L</script> 為最佳解：</p>

<script type="math/tex; mode=display">

L = \frac{1}{T}\sum_{t} log(w_{t},w_{t-1},...,w_{t-n+1},\theta) +R(\theta)

</script>

<p>其中，<script type="math/tex">T</script> 為訓練資料的總字數，而 <script type="math/tex">R(\theta)</script> 作為 <em>regularization</em> 的用途。</p>

<p>可用 <em>Stochastic Gradient Descent</em> 的方式來求得最佳解，其中 <script type="math/tex">\epsilon</script> 為 <em>learning rate</em> ：</p>

<script type="math/tex; mode=display">

\theta \leftarrow \theta + \epsilon \dfrac{\partial log P(w_{t} \mid w_{t-1} , ... , w_{t-n+1} )}{\partial \theta}

</script>

<p>一般而言，類神經網路都是以 <em>Backward Propogation</em> 的方法來訓練，此方法分為兩個階段： <em>Forward Phase</em> 與 <em>Backward Phase</em> 。即是先計算從 <em>input</em> 透過中間的每一層把值傳遞到 <em>output</em> ，再從 <em>output</em> 往回傳遞 <em>error</em> 到中間的每一層，去更新層與層之間權重的參數 。</p>

<p>在此類神經網路的訓練過程中，運算的瓶頸在於從 <em>hidden layer</em> 到 <em>output layer</em> 這段，需要計算 <script type="math/tex">\mid V\mid (1+(n−1)m+h)</script> 個參數，因此通常採取平行化的運算，將此步驟的運算分配到不同的 <em>CPU</em> 。</p>

<p>由於 <em>output layer</em>  的大小為 <script type="math/tex">\mid V\mid </script> ，若總共有 <script type="math/tex">M</script> 個 <em>CPU</em> ，則可將 <em>output layer</em> 分成 <script type="math/tex">M</script> 個區塊，則每個 <em>CPU</em> 負責計算一個區塊，也就是 <script type="math/tex"> ⌈\mid V\mid /M⌉ </script> 個 <em>output unit</em> ，而第 <script type="math/tex">i</script> 個 <em>CPU</em> 則從第 <script type="math/tex">i × ⌈\mid V \mid /M⌉ </script> 個 <em>output unit</em> 開始計算，詳細過程如下：</p>

<p><strong>1.Forward Phase</strong></p>

<p>(a) 根據 <em>input</em> 的詞彙，從矩陣 <script type="math/tex">C</script> 中取出相對應的語意向量：</p>

<script type="math/tex; mode=display">% <![CDATA[


\begin{align}

& x(k) \leftarrow C(w_{t-k}) \\

& x = (x(1),x(2),...,x(n-1))

\end{align}

 %]]></script>

<p>(b) 將 <em>input</em> 的值傳遞到 <em>hidden layer</em> ：</p>

<script type="math/tex; mode=display">% <![CDATA[


\begin{align}

& o \leftarrow d + H \times x \\

& a \leftarrow tanh(o)

\end{align}

 %]]></script>

<p>(c) 將 <em>hidden layer</em> 的值傳到 <em>output layer</em> 。</p>

<p>這步驟要平行運算，即第 <script type="math/tex">i</script> 個 <em>CPU</em> 負責運算第 <script type="math/tex">i</script> 個區塊的 <em>output layer</em> ：</p>

<script type="math/tex; mode=display">% <![CDATA[


\begin{align}

& s_{i} \leftarrow 0 \\ 

& \text{Loop over } j \text{ in the i-th block }  \\

& 1. \mspace{20mu}	y_{j} \leftarrow b_{j} + U_{j} \times a + W_{j} \times x  \\

& 2. \mspace{20mu}	p_{j} \leftarrow e^{y_{j}} \\

& 3. \mspace{20mu} s_{i} \leftarrow s_{i} + p_{j} \\


\end{align}

 %]]></script>

<p>(d) 合併各個 <em>CPU</em> 所算出來的 <script type="math/tex">s_{i}</script>：</p>

<script type="math/tex; mode=display">

\begin{align}

S = \Sigma_{i}s_{i}

\end{align}

</script>

<p>(e) 將每個 <script type="math/tex">P_{w_{t}}</script> 正規化後取 <em>log</em> ，得出它們的 <script type="math/tex">L</script> ：</p>

<script type="math/tex; mode=display">% <![CDATA[


\begin{align}

& p_{w_{t}}  \leftarrow \frac{ p_{w_{t}} }{ S }\\

& L = log(p_{w_{t}}) 

\end{align}

 %]]></script>

<p><em>註：本文所寫的演算法省略掉 Regularization 的過程。</em></p>

<p><strong>2.Backward Phase</strong></p>

<p>(a) 從 <em>output layer</em> 執行 <em>back propogation</em> 。</p>

<p>在此過程中，第 <script type="math/tex">i</script> 個 <em>CPU</em> 負責計算 <em>output layer</em> 中第 <script type="math/tex">i</script> 個區塊的 <em>gradient</em> ：</p>

<script type="math/tex; mode=display">% <![CDATA[


\begin{align}

&\dfrac{ \partial L} { \partial a} \leftarrow 0 \\

&\dfrac{ \partial L} { \partial x} \leftarrow 0 \\

& \text{Loop over } j \text{ in the i-th block }  \\

& 1. \mspace{20mu}	\dfrac{ \partial L} { \partial y_{j}}  

\leftarrow 1_{j == w_{t}} - p_{j} \\

& 2. \mspace{20mu}	b_{j} \leftarrow b_{j} + \epsilon \dfrac{\partial L }{\partial y_{j}} \\

& 3. \mspace{20mu} \dfrac{ \partial L} { \partial x} \leftarrow 

\dfrac{ \partial L} { \partial x} + \dfrac{ \partial L} { \partial y_{j}}W_{j} \\

& 4. \mspace{20mu} \dfrac{ \partial L} { \partial a} \leftarrow 

\dfrac{ \partial L} { \partial a} + \dfrac{ \partial L} { \partial y_{j}}U_{j} \\

& 5. \mspace{20mu} W_{j} \leftarrow W_{j} + \epsilon\dfrac{ \partial L} { \partial y_{j}}x

\\

& 6. \mspace{20mu} U_{j} \leftarrow U_{j} + \epsilon\dfrac{ \partial L} { \partial y_{j}}a

\end{align}

 %]]></script>

<p>(b) 把每個 <em>CPU</em> 算出來的 <script type="math/tex">\dfrac{ \partial L} { \partial a}</script> 與 <script type="math/tex">\dfrac{ \partial L} { \partial x}</script> 都加起來。</p>

<p>(c) 用 <em>back propogation</em> 更新 <em>hidden layer</em> 到 <em>input layer</em> 之間的參數：</p>

<script type="math/tex; mode=display">% <![CDATA[


\begin{align}

& \text{Loop over } k \text{ between } 1 \text{ and } h  \\

& \mspace{20mu} \dfrac{\partial L}{\partial o_{k}} \leftarrow (1-a_{k}^{2}) \dfrac{\partial L}{\partial a_{k}} \\

& \dfrac{\partial L}{\partial x} \leftarrow \dfrac{\partial L}{\partial x} + H'\dfrac{\partial  L}{\partial o} \\

& d \leftarrow d + \epsilon \dfrac{\partial L}{\partial o} \\

& H \leftarrow H + \epsilon \dfrac{\partial L}{\partial o} x'

\end{align}

 %]]></script>

<p>(d) 更新 <em>input layer</em> 的語意向量值（從第一個字到第 <script type="math/tex">n-1</script> 個字）：</p>

<script type="math/tex; mode=display">% <![CDATA[


\begin{align}

& \text{Loop over } k \text{ between } 1 \text{ and } n-1  \\

& \mspace{20mu} C(w_{t-k}) \leftarrow C(w_{t-k}) + \epsilon \dfrac{\partial L}{\partial x(k)}

\end{align}

 %]]></script>

<h2 id="conclusion">Conclusion</h2>

<p><em>Neural Probabilistic Language Model</em> 是2003年期間所提出的語言模型，但受限於當時的電腦運算能力，這個模型的複雜度實在太高，難以實際應用。</p>

<p>近幾年來由於平行運算和 <em>GPU</em> 的發展，大幅提升了矩陣運算的效率，促成類神經網路與深度學習的發展。因此，建構在 <em>Neural Probabilistic Language Model</em> 的基礎之上，發展出了各種類神經網路相關的語言模型，一再突破了以往自然語言處理的瓶頸，包括近兩年來很熱門的 <em>word2vec</em> ，也是從它發展而來。</p>

<h2 id="reference">Reference</h2>

<p>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. J. Mach. Learn. Res. 3 (March 2003), 1137-1155.</p>

</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Mark Chang</span></span>

      




<time class='entry-date' datetime='2015-05-15T02:38:00+08:00'><span class='date'><span class='date-month'>May</span> <span class='date-day'>15</span><span class='date-suffix'>th</span>, <span class='date-year'>2015</span></span> <span class='time'>2:38 am</span></time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/natural-language-processing/'>natural language processing</a>, <a class='category' href='/blog/categories/neural-networks/'>neural networks</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://ckmarkoh.github.io/blog/2015/05/15/neural-network-neural-probabilistic-language-model/" data-via="" data-counturl="http://ckmarkoh.github.io/blog/2015/05/15/neural-network-neural-probabilistic-language-model/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2015/02/21/mt-ibm-model-1/" title="Previous Post: Machine Translation : IBM Model 1">&laquo; Machine Translation : IBM Model 1</a>
      
      
        <a class="basic-alignment right" href="/blog/2015/05/23/-hierarchical-probabilistic-neural-networks-neural-network-language-model/" title="Next Post: Hierarchical Probabilistic Neural Network Language Model (Hierarchical Softmax)">Hierarchical Probabilistic Neural Network Language Model (Hierarchical Softmax) &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Categories</h1>
    <ul id="category-list"><li><a href='/blog/categories/functional-programming/'>functional programming (3)</a></li><li><a href='/blog/categories/information-retrieval/'>information retrieval (2)</a></li><li><a href='/blog/categories/information-theory/'>information theory (1)</a></li><li><a href='/blog/categories/machine-learning/'>machine learning (9)</a></li><li><a href='/blog/categories/natural-language-processing/'>natural language processing (27)</a></li><li><a href='/blog/categories/neural-networks/'>neural networks (8)</a></li><li><a href='/blog/categories/nltk/'>nltk (6)</a></li><li><a href='/blog/categories/optimization-methods/'>optimization methods (5)</a></li><li><a href='/blog/categories/probabilistic-graphical-models/'>probabilistic graphical models (3)</a></li><li><a href='/blog/categories/python-programming/'>python programming (9)</a></li><li><a href='/blog/categories/r-programming/'>r programming (1)</a></li><li><a href='/blog/categories/ruby-programming/'>ruby programming (2)</a></li><li><a href='/blog/categories/semantics/'>semantics (7)</a></li><li><a href='/blog/categories/text-mining/'>text mining (3)</a></li><li><a href='/blog/categories/xpath/'>xpath (1)</a></li></ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2016/08/29/neural-network-word2vec-part-3-implementation/">Word2vec (Part 3 : Implementation)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/07/12/-word2vec-neural-networks-part-2-backward-propagation/">Word2vec (Part 2 : Backward Propagation)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/07/12/neural-network-word2vec-part-1-overview/">Word2vec (Part 1 : Overview)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/07/10/nlp-vector-space-semantics/">Vector Space of Semantics</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/07/09/pgm-gibbs-sampling/">Gibbs Sampling</a>
      </li>
    
  </ul>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2016 - Mark Chang -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'ckmarkoh-pages';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://ckmarkoh.github.io/blog/2015/05/15/neural-network-neural-probabilistic-language-model/';
        var disqus_url = 'http://ckmarkoh.github.io/blog/2015/05/15/neural-network-neural-probabilistic-language-model/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
